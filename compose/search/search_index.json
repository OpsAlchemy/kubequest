{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Concise Notes for CKA","text":"<p>Always focus on practical tips only.</p> <ol> <li> <p>Always use the command line - it is much faster than editing YAML manually.    Example: create or modify resources directly using CLI flags instead of opening an editor.</p> </li> <li> <p>Always take a backup before making any changes.    Example: save the current state of a resource before modifying it.</p> </li> <li> <p>Use <code>kubectl explain</code> to verify syntax and object structure.    Example: check available fields before writing or updating a manifest.</p> </li> </ol> <p>k api-resoruces is good thing</p>"},{"location":"01-autoscaling/hpa/","title":"Horizontal Pod Autoscaler (HPA)","text":""},{"location":"01-autoscaling/hpa/#metrics-server-installation-configuration","title":"\ud83d\udcca Metrics Server Installation &amp; Configuration","text":""},{"location":"01-autoscaling/hpa/#purpose","title":"Purpose","text":"<p>The Metrics Server collects resource utilization data (CPU/Memory) from Kubernetes nodes and pods, enabling the <code>kubectl top</code> command and providing metrics to the Horizontal Pod Autoscaler (HPA).</p>"},{"location":"01-autoscaling/hpa/#core-issue-in-labs","title":"Core Issue in Labs","text":"<p>Lab environments like KillerCoda often have self-signed or untrusted TLS certificates between components. Without bypassing TLS verification, the Metrics Server cannot scrape metrics from kubelets.</p>"},{"location":"01-autoscaling/hpa/#recommended-installation-method-helm","title":"Recommended Installation Method (Helm)","text":""},{"location":"01-autoscaling/hpa/#standard-installation","title":"Standard Installation","text":"<pre><code># Add Helm repository\nhelm repo add metrics-server https://kubernetes-sigs.github.io/metrics-server/\nhelm repo update\n\n# Install with lab-friendly configuration\nhelm install metrics-server metrics-server/metrics-server \\\n  --namespace kube-system \\\n  --set 'args={--cert-dir=/tmp,--secure-port=4443,--kubelet-insecure-tls,--kubelet-preferred-address-types=InternalIP}' \\\n  --set containerPort=4443 \\\n  --set 'livenessProbe.httpGet.path=/livez' \\\n  --set 'livenessProbe.httpGet.port=4443' \\\n  --set 'livenessProbe.httpGet.scheme=HTTPS' \\\n  --set 'readinessProbe.httpGet.path=/readyz' \\\n  --set 'readinessProbe.httpGet.port=4443' \\\n  --set 'readinessProbe.httpGet.scheme=HTTPS' \\\n  --set service.port=4443 \\\n  --set service.targetPort=4443\n</code></pre>"},{"location":"01-autoscaling/hpa/#key-configuration-parameters-explained","title":"Key Configuration Parameters Explained","text":"Parameter Purpose Why It's Needed <code>--kubelet-insecure-tls</code> Disables TLS verification Lab environments have self-signed certs <code>--kubelet-preferred-address-types=InternalIP</code> Uses internal IPs Ensures correct network connectivity <code>containerPort: 4443</code> Explicit port definition Avoids conflicts with default ports Port 4443 in probes Consistent port usage Health checks match actual listening port HTTPS scheme Secure connections Required for metrics server security"},{"location":"01-autoscaling/hpa/#verification-steps","title":"Verification Steps","text":"<pre><code># Wait 20-30 seconds for initialization\nsleep 20\n\n# Check pod status\nkubectl get pods -n kube-system -l app.kubernetes.io/name=metrics-server\n\n# Test metrics collection\nkubectl top nodes\nkubectl top pods\n\n# Verify API service\nkubectl get apiservice v1beta1.metrics.k8s.io\n# Should show: AVAILABLE=True\n</code></pre>"},{"location":"01-autoscaling/hpa/#troubleshooting-metrics-server","title":"Troubleshooting Metrics Server","text":"<pre><code># Check logs for connection issues\nkubectl logs -n kube-system deployment/metrics-server\n\n# If 'Failed to scrape node', verify network policies\nkubectl get networkpolicies -A\n\n# Test direct metrics API access\nkubectl get --raw \"/apis/metrics.k8s.io/v1beta1/nodes\" | head -20\n</code></pre>"},{"location":"01-autoscaling/hpa/#horizontal-pod-autoscaler-hpa-complete-guide","title":"\u2699\ufe0f Horizontal Pod Autoscaler (HPA) - Complete Guide","text":""},{"location":"01-autoscaling/hpa/#what-hpa-does","title":"What HPA Does","text":"<p>HPA automatically adjusts the number of pod replicas in a deployment based on observed metrics, maintaining your defined target utilization.</p> <p>Core Formula: <code>desiredReplicas = ceil(currentReplicas \u00d7 (currentMetricValue / desiredMetricValue))</code></p>"},{"location":"01-autoscaling/hpa/#hpa-architecture-diagram","title":"HPA Architecture Diagram","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                          HPA Controller                              \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502\n\u2502  \u2502  Get Metrics \u2502\u2500\u2500\u2500\u2500\u25b6\u2502  Calculate   \u2502\u2500\u2500\u2500\u2500\u25b6\u2502  Update      \u2502         \u2502\n\u2502  \u2502  from API    \u2502     \u2502  Desired     \u2502     \u2502  Deployment  \u2502         \u2502\n\u2502  \u2502              \u2502     \u2502  Replicas    \u2502     \u2502  Replicas    \u2502         \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                            \u2502\n         \u25bc                            \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Metrics API   \u2502         \u2502   Deployment    \u2502\n\u2502   (Metrics-     \u2502         \u2502   Controller    \u2502\n\u2502    Server)      \u2502         \u2502                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502    Kubelet      \u2502\n\u2502   (Nodes/Pods)  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"01-autoscaling/hpa/#basic-hpa-structure","title":"Basic HPA Structure","text":"<pre><code>apiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: app-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: your-app\n\n  minReplicas: 1        # Minimum pods for availability\n  maxReplicas: 10       # Maximum pods for cost control\n\n  metrics:              # What to monitor\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 70  # Target 70% CPU usage\n\n  behavior:             # How to scale (optional but recommended)\n    scaleUp:\n      stabilizationWindowSeconds: 0\n      policies: [...]\n    scaleDown:\n      stabilizationWindowSeconds: 300\n      policies: [...]\n</code></pre>"},{"location":"01-autoscaling/hpa/#prerequisite-resource-requests","title":"Prerequisite: Resource Requests","text":"<p>Critical: Pods must have resource requests for HPA to calculate utilization percentages. <pre><code># In your deployment template\nresources:\n  requests:\n    memory: \"256Mi\"    # HPA uses: (actual usage / 256Mi) \u00d7 100%\n    cpu: \"250m\"        # Requested = 0.25 CPU cores\n  limits:\n    memory: \"512Mi\"\n    cpu: \"500m\"\n</code></pre></p> <p>Common Mistake: Using wrong units - \u274c CPU: <code>\"256Mi\"</code> (Mi is for memory!) - \u2705 CPU: <code>\"250m\"</code> (millicores) or <code>\"0.25\"</code> - \u2705 Memory: <code>\"256Mi\"</code> (mebibytes) or <code>\"512Mi\"</code></p>"},{"location":"01-autoscaling/hpa/#hpa-behavior-configuration-complete-details","title":"\ud83c\udf9b\ufe0f HPA Behavior Configuration - Complete Details","text":""},{"location":"01-autoscaling/hpa/#purpose-of-behavior-settings","title":"Purpose of Behavior Settings","text":"<p>Control the timing, speed, and magnitude of scaling operations to prevent rapid oscillations (\"flapping\") and ensure stable application performance.</p>"},{"location":"01-autoscaling/hpa/#core-components-explained","title":"Core Components Explained","text":""},{"location":"01-autoscaling/hpa/#1-stabilizationwindowseconds","title":"1. <code>stabilizationWindowSeconds</code>","text":"<p>A waiting period after a metric change before taking scaling action.</p> <pre><code>scaleDown:\n  stabilizationWindowSeconds: 300  # Wait 5 minutes of low usage before scaling down\n</code></pre> Use Case Recommended Value Scale Up (responsive) 0-30 seconds Scale Down (conservative) 300-600 seconds Stateful applications 600+ seconds"},{"location":"01-autoscaling/hpa/#2-policies-scaling-rules","title":"2. <code>policies</code> - Scaling Rules","text":"<p>Define how many pods can be added/removed in a time window.</p> <pre><code>policies:\n- type: Pods        # Fixed number of pods\n  value: 2          # Add/remove 2 pods max\n  periodSeconds: 60 # Every 60 seconds\n\n- type: Percent     # Percentage of current pods\n  value: 50         # Add/remove 50% of current pods\n  periodSeconds: 30 # Every 30 seconds\n</code></pre> <p>Policy Types Comparison: | Type | Best For | Example | Result (10 pods \u2192 ?) | |------|----------|---------|----------------------| | <code>Pods</code> | Predictable scaling | <code>value: 3</code> | Add/remove exactly 3 pods | | <code>Percent</code> | Proportional scaling | <code>value: 50</code> | Add/remove 5 pods (50% of 10) |</p>"},{"location":"01-autoscaling/hpa/#3-selectpolicy-choosing-between-policies","title":"3. <code>selectPolicy</code> - Choosing Between Policies","text":"<p>When multiple policies exist, which one to apply?</p> <pre><code>selectPolicy: Max  # Use the policy allowing the BIGGEST change\n</code></pre> Policy Behavior Use Case <code>Max</code> Uses policy allowing largest change Fast scaling response <code>Min</code> Uses policy allowing smallest change Conservative, safe scaling <code>Disabled</code> No scaling in this direction Disable scale-up/down <p>Example with <code>Max</code>: <pre><code>policies:\n- type: Pods\n  value: 2          # Can change 2 pods\n- type: Percent\n  value: 100        # Can change 100% of pods (all of them!)\n\n# With selectPolicy: Max \u2192 Uses Percent: 100 (bigger change)\n# With selectPolicy: Min \u2192 Uses Pods: 2 (smaller change)\n</code></pre></p>"},{"location":"01-autoscaling/hpa/#complete-behavior-configuration-examples","title":"Complete Behavior Configuration Examples","text":""},{"location":"01-autoscaling/hpa/#1-web-application-pattern-responsive","title":"1. Web Application Pattern (Responsive)","text":"<pre><code>behavior:\n  scaleUp:\n    stabilizationWindowSeconds: 0      # React immediately to traffic spikes\n    policies:\n    - type: Pods\n      value: 4\n      periodSeconds: 15\n    - type: Percent\n      value: 100\n      periodSeconds: 15\n    selectPolicy: Max                  # Scale aggressively during spikes\n\n  scaleDown:\n    stabilizationWindowSeconds: 300    # Wait 5 minutes before scaling down\n    policies:\n    - type: Percent\n      value: 50\n      periodSeconds: 60\n    selectPolicy: Max\n</code></pre>"},{"location":"01-autoscaling/hpa/#2-batch-processing-pattern-conservative","title":"2. Batch Processing Pattern (Conservative)","text":"<pre><code>behavior:\n  scaleUp:\n    stabilizationWindowSeconds: 60     # Wait 1 minute to confirm need\n    policies:\n    - type: Pods\n      value: 1\n      periodSeconds: 90\n    selectPolicy: Max\n\n  scaleDown:\n    stabilizationWindowSeconds: 600    # Wait 10 minutes (long jobs)\n    policies:\n    - type: Pods\n      value: 1\n      periodSeconds: 180\n    selectPolicy: Min                  # Scale down very slowly\n</code></pre>"},{"location":"01-autoscaling/hpa/#3-real-timestreaming-pattern-aggressive","title":"3. Real-time/Streaming Pattern (Aggressive)","text":"<pre><code>behavior:\n  scaleUp:\n    stabilizationWindowSeconds: 0\n    policies:\n    - type: Percent\n      value: 200      # Can double pod count\n      periodSeconds: 10\n    selectPolicy: Max\n\n  scaleDown:\n    stabilizationWindowSeconds: 60\n    policies:\n    - type: Percent\n      value: 100      # Can remove all extra pods\n      periodSeconds: 30\n    selectPolicy: Max\n</code></pre>"},{"location":"01-autoscaling/hpa/#load-testing-with-stress-command","title":"\ud83e\uddea Load Testing with <code>stress</code> Command","text":""},{"location":"01-autoscaling/hpa/#purpose-of-load-testing","title":"Purpose of Load Testing","text":"<ul> <li>Verify HPA triggers at correct thresholds</li> <li>Test scaling behavior under controlled conditions</li> <li>Identify optimal resource requests and limits</li> </ul>"},{"location":"01-autoscaling/hpa/#using-the-polinuxstress-image","title":"Using the <code>polinux/stress</code> Image","text":""},{"location":"01-autoscaling/hpa/#basic-test-pod","title":"Basic Test Pod","text":"<pre><code># Create a pod for testing\nkubectl run stress-test --image=polinux/stress --command -- sleep 3600\n\n# Run stress commands inside the pod\nkubectl exec stress-test -- stress --vm 1 --vm-bytes 100M --timeout 60\n</code></pre>"},{"location":"01-autoscaling/hpa/#memory-stress-testing","title":"Memory Stress Testing","text":"<pre><code># 1. Memory Stress Only\n# Syntax: --vm N (workers) --vm-bytes X (memory per worker) --timeout T (seconds)\nkubectl exec &lt;pod&gt; -- stress --vm 2 --vm-bytes 150M --timeout 120\n\n# 2. Heavy Memory Load\nkubectl exec &lt;pod&gt; -- stress --vm 4 --vm-bytes 500M --timeout 180\n\n# 3. Continuous Memory Stress (background)\nkubectl exec &lt;pod&gt; -- stress --vm 1 --vm-bytes 100M --timeout 600 &amp;\n</code></pre>"},{"location":"01-autoscaling/hpa/#cpu-stress-testing","title":"CPU Stress Testing","text":"<pre><code># 1. CPU Stress Only\n# Syntax: --cpu N (workers) --timeout T (seconds)\nkubectl exec &lt;pod&gt; -- stress --cpu 4 --timeout 90\n\n# 2. Heavy CPU Load (all cores)\nkubectl exec &lt;pod&gt; -- stress --cpu 8 --timeout 120\n\n# 3. Mixed CPU Load (varying intensity)\nkubectl exec &lt;pod&gt; -- stress --cpu 2 --timeout 60 &amp; \\\nkubectl exec &lt;pod&gt; -- stress --cpu 4 --timeout 60\n</code></pre>"},{"location":"01-autoscaling/hpa/#combined-stress-testing","title":"Combined Stress Testing","text":"<pre><code># 1. Memory + CPU Combined Stress\nkubectl exec &lt;pod&gt; -- stress --vm 1 --vm-bytes 200M --cpu 2 --timeout 180\n\n# 2. Heavy Combined Load\nkubectl exec &lt;pod&gt; -- stress --vm 2 --vm-bytes 300M --cpu 4 --timeout 240\n\n# 3. Sequential Stress Testing\nkubectl exec &lt;pod&gt; -- stress --cpu 4 --timeout 60\nkubectl exec &lt;pod&gt; -- stress --vm 2 --vm-bytes 100M --timeout 60\nkubectl exec &lt;pod&gt; -- stress --cpu 2 --vm 1 --vm-bytes 150M --timeout 90\n</code></pre>"},{"location":"01-autoscaling/hpa/#testing-hpa-with-memory-metrics","title":"Testing HPA with Memory Metrics","text":"<pre><code># Given: HPA target is 70% memory utilization\n# Given: Pod requests 128Mi memory\n# Calculation: Need ~90Mi usage to trigger scaling (128Mi \u00d7 70% = 89.6Mi)\n\n# Trigger scaling\nkubectl exec deployment/your-app -- stress --vm 1 --vm-bytes 100M --timeout 300\n\n# Monitor scaling\nwatch -n 5 'kubectl get hpa,pods &amp;&amp; echo \"---\" &amp;&amp; kubectl top pods'\n</code></pre>"},{"location":"01-autoscaling/hpa/#testing-hpa-with-cpu-metrics","title":"Testing HPA with CPU Metrics","text":"<pre><code># Given: HPA target is 50% CPU utilization  \n# Given: Pod requests 500m CPU (0.5 cores)\n# Calculation: Need ~250m usage to trigger scaling (500m \u00d7 50% = 250m)\n\n# Trigger CPU scaling\nkubectl exec deployment/your-app -- stress --cpu 2 --timeout 300\n\n# Monitor CPU-based scaling\nwatch -n 3 'kubectl get hpa &amp;&amp; echo \"CPU Usage:\" &amp;&amp; kubectl top pods | grep your-app'\n</code></pre>"},{"location":"01-autoscaling/hpa/#multi-metric-hpa-testing","title":"Multi-Metric HPA Testing","text":"<pre><code># Create HPA watching both CPU and Memory\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: multi-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: your-app\n  minReplicas: 1\n  maxReplicas: 10\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 60\n  - type: Resource\n    resource:\n      name: memory\n      target:\n        type: Utilization\n        averageUtilization: 70\nEOF\n\n# Test both resources\nkubectl exec deployment/your-app -- stress --cpu 4 --vm 2 --vm-bytes 200M --timeout 300\n</code></pre>"},{"location":"01-autoscaling/hpa/#hpa-status-interpretation","title":"\ud83d\udd0d HPA Status Interpretation","text":""},{"location":"01-autoscaling/hpa/#checking-hpa-status","title":"Checking HPA Status","text":"<pre><code>kubectl get hpa\n# Output shows: REFERENCE, TARGETS, MINPODS, MAXPODS, REPLICAS\n\nkubectl describe hpa your-hpa\n# Shows conditions, events, and detailed metrics\n</code></pre>"},{"location":"01-autoscaling/hpa/#key-hpa-conditions","title":"Key HPA Conditions","text":"Condition Status Meaning Action Required <code>AbleToScale</code> True/False Can HPA modify replicas? Check permissions if False <code>ScalingActive</code> True/False Getting metrics successfully? Fix metrics server if False <code>ScalingLimited</code> True/False Hit min/max replica limits? Adjust min/max if True"},{"location":"01-autoscaling/hpa/#common-hpa-events","title":"Common HPA Events","text":"<pre><code>kubectl describe hpa | grep -A5 \"Events:\"\n</code></pre> Event Meaning Typical Cause <code>SuccessfulRescale</code> HPA changed replica count Normal operation <code>FailedGetResourceMetric</code> Can't fetch metrics Metrics server down <code>FailedComputeMetricsReplicas</code> Can't calculate desired replicas Invalid metric configuration"},{"location":"01-autoscaling/hpa/#troubleshooting-common-issues","title":"\ud83d\udea8 Troubleshooting Common Issues","text":""},{"location":"01-autoscaling/hpa/#issue-1-hpa-shows-unknown-targets","title":"Issue 1: HPA shows <code>&lt;unknown&gt;</code> targets","text":"<p>Cause: Metrics Server not working Solution: <pre><code># Check metrics-server pod\nkubectl get pods -n kube-system | grep metrics-server\n\n# Check logs\nkubectl logs -n kube-system deployment/metrics-server\n\n# Reinstall if needed (with correct flags)\nhelm upgrade metrics-server --reuse-values --set 'args={--kubelet-insecure-tls}'\n</code></pre></p>"},{"location":"01-autoscaling/hpa/#issue-2-hpa-not-scaling-when-expected","title":"Issue 2: HPA not scaling when expected","text":"<p>Checklist: 1. Verify pod has resource requests (<code>kubectl get pod -o yaml | grep requests</code>) 2. Check current vs target metrics (<code>kubectl describe hpa</code>) 3. Verify HPA is not limited (<code>ScalingLimited</code> condition) 4. Ensure metrics are above target threshold</p>"},{"location":"01-autoscaling/hpa/#issue-3-rapid-scaling-oscillations-flapping","title":"Issue 3: Rapid scaling oscillations (\"flapping\")","text":"<p>Solution: Adjust behavior settings <pre><code>behavior:\n  scaleDown:\n    stabilizationWindowSeconds: 600  # Increase from 300 to 600\n    policies:\n    - type: Percent\n      value: 20  # Reduce from 50% to 20%\n      periodSeconds: 120\n</code></pre></p>"},{"location":"01-autoscaling/hpa/#quick-reference-commands","title":"\ud83d\udccb Quick Reference Commands","text":""},{"location":"01-autoscaling/hpa/#hpa-management","title":"HPA Management","text":"<pre><code># Create HPA with kubectl\nkubectl autoscale deployment/my-app --cpu-percent=50 --min=1 --max=5\n\n# Get HPA information\nkubectl get hpa\nkubectl describe hpa &lt;name&gt;\nkubectl get hpa &lt;name&gt; -o yaml\n\n# Edit HPA\nkubectl edit hpa &lt;name&gt;\n\n# Delete HPA\nkubectl delete hpa &lt;name&gt;\n</code></pre>"},{"location":"01-autoscaling/hpa/#monitoring-testing","title":"Monitoring &amp; Testing","text":"<pre><code># Watch scaling in real-time\nwatch -n 2 'kubectl get hpa,deploy,pods &amp;&amp; echo \"---\" &amp;&amp; kubectl top pods 2&gt;/dev/null || echo \"Metrics loading...\"'\n\n# Generate memory load for testing\nkubectl exec deployment/&lt;name&gt; -- stress --vm 1 --vm-bytes 150M --timeout 120\n\n# Generate CPU load for testing\nkubectl exec deployment/&lt;name&gt; -- stress --cpu 4 --timeout 120\n\n# Check resource usage\nkubectl top pods\nkubectl top pods --containers  # Show container-level usage\n</code></pre>"},{"location":"01-autoscaling/hpa/#debugging","title":"Debugging","text":"<pre><code># Check HPA events\nkubectl get events --field-selector involvedObject.kind=HorizontalPodAutoscaler --sort-by=.metadata.creationTimestamp\n\n# Verify API access\nkubectl get --raw \"/apis/metrics.k8s.io/v1beta1/nodes\" | jq '.items[0].usage'\n\n# Check pod details\nkubectl describe pod &lt;pod-name&gt; | grep -A10 \"Resources:\"\n</code></pre>"},{"location":"01-autoscaling/hpa/#best-practices-summary","title":"\ud83d\udca1 Best Practices Summary","text":"<ol> <li>Always set resource requests in deployments (required for HPA calculations)</li> <li>Start with conservative behavior and adjust based on observations</li> <li>Test both CPU and memory scaling before production deployment</li> <li>Monitor HPA events for failed scaling attempts</li> <li>Use appropriate min/max values based on application needs and cluster capacity</li> <li>Regularly review metrics to ensure targets are appropriate</li> <li>Consider combining with VPA (Vertical Pod Autoscaler) for right-sizing requests</li> </ol>"},{"location":"01-autoscaling/hpa/#common-pitfalls-to-avoid","title":"\u26a0\ufe0f Common Pitfalls to Avoid","text":"<ul> <li>\u274c Forgetting to install/configure Metrics Server</li> <li>\u274c Not setting resource requests in pods</li> <li>\u274c Setting <code>minReplicas</code> to 0 for stateful applications</li> <li>\u274c Too aggressive scale-down causing application instability</li> <li>\u274c Incorrect metric units (using <code>Mi</code> for CPU, <code>m</code> for memory)</li> <li>\u274c Not monitoring HPA events for failures</li> <li>\u274c Testing only one resource type (CPU or Memory) when both matter</li> </ul>"},{"location":"01-autoscaling/keywords/","title":"Search Keywords+","text":""},{"location":"01-autoscaling/keywords/#keyword-hpa-example","title":"Keyword: <code>hpa example</code>","text":"<p>Recommendation: Recommended https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/</p>"},{"location":"01-autoscaling/keywords/#keyword-hpa","title":"Keyword: <code>hpa</code>","text":"<p>Recommendation: Not recommended https://kubernetes.io/docs/concepts/workloads/autoscaling/horizontal-pod-autoscale/</p>"},{"location":"01-autoscaling/keywords/#keyword-vpa","title":"Keyword: <code>vpa</code>","text":"<p>Recommendation: Recommended https://kubernetes.io/docs/concepts/workloads/autoscaling/vertical-pod-autoscale/</p>"},{"location":"01-autoscaling/question/","title":"HPA &amp; VPA Practice Questions","text":"<p>This document contains detailed, step-by-step practice questions for understanding Horizontal Pod Autoscaler (HPA) and Vertical Pod Autoscaler (VPA) behavior using realistic workloads. All workloads are intentionally simple so that autoscaling behavior is clearly observable.</p>"},{"location":"01-autoscaling/question/#hpa-practice-questions","title":"\ud83d\udd25 HPA PRACTICE QUESTIONS","text":""},{"location":"01-autoscaling/question/#question-1-memory-based-hpa","title":"Question 1: Memory-based HPA","text":""},{"location":"01-autoscaling/question/#objective","title":"Objective","text":"<p>Practice configuring an HPA that scales based on memory utilization. Understand how memory usage is calculated relative to memory requests, not limits.</p>"},{"location":"01-autoscaling/question/#requirements","title":"Requirements","text":"<ul> <li>Create a Deployment named <code>mem-hpa</code></li> <li>Container image: <code>polinux/stress</code></li> <li>Container must stay running using:</li> </ul> <p><pre><code>command: [\"sleep\", \"3600\"]\n</code></pre> * Set memory request to:</p> <p><pre><code>memory: 50Mi\n</code></pre> * Configure an HPA with:</p> <ul> <li>Metric: memory utilization</li> <li>Target: 80%</li> <li>Minimum replicas: 1</li> <li>Maximum replicas: 4</li> </ul>"},{"location":"01-autoscaling/question/#expected-behavior","title":"Expected Behavior","text":"<ul> <li> <p>When memory usage crosses ~40Mi per pod:</p> </li> <li> <p>HPA should calculate a higher desired replica count</p> </li> <li>New pods should be created</li> <li> <p>When memory load stops:</p> </li> <li> <p>Replica count should eventually reduce</p> </li> </ul>"},{"location":"01-autoscaling/question/#load-generation","title":"Load Generation","text":"<pre><code>kubectl exec deploy/mem-hpa -it -- stress --vm 1 --vm-bytes 45M --timeout 600\n</code></pre>"},{"location":"01-autoscaling/question/#observation","title":"Observation","text":"<pre><code>watch -n 5 '\nkubectl get hpa mem-hpa;\necho \"---\";\nkubectl get pods -l app=mem-hpa\n'\n</code></pre>"},{"location":"01-autoscaling/question/#question-2-cpu-based-hpa","title":"Question 2: CPU-based HPA","text":""},{"location":"01-autoscaling/question/#objective_1","title":"Objective","text":"<p>Understand CPU utilization\u2013based scaling and the importance of CPU requests.</p>"},{"location":"01-autoscaling/question/#requirements_1","title":"Requirements","text":"<ul> <li>Deployment name: <code>cpu-hpa</code></li> <li>Image: <code>polinux/stress</code></li> <li>Command:</li> </ul> <p><pre><code>[\"sleep\", \"3600\"]\n</code></pre> * Set CPU request to:</p> <p><pre><code>cpu: 50m\n</code></pre> * HPA configuration:</p> <ul> <li>Metric: CPU utilization</li> <li>Target: 70%</li> <li>Min replicas: 2</li> <li>Max replicas: 6</li> </ul>"},{"location":"01-autoscaling/question/#expected-behavior_1","title":"Expected Behavior","text":"<ul> <li> <p>Sustained CPU usage above ~35m per pod should:</p> </li> <li> <p>Trigger scale-up</p> </li> <li> <p>Removing CPU stress should:</p> </li> <li> <p>Gradually scale replicas down</p> </li> </ul>"},{"location":"01-autoscaling/question/#load-generation_1","title":"Load Generation","text":"<pre><code>kubectl exec deploy/cpu-hpa -it -- stress --cpu 4 --timeout 600\n</code></pre>"},{"location":"01-autoscaling/question/#observation_1","title":"Observation","text":"<pre><code>watch -n 10 '\nkubectl get hpa cpu-hpa;\nkubectl top pods -l app=cpu-hpa\n'\n</code></pre>"},{"location":"01-autoscaling/question/#question-3-multi-metric-hpa-cpu-or-memory","title":"Question 3: Multi-Metric HPA (CPU OR Memory)","text":""},{"location":"01-autoscaling/question/#objective_2","title":"Objective","text":"<p>Understand how HPA behaves when multiple metrics are configured. HPA scales if any single metric breaches its target.</p>"},{"location":"01-autoscaling/question/#requirements_2","title":"Requirements","text":"<ul> <li>Deployment name: <code>multi-hpa</code></li> <li>Image: <code>polinux/stress</code></li> <li>Command:</li> </ul> <p><pre><code>[\"sleep\", \"3600\"]\n</code></pre> * Resource requests:</p> <p><pre><code>cpu: 30m\nmemory: 40Mi\n</code></pre> * HPA rules:</p> <ul> <li>CPU utilization &gt; 60%</li> <li>OR memory utilization &gt; 75%</li> <li>Replica range: 1 \u2192 5</li> </ul>"},{"location":"01-autoscaling/question/#expected-behavior_2","title":"Expected Behavior","text":"<ul> <li>Either CPU or memory pressure alone can trigger scaling</li> <li>HPA uses the largest calculated replica count across metrics</li> </ul>"},{"location":"01-autoscaling/question/#load-generation_2","title":"Load Generation","text":"<pre><code>kubectl exec deploy/multi-hpa -it -- \\\n  stress --cpu 2 --vm 1 --vm-bytes 35M --timeout 600\n</code></pre>"},{"location":"01-autoscaling/question/#observation_2","title":"Observation","text":"<pre><code>watch -n 15 '\nkubectl describe hpa multi-hpa | grep -A12 \"Metrics:\"\n'\n</code></pre>"},{"location":"01-autoscaling/question/#vpa-practice-questions","title":"\ud83d\ude80 VPA PRACTICE QUESTIONS","text":""},{"location":"01-autoscaling/question/#question-4-vpa-initial-mode","title":"Question 4: VPA Initial Mode","text":""},{"location":"01-autoscaling/question/#objective_3","title":"Objective","text":"<p>Learn how VPA:</p> <ul> <li>Observes historical usage</li> <li>Builds recommendation profiles</li> <li>Injects resource requests only at pod creation time</li> </ul>"},{"location":"01-autoscaling/question/#requirements_3","title":"Requirements","text":"<ul> <li>Deployment name: <code>vpa-initial</code></li> <li>Image: <code>polinux/stress</code></li> <li>Command:</li> </ul> <p><pre><code>[\"sleep\", \"3600\"]\n</code></pre> * Initial requests:</p> <p><pre><code>cpu: 10m\nmemory: 20Mi\n</code></pre> * VPA configuration:</p> <ul> <li>Update mode: <code>Initial</code></li> <li>Minimum allowed: <code>20m / 30Mi</code></li> <li>Maximum allowed: <code>100m / 100Mi</code></li> <li>Controlled resources: CPU and memory</li> </ul>"},{"location":"01-autoscaling/question/#expected-behavior_3","title":"Expected Behavior","text":"<ul> <li>Running pods remain unchanged</li> <li>VPA generates recommendations</li> <li>New pods receive injected requests</li> </ul>"},{"location":"01-autoscaling/question/#load-generation_3","title":"Load Generation","text":"<pre><code>kubectl exec deploy/vpa-initial -it -- \\\n  stress --cpu 1 --vm 1 --vm-bytes 25M --timeout 600\n</code></pre>"},{"location":"01-autoscaling/question/#observation_3","title":"Observation","text":"<pre><code>watch -n 30 '\nkubectl describe vpa vpa-initial | grep -A6 -B6 \"Recommendation\"\n'\n</code></pre>"},{"location":"01-autoscaling/question/#question-5-vpa-auto-mode","title":"Question 5: VPA Auto Mode","text":""},{"location":"01-autoscaling/question/#objective_4","title":"Objective","text":"<p>Understand how VPA actively enforces resource sizing by restarting pods.</p>"},{"location":"01-autoscaling/question/#requirements_4","title":"Requirements","text":"<ul> <li>Deployment name: <code>vpa-auto</code></li> <li>Initial requests:</li> </ul> <p><pre><code>cpu: 15m\nmemory: 25Mi\n</code></pre> * VPA update mode: <code>Auto</code> * Minimum allowed: <code>20m / 30Mi</code> * Maximum allowed: <code>100m / 150Mi</code></p>"},{"location":"01-autoscaling/question/#expected-behavior_4","title":"Expected Behavior","text":"<ul> <li>VPA detects sustained overuse</li> <li>Pods are evicted and recreated</li> <li>New pods have updated requests</li> </ul>"},{"location":"01-autoscaling/question/#load-generation_4","title":"Load Generation","text":"<pre><code>kubectl exec deploy/vpa-auto -it -- \\\n  stress --cpu 3 --vm 1 --vm-bytes 80M --timeout 300\n</code></pre>"},{"location":"01-autoscaling/question/#observation_4","title":"Observation","text":"<pre><code>watch -n 20 '\nkubectl get pods -l app=vpa-auto\n'\n</code></pre>"},{"location":"01-autoscaling/question/#question-6-vpa-off-mode","title":"Question 6: VPA Off Mode","text":""},{"location":"01-autoscaling/question/#objective_5","title":"Objective","text":"<p>Confirm that <code>Off</code> mode:</p> <ul> <li>Collects metrics</li> <li>Produces recommendations</li> <li>Never modifies running pods</li> </ul>"},{"location":"01-autoscaling/question/#requirements_5","title":"Requirements","text":"<ul> <li>Deployment name: <code>vpa-off</code></li> <li>Requests:</li> </ul> <p><pre><code>cpu: 20m\nmemory: 30Mi\n</code></pre> * VPA update mode: <code>Off</code> * Minimum allowed: <code>30m / 40Mi</code> * Maximum allowed: <code>200m / 200Mi</code></p>"},{"location":"01-autoscaling/question/#expected-behavior_5","title":"Expected Behavior","text":"<ul> <li>No pod restarts</li> <li>No resource injection</li> <li>Recommendations visible only via VPA status</li> </ul>"},{"location":"01-autoscaling/question/#load-generation_5","title":"Load Generation","text":"<pre><code>kubectl exec deploy/vpa-off -it -- \\\n  stress --cpu 2 --vm 1 --vm-bytes 100M --timeout 600\n</code></pre>"},{"location":"01-autoscaling/question/#observation_5","title":"Observation","text":"<pre><code>kubectl describe vpa vpa-off\n</code></pre>"},{"location":"01-autoscaling/question/#question-7-combined-hpa-vpa","title":"Question 7: Combined HPA + VPA","text":""},{"location":"01-autoscaling/question/#objective_6","title":"Objective","text":"<p>Practice the safe and recommended combination:</p> <ul> <li>HPA controls replica count</li> <li>VPA controls initial resource sizing</li> </ul>"},{"location":"01-autoscaling/question/#requirements_6","title":"Requirements","text":"<ul> <li>Deployment name: <code>combined-app</code></li> <li>Requests:</li> </ul> <p><pre><code>cpu: 20m\nmemory: 30Mi\n</code></pre> * VPA:</p> <ul> <li>Update mode: <code>Initial</code></li> <li> <p>HPA:</p> </li> <li> <p>Metric: CPU</p> </li> <li>Target: 65%</li> <li>Replica range: 2 \u2192 4</li> </ul>"},{"location":"01-autoscaling/question/#expected-behavior_6","title":"Expected Behavior","text":"<ul> <li>VPA injects requests for new pods only</li> <li>HPA scales replicas independently</li> <li>No feedback loop or oscillation</li> </ul>"},{"location":"01-autoscaling/question/#load-generation_6","title":"Load Generation","text":"<pre><code>kubectl exec deploy/combined-app -it -- \\\n  stress --cpu 2 --vm 1 --vm-bytes 25M --timeout 600\n</code></pre>"},{"location":"01-autoscaling/question/#observation_6","title":"Observation","text":"<pre><code>watch -n 10 '\nkubectl get hpa,vpa,pods -l app=combined-app\n'\n</code></pre>"},{"location":"01-autoscaling/question/#running-stress-in-the-background-correct-ways","title":"\u2699\ufe0f Running <code>stress</code> in the Background (Correct Ways)","text":""},{"location":"01-autoscaling/question/#method-1-separate-terminals","title":"Method 1: Separate terminals","text":"<ul> <li>Terminal 1 runs stress (blocking)</li> <li>Terminal 2 is used for monitoring</li> </ul>"},{"location":"01-autoscaling/question/#method-2-nohup-inside-the-pod","title":"Method 2: <code>nohup</code> inside the pod","text":"<pre><code>kubectl exec deploy/app -it -- bash\nnohup stress --vm 1 --vm-bytes 40M --timeout 600 &gt; /dev/null 2&gt;&amp;1 &amp;\nexit\n</code></pre>"},{"location":"01-autoscaling/question/#method-3-tmux-screen","title":"Method 3: tmux / screen","text":"<pre><code>tmux new-session -d -s stress-test \\\n  'kubectl exec deploy/app -it -- stress --cpu 2 --timeout 600'\n</code></pre>"},{"location":"01-autoscaling/question/#method-4-run-stress-on-all-replicas","title":"Method 4: Run stress on all replicas","text":"<pre><code>for pod in $(kubectl get pods -l app=app -o name); do\n  kubectl exec $pod -- stress --cpu 1 --timeout 300 &amp;\ndone\n</code></pre>"},{"location":"01-autoscaling/question/#yaml-templates-full","title":"\ud83d\udccb YAML TEMPLATES (FULL)","text":""},{"location":"01-autoscaling/question/#deployment-template","title":"Deployment Template","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app\n  labels:\n    app: app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: app\n  template:\n    metadata:\n      labels:\n        app: app\n    spec:\n      containers:\n      - name: stress\n        image: polinux/stress\n        command: [\"sleep\", \"3600\"]\n        resources:\n          requests:\n            cpu: 50m\n            memory: 50Mi\n</code></pre>"},{"location":"01-autoscaling/question/#hpa-template","title":"HPA Template","text":"<pre><code>apiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: app-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: app\n  minReplicas: 1\n  maxReplicas: 4\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 70\n</code></pre>"},{"location":"01-autoscaling/question/#vpa-template","title":"VPA Template","text":"<pre><code>apiVersion: autoscaling.k8s.io/v1\nkind: VerticalPodAutoscaler\nmetadata:\n  name: app-vpa\nspec:\n  targetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: app\n  updatePolicy:\n    updateMode: Initial\n  resourcePolicy:\n    containerPolicies:\n    - containerName: stress\n      minAllowed:\n        cpu: 20m\n        memory: 30Mi\n      maxAllowed:\n        cpu: 100m\n        memory: 100Mi\n      controlledResources:\n      - cpu\n      - memory\n</code></pre>"},{"location":"01-autoscaling/vpa/","title":"Vertical Pod Autoscaler (VPA)","text":""},{"location":"01-autoscaling/vpa/#what-is-vpa","title":"\ud83c\udfaf What is VPA?","text":"<p>Vertical Pod Autoscaler (VPA) automatically adjusts the CPU and memory requests/limits of your pods based on actual usage patterns. Unlike HPA (which adds/removes pods), VPA right-sizes individual pods by modifying their resource specifications.</p> <p>\ud83d\udca1 Explanation: Think of your pods wearing clothes. HPA buys more shirts when you have more people (adds pods). VPA measures each person and tailors their shirt to fit perfectly (adjusts CPU/memory per pod). If someone grows or shrinks, VPA remeasures and makes them a new shirt.</p>"},{"location":"01-autoscaling/vpa/#core-analogy-tailoring-clothes","title":"Core Analogy: Tailoring Clothes","text":"<ul> <li>HPA: Buys more shirts when you have more people</li> <li>VPA: Resizes each shirt to perfectly fit each person</li> </ul>"},{"location":"01-autoscaling/vpa/#vpa-vs-hpa-comparison","title":"\ud83d\udcca VPA vs HPA Comparison","text":"Aspect Horizontal Pod Autoscaler (HPA) Vertical Pod Autoscaler (VPA) Scales Number of pod replicas CPU/Memory per pod Direction Horizontal (more/fewer pods) Vertical (more/fewer resources) Best for Variable traffic Predictable resource patterns Disruption None (adds/removes pods) Pod recreation required Typical Use Web apps, APIs Databases, memory-heavy apps <p>\ud83d\udca1 Key Difference: HPA changes quantity (pod count), VPA changes quality (resources per pod). They can work together: VPA makes each pod the right size, HPA decides how many of those right-sized pods you need.</p>"},{"location":"01-autoscaling/vpa/#vpa-architecture-components","title":"\ud83c\udfd7\ufe0f VPA Architecture &amp; Components","text":""},{"location":"01-autoscaling/vpa/#vpa-component-diagram","title":"VPA Component Diagram","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                  VPA Controller                          \u2502\n\u2502                                                         \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502              1. RECOMMENDER                     \u2502   \u2502\n\u2502  \u2502  \u2022 Analyzes historical usage                    \u2502   \u2502\n\u2502  \u2502  \u2022 Creates resource profiles                    \u2502   \u2502\n\u2502  \u2502  \u2022 Suggests optimal requests/limits            \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502                                                         \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502              2. UPDATER                         \u2502   \u2502\n\u2502  \u2502  \u2022 Evicts pods needing updates                 \u2502   \u2502\n\u2502  \u2502  \u2022 Only in \"Auto\" mode                         \u2502   \u2502\n\u2502  \u2502  \u2022 Gracefully terminates pods                  \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502                                                         \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502        3. ADMISSION CONTROLLER                  \u2502   \u2502\n\u2502  \u2502  \u2022 Intercepts pod creation                      \u2502   \u2502\n\u2502  \u2502  \u2022 Injects recommended resources               \u2502   \u2502\n\u2502  \u2502  \u2022 Mutating webhook                            \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502\n                         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502               METRICS HISTORY                           \u2502\n\u2502          (Metrics Server / Prometheus)                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>\ud83d\udca1 How They Work Together: 1. Recommender = The \"brain\" that learns what resources pods actually need 2. Updater = The \"action taker\" that replaces pods when they need new sizes (only in Auto mode) 3. Admission Controller = The \"gatekeeper\" that gives new pods the right resources from the start 4. Metrics = The \"memory\" that stores what happened in the past</p>"},{"location":"01-autoscaling/vpa/#vpa-installation","title":"\ud83d\ude80 VPA Installation","text":""},{"location":"01-autoscaling/vpa/#method-1-official-release-recommended","title":"Method 1: Official Release (Recommended)","text":"<pre><code># Clone VPA repository\ngit clone https://github.com/kubernetes/autoscaler.git\ncd autoscaler/vertical-pod-autoscaler\n\n# Install all components\n./hack/vpa-up.sh\n\n# Verify installation\nkubectl get pods -n kube-system | grep vpa\n</code></pre> <p>\ud83d\udca1 What This Installs: - <code>vpa-recommender</code>: Learns and suggests resource sizes - <code>vpa-updater</code>: Takes action to resize pods (in Auto mode) - <code>vpa-admission-controller</code>: Updates new pods as they're created - <code>vpa-crd</code>: Custom Resource Definition for VPA objects</p>"},{"location":"01-autoscaling/vpa/#method-2-helm-installation","title":"Method 2: Helm Installation","text":"<pre><code># Add Helm repository\nhelm repo add fairwinds-stable https://charts.fairwinds.com/stable\nhelm repo update\n\n# Install VPA\nhelm install vpa fairwinds-stable/vpa \\\n  --namespace kube-system \\\n  --set recommender.enabled=true \\\n  --set updater.enabled=true \\\n  --set admissionController.enabled=true\n</code></pre> <p>\ud83d\udca1 Helm Benefits: Easier upgrades, configuration management, and clean uninstalls.</p>"},{"location":"01-autoscaling/vpa/#method-3-manifest-files","title":"Method 3: Manifest Files","text":"<pre><code># Apply individual components\nkubectl apply -f https://github.com/kubernetes/autoscaler/releases/latest/download/vertical-pod-autoscaler-recommender.yaml\nkubectl apply -f https://github.com/kubernetes/autoscaler/releases/latest/download/vertical-pod-autoscaler-updater.yaml\nkubectl apply -f https://github.com/kubernetes/autoscaler/releases/latest/download/vertical-pod-autoscaler-admission-controller.yaml\n\n# Verify\nkubectl get pods -n kube-system -l app=vpa\n</code></pre>"},{"location":"01-autoscaling/vpa/#vpa-components-explained","title":"\u2699\ufe0f VPA Components Explained","text":""},{"location":"01-autoscaling/vpa/#1-vpa-recommender","title":"1. VPA Recommender","text":"<ul> <li>Purpose: Analyzes historical resource usage</li> <li>Output: Resource recommendations stored in VPA object</li> <li>Data Source: Metrics Server (last 8 days by default)</li> <li>Algorithm: Uses histogram of usage patterns</li> </ul> <p>\ud83d\udca1 How It Learns: Imagine you have a pod that uses between 200-400m CPU over time. Recommender watches for 8 days, builds a histogram, and says: \"This pod usually needs 300m CPU, but sometimes hits 400m. Let's recommend 350m to be safe.\"</p>"},{"location":"01-autoscaling/vpa/#2-vpa-updater","title":"2. VPA Updater","text":"<ul> <li>Purpose: Evicts pods that need resource adjustments</li> <li>Action: Deletes pods with incorrect allocations</li> <li>Trigger: When recommendations differ significantly from current</li> <li>Mode: Only active in <code>Auto</code> mode</li> </ul> <p>\ud83d\udca1 The \"Pod Killer\": Updater is like a tailor who says \"Your shirt doesn't fit anymore!\" and makes you take it off so they can give you a new one. It doesn't resize the shirt while you're wearing it\u2014it makes you change shirts entirely.</p>"},{"location":"01-autoscaling/vpa/#3-vpa-admission-controller","title":"3. VPA Admission Controller","text":"<ul> <li>Purpose: Modifies pod specs during creation</li> <li>How: Mutating admission webhook</li> <li>When: Intercepts all pod creation requests</li> <li>What: Replaces resource requests/limits with recommendations</li> </ul> <p>\ud83d\udca1 The \"Birth Certificate\": When a pod is born, Admission Controller checks the Recommender's notes and says: \"This pod should have 350m CPU, not the 100m written in its DNA (YAML file).\" It secretly changes the birth certificate before anyone notices.</p>"},{"location":"01-autoscaling/vpa/#vpa-configuration-modes","title":"\ud83d\udcdd VPA Configuration Modes","text":""},{"location":"01-autoscaling/vpa/#four-update-modes","title":"Four Update Modes","text":""},{"location":"01-autoscaling/vpa/#1-initial-mode-most-common","title":"1. <code>Initial</code> Mode (Most Common)","text":"<p><pre><code>updatePolicy:\n  updateMode: \"Initial\"\n</code></pre> - Behavior: Only sets resources on pod creation - Existing pods: Never updated - New pods: Get recommended resources - Best for: Safe production use, testing</p> <p>\ud83d\udca1 Analogy: Like a hospital that measures every newborn baby and gives them the right size diaper, but doesn't change diapers on babies who've already left the hospital.</p>"},{"location":"01-autoscaling/vpa/#2-auto-mode-aggressive","title":"2. <code>Auto</code> Mode (Aggressive)","text":"<p><pre><code>updatePolicy:\n  updateMode: \"Auto\"\n</code></pre> - Behavior: Updates resources and recreates pods - Existing pods: Evicted and recreated with new resources - Risk: Pod disruptions, potential downtime - Best for: Non-critical workloads, dev environments</p> <p>\ud83d\udca1 Analogy: Like a strict parent who makes you change clothes immediately if your shirt doesn't fit perfectly, even if you're in the middle of dinner.</p>"},{"location":"01-autoscaling/vpa/#3-recreate-mode-rarely-used","title":"3. <code>Recreate</code> Mode (Rarely Used)","text":"<p><pre><code>updatePolicy:\n  updateMode: \"Recreate\"\n</code></pre> - Behavior: Only recreates pods (no resource updates) - Use case: When pod recreation needed without resource changes</p> <p>\ud83d\udca1 When to Use: If you want VPA to restart pods periodically but not change their resources. Rare case.</p>"},{"location":"01-autoscaling/vpa/#4-off-mode-monitor-only","title":"4. <code>Off</code> Mode (Monitor Only)","text":"<p><pre><code>updatePolicy:\n  updateMode: \"Off\"\n</code></pre> - Behavior: Only provides recommendations - Action: No automatic changes - Best for: Learning patterns, manual optimization</p> <p>\ud83d\udca1 Analogy: Like a nutritionist who tells you \"You should eat 2000 calories per day\" but doesn't stop you from eating a whole pizza.</p>"},{"location":"01-autoscaling/vpa/#basic-vpa-configuration","title":"\ud83d\udd27 Basic VPA Configuration","text":""},{"location":"01-autoscaling/vpa/#minimal-vpa-example","title":"Minimal VPA Example","text":"<pre><code>apiVersion: autoscaling.k8s.io/v1\nkind: VerticalPodAutoscaler\nmetadata:\n  name: myapp-vpa\nspec:\n  targetRef:\n    apiVersion: \"apps/v1\"\n    kind: Deployment\n    name: myapp-deployment\n\n  updatePolicy:\n    updateMode: \"Initial\"  # Safe mode\n\n  resourcePolicy:\n    containerPolicies:\n    - containerName: \"*\"   # Apply to all containers\n      minAllowed:\n        cpu: \"100m\"\n        memory: \"128Mi\"\n      maxAllowed:\n        cpu: \"2\"\n        memory: \"2Gi\"\n      controlledResources: [\"cpu\", \"memory\"]\n</code></pre> <p>\ud83d\udca1 Breaking It Down: - <code>targetRef</code>: Which deployment/statefulset to watch (like tagging a person for measurement) - <code>updateMode: \"Initial\"</code>: Safe mode\u2014only fix new pods - <code>containerName: \"*\"</code>: Apply to ALL containers in the pod (asterisk = wildcard) - <code>minAllowed/maxAllowed</code>: Safety rails so VPA doesn't recommend crazy sizes - <code>controlledResources</code>: Which resources to adjust (CPU, memory, or both)</p>"},{"location":"01-autoscaling/vpa/#complete-vpa-with-all-options","title":"Complete VPA with All Options","text":"<pre><code>apiVersion: autoscaling.k8s.io/v1\nkind: VerticalPodAutoscaler\nmetadata:\n  name: complete-vpa\nspec:\n  targetRef:\n    apiVersion: \"apps/v1\"\n    kind: Deployment       # Can be: Deployment, StatefulSet, DaemonSet\n    name: my-application\n\n  updatePolicy:\n    updateMode: \"Auto\"     # Initial, Auto, Recreate, or Off\n    minReplicas: 2         # Optional: Minimum pods to consider\n\n  resourcePolicy:\n    containerPolicies:\n    - containerName: \"app\"\n      mode: \"Auto\"         # Auto, Off, or Initial\n      minAllowed:\n        cpu: \"100m\"\n        memory: \"128Mi\"\n      maxAllowed:\n        cpu: \"4\"\n        memory: \"8Gi\"\n      controlledResources: [\"cpu\", \"memory\"]\n      controlledValues: \"RequestsAndLimits\"  # or \"RequestsOnly\"\n\n    - containerName: \"sidecar\"\n      mode: \"Off\"          # Don't autoscale this container\n</code></pre> <p>\ud83d\udca1 Advanced Options Explained: - <code>minReplicas</code>: Only start recommending if you have at least this many pods (more data = better recommendations) - <code>container-specific mode</code>: Different rules per container in same pod - <code>controlledValues</code>: \"RequestsOnly\" = only adjust requests, not limits (limits stay as you set them)</p>"},{"location":"01-autoscaling/vpa/#resource-policy-explained","title":"\ud83d\udcca Resource Policy Explained","text":""},{"location":"01-autoscaling/vpa/#container-policies","title":"Container Policies","text":"<pre><code>resourcePolicy:\n  containerPolicies:\n  - containerName: \"webapp\"      # Specific container\n    minAllowed:                  # Minimum VPA can recommend\n      cpu: \"100m\"\n      memory: \"256Mi\"\n    maxAllowed:                  # Maximum VPA can recommend\n      cpu: \"2\"\n      memory: \"4Gi\"\n    controlledResources:         # Which resources to adjust\n    - \"cpu\"\n    - \"memory\"\n    controlledValues: \"RequestsAndLimits\"  # Options:\n                                           # - RequestsAndLimits (default)\n                                           # - RequestsOnly\n</code></pre> <p>\ud83d\udca1 Why Min/Max Matters: Without these, VPA might recommend 0.001m CPU (too small to run) or 1000 CPU cores (breaks your cluster). These are guardrails.</p>"},{"location":"01-autoscaling/vpa/#wildcard-vs-specific-containers","title":"Wildcard vs Specific Containers","text":"<pre><code># Option 1: All containers\n- containerName: \"*\"  # Apply to EVERY container\n\n# Option 2: Specific containers\n- containerName: \"app-server\"   # Only this container\n- containerName: \"cache\"        # Different policy for cache\n- containerName: \"logger\"       # Don't set mode: \"Off\"\n</code></pre> <p>\ud83d\udca1 Real Example: Your pod has 3 containers: app (needs lots of CPU), redis (needs lots of memory), logger (tiny, fixed needs). Use specific names to treat each differently.</p>"},{"location":"01-autoscaling/vpa/#practical-vpa-examples","title":"\ud83d\udee0\ufe0f Practical VPA Examples","text":""},{"location":"01-autoscaling/vpa/#example-1-web-application","title":"Example 1: Web Application","text":"<pre><code>apiVersion: autoscaling.k8s.io/v1\nkind: VerticalPodAutoscaler\nmetadata:\n  name: webapp-vpa\nspec:\n  targetRef:\n    apiVersion: \"apps/v1\"\n    kind: Deployment\n    name: webapp\n\n  updatePolicy:\n    updateMode: \"Initial\"  # Safe for production\n\n  resourcePolicy:\n    containerPolicies:\n    - containerName: \"nginx\"\n      minAllowed:\n        cpu: \"100m\"\n        memory: \"128Mi\"\n      maxAllowed:\n        cpu: \"1\"\n        memory: \"1Gi\"\n    - containerName: \"app\"\n      minAllowed:\n        cpu: \"200m\"\n        memory: \"256Mi\"\n      maxAllowed:\n        cpu: \"2\"\n        memory: \"2Gi\"\n</code></pre> <p>\ud83d\udca1 Two Containers, Different Needs: Nginx (web server) vs App (application logic) have different resource patterns. VPA handles each separately.</p>"},{"location":"01-autoscaling/vpa/#example-2-database-with-statefulset","title":"Example 2: Database with StatefulSet","text":"<pre><code>apiVersion: autoscaling.k8s.io/v1\nkind: VerticalPodAutoscaler\nmetadata:\n  name: postgres-vpa\nspec:\n  targetRef:\n    apiVersion: \"apps/v1\"\n    kind: StatefulSet       # Works with StatefulSets!\n    name: postgres\n\n  updatePolicy:\n    updateMode: \"Initial\"   # CRITICAL: Databases hate pod restarts!\n\n  resourcePolicy:\n    containerPolicies:\n    - containerName: \"postgres\"\n      minAllowed:\n        cpu: \"500m\"\n        memory: \"1Gi\"\n      maxAllowed:\n        cpu: \"4\"\n        memory: \"16Gi\"\n</code></pre> <p>\ud83d\udca1 StatefulSet Warning: Databases store data on disk. If VPA kills the pod in <code>Auto</code> mode, database might get confused. <code>Initial</code> mode is safer\u2014only fixes new pods when they're created during maintenance.</p>"},{"location":"01-autoscaling/vpa/#example-3-multi-container-microservice","title":"Example 3: Multi-Container Microservice","text":"<pre><code>apiVersion: autoscaling.k8s.io/v1\nkind: VerticalPodAutoscaler\nmetadata:\n  name: microservice-vpa\nspec:\n  targetRef:\n    apiVersion: \"apps/v1\"\n    kind: Deployment\n    name: order-service\n\n  updatePolicy:\n    updateMode: \"Auto\"      # Will recreate pods\n\n  resourcePolicy:\n    containerPolicies:\n    - containerName: \"order-processor\"\n      minAllowed:\n        cpu: \"200m\"\n        memory: \"512Mi\"\n      maxAllowed:\n        cpu: \"2\"\n        memory: \"4Gi\"\n      controlledValues: \"RequestsAndLimits\"\n\n    - containerName: \"redis-sidecar\"\n      mode: \"Off\"           # Don't autoscale sidecar\n      # No min/max needed when mode is Off\n</code></pre> <p>\ud83d\udca1 Sidecar Pattern: Redis runs alongside your app as a cache. You might not want VPA changing it because Redis has known memory patterns. <code>mode: \"Off\"</code> tells VPA to leave it alone.</p>"},{"location":"01-autoscaling/vpa/#how-vpa-makes-recommendations","title":"\ud83d\udd0d How VPA Makes Recommendations","text":""},{"location":"01-autoscaling/vpa/#recommendation-algorithm","title":"Recommendation Algorithm","text":"<pre><code>VPA Recommendation Process:\n1. Collect 8 days of usage data (default)\n2. Build histogram of CPU/Memory usage\n3. Calculate:\n   - Target: 90th percentile of usage + safety margin\n   - Lower Bound: 50th percentile\n   - Upper Bound: 95th percentile + safety margin\n4. Store in VPA object status\n</code></pre> <p>\ud83d\udca1 What This Means: If your pod uses 100m, 200m, 300m, 400m CPU over time: - 50th percentile (Lower Bound): 250m (half the time it uses less than this) - 90th percentile (Target): 380m (90% of the time it uses less than this) - 95th percentile (Upper Bound): 390m (almost never uses more than this)</p> <p>VPA recommends Target (380m) as the \"right size\" with safety margins.</p>"},{"location":"01-autoscaling/vpa/#viewing-recommendations","title":"Viewing Recommendations","text":"<pre><code># Check VPA status\nkubectl get vpa\n\n# Detailed view with recommendations\nkubectl describe vpa myapp-vpa\n\n# Raw YAML with recommendations\nkubectl get vpa myapp-vpa -o yaml\n</code></pre>"},{"location":"01-autoscaling/vpa/#sample-vpa-output","title":"Sample VPA Output","text":"<pre><code>$ kubectl describe vpa/webapp-vpa\n\nStatus:\n  Conditions:\n    Status: True\n    Type: RecommendationProvided\n  Recommendation:\n    Container Recommendations:\n      Container Name:  nginx\n      Lower Bound:     # Minimum safe\n        Cpu:     100m\n        Memory:  128Mi\n      Target:          # \u2b50 RECOMMENDED VALUE \u2b50\n        Cpu:     350m\n        Memory:  512Mi\n      Uncapped Target: # Without min/max constraints\n        Cpu:     420m\n        Memory:  600Mi\n      Upper Bound:     # Maximum safe\n        Cpu:     500m\n        Memory:  1Gi\n</code></pre> <p>\ud83d\udca1 Reading This Output: - Target (350m CPU): What VPA wants to set your pod to - Lower Bound (100m): Below this is dangerously small - Upper Bound (500m): Above this is wastefully large - Uncapped Target (420m): What VPA would recommend if you had no min/max limits</p>"},{"location":"01-autoscaling/vpa/#testing-vpa-step-by-step","title":"\ud83e\uddea Testing VPA Step by Step","text":""},{"location":"01-autoscaling/vpa/#step-1-create-test-deployment","title":"Step 1: Create Test Deployment","text":"<pre><code># test-deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vpa-test\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: vpa-test\n  template:\n    metadata:\n      labels:\n        app: vpa-test\n    spec:\n      containers:\n      - name: test-app\n        image: polinux/stress\n        command: [\"sleep\", \"3600\"]\n        resources:\n          requests:\n            cpu: \"50m\"     # Intentionally TOO LOW\n            memory: \"64Mi\" # Intentionally TOO LOW\n</code></pre> <p>\ud83d\udca1 Setting Up the Test: We create pods with obviously wrong resources (50m CPU) so VPA has something to fix.</p>"},{"location":"01-autoscaling/vpa/#step-2-create-vpa","title":"Step 2: Create VPA","text":"<pre><code># test-vpa.yaml\napiVersion: autoscaling.k8s.io/v1\nkind: VerticalPodAutoscaler\nmetadata:\n  name: vpa-test-vpa\nspec:\n  targetRef:\n    apiVersion: \"apps/v1\"\n    kind: Deployment\n    name: vpa-test\n  updatePolicy:\n    updateMode: \"Auto\"  # Will show immediate effect\n  resourcePolicy:\n    containerPolicies:\n    - containerName: \"*\"\n      minAllowed:\n        cpu: \"100m\"\n        memory: \"128Mi\"\n      maxAllowed:\n        cpu: \"2\"\n        memory: \"2Gi\"\n</code></pre> <p>\ud83d\udca1 Using Auto Mode for Testing: In real life, start with <code>Initial</code>. For testing, <code>Auto</code> shows VPA in action immediately.</p>"},{"location":"01-autoscaling/vpa/#step-3-apply-and-generate-load","title":"Step 3: Apply and Generate Load","text":"<pre><code># Apply configurations\nkubectl apply -f test-deployment.yaml\nkubectl apply -f test-vpa.yaml\n\n# Generate load to help VPA learn\nkubectl exec deployment/vpa-test -- stress --cpu 2 --vm 1 --vm-bytes 200M --timeout 300\n\n# Wait for VPA to analyze (5-10 minutes)\nsleep 300\n\n# Check recommendations\nkubectl describe vpa vpa-test-vpa\n\n# Watch pods get recreated (Auto mode)\nkubectl get pods -l app=vpa-test -w\n</code></pre> <p>\ud83d\udca1 The Learning Process: VPA needs to see actual usage. <code>stress</code> command simulates load so VPA can say \"Hey, this pod needs more than 50m CPU!\"</p>"},{"location":"01-autoscaling/vpa/#vpa-troubleshooting","title":"\ud83d\udea8 VPA Troubleshooting","text":""},{"location":"01-autoscaling/vpa/#common-issues-solutions","title":"Common Issues &amp; Solutions","text":""},{"location":"01-autoscaling/vpa/#issue-1-vpa-shows-no-recommendations","title":"Issue 1: VPA shows no recommendations","text":"<pre><code># Check VPA components are running\nkubectl get pods -n kube-system | grep vpa\n\n# Check logs\nkubectl logs -n kube-system deployment/vpa-recommender\n\n# Verify metrics are available\nkubectl top pods\n\n# Check VPA object\nkubectl describe vpa &lt;name&gt;\n</code></pre> <p>\ud83d\udca1 Likely Causes: Metrics server not running, VPA pods crashed, or not enough time has passed (VPA needs hours of data).</p>"},{"location":"01-autoscaling/vpa/#issue-2-pods-not-being-updated-in-auto-mode","title":"Issue 2: Pods not being updated in Auto mode","text":"<pre><code># Check update mode\nkubectl get vpa &lt;name&gt; -o yaml | grep updateMode\n\n# Check if recommendations exist\nkubectl describe vpa &lt;name&gt; | grep -A5 \"Recommendation\"\n\n# Check updater logs\nkubectl logs -n kube-system deployment/vpa-updater\n\n# Check events\nkubectl get events | grep -i vpa\n</code></pre> <p>\ud83d\udca1 Common Reason: Recommendations don't differ enough from current resources. VPA won't restart pods for tiny changes.</p>"},{"location":"01-autoscaling/vpa/#issue-3-admission-controller-not-working","title":"Issue 3: Admission controller not working","text":"<pre><code># Check mutating webhook\nkubectl get mutatingwebhookconfigurations\n\n# Check webhook logs\nkubectl logs -n kube-system deployment/vpa-admission-controller\n\n# Test pod creation\nkubectl run test --image=nginx --dry-run=client -o yaml | kubectl apply -f -\n</code></pre> <p>\ud83d\udca1 Webhook Issues: Admission controller is a webhook that intercepts pod creation. If it's down, new pods won't get VPA recommendations.</p>"},{"location":"01-autoscaling/vpa/#diagnostic-commands","title":"Diagnostic Commands","text":"<pre><code># Get all VPA resources\nkubectl get vpa --all-namespaces\n\n# Check VPA system status\nkubectl get pods -n kube-system -l app=vpa\nkubectl get deployments -n kube-system -l app=vpa\nkubectl get services -n kube-system -l app=vpa\n\n# Check events\nkubectl get events --field-selector involvedObject.kind=VerticalPodAutoscaler\nkubectl get events --sort-by=.metadata.creationTimestamp | tail -20\n\n# Check resource usage history\nkubectl get --raw \"/apis/metrics.k8s.io/v1beta1/pods\" | jq '.items[] | select(.metadata.name | contains(\"your-pod\"))'\n</code></pre>"},{"location":"01-autoscaling/vpa/#vpa-best-practices","title":"\u26a1 VPA Best Practices","text":""},{"location":"01-autoscaling/vpa/#1-start-with-initial-mode","title":"1. Start with <code>Initial</code> Mode","text":"<pre><code>updatePolicy:\n  updateMode: \"Initial\"  # Always start here\n</code></pre> <p>\ud83d\udca1 Why: Zero risk. Existing pods keep running, only new pods get changes. Like dipping your toe in water before jumping in.</p>"},{"location":"01-autoscaling/vpa/#2-set-conservative-bounds","title":"2. Set Conservative Bounds","text":"<pre><code>minAllowed:\n  cpu: \"100m\"     # Prevent too-small pods\n  memory: \"128Mi\"\nmaxAllowed:\n  cpu: \"4\"        # Prevent runaway growth\n  memory: \"8Gi\"\n</code></pre> <p>\ud83d\udca1 Safety Rails: Without bounds, VPA might recommend 0.1m CPU (pod won't start) or 100 CPU cores (bankrupts your cloud bill).</p>"},{"location":"01-autoscaling/vpa/#3-monitor-before-switching-to-auto","title":"3. Monitor Before Switching to Auto","text":"<pre><code># Run in Initial/Off mode for 1-2 weeks\n# Check recommendations are stable\nkubectl describe vpa &lt;name&gt; | grep -A10 \"Recommendation\"\n\n# Only switch to Auto when:\n# 1. Recommendations are stable for 7+ days\n# 2. You understand the impact of pod recreation\n# 3. Your app can handle pod restarts\n</code></pre> <p>\ud83d\udca1 The 7-Day Rule: VPA needs to see weekly patterns (weekday vs weekend, business hours vs night). Don't trust day 1 recommendations.</p>"},{"location":"01-autoscaling/vpa/#4-combine-with-hpa","title":"4. Combine with HPA","text":"<pre><code># VPA optimizes resource per pod\n# HPA adjusts number of pods\n# Perfect combination for variable workloads\n</code></pre> <p>\ud83d\udca1 Dream Team: VPA makes each pod the perfect size. HPA decides how many perfect-sized pods you need based on traffic.</p>"},{"location":"01-autoscaling/vpa/#5-regular-reviews","title":"5. Regular Reviews","text":"<ul> <li>Weekly: Check VPA recommendations</li> <li>Monthly: Adjust min/max bounds if needed</li> <li>Quarterly: Review if VPA still needed</li> </ul> <p>\ud83d\udca1 VPA Isn't Fire-and-Forget: Like a garden, it needs occasional checking. Applications change, usage patterns shift.</p>"},{"location":"01-autoscaling/vpa/#vpa-with-hpa-combined-strategy","title":"\ud83d\udcc8 VPA with HPA: Combined Strategy","text":""},{"location":"01-autoscaling/vpa/#why-combine-both","title":"Why Combine Both?","text":"<ul> <li>VPA: Gets resource requests right for each pod</li> <li>HPA: Scales pod count based on those correct resources</li> <li>Result: Optimal scaling in both dimensions</li> </ul>"},{"location":"01-autoscaling/vpa/#implementation-example","title":"Implementation Example","text":"<pre><code># 1. Deployment with placeholder resources\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: combined-app\nspec:\n  replicas: 2\n  template:\n    spec:\n      containers:\n      - name: app\n        image: nginx\n        resources:\n          requests:\n            cpu: \"100m\"    # Placeholder - VPA will fix\n            memory: \"256Mi\" # Placeholder - VPA will fix\n\n---\n# 2. VPA for resource optimization\napiVersion: autoscaling.k8s.io/v1\nkind: VerticalPodAutoscaler\nmetadata:\n  name: combined-vpa\nspec:\n  targetRef:\n    apiVersion: \"apps/v1\"\n    kind: Deployment\n    name: combined-app\n  updatePolicy:\n    updateMode: \"Initial\"  # Safe mode\n\n---\n# 3. HPA for replica scaling\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: combined-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: combined-app\n  minReplicas: 2\n  maxReplicas: 10\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 70  # Scale based on VPA-optimized resources\n</code></pre>"},{"location":"01-autoscaling/vpa/#important-interaction","title":"Important Interaction","text":"<p>HPA uses: <code>(actual usage / requested resources) \u00d7 100%</code></p> <p>Since VPA optimizes requested resources, HPA makes better scaling decisions!</p> <p>\ud83d\udca1 Example: Your pod actually uses 300m CPU. - Without VPA: You guessed 500m request \u2192 HPA sees 60% usage (300/500) \u2192 No scaling - With VPA: VPA sets 350m request \u2192 HPA sees 85% usage (300/350) \u2192 Scales up!</p> <p>VPA makes HPA's math more accurate.</p>"},{"location":"01-autoscaling/vpa/#vpa-limitations-gotchas","title":"\u26a0\ufe0f VPA Limitations &amp; Gotchas","text":""},{"location":"01-autoscaling/vpa/#1-pod-recreation-required","title":"1. Pod Recreation Required","text":"<ul> <li>VPA cannot update running pods' resources</li> <li>Pods must be recreated (causes brief downtime)</li> <li>Not suitable for stateful applications that hate restarts</li> </ul> <p>\ud83d\udca1 The Shirt Problem: You can't resize a shirt while someone's wearing it. You must give them a new shirt (recreate pod).</p>"},{"location":"01-autoscaling/vpa/#2-learning-period-required","title":"2. Learning Period Required","text":"<ul> <li>Needs 8+ hours of metrics for good recommendations</li> <li>Longer (24-48 hours) for stable patterns</li> <li>Initial recommendations may be inaccurate</li> </ul> <p>\ud83d\udca1 Like a New Doctor: A doctor needs to examine you multiple times before understanding your health patterns. Day 1 diagnosis might be wrong.</p>"},{"location":"01-autoscaling/vpa/#3-container-name-dependency","title":"3. Container Name Dependency","text":"<pre><code># VPA tracks by CONTAINER NAME\ncontainers:\n- name: \"app\"          # Profile tied to THIS name\n- name: \"app-v2\"       # Different name = different profile!\n</code></pre> <p>\ud83d\udca1 Name Change = New Person: If you rename container from \"app\" to \"app-v2\", VPA thinks it's a completely different container and starts learning from scratch.</p>"},{"location":"01-autoscaling/vpa/#4-not-for-all-workloads","title":"4. Not for All Workloads","text":"<p>Avoid VPA for: - \u274c Short-lived Jobs (&lt; 5 minutes) - \u274c StatefulSets with persistent data - \u274c Applications with bursty, unpredictable patterns - \u274c When pod recreation causes issues</p> <p>\ud83d\udca1 Wrong Tool for the Job: VPA is great for steady workloads. For spiky, unpredictable workloads, HPA is better.</p>"},{"location":"01-autoscaling/vpa/#5-resource-quota-conflicts","title":"5. Resource Quota Conflicts","text":"<ul> <li>VPA might recommend resources exceeding namespace quotas</li> <li>Can cause pod creation failures</li> <li>Monitor quotas when using VPA</li> </ul> <p>\ud83d\udca1 Quota Jail: If your namespace has 2 CPU quota and VPA wants to give one pod 3 CPU, that pod can't be created.</p>"},{"location":"01-autoscaling/vpa/#advanced-vpa-configuration","title":"\ud83d\udd27 Advanced VPA Configuration","text":""},{"location":"01-autoscaling/vpa/#custom-metrics-integration","title":"Custom Metrics Integration","text":"<pre><code># VPA can use Prometheus metrics\napiVersion: autoscaling.k8s.io/v1\nkind: VerticalPodAutoscaler\nmetadata:\n  name: custom-vpa\n  annotations:\n    vpa.custom.metrics.prometheus.io/cpu: |\n      rate(container_cpu_usage_seconds_total{container=\"myapp\"}[5m])\n    vpa.custom.metrics.prometheus.io/memory: |\n      container_memory_working_set_bytes{container=\"myapp\"}\n</code></pre> <p>\ud83d\udca1 Beyond CPU/Memory: In theory, VPA could use any metric (requests per second, queue length), but CPU/memory are the standard ones.</p>"},{"location":"01-autoscaling/vpa/#resource-specific-controls","title":"Resource-Specific Controls","text":"<pre><code>controlledValues: \"RequestsOnly\"  # Only adjust requests, not limits\n# or\ncontrolledValues: \"RequestsAndLimits\"  # Adjust both (default)\n\ncontrolledResources: [\"cpu\"]  # Only adjust CPU, not memory\n# or\ncontrolledResources: [\"cpu\", \"memory\"]  # Adjust both (default)\n</code></pre> <p>\ud83d\udca1 Requests vs Limits:  - Requests: What Kubernetes guarantees you - Limits: Maximum you can use - Most people let VPA adjust both, but <code>RequestsOnly</code> is safer.</p>"},{"location":"01-autoscaling/vpa/#target-cpumemory-percentiles","title":"Target CPU/Memory Percentiles","text":"<pre><code># Adjust safety margins (advanced)\n# These are VPA recommender flags, not in VPA spec\n# Set as command-line args to recommender\nspec:\n  containers:\n  - name: recommender\n    args:\n    - --cpu-histogram-decay-half-life=24h\n    - --memory-histogram-decay-half-life=48h\n    - --target-cpu-utilization=0.9  # 90th percentile\n    - --target-memory-utilization=0.9\n</code></pre> <p>\ud83d\udca1 Tuning Knobs:  - <code>decay-half-life</code>: How quickly VPA \"forgets\" old data (24h = yesterday's data matters half as much as today's) - <code>target-utilization</code>: How aggressive to be (0.9 = 90th percentile = 10% safety margin)</p>"},{"location":"01-autoscaling/vpa/#monitoring-vpa","title":"\ud83d\udcca Monitoring VPA","text":""},{"location":"01-autoscaling/vpa/#key-metrics-to-track","title":"Key Metrics to Track","text":"<pre><code># 1. Resource optimization\nkubectl describe vpa &lt;name&gt; | grep -A5 \"Recommendation\"\n\n# 2. Pod evictions (Auto mode)\nkubectl get events | grep \"vpa.*evict\"\n\n# 3. Cost savings\n# Compare before/after resource requests\n\n# 4. Application performance\n# Monitor app metrics after VPA changes\n</code></pre>"},{"location":"01-autoscaling/vpa/#prometheus-metrics-if-exposed","title":"Prometheus Metrics (if exposed)","text":"<pre><code># VPA recommender metrics\nvpa_recommendation_cpu_cores\nvpa_recommendation_memory_bytes\nvpa_checkpoint_created_total\n\n# VPA updater metrics\nvpa_updater_evictions_total\nvpa_updater_errors_total\n</code></pre>"},{"location":"01-autoscaling/vpa/#when-to-use-vpa-decision-guide","title":"\ud83c\udfaf When to Use VPA - Decision Guide","text":""},{"location":"01-autoscaling/vpa/#use-vpa-when","title":"Use VPA When:","text":"<ul> <li>\u2705 Memory-intensive applications (Java, .NET, Node.js)</li> <li>\u2705 Applications with growing resource needs</li> <li>\u2705 You're unsure of resource requirements</li> <li>\u2705 Cost optimization is important</li> <li>\u2705 Combined with HPA for complete autoscaling</li> </ul> <p>\ud83d\udca1 Perfect Example: A Java microservice that starts with 1GB heap but grows to need 2GB over months. VPA notices and adjusts automatically.</p>"},{"location":"01-autoscaling/vpa/#dont-use-vpa-when","title":"Don't Use VPA When:","text":"<ul> <li>\u274c Stateful applications (databases with persistent storage)</li> <li>\u274c Short-lived batch jobs</li> <li>\u274c When pod recreation causes business impact</li> <li>\u274c You have precise, known resource requirements</li> <li>\u274c Your cluster has strict resource quotas</li> </ul> <p>\ud83d\udca1 Bad Example: PostgreSQL database. If VPA kills the pod to resize it, database recovery might take minutes.</p>"},{"location":"01-autoscaling/vpa/#recommended-vpa-strategy","title":"Recommended VPA Strategy","text":"<ol> <li>Week 1: Deploy VPA in <code>Off</code> mode, monitor recommendations</li> <li>Week 2: Switch to <code>Initial</code> mode, verify new pods get correct resources</li> <li>Week 3+:: If stable, consider <code>Auto</code> mode for continuous optimization</li> <li>Ongoing: Combine with HPA, monitor monthly</li> </ol> <p>\ud83d\udca1 The VPA Journey: Off \u2192 Initial \u2192 (maybe) Auto. Like learning to drive: Parking lot (Off) \u2192 Quiet streets (Initial) \u2192 Highway (Auto, if you're brave).</p>"},{"location":"01-autoscaling/vpa/#quick-reference-commands","title":"\ud83d\udccb Quick Reference Commands","text":""},{"location":"01-autoscaling/vpa/#vpa-management","title":"VPA Management","text":"<pre><code># Basic commands\nkubectl get vpa                          # List VPAs\nkubectl describe vpa &lt;name&gt;              # Detailed view\nkubectl edit vpa &lt;name&gt;                  # Edit VPA\nkubectl delete vpa &lt;name&gt;                # Delete VPA\n\n# Check components\nkubectl get pods -n kube-system -l app=vpa\nkubectl get deployments -n kube-system -l app=vpa\nkubectl get services -n kube-system -l app=vpa\n\n# Debugging\nkubectl logs -n kube-system deployment/vpa-recommender\nkubectl logs -n kube-system deployment/vpa-updater\nkubectl logs -n kube-system deployment/vpa-admission-controller\n</code></pre>"},{"location":"01-autoscaling/vpa/#testing-validation","title":"Testing &amp; Validation","text":"<pre><code># Check current resource usage\nkubectl top pods\nkubectl describe pod &lt;pod&gt; | grep -A10 \"Resources\"\n\n# Generate test load\nkubectl exec &lt;pod&gt; -- stress --cpu 2 --vm 1 --vm-bytes 200M --timeout 300\n\n# Monitor VPA actions\nwatch -n 5 'kubectl get vpa,pods &amp;&amp; echo \"---\" &amp;&amp; kubectl describe vpa &lt;name&gt; | grep -A5 \"Recommendation\"'\n</code></pre>"},{"location":"01-autoscaling/vpa/#pro-tips","title":"\ud83d\udca1 Pro Tips","text":"<ol> <li>Always set min/max bounds to prevent extreme recommendations</li> <li>Start with <code>Initial</code> mode in production</li> <li>Monitor for 1-2 weeks before switching to <code>Auto</code></li> <li>Combine with HPA for complete autoscaling</li> <li>Regularly review recommendations - VPA isn't \"set and forget\"</li> <li>Test pod recreation impact before enabling <code>Auto</code> mode</li> <li>Use with resource quotas to prevent runaway growth</li> </ol>"},{"location":"01-autoscaling/vpa/#summary-vpa-in-one-page","title":"\ud83c\udf93 Summary: VPA in One Page","text":"Aspect Recommendation Why Installation Use Helm or official manifests Clean management Initial Mode <code>updateMode: \"Initial\"</code> Safe start Resource Bounds Always set min/max Prevent extremes Monitoring Period 1-2 weeks before trusting Learn patterns Production Use Start with <code>Initial</code>, move to <code>Auto</code> cautiously Avoid surprises Best Combo VPA + HPA Complete autoscaling Avoid For Stateful apps, short jobs Wrong tool <p>Remember: VPA is a powerful tool for resource optimization, but requires careful implementation and monitoring. Start small, learn patterns, and expand gradually!</p> <p>Final Thought: VPA is like having a personal tailor for your pods. A good tailor measures carefully, makes small adjustments, and never ruins your favorite suit. A bad tailor cuts without measuring and leaves you with clothes that don't fit. Be a good VPA tailor.</p>"},{"location":"02%20helm/commands/","title":"Helm Comprehensive Guide","text":""},{"location":"02%20helm/commands/#table-of-contents","title":"TABLE OF CONTENTS","text":"<ol> <li>Helm Fundamentals</li> <li>Repository Management</li> <li>Release Lifecycle</li> <li>Configuration Management</li> <li>Advanced Operations</li> <li>Debugging &amp; Troubleshooting</li> <li>Best Practices</li> <li>Real-world Examples</li> </ol>"},{"location":"02%20helm/commands/#1-helm-fundamentals","title":"1. HELM FUNDAMENTALS","text":""},{"location":"02%20helm/commands/#what-is-helm","title":"What is Helm?","text":"<p>Helm is the package manager for Kubernetes, allowing you to: - Define, install, and upgrade Kubernetes applications - Package applications as charts - Manage dependencies between applications - Share applications through repositories</p>"},{"location":"02%20helm/commands/#key-concepts","title":"Key Concepts","text":"Concept Description Example Chart Package containing all resource definitions <code>bitnami/mysql</code>, <code>nginx-ingress</code> Release Instance of a chart running in Kubernetes <code>mysql-release</code>, <code>webapp-prod</code> Repository Collection of charts <code>bitnami</code>, <code>stable</code>, <code>jetstack</code> Values Configurable parameters for a chart Database passwords, replica counts"},{"location":"02%20helm/commands/#version-types","title":"Version Types","text":"<pre><code># Chart outputs show both versions:\nCHART NAME: tomcat\nCHART VERSION: 13.3.0    # \u2190 Helm chart version (templates/config)\nAPP VERSION: 11.0.15     # \u2190 Application version (actual software)\n</code></pre> <p>Key Difference: - Chart Version: Changes when Helm templates, dependencies, or configurations change - App Version: Changes when the actual application software version changes</p>"},{"location":"02%20helm/commands/#2-repository-management","title":"2. REPOSITORY MANAGEMENT","text":""},{"location":"02%20helm/commands/#core-commands","title":"Core Commands","text":"<pre><code># Add a repository\nhelm repo add bitnami https://charts.bitnami.com/bitnami\n\n# List all repositories\nhelm repo list\n\n# Update repository index\nhelm repo update\n\n# Search for charts\nhelm search repo apache                   # Stable releases\nhelm search repo mysql --devel            # Include development versions\nhelm search repo --versions nginx         # Show all versions\n\n# Remove a repository\nhelm repo remove bitnami\n</code></pre>"},{"location":"02%20helm/commands/#bitnami-repository-specifics","title":"Bitnami Repository Specifics","text":"<pre><code># Since August 2025, Bitnami requires subscription for latest images\n# Free tier workaround:\nhelm install mydb bitnami/mysql --set image.repository=bitnamilegacy/mysql\n\n# Warnings you'll see:\n# 1. Limited free tier images available\n# 2. Rolling tags (:latest) not recommended for production\n# 3. Resources should be explicitly set\n</code></pre>"},{"location":"02%20helm/commands/#3-release-lifecycle","title":"3. RELEASE LIFECYCLE","text":""},{"location":"02%20helm/commands/#installation","title":"Installation","text":"<pre><code># Basic installation\nhelm install mysql wso2/mysql\n\n# With custom name\nhelm install my-database bitnami/mysql\n\n# Generate random name\nhelm install bitnami/apache --generate-name\n\n# Custom naming template\nhelm install bitnami/apache --generate-name --name-template \"web-{{randAlpha 5 | lower}}\"\n</code></pre>"},{"location":"02%20helm/commands/#upgrade-rollback","title":"Upgrade &amp; Rollback","text":"<pre><code># Upgrade with new values\nhelm upgrade mysql wso2/mysql --set testFramework.enabled=false\n\n# Upgrade with values file\nhelm upgrade mysql bitnami/mysql -f mysql-values.yaml\n\n# Reuse previous values (dangerous - may use defaults for missing values)\nhelm upgrade mydb bitnami/mysql --reuse-values\n\n# Force upgrade (deletes and recreates resources - causes downtime)\nhelm upgrade myapp bitnami/tomcat --force\n\n# Check upgrade history\nhelm history mysql\n\n# Rollback to specific revision\nhelm rollback mysql 1\n</code></pre>"},{"location":"02%20helm/commands/#uninstallation","title":"Uninstallation","text":"<pre><code># Complete removal\nhelm uninstall mysql\n\n# Keep history (resources remain, only Helm metadata removed)\nhelm uninstall server --keep-history\n</code></pre>"},{"location":"02%20helm/commands/#status-inspection","title":"Status &amp; Inspection","text":"<pre><code># List all releases\nhelm ls\nhelm list\nhelm list --all-namespaces\n\n# Get release information\nhelm status mysql\nhelm get notes mysql                    # Show NOTES.txt\nhelm get values mysql                   # Show user-supplied values\nhelm get values mysql --all            # Show all values (including defaults)\nhelm get values mysql --revision 2     # Show values for specific revision\n\n# Get manifest\nhelm get manifest mysql\n</code></pre>"},{"location":"02%20helm/commands/#4-configuration-management","title":"4. CONFIGURATION MANAGEMENT","text":""},{"location":"02%20helm/commands/#setting-values","title":"Setting Values","text":""},{"location":"02%20helm/commands/#method-1-command-line-set","title":"Method 1: Command Line (--set)","text":"<pre><code># Single value\nhelm install mysql bitnami/mysql --set auth.rootPassword=Secret123\n\n# Nested values\nhelm install mysql bitnami/mysql \\\n  --set auth.rootPassword=Secret123 \\\n  --set auth.database=appdb \\\n  --set primary.persistence.size=10Gi\n\n# Arrays/lists\nhelm install myapp bitnami/app --set \"podAnnotations.key=value\"\n</code></pre>"},{"location":"02%20helm/commands/#method-2-values-file-recommended","title":"Method 2: Values File (Recommended)","text":"<pre><code># mysql-values.yaml\nauth:\n  rootPassword: StrongRootPass123\n  database: appdb\n  username: appuser\n  password: AppUserPass123\n\nprimary:\n  persistence:\n    enabled: true\n    size: 10Gi\n    storageClass: standard\n\n  resources:\n    requests:\n      cpu: 250m\n      memory: 512Mi\n    limits:\n      cpu: 500m\n      memory: 1Gi\n\nservice:\n  type: ClusterIP\n</code></pre> <pre><code># Install with values file\nhelm install mysql bitnami/mysql -f mysql-values.yaml\n\n# Override values file with command line\nhelm install mysql bitnami/mysql -f mysql-values.yaml --set replicaCount=3\n</code></pre>"},{"location":"02%20helm/commands/#method-3-multiple-values-files","title":"Method 3: Multiple Values Files","text":"<pre><code># Base values + environment-specific overrides\nhelm install myapp bitnami/app -f values.yaml -f production.yaml\n</code></pre>"},{"location":"02%20helm/commands/#template-rendering-validation","title":"Template Rendering &amp; Validation","text":"<pre><code># Dry run (see what would be deployed)\nhelm install server bitnami/tomcat --dry-run=client\n\n# Render templates locally\nhelm template server bitnami/tomcat\n\n# Debug template rendering\nhelm template server bitnami/tomcat --debug\n\n# Lint chart (validate)\nhelm lint ./mychart\n</code></pre>"},{"location":"02%20helm/commands/#5-advanced-operations","title":"5. ADVANCED OPERATIONS","text":""},{"location":"02%20helm/commands/#installation-options","title":"Installation Options","text":"<pre><code># Wait for resources to be ready\nhelm install myapp bitnami/nginx --wait\nhelm install myapp bitnami/nginx --wait --timeout 60s  # Custom timeout\n\n# Atomic installation (auto-rollback on failure)\nhelm install myapp bitnami/nginx --atomic\n\n# Create namespace if it doesn't exist\nhelm install myapp bitnami/nginx --namespace myns --create-namespace\n\n# Skip CRDs\nhelm install myapp bitnami/app --skip-crds\n\n# Wait for jobs to complete\nhelm install myapp bitnami/app --wait-for-jobs\n</code></pre>"},{"location":"02%20helm/commands/#namespace-management","title":"Namespace Management","text":"<pre><code># Set namespace context\nkubectl config set-context --current --namespace tomcat\n\n# Install in specific namespace\nhelm install tomcat bitnami/tomcat --namespace tomcat\n\n# List releases across all namespaces\nhelm list --all-namespaces\n\n# Get values from specific namespace\nhelm get values mysql --namespace production\n</code></pre>"},{"location":"02%20helm/commands/#upgrade-strategies","title":"Upgrade Strategies","text":"<pre><code># Combined install/upgrade\nhelm upgrade --install myapp bitnami/nginx\n\n# Three-way merge strategy (Helm 3)\nhelm upgrade myapp bitnami/nginx --three-way-merge\n\n# Force resource update\nhelm upgrade myapp bitnami/nginx --force\n\n# Reset values to defaults\nhelm upgrade myapp bitnami/nginx --reset-values\n</code></pre>"},{"location":"02%20helm/commands/#6-debugging-troubleshooting","title":"6. DEBUGGING &amp; TROUBLESHOOTING","text":""},{"location":"02%20helm/commands/#common-issues-solutions","title":"Common Issues &amp; Solutions","text":""},{"location":"02%20helm/commands/#issue-1-image-pull-errors-bitnami-free-tier","title":"Issue 1: Image Pull Errors (Bitnami Free Tier)","text":"<pre><code># Error: Image not available in free tier\n# Solution: Use legacy images\nhelm install mysql bitnami/mysql --set image.repository=bitnamilegacy/mysql\n</code></pre>"},{"location":"02%20helm/commands/#issue-2-readiness-probe-failures","title":"Issue 2: Readiness Probe Failures","text":"<pre><code># Check pod status\nkubectl describe pod &lt;pod-name&gt;\n\n# Check logs\nkubectl logs &lt;pod-name&gt;\n\n# Common fix: Increase initial delay\nhelm upgrade app bitnami/app --set readinessProbe.initialDelaySeconds=60\n</code></pre>"},{"location":"02%20helm/commands/#issue-3-persistent-volume-claims-pending","title":"Issue 3: Persistent Volume Claims Pending","text":"<pre><code># Check PVC status\nkubectl get pvc\n\n# Check storage class\nkubectl get storageclass\n\n# Fix: Specify storage class or reduce size\nhelm upgrade mysql bitnami/mysql --set primary.persistence.storageClass=standard\n</code></pre>"},{"location":"02%20helm/commands/#diagnostic-commands","title":"Diagnostic Commands","text":"<pre><code># Check Kubernetes resources created by Helm\nkubectl get all -l app.kubernetes.io/instance=mysql\n\n# View Helm release secrets\nkubectl get secrets | grep helm.release\n\n# Decode secret values\nkubectl get secret mysql -o jsonpath=\"{.data.mysql-root-password}\" | base64 --decode\n\n# Test database connection (example)\nMYSQL_ROOT_PASSWORD=$(kubectl get secret mysql -o jsonpath=\"{.data.mysql-root-password}\" | base64 --decode)\nmysql -h 127.0.0.1 -P 3306 -u root -p$MYSQL_ROOT_PASSWORD\n</code></pre>"},{"location":"02%20helm/commands/#port-forwarding-for-testing","title":"Port Forwarding for Testing","text":"<pre><code># Forward service port locally\nkubectl port-forward svc/mysql 3306:3306\n\n# Connect to forwarded service\nmysql -h 127.0.0.1 -P 3306 -u root -p$(kubectl get secret mysql -o jsonpath=\"{.data.mysql-root-password}\" | base64 --decode)\n</code></pre>"},{"location":"02%20helm/commands/#7-best-practices","title":"7. BEST PRACTICES","text":""},{"location":"02%20helm/commands/#configuration-management","title":"Configuration Management","text":"<p>\u2705 DO: - Use values files for production deployments - Version control your values files - Use separate values files per environment - Set explicit resource limits</p> <p>\u274c DON'T: - Use <code>:latest</code> tags in production - Store secrets in values files (use Kubernetes Secrets) - Use <code>--reuse-values</code> without understanding implications</p>"},{"location":"02%20helm/commands/#release-management","title":"Release Management","text":"<pre><code># Good: Version-controlled values\nhelm upgrade myapp ./chart -f values/production.yaml\n\n# Good: Atomic deployments\nhelm upgrade myapp ./chart --atomic --wait\n\n# Good: Test before applying\nhelm upgrade myapp ./chart --dry-run\n\n# Bad: Unreproducible\nhelm upgrade myapp ./chart --set key1=val1 --set key2=val2\n</code></pre>"},{"location":"02%20helm/commands/#security-practices","title":"Security Practices","text":"<ol> <li> <p>Use Secrets for sensitive data <pre><code># Instead of:\npassword: PlainTextPassword\n\n# Use:\nexistingSecret: mysql-secret\nsecretKey: password\n</code></pre></p> </li> <li> <p>Enable security contexts <pre><code>helm install app bitnami/app --set securityContext.enabled=true\n</code></pre></p> </li> <li> <p>Regular updates <pre><code># Check for updates\nhelm search repo bitnami/mysql --versions\n\n# Update dependencies\nhelm dependency update ./mychart\n</code></pre></p> </li> </ol>"},{"location":"02%20helm/commands/#performance-optimization","title":"Performance Optimization","text":"<pre><code># Set appropriate resource limits\nhelm install app bitnami/app \\\n  --set resources.requests.cpu=100m \\\n  --set resources.requests.memory=256Mi \\\n  --set resources.limits.cpu=500m \\\n  --set resources.limits.memory=512Mi\n\n# Configure probes\nhelm install app bitnami/app \\\n  --set livenessProbe.initialDelaySeconds=30 \\\n  --set readinessProbe.initialDelaySeconds=5\n</code></pre>"},{"location":"02%20helm/commands/#8-real-world-examples","title":"8. REAL-WORLD EXAMPLES","text":""},{"location":"02%20helm/commands/#complete-mysql-deployment","title":"Complete MySQL Deployment","text":"<pre><code># Step 1: Add repository\nhelm repo add bitnami https://charts.bitnami.com/bitnami\nhelm repo update\n\n# Step 2: Create values file\ncat &gt; mysql-prod.yaml &lt;&lt;EOF\nauth:\n  rootPassword: \"$(openssl rand -base64 16)\"\n  database: appdb\n  username: appuser\n  password: \"$(openssl rand -base64 16)\"\n\nprimary:\n  persistence:\n    enabled: true\n    size: 20Gi\n    storageClass: gp2\n  resources:\n    requests:\n      cpu: 500m\n      memory: 1Gi\n    limits:\n      cpu: 1\n      memory: 2Gi\n\nservice:\n  type: ClusterIP\n\nmetrics:\n  enabled: true\nEOF\n\n# Step 3: Install\nhelm install mysql-prod bitnami/mysql \\\n  -f mysql-prod.yaml \\\n  --namespace database \\\n  --create-namespace \\\n  --wait \\\n  --timeout 5m\n\n# Step 4: Verify\nhelm status mysql-prod --namespace database\nkubectl get pods -n database\n</code></pre>"},{"location":"02%20helm/commands/#tomcat-with-custom-configuration","title":"Tomcat with Custom Configuration","text":"<pre><code># Create values file\ncat &gt; tomcat-custom.yaml &lt;&lt;EOF\nimage:\n  repository: bitnamilegacy/tomcat\n  tag: \"11.0\"\n\nservice:\n  type: LoadBalancer\n  port: 80\n\ntomcat:\n  username: admin\n  password: \"$(openssl rand -base64 12)\"\n\npersistence:\n  enabled: true\n  size: 10Gi\n\nresources:\n  requests:\n    cpu: 200m\n    memory: 512Mi\n  limits:\n    cpu: 500m\n    memory: 1Gi\n\nreadinessProbe:\n  initialDelaySeconds: 60\n  periodSeconds: 10\n\nlivenessProbe:\n  initialDelaySeconds: 120\n  periodSeconds: 20\nEOF\n\n# Install\nhelm install tomcat-app bitnami/tomcat \\\n  -f tomcat-custom.yaml \\\n  --wait \\\n  --atomic\n</code></pre>"},{"location":"02%20helm/commands/#application-stack-multi-chart","title":"Application Stack (Multi-chart)","text":"<pre><code># Deploy database\nhelm install postgresql bitnami/postgresql \\\n  --set auth.database=myapp \\\n  --set auth.username=myuser \\\n  --set primary.persistence.size=10Gi\n\n# Deploy Redis cache\nhelm install redis bitnami/redis \\\n  --set architecture=standalone \\\n  --set master.persistence.size=5Gi\n\n# Deploy application\nhelm install myapp ./myapp-chart \\\n  --set database.host=postgresql \\\n  --set redis.host=redis\n</code></pre>"},{"location":"02%20helm/commands/#quick-reference-cheat-sheet","title":"QUICK REFERENCE CHEAT SHEET","text":""},{"location":"02%20helm/commands/#essential-commands","title":"Essential Commands","text":"<pre><code># Repository\nhelm repo add &lt;name&gt; &lt;url&gt;\nhelm repo update\nhelm search repo &lt;term&gt;\n\n# Installation\nhelm install &lt;name&gt; &lt;chart&gt; [flags]\nhelm install &lt;name&gt; &lt;chart&gt; -f values.yaml\nhelm upgrade --install &lt;name&gt; &lt;chart&gt;\n\n# Management\nhelm ls\nhelm status &lt;release&gt;\nhelm history &lt;release&gt;\nhelm rollback &lt;release&gt; &lt;revision&gt;\nhelm uninstall &lt;release&gt;\n\n# Configuration\nhelm get values &lt;release&gt;\nhelm get manifest &lt;release&gt;\nhelm template &lt;chart&gt;\n</code></pre>"},{"location":"02%20helm/commands/#common-flags","title":"Common Flags","text":"<pre><code>--namespace &lt;ns&gt;           # Deploy to specific namespace\n--create-namespace         # Create namespace if needed\n--wait                     # Wait for resources ready\n--timeout &lt;duration&gt;       # Wait timeout (e.g., 5m)\n--atomic                   # Rollback on failure\n--dry-run                  # Simulate installation\n--debug                    # Enable debug output\n--set key=value            # Set individual values\n-f values.yaml            # Use values file\n--values values.yaml      # Alias for -f\n--version &lt;version&gt;       # Specific chart version\n</code></pre>"},{"location":"02%20helm/commands/#troubleshooting-flow","title":"Troubleshooting Flow","text":"<pre><code>1. helm status &lt;release&gt;           # Check release status\n2. kubectl get pods                # Check pod status\n3. kubectl describe pod &lt;pod&gt;      # Get pod details\n4. kubectl logs &lt;pod&gt;              # Check logs\n5. helm get values &lt;release&gt;       # Check configuration\n6. helm history &lt;release&gt;          # Check revision history\n7. helm rollback &lt;release&gt; &lt;rev&gt;   # Rollback if needed\n</code></pre> <p>This comprehensive guide covers Helm from basic to advanced usage, with practical examples and best practices for production deployments.</p>"},{"location":"02%20helm/create/","title":"Creating Helm Charts","text":""},{"location":"02%20helm/create/#basic-chart-structure","title":"Basic Chart Structure","text":"<pre><code>helm create firstchart\ntree firstchart/\nfirstchart/\n\u251c\u2500\u2500 Chart.yaml          # Chart metadata\n\u251c\u2500\u2500 values.yaml         # Default values\n\u251c\u2500\u2500 templates/          # Kubernetes manifests\n\u2502   \u251c\u2500\u2500 deployment.yaml\n\u2502   \u251c\u2500\u2500 service.yaml\n\u2502   \u251c\u2500\u2500 ingress.yaml\n\u2502   \u251c\u2500\u2500 _helpers.tpl    # Helper templates\n\u2502   \u2514\u2500\u2500 tests/          # Test files\n\u2502       \u2514\u2500\u2500 test-connection.yaml\n\u2514\u2500\u2500 charts/             # Subcharts/dependencies\n</code></pre>"},{"location":"02%20helm/create/#chartyaml-essential-fields","title":"Chart.yaml - Essential Fields","text":"<pre><code>apiVersion: v2\nname: firstchart\ndescription: A Helm chart for Kubernetes\ntype: application        # application or library\nversion: 0.1.0           # Chart version\nappVersion: \"1.0.0\"      # Application version\n</code></pre>"},{"location":"02%20helm/create/#packaging-charts","title":"Packaging Charts","text":"<pre><code># Create .tgz package\nhelm package firstchart/\n# Output: firstchart-0.1.0.tgz\n\n# Package with custom directory\nhelm package firstchart/ -d ./packaged\n\n# Update dependencies in chart\nhelm package firstchart/ -u     # Update dependencies first\n</code></pre>"},{"location":"02%20helm/create/#testing-linting","title":"Testing &amp; Linting","text":"<pre><code># Test a release (requires release name, not version)\nhelm test chart          # Not v1.0.0 or v2\n\n# Lint your chart\nhelm lint firstchart/\n\n# Lint with strict mode\nhelm lint firstchart/ --strict\n\n# Dry run install\nhelm install myrelease firstchart/ --dry-run\n\n# Template rendering\nhelm template myrelease firstchart/\n</code></pre>"},{"location":"02%20helm/create/#chart-development-workflow","title":"Chart Development Workflow","text":"<pre><code># 1. Create chart\nhelm create myapp\n\n# 2. Edit Chart.yaml, values.yaml, templates\n\n# 3. Lint\nhelm lint myapp/\n\n# 4. Dry run\nhelm install myapp myapp/ --dry-run\n\n# 5. Package\nhelm package myapp/\n\n# 6. Install\nhelm install myapp myapp-0.1.0.tgz\n\n# 7. Test (after install)\nhelm test myapp\n</code></pre>"},{"location":"02%20helm/create/#quick-tips","title":"Quick Tips","text":"<ol> <li>Test command needs release name (not version number)</li> <li>Package creates .tgz file in current directory</li> <li>Lint catches YAML syntax errors</li> <li>Dry-run shows what will be deployed</li> <li>port-forward from NOTES.txt to test locally</li> </ol>"},{"location":"02%20helm/notes/","title":"Notes","text":""},{"location":"02%20helm/notes/#sed-surface-newlines-no-duplicates","title":"<code>sed</code> (surface newlines, no duplicates)","text":"<pre><code>helm template . | sed -n 'l'\n</code></pre> <p>Shows line endings explicitly (<code>$</code> = <code>\\n</code>, <code>\\r$</code> = CRLF) without printing each line twice; works consistently on Linux, WSL, and macOS.</p>"},{"location":"02%20helm/notes/#yamllint","title":"yamllint","text":"<p>Validates rendered output for basic YAML syntax and indentation errors before Kubernetes processing.</p>"},{"location":"02%20helm/notes/#kubeconform","title":"kubeconform","text":"<p>Validates rendered manifests against Kubernetes API schemas to catch invalid kinds, fields, or apiVersions.</p> <p>Output only the final content. No preface, no acknowledgements, no closing remarks.</p> <p>{{- }}</p> <p>{{ .Values.my.custome.data }} {{ .Chart.Name }} {{ .Chart.Version }} {{ .Chart.AppVersion }} {{ .Chart.Annotation }}</p> <p>{{.Release.Name}} {{.Release.Namespace}} {{.Release.IsInstall}} {{.Release.IsUpgrade}} {{.Release.Service}}</p> <p>{{Template.Name}} {{Template.BasePath}}</p> <p>--- Pipeline ---</p> <p>output of one command -&gt; passed as input to right side</p> <p>{{ .Values.my.custom.data | default \"testdefault\" | upper | quote }}</p>"},{"location":"02%20helm/practice-questions/","title":"Helm Practice Questions - Progressive Learning Path","text":""},{"location":"02%20helm/practice-questions/#level-1-fundamentals-basic-templates-values","title":"Level 1: Fundamentals (Basic Templates &amp; Values)","text":""},{"location":"02%20helm/practice-questions/#question-11-simple-deployment-chart","title":"Question 1.1: Simple Deployment Chart","text":"<p>Objective: Create a basic Helm chart that deploys a single Nginx application.</p> <p>Requirements: - Chart should accept the following configurable parameters through <code>values.yaml</code>:   - Application name   - Container image and tag   - Number of replicas   - Container port   - Service type (ClusterIP, NodePort, or LoadBalancer) - Generate Deployment and Service manifests - Service should expose the application on the configured port - Use proper naming conventions with release name and chart name</p> <p>Constraints: - Do not hardcode any values in templates - All configuration must come from <code>values.yaml</code></p>"},{"location":"02%20helm/practice-questions/#question-12-configmap-and-environment-variables","title":"Question 1.2: ConfigMap and Environment Variables","text":"<p>Objective: Deploy an application that uses ConfigMap for configuration management.</p> <p>Requirements: - Create a chart that deploys an application with environment variables - ConfigMap should store:   - Application log level   - API endpoint URL   - Database host - Deployment should inject these values as environment variables - Allow overriding all ConfigMap values through <code>values.yaml</code></p> <p>Constraints: - ConfigMap data must be stored separately from deployment spec - Environment variables should be clearly documented</p>"},{"location":"02%20helm/practice-questions/#question-13-multi-container-pod","title":"Question 1.3: Multi-Container Pod","text":"<p>Objective: Create a Deployment with multiple containers in a single pod.</p> <p>Requirements: - Main application container (app container) - Sidecar container (logging aggregator or monitoring agent) - Both containers should have:   - Configurable image and tag   - Configurable resource requests/limits   - Configurable container port - Pod should expose both container ports through the Service</p> <p>Constraints: - Each container must have its own configuration section in <code>values.yaml</code> - Resource limits must be mandatory (not optional)</p>"},{"location":"02%20helm/practice-questions/#level-2-template-logic-control-flow","title":"Level 2: Template Logic &amp; Control Flow","text":""},{"location":"02%20helm/practice-questions/#question-21-conditional-template-rendering","title":"Question 2.1: Conditional Template Rendering","text":"<p>Objective: Create a chart that conditionally includes/excludes Kubernetes resources based on configuration.</p> <p>Requirements: - Deployment should only include:   - Ingress resource when <code>ingress.enabled</code> is true   - HorizontalPodAutoscaler when <code>autoscaling.enabled</code> is true   - ResourceQuota when <code>resourceQuota.enabled</code> is true - Each optional resource must have its complete configuration in <code>values.yaml</code> - Template conditions should be clear and documented</p> <p>Constraints: - Use only Helm template conditionals (if/else) - No resource duplication</p>"},{"location":"02%20helm/practice-questions/#question-22-loops-and-template-iteration","title":"Question 2.2: Loops and Template Iteration","text":"<p>Objective: Deploy an application with multiple configuration volumes and volume mounts.</p> <p>Requirements: - Support mounting multiple ConfigMaps as volumes - Support mounting multiple Secrets as volumes - Each volume should be independently configurable with:   - Name   - Mount path   - Permissions (optional) - Deployment spec should iterate through all configured volumes</p> <p>Constraints: - Volumes list must be defined in <code>values.yaml</code> as an array - Template must use range/loop constructs</p>"},{"location":"02%20helm/practice-questions/#question-23-template-functions-and-filters","title":"Question 2.3: Template Functions and Filters","text":"<p>Objective: Create templates that transform and format values using Helm functions.</p> <p>Requirements: - Application labels should be generated dynamically with:   - Proper casing transformations   - Version tagging   - Environment tagging - Resource names should be truncated to valid Kubernetes length (using trunc function) - Annotations should concatenate multiple values - Template should include:   - String manipulation (upper, lower, title case)   - Quote functions for proper YAML formatting   - Default value handling</p> <p>Constraints: - Must demonstrate at least 5 different Helm template functions - All transformations must be properly quoted for YAML validity</p>"},{"location":"02%20helm/practice-questions/#level-3-advanced-templating-helm-features","title":"Level 3: Advanced Templating &amp; Helm Features","text":""},{"location":"02%20helm/practice-questions/#question-31-helm-hooks-and-lifecycle","title":"Question 3.1: Helm Hooks and Lifecycle","text":"<p>Objective: Create a chart that executes pre and post deployment tasks.</p> <p>Requirements: - Pre-install hook: Job to validate prerequisites - Pre-upgrade hook: Job to backup current state - Post-install hook: Job to initialize application - Each hook should:   - Have appropriate <code>deletion-policy</code> annotation   - Be weight-ordered for execution sequence   - Include proper RBAC if needed - Hooks should be optional (can be disabled via <code>values.yaml</code>)</p> <p>Constraints: - Hooks must not cause deployment failure if they fail (unless critical) - Proper cleanup policies must be defined</p>"},{"location":"02%20helm/practice-questions/#question-32-named-templates-and-template-reuse","title":"Question 3.2: Named Templates and Template Reuse","text":"<p>Objective: Create reusable template components for common patterns.</p> <p>Requirements: - Create named templates for:   - Pod label generation   - Common annotations generation   - Resource requests/limits   - Security context   - Probe configuration - Main templates should use these named templates extensively - Support both individual container probes and pod-wide security settings</p> <p>Constraints: - Each named template must be in separate <code>_*.tpl</code> file - Demonstrate template scope and parameter passing - Minimize code duplication</p>"},{"location":"02%20helm/practice-questions/#question-33-helm-values-validation-defaults","title":"Question 3.3: Helm Values Validation &amp; Defaults","text":"<p>Objective: Create a chart with complex nested values and validation.</p> <p>Requirements: - Define a complex <code>values.yaml</code> with:   - Nested objects for database configuration   - Nested objects for cache configuration   - Nested arrays for multiple environments   - Proper defaults for all optional values - Template must:   - Validate that required values are provided   - Apply defaults for missing optional values   - Handle type mismatches gracefully - Include template that documents all available values</p> <p>Constraints: - Use <code>required</code> function for mandatory values - Use nested object structure (3+ levels deep)</p>"},{"location":"02%20helm/practice-questions/#question-34-multi-chart-dependencies","title":"Question 3.4: Multi-Chart Dependencies","text":"<p>Objective: Create a parent chart that depends on child charts.</p> <p>Requirements: - Create main application chart - Create dependency charts for:   - Database (PostgreSQL/MySQL)   - Cache (Redis)   - Message Queue (RabbitMQ) - Parent chart should:   - Declare dependencies in <code>Chart.yaml</code>   - Override dependency values from parent <code>values.yaml</code>   - Control which dependencies are installed   - Pass values to child charts with proper scoping</p> <p>Constraints: - Dependencies must be properly versioned - Parent chart must demonstrate selective dependency enablement</p>"},{"location":"02%20helm/practice-questions/#level-4-production-patterns-advanced-features","title":"Level 4: Production Patterns &amp; Advanced Features","text":""},{"location":"02%20helm/practice-questions/#question-41-multi-environment-namespace-strategy","title":"Question 4.1: Multi-Environment &amp; Namespace Strategy","text":"<p>Objective: Create a single chart that deploys across development, staging, and production environments.</p> <p>Requirements: - Chart should support values files for each environment:   - <code>values-dev.yaml</code> - for development   - <code>values-stg.yaml</code> - for staging   - <code>values-prod.yaml</code> - for production - Each environment should have different:   - Replica counts   - Resource limits   - Security policies   - Ingress configurations   - Storage configurations - Template should conditionally include environment-specific ConfigMaps and Secrets</p> <p>Constraints: - Single chart.yaml, multiple values files - No environment hardcoding in templates</p>"},{"location":"02%20helm/practice-questions/#question-42-secrets-management-integration","title":"Question 4.2: Secrets Management Integration","text":"<p>Objective: Create templates that integrate with external secret management systems.</p> <p>Requirements: - Support multiple secret backends:   - Kubernetes native Secrets   - External Secrets Operator references   - HashiCorp Vault integration - Templates should:   - Reference secrets by name/path   - Handle secret rotation annotations   - Support sealed secrets compatibility - Deployment should mount secrets:   - As environment variables   - As volume mounts   - Both methods should be configurable</p> <p>Constraints: - No secret values in <code>values.yaml</code> - Template must support switching between secret backends via configuration</p>"},{"location":"02%20helm/practice-questions/#question-43-rbac-and-security-context-templates","title":"Question 4.3: RBAC and Security Context Templates","text":"<p>Objective: Create comprehensive RBAC and security-focused templates.</p> <p>Requirements: - Generate:   - ServiceAccount   - Role/ClusterRole (with appropriate permissions)   - RoleBinding/ClusterRoleBinding   - Pod Security Policy or Security Context   - Network Policy (optional) - Configuration should include:   - Custom RBAC rules (configurable)   - Pod security context (read-only filesystem, non-root user)   - Container security context (capabilities dropping)   - Service Account Token automounting control</p> <p>Constraints: - RBAC rules must be minimal and specific - Security context must follow Kubernetes best practices</p>"},{"location":"02%20helm/practice-questions/#question-44-complex-deployment-strategy","title":"Question 4.4: Complex Deployment Strategy","text":"<p>Objective: Create templates supporting multiple deployment strategies.</p> <p>Requirements: - Support multiple deployment patterns:   - Rolling update (standard)   - Blue-green deployment (two separate deployments)   - Canary deployment (with weighted traffic)   - Feature flag driven releases - For each strategy:   - Define required configuration in <code>values.yaml</code>   - Template conditionally generates appropriate resources   - Support gradual traffic shifting - Include status tracking resources (optional)</p> <p>Constraints: - Single chart must support all strategies - Switching strategies should only require values.yaml changes</p>"},{"location":"02%20helm/practice-questions/#question-45-helm-testing-validation","title":"Question 4.5: Helm Testing &amp; Validation","text":"<p>Objective: Create test charts and validation templates.</p> <p>Requirements: - Create Helm test pods that validate:   - Application connectivity   - Configuration correctness   - Environment variables   - Volume mounts   - Security context - Test pod should:   - Run after deployment   - Have proper cleanup policies   - Exit with appropriate status   - Generate meaningful output - Include template that validates values.yaml schema</p> <p>Constraints: - Tests must not require external dependencies - Test cleanup must be automatic</p>"},{"location":"02%20helm/practice-questions/#level-5-expert-scenarios-real-world-patterns","title":"Level 5: Expert Scenarios &amp; Real-World Patterns","text":""},{"location":"02%20helm/practice-questions/#question-51-stateful-application-with-storage","title":"Question 5.1: Stateful Application with Storage","text":"<p>Objective: Create a chart for a stateful application with persistent storage.</p> <p>Requirements: - Statefulset for ordered, stable Pod identities - Storage configuration:   - PersistentVolumeClaim generation   - Storage class selection   - Mount path configuration - Support:   - StatefulSet specific features (headless service, pod DNS)   - Rolling updates with PVC retention   - Backup/restore hooks   - Data migration between storage classes</p> <p>Constraints: - Must use StatefulSet (not Deployment) - Storage configuration must be flexible</p>"},{"location":"02%20helm/practice-questions/#question-52-helm-plugins-and-custom-functions","title":"Question 5.2: Helm Plugins and Custom Functions","text":"<p>Objective: Create templates that use custom plugin functions or advanced templating.</p> <p>Requirements: - Implement custom template functions for:   - Environment-specific variable substitution   - Kubernetes resource validation   - Image digest resolution   - Custom label generation - Template must:   - Use these custom functions effectively   - Handle function failures gracefully   - Document custom function usage</p> <p>Constraints: - Must create actual working functions (not pseudocode) - Functions must be reusable across templates</p>"},{"location":"02%20helm/practice-questions/#question-53-gitops-and-helm-operator-integration","title":"Question 5.3: GitOps and Helm Operator Integration","text":"<p>Objective: Create a chart designed for GitOps workflow.</p> <p>Requirements: - Chart structure optimized for:   - Flux CD or ArgoCD integration   - HelmRelease resource generation   - Automated sync and monitoring - Include:   - Health check configuration   - Sync behavior controls   - Rollback capabilities   - Notification annotations - Support value-driven deployments from Git</p> <p>Constraints: - Chart must be compatible with popular GitOps tools - Proper CRD references if needed</p>"},{"location":"02%20helm/practice-questions/#question-54-umbrella-chart-with-microservices","title":"Question 5.4: Umbrella Chart with Microservices","text":"<p>Objective: Create a parent chart managing multiple microservices.</p> <p>Requirements: - Parent chart (umbrella) that orchestrates:   - API service   - Frontend service   - Backend service   - Worker service   - Database   - Cache - Features:   - Selective component enablement   - Shared configuration management   - Service discovery and DNS configuration   - Cross-service communication security   - Centralized logging configuration</p> <p>Constraints: - Must handle 6+ child charts - Shared values must be properly scoped</p>"},{"location":"02%20helm/practice-questions/#question-55-production-readiness-checklist","title":"Question 5.5: Production Readiness Checklist","text":"<p>Objective: Create a comprehensive production-ready Helm chart.</p> <p>Requirements: - Chart must include and configure:   - Health checks (liveness and readiness probes)   - Resource requests and limits   - Horizontal Pod Autoscaling   - Pod Disruption Budgets   - Affinity rules (node/pod affinity)   - Tolerations for node taints   - Proper logging and monitoring hooks   - RBAC and security contexts   - Network policies   - Service mesh integration (if applicable) - Documentation:   - Values.yaml documentation   - Deployment guide   - Troubleshooting guide   - Upgrade procedure</p> <p>Constraints: - All components must be optional (configurable) - Security requirements must be non-negotiable - Performance/reliability cannot be compromised</p>"},{"location":"02%20helm/practice-questions/#learning-path-recommendation","title":"Learning Path Recommendation","text":"<ol> <li>Start with Level 1 (1.1 \u2192 1.2 \u2192 1.3) to understand basic Helm mechanics</li> <li>Progress to Level 2 (2.1 \u2192 2.2 \u2192 2.3) to learn template logic and features</li> <li>Move to Level 3 (3.1 \u2192 3.2 \u2192 3.3 \u2192 3.4) for advanced templating patterns</li> <li>Explore Level 4 (4.1 \u2192 4.2 \u2192 4.3 \u2192 4.4 \u2192 4.5) for production scenarios</li> <li>Master Level 5 (5.1 \u2192 5.2 \u2192 5.3 \u2192 5.4 \u2192 5.5) for expert implementations</li> </ol> <p>Each level builds on previous knowledge. Complete at least 2-3 questions per level before advancing.</p>"},{"location":"02%20helm/question1/","title":"Question1","text":""},{"location":"02%20helm/question1/#helm-practice-exercise-1","title":"Helm Practice Exercise - 1","text":"<p>These exercises are designed to help you master Helm mechanics only\u2014chart structure, templating, values, conditionals, hooks, dependencies, upgrades, and reuse\u2014using simple dummy workloads (<code>busybox</code>, <code>sleep</code>, <code>echo</code>).</p> <p>There is no application logic complexity involved.</p>"},{"location":"02%20helm/question1/#level-1-basic-chart-structure","title":"Level 1: Basic Chart Structure","text":""},{"location":"02%20helm/question1/#11-simple-dummy-application","title":"1.1 Simple Dummy Application","text":"<p>Objective: Create a Helm chart named <code>simple-app</code> that deploys a container which only sleeps.</p> <p>Requirements:</p> <ul> <li> <p>Chart metadata:</p> </li> <li> <p><code>name</code>: <code>simple-app</code></p> </li> <li><code>version</code>: <code>0.1.0</code></li> <li><code>appVersion</code>: <code>1.0</code></li> <li> <p>One <code>Deployment</code></p> </li> <li> <p><code>replicas: 1</code></p> </li> <li>Image: <code>busybox:latest</code></li> <li> <p>Command:</p> <p><pre><code>[\"sh\", \"-c\", \"echo 'App started' &amp;&amp; sleep 3600\"]\n</code></pre> * One <code>Service</code></p> </li> <li> <p>Exposes port <code>8080</code> (even if unused)</p> </li> </ul> <p>Constraint:</p> <ul> <li>All configuration must be hardcoded in the manifests.</li> <li>No <code>values.yaml</code> usage yet.</li> </ul>"},{"location":"02%20helm/question1/#12-add-configuration-values","title":"1.2 Add Configuration Values","text":"<p>Objective: Parameterize the chart using <code>values.yaml</code>.</p> <p>Requirements: Create <code>values.yaml</code> with:</p> <pre><code>replicaCount: 2\n\nimage: busybox:latest\n\ncommand:\n  - sh\n  - -c\n  - echo 'Default command' &amp;&amp; sleep 3600\n\nservice:\n  port: 8080\n</code></pre> <p>Template Expectations:</p> <ul> <li>All values must be referenced using <code>{{ .Values.* }}</code></li> </ul> <p>Test:</p> <pre><code>helm install test simple-app --set replicaCount=3\n</code></pre> <p>Verify that 3 Pods are created.</p>"},{"location":"02%20helm/question1/#level-2-template-logic","title":"Level 2: Template Logic","text":""},{"location":"02%20helm/question1/#21-conditional-sidecar-container","title":"2.1 Conditional Sidecar Container","text":"<p>Objective: Add a sidecar container conditionally.</p> <p>Requirements (values.yaml):</p> <pre><code>sidecar:\n  enabled: false\n  image: nginx:alpine\n  port: 80\n</code></pre> <p>Template Logic:</p> <ul> <li>Use:</li> </ul> <p><pre><code>{{- if .Values.sidecar.enabled }}\n</code></pre> * Add a second container:</p> <ul> <li>Name: <code>sidecar</code></li> <li>Image: from values</li> <li>Exposes <code>containerPort</code></li> </ul> <p>Test:</p> <ul> <li>Install normally \u2192 no sidecar</li> <li>Install with:</li> </ul> <pre><code>--set sidecar.enabled=true\n</code></pre>"},{"location":"02%20helm/question1/#22-dynamic-names-and-labels","title":"2.2 Dynamic Names and Labels","text":"<p>Objective: Generate labels and names dynamically.</p> <p>Requirements (values.yaml):</p> <pre><code>environment: dev\nteam: platform\n</code></pre> <p>Deployment Labels:</p> <ul> <li><code>environment: &lt;value&gt;</code></li> <li><code>team: &lt;value&gt;</code></li> <li><code>fullname: &lt;release-name&gt;-&lt;chart-name&gt;</code></li> </ul> <p>Additional Resource: Create a <code>ConfigMap</code>:</p> <ul> <li>Name:</li> </ul> <p><pre><code>{{ .Release.Name }}-config\n</code></pre> * Data:</p> <pre><code>environment: &lt;value&gt;\n</code></pre> <p>Key Challenge:</p> <ul> <li>Proper quoting and sanitization of label values.</li> </ul>"},{"location":"02%20helm/question1/#level-3-advanced-templating","title":"Level 3: Advanced Templating","text":""},{"location":"02%20helm/question1/#31-looping-environment-variables","title":"3.1 Looping Environment Variables","text":"<p>Objective: Dynamically inject environment variables.</p> <p>values.yaml:</p> <pre><code>env:\n  - name: LOG_LEVEL\n    value: \"INFO\"\n  - name: TIMEOUT\n    value: \"30\"\n\nenvSecrets: []\n</code></pre> <p>Requirements:</p> <ul> <li>Use:</li> </ul> <pre><code>{{- range .Values.env }}\n</code></pre> <p>to render environment variables. * Create a demo Secret with a similar structure. * Conditionally mount secrets only if <code>envSecrets</code> is non-empty.</p>"},{"location":"02%20helm/question1/#32-configmap-from-file-with-tpl","title":"3.2 ConfigMap from File with <code>tpl</code>","text":"<p>Objective: Render templated config content stored in values.</p> <p>values.yaml:</p> <pre><code>configData: |\n  app.name: {{ .Chart.Name }}\n  app.version: {{ .Chart.Version }}\n</code></pre> <p>templates/config.yaml:</p> <ul> <li>Use:</li> </ul> <pre><code>{{ tpl .Values.configData . }}\n</code></pre> <p>Mounting:</p> <ul> <li>Mount ConfigMap at:</li> </ul> <pre><code>/app/config/app.conf\n</code></pre> <p>Test:</p> <ul> <li>Verify rendered ConfigMap shows actual chart name and version, not template text.</li> </ul>"},{"location":"02%20helm/question1/#level-4-named-templates-helper-functions","title":"Level 4: Named Templates &amp; Helper Functions","text":""},{"location":"02%20helm/question1/#41-create-_helperstpl-for-labels","title":"4.1 Create <code>_helpers.tpl</code> for Labels","text":"<p>Objective: Extract common labels into reusable named templates.</p> <p>Create <code>templates/_helpers.tpl</code>:</p> <p>Define these named templates:</p> <ol> <li><code>simple-app.labels</code> - standard labels</li> <li><code>simple-app.selectorLabels</code> - selector labels only</li> <li><code>simple-app.name</code> - chart name</li> <li><code>simple-app.fullname</code> - full resource name</li> </ol> <p>Example Structure:</p> <pre><code>{{- define \"simple-app.labels\" -}}\napp.kubernetes.io/name: {{ include \"simple-app.name\" . }}\napp.kubernetes.io/instance: {{ .Release.Name }}\napp.kubernetes.io/version: {{ .Chart.AppVersion | quote }}\napp.kubernetes.io/managed-by: {{ .Release.Service }}\n{{- end }}\n</code></pre> <p>Requirements:</p> <ul> <li>Update Deployment and Service to use:</li> </ul> <p><pre><code>labels:\n  {{- include \"simple-app.labels\" . | nindent 4 }}\n</code></pre> * Use proper <code>nindent</code> for alignment</p>"},{"location":"02%20helm/question1/#42-string-manipulation-functions","title":"4.2 String Manipulation Functions","text":"<p>Objective: Practice Helm string functions.</p> <p>values.yaml additions:</p> <pre><code>appName: \"My Demo App\"\nnamespace: \"production\"\nimageTag: \"v1.2.3-alpha\"\n</code></pre> <p>Template Requirements:</p> <p>Create a ConfigMap that demonstrates:</p> <ul> <li><code>upper</code> - convert appName to uppercase</li> <li><code>lower</code> - convert namespace to lowercase</li> <li><code>title</code> - title case the appName</li> <li><code>trim</code> - remove whitespace</li> <li><code>trimSuffix</code> - remove <code>-alpha</code> from tag</li> <li><code>replace</code> - replace spaces with dashes</li> <li><code>quote</code> - properly quote values</li> </ul> <p>Example Data:</p> <pre><code>data:\n  APP_NAME_UPPER: {{ .Values.appName | upper | quote }}\n  NAMESPACE_LOWER: {{ .Values.namespace | lower | quote }}\n  TAG_CLEAN: {{ .Values.imageTag | trimSuffix \"-alpha\" | quote }}\n  APP_SLUG: {{ .Values.appName | replace \" \" \"-\" | lower | quote }}\n</code></pre>"},{"location":"02%20helm/question1/#43-default-values-with-default-function","title":"4.3 Default Values with <code>default</code> Function","text":"<p>Objective: Handle missing or optional values gracefully.</p> <p>values.yaml:</p> <pre><code>service:\n  type: ClusterIP\n  # port is intentionally missing\n\nresources: {}\n  # limits and requests are optional\n</code></pre> <p>Template Logic:</p> <p>In Service template:</p> <pre><code>port: {{ default 8080 .Values.service.port }}\n</code></pre> <p>In Deployment template:</p> <pre><code>resources:\n  {{- if .Values.resources.limits }}\n  limits:\n    cpu: {{ default \"100m\" .Values.resources.limits.cpu }}\n    memory: {{ default \"128Mi\" .Values.resources.limits.memory }}\n  {{- end }}\n  requests:\n    cpu: {{ default \"50m\" .Values.resources.requests.cpu }}\n    memory: {{ default \"64Mi\" .Values.resources.requests.memory }}\n</code></pre> <p>Test:</p> <ul> <li>Install without setting values \u2192 uses defaults</li> <li>Override specific values \u2192 uses overrides</li> </ul>"},{"location":"02%20helm/question1/#level-5-data-structures-advanced-logic","title":"Level 5: Data Structures &amp; Advanced Logic","text":""},{"location":"02%20helm/question1/#51-working-with-lists-and-range","title":"5.1 Working with Lists and <code>range</code>","text":"<p>Objective: Master iteration over lists and maps.</p> <p>values.yaml:</p> <pre><code>additionalLabels:\n  cost-center: \"engineering\"\n  project: \"demo\"\n  owner: \"platform-team\"\n\nvolumes:\n  - name: cache\n    mountPath: /cache\n    emptyDir: {}\n  - name: data\n    mountPath: /data\n    emptyDir:\n      sizeLimit: 1Gi\n</code></pre> <p>Deployment Template:</p> <p>Add labels section:</p> <pre><code>labels:\n  {{- include \"simple-app.labels\" . | nindent 4 }}\n  {{- range $key, $value := .Values.additionalLabels }}\n  {{ $key }}: {{ $value | quote }}\n  {{- end }}\n</code></pre> <p>Add volumes and volumeMounts:</p> <pre><code>volumes:\n{{- range .Values.volumes }}\n- name: {{ .name }}\n  emptyDir:\n    {{- toYaml .emptyDir | nindent 4 }}\n{{- end }}\n\nvolumeMounts:\n{{- range .Values.volumes }}\n- name: {{ .name }}\n  mountPath: {{ .mountPath }}\n{{- end }}\n</code></pre>"},{"location":"02%20helm/question1/#52-conditionals-with-and-or-not","title":"5.2 Conditionals with <code>and</code>, <code>or</code>, <code>not</code>","text":"<p>Objective: Use logical operators for complex conditions.</p> <p>values.yaml:</p> <pre><code>monitoring:\n  enabled: true\n  prometheus: true\n  grafana: false\n\nsecurity:\n  enabled: true\n  readOnlyRootFilesystem: true\n  runAsNonRoot: true\n</code></pre> <p>Template Logic:</p> <pre><code>{{- if and .Values.monitoring.enabled .Values.monitoring.prometheus }}\nannotations:\n  prometheus.io/scrape: \"true\"\n  prometheus.io/port: \"8080\"\n{{- end }}\n\n{{- if or .Values.security.enabled .Values.security.runAsNonRoot }}\nsecurityContext:\n  {{- if .Values.security.runAsNonRoot }}\n  runAsNonRoot: true\n  runAsUser: 1000\n  {{- end }}\n  {{- if .Values.security.readOnlyRootFilesystem }}\n  readOnlyRootFilesystem: true\n  {{- end }}\n{{- end }}\n</code></pre>"},{"location":"02%20helm/question1/#53-required-values-validation","title":"5.3 Required Values Validation","text":"<p>Objective: Ensure critical values are provided.</p> <p>values.yaml:</p> <pre><code># image is required - no default\nimage: \"\"\n\ndatabase:\n  # host is required\n  host: \"\"\n  port: 5432\n</code></pre> <p>Template with Validation:</p> <pre><code>image: {{ required \"A valid .Values.image is required!\" .Values.image }}\n\nenv:\n- name: DB_HOST\n  value: {{ required \"database.host is required!\" .Values.database.host | quote }}\n- name: DB_PORT\n  value: {{ .Values.database.port | quote }}\n</code></pre> <p>Test:</p> <ul> <li>Install without image \u2192 should fail with error message</li> <li>Install with image \u2192 succeeds</li> </ul>"},{"location":"02%20helm/question1/#level-6-yaml-formatting-complex-types","title":"Level 6: YAML Formatting &amp; Complex Types","text":""},{"location":"02%20helm/question1/#61-working-with-toyaml-and-nindent","title":"6.1 Working with <code>toYaml</code> and <code>nindent</code>","text":"<p>Objective: Properly render complex YAML structures.</p> <p>values.yaml:</p> <pre><code>podAnnotations:\n  backup.velero.io/backup-volumes: data\n  prometheus.io/scrape: \"true\"\n\ntolerations:\n  - key: \"node.kubernetes.io/disk-pressure\"\n    operator: \"Exists\"\n    effect: \"NoSchedule\"\n  - key: \"environment\"\n    operator: \"Equal\"\n    value: \"production\"\n    effect: \"NoSchedule\"\n\nnodeSelector:\n  disktype: ssd\n  zone: us-west-1a\n</code></pre> <p>Deployment Template:</p> <pre><code>{{- with .Values.podAnnotations }}\nannotations:\n  {{- toYaml . | nindent 2 }}\n{{- end }}\n\nspec:\n  {{- with .Values.nodeSelector }}\n  nodeSelector:\n    {{- toYaml . | nindent 4 }}\n  {{- end }}\n\n  {{- with .Values.tolerations }}\n  tolerations:\n    {{- toYaml . | nindent 2 }}\n  {{- end }}\n</code></pre> <p>Key Concepts:</p> <ul> <li><code>with</code> for scoping</li> <li><code>toYaml</code> for complex structures</li> <li>Correct <code>nindent</code> values for alignment</li> </ul>"},{"location":"02%20helm/question1/#62-merging-values-with-merge-and-mustmerge","title":"6.2 Merging Values with <code>merge</code> and <code>mustMerge</code>","text":"<p>Objective: Combine multiple value sources.</p> <p>values.yaml:</p> <pre><code>commonLabels:\n  team: platform\n  managed-by: helm\n\ndeploymentLabels:\n  component: backend\n\npodLabels:\n  version: v1\n</code></pre> <p>Template:</p> <pre><code>{{- $labels := merge .Values.podLabels .Values.deploymentLabels .Values.commonLabels }}\nlabels:\n  {{- range $key, $value := $labels }}\n  {{ $key }}: {{ $value | quote }}\n  {{- end }}\n</code></pre> <p>Expected Result: All three label sets merged together (podLabels takes precedence).</p>"},{"location":"02%20helm/question1/#63-type-conversion-functions","title":"6.3 Type Conversion Functions","text":"<p>Objective: Convert between types safely.</p> <p>values.yaml:</p> <pre><code>replicas: \"3\"  # string instead of int\nport: 8080     # int\n\nfeatures:\n  caching: \"true\"  # string bool\n  debug: false     # real bool\n</code></pre> <p>Template:</p> <pre><code>replicas: {{ .Values.replicas | int }}\n\nenv:\n- name: CACHE_ENABLED\n  value: {{ .Values.features.caching | toString | quote }}\n- name: DEBUG\n  value: {{ .Values.features.debug | toString | quote }}\n</code></pre> <p>Functions to Practice:</p> <ul> <li><code>int</code> - convert to integer</li> <li><code>toString</code> - convert to string</li> <li><code>float64</code> - convert to float</li> </ul>"},{"location":"02%20helm/question1/#level-7-production-ready-patterns","title":"Level 7: Production-Ready Patterns","text":""},{"location":"02%20helm/question1/#71-health-checks-probes","title":"7.1 Health Checks (Probes)","text":"<p>Objective: Add comprehensive health checking.</p> <p>values.yaml:</p> <pre><code>livenessProbe:\n  httpGet:\n    path: /healthz\n    port: http\n  initialDelaySeconds: 30\n  periodSeconds: 10\n  timeoutSeconds: 5\n  failureThreshold: 3\n\nreadinessProbe:\n  httpGet:\n    path: /ready\n    port: http\n  initialDelaySeconds: 10\n  periodSeconds: 5\n  timeoutSeconds: 3\n  failureThreshold: 3\n\nstartupProbe:\n  httpGet:\n    path: /startup\n    port: http\n  initialDelaySeconds: 0\n  periodSeconds: 5\n  failureThreshold: 30\n</code></pre> <p>Deployment Template:</p> <pre><code>{{- if .Values.livenessProbe }}\nlivenessProbe:\n  {{- toYaml .Values.livenessProbe | nindent 2 }}\n{{- end }}\n\n{{- if .Values.readinessProbe }}\nreadinessProbe:\n  {{- toYaml .Values.readinessProbe | nindent 2 }}\n{{- end }}\n\n{{- if .Values.startupProbe }}\nstartupProbe:\n  {{- toYaml .Values.startupProbe | nindent 2 }}\n{{- end }}\n</code></pre>"},{"location":"02%20helm/question1/#72-resource-management","title":"7.2 Resource Management","text":"<p>Objective: Properly define resource requests and limits.</p> <p>values.yaml:</p> <pre><code>resources:\n  limits:\n    cpu: 500m\n    memory: 512Mi\n  requests:\n    cpu: 100m\n    memory: 128Mi\n\nautoscaling:\n  enabled: false\n  minReplicas: 2\n  maxReplicas: 10\n  targetCPUUtilizationPercentage: 80\n  targetMemoryUtilizationPercentage: 80\n</code></pre> <p>Deployment:</p> <pre><code>resources:\n  {{- toYaml .Values.resources | nindent 2 }}\n</code></pre> <p>Create <code>hpa.yaml</code>:</p> <pre><code>{{- if .Values.autoscaling.enabled }}\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: {{ include \"simple-app.fullname\" . }}\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: {{ include \"simple-app.fullname\" . }}\n  minReplicas: {{ .Values.autoscaling.minReplicas }}\n  maxReplicas: {{ .Values.autoscaling.maxReplicas }}\n  metrics:\n  {{- if .Values.autoscaling.targetCPUUtilizationPercentage }}\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: {{ .Values.autoscaling.targetCPUUtilizationPercentage }}\n  {{- end }}\n  {{- if .Values.autoscaling.targetMemoryUtilizationPercentage }}\n  - type: Resource\n    resource:\n      name: memory\n      target:\n        type: Utilization\n        averageUtilization: {{ .Values.autoscaling.targetMemoryUtilizationPercentage }}\n  {{- end }}\n{{- end }}\n</code></pre>"},{"location":"02%20helm/question1/#73-notestxt-template","title":"7.3 NOTES.txt Template","text":"<p>Objective: Create helpful installation output.</p> <p>Create <code>templates/NOTES.txt</code>:</p> <pre><code>Thank you for installing {{ .Chart.Name }}!\n\nYour release is named {{ .Release.Name }}.\n\nTo learn more about the release, try:\n\n  $ helm status {{ .Release.Name }}\n  $ helm get all {{ .Release.Name }}\n\n{{- if .Values.service.type }}\n\nService Type: {{ .Values.service.type }}\n\n{{- if eq .Values.service.type \"NodePort\" }}\n  export NODE_PORT=$(kubectl get --namespace {{ .Release.Namespace }} -o jsonpath=\"{.spec.ports[0].nodePort}\" services {{ include \"simple-app.fullname\" . }})\n  export NODE_IP=$(kubectl get nodes --namespace {{ .Release.Namespace }} -o jsonpath=\"{.items[0].status.addresses[0].address}\")\n  echo http://$NODE_IP:$NODE_PORT\n{{- else if eq .Values.service.type \"LoadBalancer\" }}\n  NOTE: It may take a few minutes for the LoadBalancer IP to be available.\n  Watch the status with: \n    kubectl get --namespace {{ .Release.Namespace }} svc -w {{ include \"simple-app.fullname\" . }}\n{{- else if eq .Values.service.type \"ClusterIP\" }}\n  export POD_NAME=$(kubectl get pods --namespace {{ .Release.Namespace }} -l \"app.kubernetes.io/name={{ include \"simple-app.name\" . }},app.kubernetes.io/instance={{ .Release.Name }}\" -o jsonpath=\"{.items[0].metadata.name}\")\n  echo \"Visit http://127.0.0.1:8080\"\n  kubectl --namespace {{ .Release.Namespace }} port-forward $POD_NAME 8080:{{ .Values.service.port }}\n{{- end }}\n{{- end }}\n\n{{- if .Values.autoscaling.enabled }}\n\nAutoscaling is ENABLED:\n  Min Replicas: {{ .Values.autoscaling.minReplicas }}\n  Max Replicas: {{ .Values.autoscaling.maxReplicas }}\n{{- else }}\n\nAutoscaling is DISABLED. Using {{ .Values.replicaCount }} replica(s).\n{{- end }}\n</code></pre>"},{"location":"02%20helm/question1/#final-challenge-comprehensive-chart-package","title":"Final Challenge: Comprehensive Chart Package","text":"<p>Objective: Combine everything into a production-ready chart.</p>"},{"location":"02%20helm/question1/#requirements-checklist","title":"Requirements Checklist:","text":"<ul> <li> Chart.yaml with proper metadata (name, version, description, keywords, maintainers)</li> <li> values.yaml with comprehensive defaults and comments</li> <li> _helpers.tpl with all named templates</li> <li> Deployment with:</li> <li> Named template labels</li> <li> ConfigMap/Secret mounts</li> <li> Resource limits</li> <li> All three probe types</li> <li> Security context</li> <li> Node selectors and tolerations</li> <li> Service with configurable type</li> <li> ConfigMap with templated content</li> <li> HorizontalPodAutoscaler (conditional)</li> <li> ServiceAccount (conditional)</li> <li> Ingress (conditional)</li> <li> NOTES.txt with helpful instructions</li> <li> Proper <code>toYaml</code>, <code>nindent</code>, <code>quote</code> usage throughout</li> <li> Required value validation</li> <li> No hardcoded values</li> </ul>"},{"location":"02%20helm/question1/#validation-commands","title":"Validation Commands:","text":"<pre><code># Lint the chart\nhelm lint .\n\n# Dry run with debug\nhelm install --dry-run --debug test-release .\n\n# Template with different values\nhelm template test-release . -f values-prod.yaml\n\n# Install and verify\nhelm install my-app .\nkubectl get all\nhelm get values my-app\n\n# Upgrade\nhelm upgrade my-app . --set replicaCount=3\n\n# Check history\nhelm history my-app\n</code></pre>"},{"location":"02%20helm/question1/#recommended-practice-flow","title":"Recommended Practice Flow","text":"<ol> <li>Level 1-2: Get basic structure working</li> <li>Level 3: Add templating and loops</li> <li>Level 4: Create helpers and use functions</li> <li>Level 5-6: Master data structures and YAML handling</li> <li>Level 7: Production-ready features</li> <li>Final Challenge: Package everything together</li> </ol> <p>Pro Tips:</p> <ul> <li>Always use <code>helm lint</code> before installing</li> <li>Use <code>--dry-run --debug</code> to see rendered templates</li> <li>Test with <code>helm template</code> to avoid cluster pollution</li> <li>Validate YAML syntax with <code>yamllint</code></li> <li>Keep templates readable with proper indentation</li> <li>Document values.yaml thoroughly</li> </ul>"},{"location":"02%20helm/question2/","title":"Question2","text":""},{"location":"02%20helm/question2/#comprehensive-helm-chart-practice-from-zero-to-production-expert","title":"Comprehensive Helm Chart Practice: From Zero to Production Expert","text":"<p>Chart Name: <code>webapp</code> (use this name throughout all phases)</p> <p>Principle: You build ONE chart progressively. Each phase adds features while maintaining backward compatibility.</p>"},{"location":"02%20helm/question2/#phase-1-foundation-hello-helm","title":"Phase 1: Foundation - Hello Helm","text":"<p>Duration: 15 minutes | Difficulty: Beginner</p>"},{"location":"02%20helm/question2/#objective","title":"Objective","text":"<p>Create the absolute minimum valid Helm chart and verify it renders.</p>"},{"location":"02%20helm/question2/#what-youll-learn","title":"What You'll Learn","text":"<ul> <li>Chart structure and metadata</li> <li>Basic templating syntax</li> <li>Helm template rendering</li> </ul>"},{"location":"02%20helm/question2/#requirements","title":"Requirements","text":"<p>Step 1.1 - Create Chart Structure</p> <pre><code>webapp/\n\u251c\u2500\u2500 Chart.yaml\n\u251c\u2500\u2500 values.yaml\n\u2514\u2500\u2500 templates/\n    \u2514\u2500\u2500 configmap.yaml\n</code></pre> <p>Step 1.2 - Chart.yaml</p> <pre><code>apiVersion: v2\nname: webapp\ndescription: Progressive Helm Learning Chart\ntype: application\nversion: 0.1.0\nappVersion: \"1.0\"\nkeywords:\n  - helm\n  - practice\n  - kubernetes\nmaintainers:\n  - name: Your Name\n    email: your@email.com\n</code></pre> <p>Step 1.3 - values.yaml</p> <pre><code>appName: webapp\nenabled: true\n</code></pre> <p>Step 1.4 - templates/configmap.yaml</p> <p>Create a simple ConfigMap:</p> <pre><code>{{- if .Values.enabled }}\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: {{ .Release.Name }}-config\n  namespace: {{ .Release.Namespace }}\n  labels:\n    app: {{ .Values.appName }}\ndata:\n  app-name: {{ .Values.appName }}\n  chart-name: {{ .Chart.Name }}\n  chart-version: {{ .Chart.Version }}\n{{- end }}\n</code></pre>"},{"location":"02%20helm/question2/#validation","title":"Validation","text":"<pre><code># Render the template\nhelm template my-release .\n\n# Expected output: ConfigMap resource with metadata and data\n\n# Verify disabling works\nhelm template my-release . --set enabled=false\n\n# Expected: No output (resource not rendered)\n</code></pre>"},{"location":"02%20helm/question2/#concepts-introduced","title":"Concepts Introduced","text":"<ul> <li><code>.Chart.*</code> (chart metadata)</li> <li><code>.Release.*</code> (release information)</li> <li><code>.Values.*</code> (user-provided values)</li> <li>Basic conditional <code>if/end</code></li> <li>String interpolation with <code>{{ }}</code></li> </ul>"},{"location":"02%20helm/question2/#phase-2-workloads-deployment-service","title":"Phase 2: Workloads - Deployment &amp; Service","text":"<p>Duration: 30 minutes | Difficulty: Beginner</p>"},{"location":"02%20helm/question2/#objective_1","title":"Objective","text":"<p>Deploy a functional application with Service exposure.</p>"},{"location":"02%20helm/question2/#what-youll-learn_1","title":"What You'll Learn","text":"<ul> <li>Deployment templating</li> <li>Service configuration</li> <li>Pod specification basics</li> <li>Value quoting and safety</li> </ul>"},{"location":"02%20helm/question2/#requirements_1","title":"Requirements","text":"<p>Step 2.1 - Update values.yaml</p> <pre><code>appName: webapp\nenabled: true\n\nreplicaCount: 2\nimage: busybox:latest\nimagePullPolicy: IfNotPresent\n\ncontainerPort: 8080\nservicePort: 8080\nserviceType: ClusterIP\n\ncommand:\n  - /bin/sh\n  - -c\n  - echo \"App started on port 8080\" &amp;&amp; sleep 3600\n\nnameOverride: \"\"\nfullnameOverride: \"\"\n</code></pre> <p>Step 2.2 - Create templates/_helpers.tpl</p> <pre><code>{{/*\nGenerate common labels\n*/}}\n{{- define \"webapp.labels\" -}}\napp.kubernetes.io/name: {{ .Chart.Name }}\napp.kubernetes.io/instance: {{ .Release.Name }}\napp.kubernetes.io/version: {{ .Chart.AppVersion | quote }}\napp.kubernetes.io/managed-by: {{ .Release.Service }}\n{{- end }}\n\n{{/*\nGenerate selector labels\n*/}}\n{{- define \"webapp.selectorLabels\" -}}\napp.kubernetes.io/name: {{ .Chart.Name }}\napp.kubernetes.io/instance: {{ .Release.Name }}\n{{- end }}\n\n{{/*\nExpand the name of the chart\n*/}}\n{{- define \"webapp.name\" -}}\n{{- default .Chart.Name .Values.nameOverride | trunc 63 | trimSuffix \"-\" }}\n{{- end }}\n\n{{/*\nExpand the full name\n*/}}\n{{- define \"webapp.fullname\" -}}\n{{- if .Values.fullnameOverride }}\n{{- .Values.fullnameOverride | trunc 63 | trimSuffix \"-\" }}\n{{- else }}\n{{- $name := default .Chart.Name .Values.nameOverride }}\n{{- if contains $name .Release.Name }}\n{{- .Release.Name | trunc 63 | trimSuffix \"-\" }}\n{{- else }}\n{{- printf \"%s-%s\" .Release.Name $name | trunc 63 | trimSuffix \"-\" }}\n{{- end }}\n{{- end }}\n{{- end }}\n</code></pre> <p>Step 2.3 - Create templates/deployment.yaml</p> <pre><code>{{- if .Values.enabled }}\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: {{ include \"webapp.fullname\" . }}\n  namespace: {{ .Release.Namespace }}\n  labels:\n    {{- include \"webapp.labels\" . | nindent 4 }}\nspec:\n  replicas: {{ .Values.replicaCount }}\n  selector:\n    matchLabels:\n      {{- include \"webapp.selectorLabels\" . | nindent 6 }}\n  template:\n    metadata:\n      labels:\n        {{- include \"webapp.selectorLabels\" . | nindent 8 }}\n    spec:\n      containers:\n      - name: {{ .Chart.Name }}\n        image: {{ .Values.image | quote }}\n        imagePullPolicy: {{ .Values.imagePullPolicy }}\n        ports:\n        - name: http\n          containerPort: {{ .Values.containerPort }}\n          protocol: TCP\n        command:\n          {{- toYaml .Values.command | nindent 10 }}\n        livenessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - ps aux | grep sleep || exit 1\n          initialDelaySeconds: 10\n          periodSeconds: 10\n        readinessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - ps aux | grep sleep || exit 1\n          initialDelaySeconds: 5\n          periodSeconds: 5\n{{- end }}\n</code></pre> <p>Step 2.4 - Create templates/service.yaml</p> <pre><code>{{- if .Values.enabled }}\napiVersion: v1\nkind: Service\nmetadata:\n  name: {{ include \"webapp.fullname\" . }}\n  namespace: {{ .Release.Namespace }}\n  labels:\n    {{- include \"webapp.labels\" . | nindent 4 }}\nspec:\n  type: {{ .Values.serviceType }}\n  ports:\n    - port: {{ .Values.servicePort }}\n      targetPort: http\n      protocol: TCP\n      name: http\n  selector:\n    {{- include \"webapp.selectorLabels\" . | nindent 4 }}\n{{- end }}\n</code></pre>"},{"location":"02%20helm/question2/#validation_1","title":"Validation","text":"<pre><code># Verify template rendering\nhelm template my-app . --debug\n\n# Dry-run install\nhelm install --dry-run my-app .\n\n# Real install\nhelm install my-app .\n\n# Verify resources\nkubectl get deployment,service\nkubectl logs deployment/my-app-webapp\n\n# Scale up\nhelm upgrade my-app . --set replicaCount=3\nkubectl get pods\n</code></pre>"},{"location":"02%20helm/question2/#concepts-introduced_1","title":"Concepts Introduced","text":"<ul> <li>Named templates (helpers)</li> <li><code>include</code> function</li> <li><code>nindent</code> for proper indentation</li> <li><code>toYaml</code> for array rendering</li> <li><code>trunc</code>, <code>trimSuffix</code> string functions</li> <li>Deployment and Service specs</li> <li>Label management</li> </ul>"},{"location":"02%20helm/question2/#phase-3-configuration-management","title":"Phase 3: Configuration Management","text":"<p>Duration: 45 minutes | Difficulty: Beginner-Intermediate</p>"},{"location":"02%20helm/question2/#objective_2","title":"Objective","text":"<p>Handle configuration through ConfigMaps and environment variables dynamically.</p>"},{"location":"02%20helm/question2/#what-youll-learn_2","title":"What You'll Learn","text":"<ul> <li>ConfigMap templating</li> <li>Environment variable injection</li> <li>Looping with <code>range</code></li> <li>Nested values</li> </ul>"},{"location":"02%20helm/question2/#requirements_2","title":"Requirements","text":"<p>Step 3.1 - Update values.yaml</p> <pre><code># ... previous values ...\n\n# Configuration Maps\nconfigMaps:\n  app-config:\n    data:\n      LOG_LEVEL: \"INFO\"\n      APP_VERSION: \"1.0\"\n      ENVIRONMENT: \"dev\"\n\n  app-settings:\n    data:\n      MAX_CONNECTIONS: \"100\"\n      TIMEOUT: \"30\"\n\n# Environment variables from ConfigMap\nenvFromConfigMaps:\n  - name: app-config\n  - name: app-settings\n\n# Direct environment variables\nenv:\n  - name: POD_NAME\n    valueFrom:\n      fieldRef:\n        fieldPath: metadata.name\n  - name: POD_NAMESPACE\n    valueFrom:\n      fieldRef:\n        fieldPath: metadata.namespace\n</code></pre> <p>Step 3.2 - Create templates/configmap.yaml</p> <pre><code>{{- if .Values.enabled }}\n{{- range $cmName, $cmData := .Values.configMaps }}\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: {{ include \"webapp.fullname\" $ }}-{{ $cmName }}\n  namespace: {{ $.Release.Namespace }}\n  labels:\n    {{- include \"webapp.labels\" $ | nindent 4 }}\n    config-type: {{ $cmName }}\ndata:\n  {{- range $key, $value := $cmData.data }}\n  {{ $key }}: {{ $value | quote }}\n  {{- end }}\n{{- end }}\n{{- end }}\n</code></pre> <p>Step 3.3 - Update templates/deployment.yaml</p> <p>Add envFrom section to containers:</p> <pre><code>        envFrom:\n        {{- range .Values.envFromConfigMaps }}\n        - configMapRef:\n            name: {{ include \"webapp.fullname\" . }}-{{ .name }}\n        {{- end }}\n\n        env:\n        {{- range .Values.env }}\n        - name: {{ .name }}\n          valueFrom:\n            fieldRef:\n              fieldPath: {{ .valueFrom.fieldRef.fieldPath }}\n        {{- end }}\n</code></pre>"},{"location":"02%20helm/question2/#validation_2","title":"Validation","text":"<pre><code># Check ConfigMaps created\nhelm install my-app . --debug\nkubectl get configmaps\nkubectl get configmap my-app-webapp-app-config -o yaml\n\n# Verify environment variables in pod\nkubectl exec deployment/my-app-webapp -- env | grep APP\n\n# Update config and restart\nhelm upgrade my-app . --set configMaps.app-config.data.LOG_LEVEL=DEBUG\nkubectl rollout restart deployment/my-app-webapp\nkubectl logs deployment/my-app-webapp\n</code></pre>"},{"location":"02%20helm/question2/#concepts-introduced_2","title":"Concepts Introduced","text":"<ul> <li><code>range</code> with maps and key-value pairs</li> <li>Multiple ConfigMaps from values</li> <li><code>envFrom</code> and <code>env</code> injection</li> <li>Fieldref (metadata access)</li> <li>Variable scoping with <code>$</code></li> </ul>"},{"location":"02%20helm/question2/#phase-4-conditional-features-advanced-logic","title":"Phase 4: Conditional Features &amp; Advanced Logic","text":"<p>Duration: 60 minutes | Difficulty: Intermediate</p>"},{"location":"02%20helm/question2/#objective_3","title":"Objective","text":"<p>Control feature availability and implement complex logic conditions.</p>"},{"location":"02%20helm/question2/#what-youll-learn_3","title":"What You'll Learn","text":"<ul> <li>Feature flags</li> <li>Multiple conditionals with <code>and</code>, <code>or</code>, <code>not</code></li> <li>ServiceAccount and RBAC</li> <li>Ingress configuration</li> </ul>"},{"location":"02%20helm/question2/#requirements_3","title":"Requirements","text":"<p>Step 4.1 - Update values.yaml</p> <pre><code># ... previous values ...\n\nserviceAccount:\n  create: true\n  name: \"\"\n  annotations: {}\n\nrbac:\n  create: true\n\ningress:\n  enabled: false\n  className: \"nginx\"\n  annotations:\n    cert-manager.io/cluster-issuer: \"letsencrypt-prod\"\n  hosts:\n    - host: webapp.example.com\n      paths:\n        - path: /\n          pathType: Prefix\n  tls:\n    - secretName: webapp-tls\n      hosts:\n        - webapp.example.com\n\npersistence:\n  enabled: false\n  storageClassName: \"\"\n  size: 1Gi\n  mountPath: /data\n\nmonitoring:\n  enabled: false\n  scrapeInterval: 30s\n</code></pre> <p>Step 4.2 - Create templates/serviceaccount.yaml</p> <pre><code>{{- if and .Values.enabled .Values.serviceAccount.create }}\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: {{ include \"webapp.fullname\" . }}\n  namespace: {{ .Release.Namespace }}\n  labels:\n    {{- include \"webapp.labels\" . | nindent 4 }}\n  {{- with .Values.serviceAccount.annotations }}\n  annotations:\n    {{- toYaml . | nindent 4 }}\n  {{- end }}\n{{- end }}\n</code></pre> <p>Step 4.3 - Create templates/role.yaml</p> <pre><code>{{- if and .Values.enabled .Values.rbac.create .Values.serviceAccount.create }}\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: {{ include \"webapp.fullname\" . }}\n  namespace: {{ .Release.Namespace }}\n  labels:\n    {{- include \"webapp.labels\" . | nindent 4 }}\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n- apiGroups: [\"\"]\n  resources: [\"configmaps\"]\n  verbs: [\"get\", \"list\"]\n{{- end }}\n</code></pre> <p>Step 4.4 - Create templates/rolebinding.yaml</p> <pre><code>{{- if and .Values.enabled .Values.rbac.create .Values.serviceAccount.create }}\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: {{ include \"webapp.fullname\" . }}\n  namespace: {{ .Release.Namespace }}\n  labels:\n    {{- include \"webapp.labels\" . | nindent 4 }}\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: {{ include \"webapp.fullname\" . }}\nsubjects:\n- kind: ServiceAccount\n  name: {{ include \"webapp.fullname\" . }}\n  namespace: {{ .Release.Namespace }}\n{{- end }}\n</code></pre> <p>Step 4.5 - Create templates/ingress.yaml</p> <pre><code>{{- if and .Values.enabled .Values.ingress.enabled }}\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: {{ include \"webapp.fullname\" . }}\n  namespace: {{ .Release.Namespace }}\n  labels:\n    {{- include \"webapp.labels\" . | nindent 4 }}\n  {{- with .Values.ingress.annotations }}\n  annotations:\n    {{- toYaml . | nindent 4 }}\n  {{- end }}\nspec:\n  {{- if .Values.ingress.className }}\n  ingressClassName: {{ .Values.ingress.className }}\n  {{- end }}\n  {{- if .Values.ingress.tls }}\n  tls:\n    {{- range .Values.ingress.tls }}\n    - hosts:\n        {{- range .hosts }}\n        - {{ . | quote }}\n        {{- end }}\n      secretName: {{ .secretName }}\n    {{- end }}\n  {{- end }}\n  rules:\n    {{- range .Values.ingress.hosts }}\n    - host: {{ .host | quote }}\n      http:\n        paths:\n          {{- range .paths }}\n          - path: {{ .path }}\n            pathType: {{ .pathType }}\n            backend:\n              service:\n                name: {{ include \"webapp.fullname\" $ }}\n                port:\n                  number: {{ $.Values.servicePort }}\n          {{- end }}\n    {{- end }}\n{{- end }}\n</code></pre> <p>Step 4.6 - Update templates/deployment.yaml</p> <p>Add ServiceAccount and monitoring annotations:</p> <pre><code>spec:\n  {{- if .Values.serviceAccount.create }}\n  serviceAccountName: {{ include \"webapp.fullname\" . }}\n  automountServiceAccountToken: true\n  {{- end }}\n\n  {{- if .Values.monitoring.enabled }}\n  podAnnotations:\n    prometheus.io/scrape: \"true\"\n    prometheus.io/port: \"8080\"\n    prometheus.io/path: \"/metrics\"\n  {{- end }}\n</code></pre>"},{"location":"02%20helm/question2/#validation_3","title":"Validation","text":"<pre><code># Enable features incrementally\nhelm install my-app . --set rbac.create=true\n\n# Verify RBAC\nkubectl get role,rolebinding\n\n# Enable Ingress\nhelm upgrade my-app . --set ingress.enabled=true --set ingress.hosts[0].host=app.local\nkubectl get ingress\n\n# Verify conditional logic\nhelm template . --set rbac.create=false | grep -c \"kind: Role\"  # Should be 0\nhelm template . --set rbac.create=true | grep -c \"kind: Role\"   # Should be 1\n</code></pre>"},{"location":"02%20helm/question2/#concepts-introduced_3","title":"Concepts Introduced","text":"<ul> <li><code>and</code>, <code>or</code>, <code>not</code> operators</li> <li><code>with</code> statement for scoping</li> <li>Multiple conditional levels</li> <li>RBAC templating</li> <li>Ingress with TLS</li> <li>Optional features pattern</li> </ul>"},{"location":"02%20helm/question2/#phase-5-secrets-sensitive-data","title":"Phase 5: Secrets &amp; Sensitive Data","text":"<p>Duration: 45 minutes | Difficulty: Intermediate</p>"},{"location":"02%20helm/question2/#objective_4","title":"Objective","text":"<p>Safely handle sensitive information and credential management.</p>"},{"location":"02%20helm/question2/#what-youll-learn_4","title":"What You'll Learn","text":"<ul> <li>Secret templating</li> <li>Best practices for sensitive data</li> <li>Base64 encoding</li> <li>Multiple secret sources</li> </ul>"},{"location":"02%20helm/question2/#requirements_4","title":"Requirements","text":"<p>Step 5.1 - Update values.yaml</p> <pre><code># ... previous values ...\n\nsecrets:\n  create: false\n  database:\n    username: admin\n    password: \"changeme\"\n  api:\n    key: \"your-api-key-here\"\n    secret: \"your-api-secret\"\n\nexternalSecrets:\n  enabled: false\n  backend: vault\n  secretStore: vault-backend\n  secretPath: secret/data/webapp\n</code></pre> <p>Step 5.2 - Create templates/secret.yaml</p> <pre><code>{{- if and .Values.enabled .Values.secrets.create }}\napiVersion: v1\nkind: Secret\nmetadata:\n  name: {{ include \"webapp.fullname\" . }}\n  namespace: {{ .Release.Namespace }}\n  labels:\n    {{- include \"webapp.labels\" . | nindent 4 }}\ntype: Opaque\ndata:\n  {{- range $key, $secret := .Values.secrets }}\n  {{- if and (not (eq $key \"create\")) (ne $secret nil) }}\n  {{- range $subkey, $value := $secret }}\n  {{ $key }}-{{ $subkey }}: {{ $value | b64enc | quote }}\n  {{- end }}\n  {{- end }}\n  {{- end }}\n{{- end }}\n</code></pre> <p>Step 5.3 - Create templates/externalsecrets.yaml</p> <pre><code>{{- if and .Values.enabled .Values.externalSecrets.enabled }}\napiVersion: external-secrets.io/v1beta1\nkind: SecretStore\nmetadata:\n  name: {{ include \"webapp.fullname\" . }}\n  namespace: {{ .Release.Namespace }}\nspec:\n  provider:\n    vault:\n      server: \"https://vault.example.com\"\n      path: \"{{ .Values.externalSecrets.secretPath }}\"\n      auth:\n        kubernetes:\n          mountPath: \"kubernetes\"\n          role: \"{{ .Release.Name }}\"\n---\napiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: {{ include \"webapp.fullname\" . }}\n  namespace: {{ .Release.Namespace }}\nspec:\n  refreshInterval: 1h\n  secretStoreRef:\n    name: {{ include \"webapp.fullname\" . }}\n    kind: SecretStore\n  target:\n    name: {{ include \"webapp.fullname\" . }}-external\n    creationPolicy: Owner\n  data:\n  - secretKey: database-password\n    remoteRef:\n      key: database\n      property: password\n  - secretKey: api-key\n    remoteRef:\n      key: api\n      property: key\n{{- end }}\n</code></pre> <p>Step 5.4 - Update templates/deployment.yaml</p> <p>Add secret injection:</p> <pre><code>        {{- if or .Values.secrets.create .Values.externalSecrets.enabled }}\n        envFrom:\n        - secretRef:\n            {{- if .Values.externalSecrets.enabled }}\n            name: {{ include \"webapp.fullname\" . }}-external\n            {{- else }}\n            name: {{ include \"webapp.fullname\" . }}\n            {{- end }}\n        {{- end }}\n\n        volumeMounts:\n        - name: secret-volume\n          mountPath: /etc/secrets\n          readOnly: true\n        {{- end }}\n\n      volumes:\n      {{- if or .Values.secrets.create .Values.externalSecrets.enabled }}\n      - name: secret-volume\n        secret:\n          {{- if .Values.externalSecrets.enabled }}\n          secretName: {{ include \"webapp.fullname\" . }}-external\n          {{- else }}\n          secretName: {{ include \"webapp.fullname\" . }}\n          {{- end }}\n      {{- end }}\n</code></pre>"},{"location":"02%20helm/question2/#validation_4","title":"Validation","text":"<pre><code># Create secrets in values\nhelm install my-app . --set secrets.create=true --set-string secrets.database.password='mysecretpass'\n\n# Verify secret created\nkubectl get secret my-app-webapp -o yaml | grep database-username\n\n# Decode and verify\nkubectl get secret my-app-webapp -o jsonpath='{.data.database-password}' | base64 -d\n\n# Test external secrets\nhelm install my-app . --set externalSecrets.enabled=true\nkubectl get externalsecrets\n</code></pre>"},{"location":"02%20helm/question2/#concepts-introduced_4","title":"Concepts Introduced","text":"<ul> <li><code>b64enc</code> function for base64 encoding</li> <li>Secret creation patterns</li> <li>External Secrets operator</li> <li>Conditional secret sources</li> <li>Volume mounting secrets</li> </ul>"},{"location":"02%20helm/question2/#phase-6-storage-persistence","title":"Phase 6: Storage &amp; Persistence","text":"<p>Duration: 45 minutes | Difficulty: Intermediate-Advanced</p>"},{"location":"02%20helm/question2/#objective_5","title":"Objective","text":"<p>Implement persistent storage and volume management.</p>"},{"location":"02%20helm/question2/#what-youll-learn_5","title":"What You'll Learn","text":"<ul> <li>PersistentVolumeClaim templating</li> <li>Storage class configuration</li> <li>StatefulSet basics</li> <li>Volume management patterns</li> </ul>"},{"location":"02%20helm/question2/#requirements_5","title":"Requirements","text":"<p>Step 6.1 - Update values.yaml</p> <pre><code># ... previous values ...\n\npersistence:\n  enabled: false\n  type: pvc  # pvc or emptyDir\n  storageClassName: standard\n  accessMode: ReadWriteOnce\n  size: 5Gi\n  mountPath: /data\n  subPath: \"\"\n\n  # For StatefulSet-style storage\n  volumeClaimTemplates:\n    - name: data\n      size: 5Gi\n      mountPath: /data\n\nemptyDir:\n  enabled: false\n  sizeLimit: 1Gi\n</code></pre> <p>Step 6.2 - Create templates/pvc.yaml</p> <pre><code>{{- if and .Values.enabled .Values.persistence.enabled (eq .Values.persistence.type \"pvc\") }}\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: {{ include \"webapp.fullname\" . }}-data\n  namespace: {{ .Release.Namespace }}\n  labels:\n    {{- include \"webapp.labels\" . | nindent 4 }}\nspec:\n  accessModes:\n    - {{ .Values.persistence.accessMode }}\n  storageClassName: {{ .Values.persistence.storageClassName }}\n  resources:\n    requests:\n      storage: {{ .Values.persistence.size }}\n{{- end }}\n</code></pre> <p>Step 6.3 - Update templates/deployment.yaml</p> <p>Add volume mounts:</p> <pre><code>        volumeMounts:\n        {{- if .Values.persistence.enabled }}\n        {{- if eq .Values.persistence.type \"pvc\" }}\n        - name: data\n          mountPath: {{ .Values.persistence.mountPath }}\n          {{- if .Values.persistence.subPath }}\n          subPath: {{ .Values.persistence.subPath }}\n          {{- end }}\n        {{- end }}\n        {{- end }}\n        {{- if .Values.emptyDir.enabled }}\n        - name: cache\n          mountPath: /cache\n        {{- end }}\n\n      volumes:\n      {{- if and .Values.persistence.enabled (eq .Values.persistence.type \"pvc\") }}\n      - name: data\n        persistentVolumeClaim:\n          claimName: {{ include \"webapp.fullname\" . }}-data\n      {{- end }}\n      {{- if .Values.emptyDir.enabled }}\n      - name: cache\n        emptyDir:\n          sizeLimit: {{ .Values.emptyDir.sizeLimit }}\n      {{- end }}\n</code></pre>"},{"location":"02%20helm/question2/#validation_5","title":"Validation","text":"<pre><code># Enable persistence\nhelm install my-app . --set persistence.enabled=true\n\n# Verify PVC\nkubectl get pvc\nkubectl describe pvc my-app-webapp-data\n\n# Check volume mounted in pod\nkubectl exec deployment/my-app-webapp -- ls -la /data\n\n# Write data and verify persistence\nkubectl exec deployment/my-app-webapp -- sh -c 'echo \"test\" &gt; /data/test.txt'\nkubectl delete pod -l app.kubernetes.io/instance=my-app\nkubectl exec deployment/my-app-webapp -- cat /data/test.txt  # Should still exist\n</code></pre>"},{"location":"02%20helm/question2/#concepts-introduced_5","title":"Concepts Introduced","text":"<ul> <li>PersistentVolumeClaim templates</li> <li>Storage class configuration</li> <li>EmptyDir volumes</li> <li>Volume mounting patterns</li> <li>Data persistence verification</li> </ul>"},{"location":"02%20helm/question2/#phase-7-scaling-performance","title":"Phase 7: Scaling &amp; Performance","text":"<p>Duration: 60 minutes | Difficulty: Intermediate-Advanced</p>"},{"location":"02%20helm/question2/#objective_6","title":"Objective","text":"<p>Implement autoscaling, resource management, and performance tuning.</p>"},{"location":"02%20helm/question2/#what-youll-learn_6","title":"What You'll Learn","text":"<ul> <li>HorizontalPodAutoscaler</li> <li>Resource requests and limits</li> <li>PodDisruptionBudget</li> <li>Pod affinity</li> </ul>"},{"location":"02%20helm/question2/#requirements_6","title":"Requirements","text":"<p>Step 7.1 - Update values.yaml</p> <pre><code># ... previous values ...\n\nresources:\n  limits:\n    cpu: 500m\n    memory: 512Mi\n  requests:\n    cpu: 250m\n    memory: 256Mi\n\nautoscaling:\n  enabled: false\n  minReplicas: 2\n  maxReplicas: 10\n  targetCPUUtilizationPercentage: 80\n  targetMemoryUtilizationPercentage: 80\n\npodDisruptionBudget:\n  enabled: false\n  minAvailable: 1\n  # maxUnavailable: 1\n\naffinity:\n  podAntiAffinity: soft  # soft or hard\n  nodeAffinity: {}\n  # Example nodeAffinity:\n  #   requiredDuringSchedulingIgnoredDuringExecution:\n  #     nodeSelectorTerms:\n  #     - matchExpressions:\n  #       - key: node-role.kubernetes.io/master\n  #         operator: DoesNotExist\n\ntopologySpreadConstraints:\n  enabled: false\n  maxSkew: 1\n  topologyKey: kubernetes.io/hostname\n</code></pre> <p>Step 7.2 - Create templates/hpa.yaml</p> <pre><code>{{- if and .Values.enabled .Values.autoscaling.enabled }}\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: {{ include \"webapp.fullname\" . }}\n  namespace: {{ .Release.Namespace }}\n  labels:\n    {{- include \"webapp.labels\" . | nindent 4 }}\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: {{ include \"webapp.fullname\" . }}\n  minReplicas: {{ .Values.autoscaling.minReplicas }}\n  maxReplicas: {{ .Values.autoscaling.maxReplicas }}\n  metrics:\n  {{- if .Values.autoscaling.targetCPUUtilizationPercentage }}\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: {{ .Values.autoscaling.targetCPUUtilizationPercentage }}\n  {{- end }}\n  {{- if .Values.autoscaling.targetMemoryUtilizationPercentage }}\n  - type: Resource\n    resource:\n      name: memory\n      target:\n        type: Utilization\n        averageUtilization: {{ .Values.autoscaling.targetMemoryUtilizationPercentage }}\n  {{- end }}\n  behavior:\n    scaleDown:\n      stabilizationWindowSeconds: 300\n      policies:\n      - type: Percent\n        value: 50\n        periodSeconds: 60\n    scaleUp:\n      stabilizationWindowSeconds: 0\n      policies:\n      - type: Percent\n        value: 100\n        periodSeconds: 30\n{{- end }}\n</code></pre> <p>Step 7.3 - Create templates/pdb.yaml</p> <pre><code>{{- if and .Values.enabled .Values.podDisruptionBudget.enabled }}\napiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: {{ include \"webapp.fullname\" . }}\n  namespace: {{ .Release.Namespace }}\n  labels:\n    {{- include \"webapp.labels\" . | nindent 4 }}\nspec:\n  {{- if .Values.podDisruptionBudget.minAvailable }}\n  minAvailable: {{ .Values.podDisruptionBudget.minAvailable }}\n  {{- end }}\n  {{- if .Values.podDisruptionBudget.maxUnavailable }}\n  maxUnavailable: {{ .Values.podDisruptionBudget.maxUnavailable }}\n  {{- end }}\n  selector:\n    matchLabels:\n      {{- include \"webapp.selectorLabels\" . | nindent 6 }}\n{{- end }}\n</code></pre> <p>Step 7.4 - Update templates/deployment.yaml</p> <p>Add resource limits, affinity, and topology spread:</p> <pre><code>spec:\n  replicas: {{ .Values.replicaCount }}\n\n  selector:\n    matchLabels:\n      {{- include \"webapp.selectorLabels\" . | nindent 6 }}\n\n  template:\n    metadata:\n      labels:\n        {{- include \"webapp.selectorLabels\" . | nindent 8 }}\n    spec:\n      {{- if .Values.affinity.podAntiAffinity }}\n      affinity:\n        podAntiAffinity:\n          {{- if eq .Values.affinity.podAntiAffinity \"hard\" }}\n          requiredDuringSchedulingIgnoredDuringExecution:\n          {{- else }}\n          preferredDuringSchedulingIgnoredDuringExecution:\n          {{- end }}\n          - podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app.kubernetes.io/name\n                  operator: In\n                  values:\n                  - {{ .Chart.Name }}\n              topologyKey: kubernetes.io/hostname\n            {{- if eq .Values.affinity.podAntiAffinity \"soft\" }}\n            weight: 100\n            {{- end }}\n        {{- with .Values.affinity.nodeAffinity }}\n        nodeAffinity:\n          {{- toYaml . | nindent 10 }}\n        {{- end }}\n      {{- end }}\n\n      {{- if .Values.topologySpreadConstraints.enabled }}\n      topologySpreadConstraints:\n      - maxSkew: {{ .Values.topologySpreadConstraints.maxSkew }}\n        topologyKey: {{ .Values.topologySpreadConstraints.topologyKey }}\n        whenUnsatisfiable: DoNotSchedule\n        labelSelector:\n          matchLabels:\n            {{- include \"webapp.selectorLabels\" . | nindent 12 }}\n      {{- end }}\n\n      containers:\n      - name: {{ .Chart.Name }}\n        resources:\n          {{- toYaml .Values.resources | nindent 10 }}\n</code></pre>"},{"location":"02%20helm/question2/#validation_6","title":"Validation","text":"<pre><code># Enable autoscaling\nhelm install my-app . --set autoscaling.enabled=true --set autoscaling.minReplicas=2\n\n# Verify HPA\nkubectl get hpa\nkubectl describe hpa my-app-webapp\n\n# Enable PDB\nhelm upgrade my-app . --set podDisruptionBudget.enabled=true\n\n# Check resource requests\nkubectl get pods -o json | jq '.items[0].spec.containers[0].resources'\n\n# Simulate load and watch HPA scale\nkubectl run -it --rm load-generator --image=busybox /bin/sh\n# Inside: while true; do wget -q -O- http://my-app-webapp:8080; done\n\n# Watch HPA respond\nkubectl get hpa -w\n</code></pre>"},{"location":"02%20helm/question2/#concepts-introduced_6","title":"Concepts Introduced","text":"<ul> <li>HorizontalPodAutoscaler (v2)</li> <li>Resource requests and limits</li> <li>PodDisruptionBudget</li> <li>Pod affinity and anti-affinity</li> <li>Topology spread constraints</li> <li>Scaling policies and behavior</li> </ul>"},{"location":"02%20helm/question2/#phase-8-monitoring-observability","title":"Phase 8: Monitoring &amp; Observability","text":"<p>Duration: 60 minutes | Difficulty: Intermediate-Advanced</p>"},{"location":"02%20helm/question2/#objective_7","title":"Objective","text":"<p>Integrate monitoring, logging, and observability features.</p>"},{"location":"02%20helm/question2/#what-youll-learn_7","title":"What You'll Learn","text":"<ul> <li>Prometheus integration</li> <li>Service Monitor templating</li> <li>Logging configuration</li> <li>Tracing support</li> </ul>"},{"location":"02%20helm/question2/#requirements_7","title":"Requirements","text":"<p>Step 8.1 - Update values.yaml</p> <pre><code># ... previous values ...\n\nmonitoring:\n  enabled: false\n  serviceMonitor:\n    enabled: false\n    interval: 30s\n    scrapeTimeout: 10s\n  prometheus:\n    rules:\n      enabled: false\n\nlogging:\n  enabled: false\n  level: INFO\n  format: json\n\ntracing:\n  enabled: false\n  jaeger:\n    enabled: false\n    agent: jaeger-agent\n    port: 6831\n</code></pre> <p>Step 8.2 - Create templates/servicemonitor.yaml</p> <pre><code>{{- if and .Values.enabled .Values.monitoring.serviceMonitor.enabled }}\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: {{ include \"webapp.fullname\" . }}\n  namespace: {{ .Release.Namespace }}\n  labels:\n    {{- include \"webapp.labels\" . | nindent 4 }}\nspec:\n  selector:\n    matchLabels:\n      {{- include \"webapp.selectorLabels\" . | nindent 6 }}\n  endpoints:\n  - port: http\n    interval: {{ .Values.monitoring.serviceMonitor.interval }}\n    scrapeTimeout: {{ .Values.monitoring.serviceMonitor.scrapeTimeout }}\n    path: /metrics\n{{- end }}\n</code></pre> <p>Step 8.3 - Create templates/prometheusrule.yaml</p> <pre><code>{{- if and .Values.enabled .Values.monitoring.prometheus.rules.enabled }}\napiVersion: monitoring.coreos.com/v1\nkind: PrometheusRule\nmetadata:\n  name: {{ include \"webapp.fullname\" . }}\n  namespace: {{ .Release.Namespace }}\n  labels:\n    {{- include \"webapp.labels\" . | nindent 4 }}\n    prometheus: kube-prometheus\nspec:\n  groups:\n  - name: {{ include \"webapp.fullname\" . }}\n    interval: {{ .Values.monitoring.serviceMonitor.interval }}\n    rules:\n    - alert: {{ include \"webapp.name\" . | upper }}HighErrorRate\n      expr: |\n        (sum(rate(http_requests_total{job=\"{{ include \"webapp.fullname\" . }}\", status=~\"5..\"}[5m])) \n        / \n        sum(rate(http_requests_total{job=\"{{ include \"webapp.fullname\" . }}\"}[5m]))) &gt; 0.05\n      for: 5m\n      labels:\n        severity: warning\n      annotations:\n        summary: \"High error rate detected\"\n        description: \"{{ include \"webapp.name\" . }} error rate is above 5%\"\n{{- end }}\n</code></pre> <p>Step 8.4 - Update templates/deployment.yaml</p> <p>Add monitoring annotations and environment:</p> <pre><code>    metadata:\n      {{- if .Values.monitoring.enabled }}\n      annotations:\n        prometheus.io/scrape: \"true\"\n        prometheus.io/port: \"{{ .Values.containerPort }}\"\n        prometheus.io/path: \"/metrics\"\n      {{- end }}\n      labels:\n        {{- include \"webapp.selectorLabels\" . | nindent 8 }}\n\n    spec:\n      containers:\n      - name: {{ .Chart.Name }}\n\n        {{- if or .Values.logging.enabled .Values.tracing.enabled }}\n        env:\n        {{- if .Values.logging.enabled }}\n        - name: LOG_LEVEL\n          value: {{ .Values.logging.level | quote }}\n        - name: LOG_FORMAT\n          value: {{ .Values.logging.format | quote }}\n        {{- end }}\n        {{- if and .Values.tracing.enabled .Values.tracing.jaeger.enabled }}\n        - name: JAEGER_AGENT_HOST\n          value: {{ .Values.tracing.jaeger.agent | quote }}\n        - name: JAEGER_AGENT_PORT\n          value: {{ .Values.tracing.jaeger.port | quote }}\n        {{- end }}\n        {{- end }}\n</code></pre>"},{"location":"02%20helm/question2/#validation_7","title":"Validation","text":"<pre><code># Enable monitoring\nhelm install my-app . --set monitoring.enabled=true --set monitoring.serviceMonitor.enabled=true\n\n# Verify ServiceMonitor\nkubectl get servicemonitor\nkubectl describe servicemonitor my-app-webapp\n\n# Enable logging\nhelm upgrade my-app . --set logging.enabled=true --set logging.level=DEBUG\n\n# Verify pod annotations\nkubectl get pods -o jsonpath='{.items[0].metadata.annotations}' | jq .\n\n# Check prometheus targets (if Prometheus is running)\nkubectl port-forward -n prometheus svc/prometheus 9090:9090\n# Visit http://localhost:9090/targets\n</code></pre>"},{"location":"02%20helm/question2/#concepts-introduced_7","title":"Concepts Introduced","text":"<ul> <li>ServiceMonitor for Prometheus</li> <li>PrometheusRule for alerting</li> <li>Monitoring annotations</li> <li>Logging configuration</li> <li>Tracing integration</li> <li>Custom Resource integration</li> </ul>"},{"location":"02%20helm/question2/#phase-9-lifecycle-hooks","title":"Phase 9: Lifecycle &amp; Hooks","text":"<p>Duration: 45 minutes | Difficulty: Advanced</p>"},{"location":"02%20helm/question2/#objective_8","title":"Objective","text":"<p>Control application lifecycle with pre/post hooks and tests.</p>"},{"location":"02%20helm/question2/#what-youll-learn_8","title":"What You'll Learn","text":"<ul> <li>Helm hooks (pre-install, post-install, etc.)</li> <li>Hook weights and deletion policies</li> <li>Helm tests</li> <li>Job templates</li> </ul>"},{"location":"02%20helm/question2/#requirements_8","title":"Requirements","text":"<p>Step 9.1 - Update values.yaml</p> <pre><code># ... previous values ...\n\nhooks:\n  preInstall:\n    enabled: false\n    image: busybox\n    command: [\"sh\", \"-c\", \"echo 'Pre-install checks'\"]\n\n  postInstall:\n    enabled: false\n    image: busybox\n    command: [\"sh\", \"-c\", \"echo 'Post-install setup'\"]\n\n  preUpgrade:\n    enabled: false\n    image: busybox\n    command: [\"sh\", \"-c\", \"echo 'Pre-upgrade validation'\"]\n\ntests:\n  enabled: false\n  image: busybox\n</code></pre> <p>Step 9.2 - Create templates/hooks/pre-install.yaml</p> <pre><code>{{- if and .Values.enabled .Values.hooks.preInstall.enabled }}\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: {{ include \"webapp.fullname\" . }}-pre-install\n  namespace: {{ .Release.Namespace }}\n  labels:\n    {{- include \"webapp.labels\" . | nindent 4 }}\n  annotations:\n    \"helm.sh/hook\": pre-install\n    \"helm.sh/hook-weight\": \"-5\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\nspec:\n  template:\n    metadata:\n      labels:\n        {{- include \"webapp.selectorLabels\" . | nindent 8 }}\n    spec:\n      serviceAccountName: {{ include \"webapp.fullname\" . }}\n      containers:\n      - name: pre-install\n        image: {{ .Values.hooks.preInstall.image }}\n        command: {{ .Values.hooks.preInstall.command | toJson }}\n      restartPolicy: Never\n  backoffLimit: 3\n{{- end }}\n</code></pre> <p>Step 9.3 - Create templates/tests/test-connection.yaml</p> <pre><code>{{- if and .Values.enabled .Values.tests.enabled }}\napiVersion: v1\nkind: Pod\nmetadata:\n  name: {{ include \"webapp.fullname\" . }}-test-connection\n  namespace: {{ .Release.Namespace }}\n  labels:\n    {{- include \"webapp.labels\" . | nindent 4 }}\n  annotations:\n    \"helm.sh/hook\": test\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\nspec:\n  containers:\n  - name: wget\n    image: {{ .Values.tests.image }}\n    command: ['wget']\n    args: ['{{ include \"webapp.fullname\" . }}:{{ .Values.servicePort }}']\n  restartPolicy: Never\n{{- end }}\n</code></pre>"},{"location":"02%20helm/question2/#validation_8","title":"Validation","text":"<pre><code># Enable hooks\nhelm install my-app . --set hooks.preInstall.enabled=true\n\n# Watch hook execution\nkubectl get jobs\nkubectl logs job/my-app-webapp-pre-install\n\n# Enable tests\nhelm install my-app . --set tests.enabled=true\n\n# Run tests\nhelm test my-app\n\n# Check test pod status\nkubectl get pods -l app.kubernetes.io/instance=my-app\n</code></pre>"},{"location":"02%20helm/question2/#concepts-introduced_8","title":"Concepts Introduced","text":"<ul> <li>Helm hooks (pre-install, post-install, pre-upgrade, etc.)</li> <li>Hook weights and execution order</li> <li>Hook deletion policies</li> <li>Helm test pods</li> <li>Job templates for hooks</li> </ul>"},{"location":"02%20helm/question2/#phase-10-production-readiness-best-practices","title":"Phase 10: Production Readiness &amp; Best Practices","text":"<p>Duration: 90 minutes | Difficulty: Advanced</p>"},{"location":"02%20helm/question2/#objective_9","title":"Objective","text":"<p>Implement complete production-grade features and best practices.</p>"},{"location":"02%20helm/question2/#what-youll-learn_9","title":"What You'll Learn","text":"<ul> <li>Chart validation and linting</li> <li>NOTES.txt documentation</li> <li>Chart values schema</li> <li>Security best practices</li> <li>Upgrade strategies</li> </ul>"},{"location":"02%20helm/question2/#requirements_9","title":"Requirements","text":"<p>Step 10.1 - Create Chart.yaml enhancements</p> <pre><code>apiVersion: v2\nname: webapp\ndescription: Production-Grade Web Application Helm Chart\ntype: application\nversion: 1.0.0\nappVersion: \"1.0\"\n\nkeywords:\n  - helm\n  - practice\n  - kubernetes\n  - production\n\nhome: https://github.com/example/webapp\nsources:\n  - https://github.com/example/webapp\n\nmaintainers:\n  - name: Your Name\n    email: your@email.com\n    url: https://github.com/yourname\n\ndependencies: []\n\nannotations:\n  category: Application\n  licenses: MIT\n</code></pre> <p>Step 10.2 - Create values.schema.json</p> <pre><code>{\n  \"$schema\": \"https://json-schema.org/draft-07/schema\",\n  \"type\": \"object\",\n  \"required\": [\"enabled\", \"replicaCount\", \"image\"],\n  \"properties\": {\n    \"enabled\": {\n      \"type\": \"boolean\",\n      \"description\": \"Enable or disable the chart\"\n    },\n    \"replicaCount\": {\n      \"type\": \"integer\",\n      \"minimum\": 1,\n      \"maximum\": 100,\n      \"description\": \"Number of replicas\"\n    },\n    \"image\": {\n      \"type\": \"string\",\n      \"pattern\": \"^[a-z0-9-]+:[a-zA-Z0-9.-]+$\",\n      \"description\": \"Docker image in format: name:tag\"\n    },\n    \"servicePort\": {\n      \"type\": \"integer\",\n      \"minimum\": 1,\n      \"maximum\": 65535,\n      \"description\": \"Service port number\"\n    },\n    \"resources\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"limits\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"cpu\": {\n              \"type\": \"string\",\n              \"pattern\": \"^[0-9]+m?$\"\n            },\n            \"memory\": {\n              \"type\": \"string\",\n              \"pattern\": \"^[0-9]+Mi$|^[0-9]+Gi$\"\n            }\n          }\n        }\n      }\n    },\n    \"autoscaling\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"enabled\": {\n          \"type\": \"boolean\"\n        },\n        \"minReplicas\": {\n          \"type\": \"integer\",\n          \"minimum\": 1\n        },\n        \"maxReplicas\": {\n          \"type\": \"integer\",\n          \"minimum\": 1\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>Step 10.3 - Create templates/NOTES.txt</p> <pre><code>1. Get the application URL by running these commands:\n{{- if .Values.ingress.enabled }}\n  {{- range .Values.ingress.hosts }}\n  {{- range .paths }}\n  http{{ if $.Values.ingress.tls }}s{{ end }}://{{ .host }}{{ .path }}\n  {{- end }}\n  {{- end }}\n{{- else if contains \"NodePort\" .Values.serviceType }}\n  export NODE_PORT=$(kubectl get --namespace {{ .Release.Namespace }} -o jsonpath=\"{.spec.ports[0].nodePort}\" services {{ include \"webapp.fullname\" . }})\n  export NODE_IP=$(kubectl get nodes --namespace {{ .Release.Namespace }} -o jsonpath=\"{.items[0].status.addresses[0].address}\")\n  echo http://$NODE_IP:$NODE_PORT\n{{- else if contains \"LoadBalancer\" .Values.serviceType }}\n  NOTE: It may take a few minutes for the LoadBalancer IP to be available.\n  You can watch the status of by running 'kubectl get svc -w {{ include \"webapp.fullname\" . }}'\n  export SERVICE_IP=$(kubectl get svc --namespace {{ .Release.Namespace }} {{ include \"webapp.fullname\" . }} --template \"{{\"{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}\"}}\")\n  echo http://$SERVICE_IP:{{ .Values.servicePort }}\n{{- else if eq .Values.serviceType \"ClusterIP\" }}\n  export POD_NAME=$(kubectl get pods --namespace {{ .Release.Namespace }} -l \"app.kubernetes.io/name={{ include \"webapp.name\" . }},app.kubernetes.io/instance={{ .Release.Name }}\" -o jsonpath=\"{.items[0].metadata.name}\")\n  export CONTAINER_PORT=$(kubectl get pod --namespace {{ .Release.Namespace }} $POD_NAME -o jsonpath=\"{.spec.containers[0].ports[0].containerPort}\")\n  echo \"Visit http://127.0.0.1:8080 to use your application\"\n  kubectl --namespace {{ .Release.Namespace }} port-forward $POD_NAME 8080:$CONTAINER_PORT\n{{- end }}\n\n2. Watch the deployment rollout status:\n  kubectl rollout status deployment/{{ include \"webapp.fullname\" . }} --namespace {{ .Release.Namespace }}\n\n3. Check the pod logs:\n  kubectl logs -f deployment/{{ include \"webapp.fullname\" . }} --namespace {{ .Release.Namespace }}\n\n4. Get more information:\n  kubectl get deployment --namespace {{ .Release.Namespace }}\n  kubectl get service --namespace {{ .Release.Namespace }}\n  helm status {{ .Release.Name }}\n\n{{- if .Values.autoscaling.enabled }}\n\n5. Autoscaling is enabled:\n  kubectl get hpa {{ include \"webapp.fullname\" . }} --namespace {{ .Release.Namespace }} -w\n{{- end }}\n\n{{- if .Values.ingress.enabled }}\n\n6. Ingress is enabled:\n  kubectl get ingress {{ include \"webapp.fullname\" . }} --namespace {{ .Release.Namespace }}\n{{- end }}\n\n{{- if .Values.persistence.enabled }}\n\n7. Persistence is enabled:\n  kubectl get pvc --namespace {{ .Release.Namespace }} -l app.kubernetes.io/instance={{ .Release.Name }}\n{{- end }}\n\nFor help and documentation, visit:\n  https://github.com/example/webapp\n</code></pre> <p>Step 10.4 - Update values.yaml with comprehensive defaults</p> <p>Ensure complete, documented values.yaml with:</p> <pre><code># Application Configuration\nappName: webapp\nenabled: true\nnameOverride: \"\"\nfullnameOverride: \"\"\n\n# Deployment Configuration\nreplicaCount: 2\nimage: busybox:latest\nimagePullPolicy: IfNotPresent\n\ncontainerPort: 8080\nservicePort: 8080\nserviceType: ClusterIP\n\ncommand:\n  - /bin/sh\n  - -c\n  - echo \"App started\" &amp;&amp; sleep 3600\n\n# ConfigMaps and Secrets\nconfigMaps:\n  app-config:\n    data:\n      LOG_LEVEL: \"INFO\"\n\nsecrets:\n  create: false\n  database:\n    username: admin\n    password: \"changeme\"\n\n# Service Account &amp; RBAC\nserviceAccount:\n  create: true\n  name: \"\"\n  annotations: {}\n\nrbac:\n  create: true\n\n# Ingress\ningress:\n  enabled: false\n  className: \"nginx\"\n  annotations: {}\n  hosts:\n    - host: webapp.example.com\n      paths:\n        - path: /\n          pathType: Prefix\n  tls: []\n\n# Storage\npersistence:\n  enabled: false\n  type: pvc\n  storageClassName: standard\n  accessMode: ReadWriteOnce\n  size: 5Gi\n  mountPath: /data\n  subPath: \"\"\n\nemptyDir:\n  enabled: false\n  sizeLimit: 1Gi\n\n# Resource Management\nresources:\n  limits:\n    cpu: 500m\n    memory: 512Mi\n  requests:\n    cpu: 250m\n    memory: 256Mi\n\n# Scaling\nautoscaling:\n  enabled: false\n  minReplicas: 2\n  maxReplicas: 10\n  targetCPUUtilizationPercentage: 80\n  targetMemoryUtilizationPercentage: 80\n\npodDisruptionBudget:\n  enabled: false\n  minAvailable: 1\n\n# Affinity\naffinity:\n  podAntiAffinity: soft\n  nodeAffinity: {}\n\ntopologySpreadConstraints:\n  enabled: false\n  maxSkew: 1\n  topologyKey: kubernetes.io/hostname\n\n# Monitoring\nmonitoring:\n  enabled: false\n  serviceMonitor:\n    enabled: false\n    interval: 30s\n    scrapeTimeout: 10s\n  prometheus:\n    rules:\n      enabled: false\n\n# Logging\nlogging:\n  enabled: false\n  level: INFO\n  format: json\n\n# Tracing\ntracing:\n  enabled: false\n  jaeger:\n    enabled: false\n    agent: jaeger-agent\n    port: 6831\n\n# Hooks\nhooks:\n  preInstall:\n    enabled: false\n    image: busybox\n    command: [\"echo\", \"Pre-install\"]\n  postInstall:\n    enabled: false\n    image: busybox\n    command: [\"echo\", \"Post-install\"]\n  preUpgrade:\n    enabled: false\n    image: busybox\n    command: [\"echo\", \"Pre-upgrade\"]\n\n# Tests\ntests:\n  enabled: false\n  image: busybox\n</code></pre> <p>Step 10.5 - Create README.md</p> <pre><code># Webapp Helm Chart\n\nProduction-ready Helm chart for deploying web applications on Kubernetes.\n\n## Prerequisites\n\n- Kubernetes 1.21+\n- Helm 3.0+\n\n## Installation\n\n```bash\nhelm repo add myrepo https://example.com/charts\nhelm repo update\nhelm install my-app myrepo/webapp\n</code></pre>"},{"location":"02%20helm/question2/#configuration","title":"Configuration","text":"<p>See values.yaml for all available options.</p> <p>Quick start:</p> <pre><code># Basic installation\nhelm install my-app .\n\n# With autoscaling\nhelm install my-app . --set autoscaling.enabled=true\n\n# With ingress\nhelm install my-app . --set ingress.enabled=true --set ingress.hosts[0].host=app.example.com\n\n# With persistence\nhelm install my-app . --set persistence.enabled=true\n</code></pre>"},{"location":"02%20helm/question2/#features","title":"Features","text":"<ul> <li>\u2705 ConfigMap and Secret management</li> <li>\u2705 ServiceAccount and RBAC</li> <li>\u2705 Horizontal Pod Autoscaler</li> <li>\u2705 Pod Disruption Budget</li> <li>\u2705 Ingress support</li> <li>\u2705 Persistent storage</li> <li>\u2705 Prometheus monitoring</li> <li>\u2705 Health checks (liveness, readiness, startup)</li> <li>\u2705 Helm tests</li> <li>\u2705 Lifecycle hooks</li> </ul>"},{"location":"02%20helm/question2/#upgrading","title":"Upgrading","text":"<pre><code>helm upgrade my-app . --values new-values.yaml\n</code></pre>"},{"location":"02%20helm/question2/#uninstalling","title":"Uninstalling","text":"<pre><code>helm uninstall my-app\n</code></pre>"},{"location":"02%20helm/question2/#validation_9","title":"Validation","text":"<pre><code># Lint the chart\nhelm lint .\n\n# Dry-run installation\nhelm install my-app . --dry-run --debug\n\n# Validate values\nhelm template my-app . --validate\n\n# Run tests\nhelm test my-app\n</code></pre>"},{"location":"02%20helm/question2/#license","title":"License","text":"<p>MIT <pre><code>### Validation\n\n```bash\n# Lint the chart\nhelm lint .\n\n# Validate against schema\nhelm template . --validate\n\n# Dry-run comprehensive install with all features\nhelm install my-app . \\\n  --set autoscaling.enabled=true \\\n  --set persistence.enabled=true \\\n  --set monitoring.enabled=true \\\n  --set ingress.enabled=true \\\n  --set rbac.create=true \\\n  --set secrets.create=true \\\n  --dry-run --debug\n\n# Real installation\nhelm install my-app .\n\n# Verify all resources\nkubectl get all -l app.kubernetes.io/instance=my-app\n\n# Run tests\nhelm test my-app\n\n# Check release info\nhelm history my-app\nhelm get values my-app\nhelm get manifest my-app\n</code></pre></p>"},{"location":"02%20helm/question2/#concepts-introduced_9","title":"Concepts Introduced","text":"<ul> <li>Chart linting and validation</li> <li>JSON schema validation</li> <li>NOTES.txt template</li> <li>README documentation</li> <li>Chart versioning strategy</li> <li>Production-grade defaults</li> <li>Comprehensive testing</li> </ul>"},{"location":"02%20helm/question2/#summary-mastery-checklist","title":"Summary &amp; Mastery Checklist","text":""},{"location":"02%20helm/question2/#what-youve-built","title":"What You've Built","text":"<p>A single, production-ready Helm chart that:</p> <p>\u2705 Starts from absolute basics (Phase 1) \u2705 Grows to include complex features (Phase 2-9) \u2705 Implements best practices (Phase 10) \u2705 Maintains backward compatibility throughout \u2705 Validates with <code>helm lint</code> and <code>helm template</code> \u2705 Includes comprehensive documentation \u2705 Supports real-world deployment patterns</p>"},{"location":"02%20helm/question2/#helm-concepts-mastered","title":"Helm Concepts Mastered","text":"<p>Templating: - <code>{{ }}</code> interpolation - <code>|</code> piping and filters - <code>if/else/end</code> conditionals - <code>range</code> iteration - Named templates (<code>define</code>) - Template helpers</p> <p>Functions: - <code>quote</code>, <code>nindent</code>, <code>toYaml</code>, <code>tojson</code> - <code>upper</code>, <code>lower</code>, <code>title</code>, <code>trimSuffix</code> - <code>default</code>, <code>required</code> - <code>include</code> for template reuse - <code>b64enc</code> for encoding - <code>trunc</code> for string truncation</p> <p>Advanced Patterns: - Conditional resources (feature flags) - Dynamic resource generation - Multi-tier templating with helpers - Scoping with <code>$</code> and <code>with</code> - Proper YAML indentation</p> <p>Kubernetes Objects: - Deployment, Service, ConfigMap, Secret - ServiceAccount, Role, RoleBinding - Ingress, PersistentVolumeClaim - HorizontalPodAutoscaler, PodDisruptionBudget - ServiceMonitor, PrometheusRule (CRDs) - Jobs (for hooks)</p> <p>Helm-Specific: - Chart metadata and versioning - Release management - Hooks (pre-install, post-install, etc.) - Tests - Values schema validation - Chart linting</p>"},{"location":"02%20helm/question2/#practice-goals","title":"Practice Goals","text":"<p>By completing all phases, you can:</p> <ol> <li>\u2705 Create charts from scratch</li> <li>\u2705 Use proper templating patterns</li> <li>\u2705 Implement feature flags and conditionals</li> <li>\u2705 Manage configuration and secrets safely</li> <li>\u2705 Scale applications with HPA</li> <li>\u2705 Implement health checks</li> <li>\u2705 Add persistence to applications</li> <li>\u2705 Integrate monitoring and observability</li> <li>\u2705 Control application lifecycle with hooks</li> <li>\u2705 Deploy production-grade applications</li> </ol>"},{"location":"02%20helm/question2/#testing-validation-throughout","title":"Testing &amp; Validation Throughout","text":"<p>At every phase, use:</p> <pre><code># Template rendering\nhelm template my-release .\n\n# Dry-run installation (no actual deployment)\nhelm install --dry-run --debug my-release .\n\n# Linting\nhelm lint .\n\n# Actual deployment\nhelm install my-release .\n\n# Verification\nkubectl get all -l app.kubernetes.io/instance=my-release\nkubectl describe deployment my-release-webapp\nkubectl logs deployment/my-release-webapp\n\n# Upgrades\nhelm upgrade my-release . --set replicaCount=5\n\n# Check history\nhelm history my-release\nhelm get values my-release\nhelm get manifest my-release\n</code></pre>"},{"location":"02%20helm/question2/#pro-tips-for-success","title":"Pro Tips for Success","text":"<ol> <li>Go slowly - Complete one phase before moving to the next</li> <li>Test constantly - Use <code>helm template</code> and <code>--dry-run</code> liberally</li> <li>Read error messages - They guide you to the problem</li> <li>Keep helpers organized - Use <code>_helpers.tpl</code> for all named templates</li> <li>Document your values - Add comments explaining each value</li> <li>Use meaningful names - Make resource names clear and descriptive</li> <li>Validate YAML - Use <code>yamllint</code> to check output</li> <li>Version your chart - Bump version when behavior changes</li> <li>Test upgrades - Ensure each phase works with upgrades</li> <li>Commit to git - Track all changes and understand evolution</li> </ol>"},{"location":"02%20helm/question2/#learning-path-summary","title":"Learning Path Summary","text":"<pre><code>Phase 1: Foundation (15 min)\n    \u2193\nPhase 2: Workloads (30 min)\n    \u2193\nPhase 3: Configuration (45 min)\n    \u2193\nPhase 4: Advanced Logic (60 min)\n    \u2193\nPhase 5: Secrets (45 min)\n    \u2193\nPhase 6: Storage (45 min)\n    \u2193\nPhase 7: Scaling (60 min)\n    \u2193\nPhase 8: Observability (60 min)\n    \u2193\nPhase 9: Lifecycle (45 min)\n    \u2193\nPhase 10: Production Ready (90 min)\n</code></pre> <p>Total Time: ~6 hours of hands-on learning</p> <p>Outcome: Complete mastery of Helm chart development for production applications.</p> <p>Good luck! \ud83d\ude80</p>"},{"location":"02%20helm/question3/","title":"Question3","text":""},{"location":"02%20helm/question3/#practice-question-3-build-a-scalable-web-application-chart-progressive-complexity","title":"Practice Question 3: Build a Scalable Web Application Chart - Progressive Complexity","text":"<p>Chart Name: <code>scalable-web-app</code></p> <p>Philosophy: One chart. One application. Progressively add features, making it production-ready step by step.</p>"},{"location":"02%20helm/question3/#phase-1-basic-deployment","title":"Phase 1: Basic Deployment","text":"<p>Objective: Create a simple Deployment with basic pod specifications.</p>"},{"location":"02%20helm/question3/#what-you-have","title":"What You Have","text":"<ul> <li>Application image: <code>scalable-web-app:latest</code></li> <li>Need to deploy to Kubernetes</li> </ul>"},{"location":"02%20helm/question3/#what-you-build","title":"What You Build","text":"<p>Create chart structure with: - <code>templates/deployment.yaml</code> - simple Deployment with 1 replica - <code>values.yaml</code> - image, tag, port configuration - <code>Chart.yaml</code> with metadata - <code>templates/service.yaml</code> - ClusterIP service for internal access</p>"},{"location":"02%20helm/question3/#requirements","title":"Requirements","text":"<ul> <li>Deployment creates 1 Pod running the application</li> <li>Service exposes the app internally</li> <li><code>helm template</code> produces valid YAML</li> <li><code>helm install</code> deploys successfully</li> </ul>"},{"location":"02%20helm/question3/#validation","title":"Validation","text":"<pre><code>helm template scalable-web-app . | kubectl apply -f -\nkubectl get deployment\nkubectl get pods\nkubectl port-forward svc/scalable-web-app 8080:8080\n# Test: curl http://localhost:8080\n</code></pre>"},{"location":"02%20helm/question3/#phase-2-configuration-management","title":"Phase 2: Configuration Management","text":"<p>Objective: Add ConfigMap for application configuration (not hardcoded).</p>"},{"location":"02%20helm/question3/#current-state","title":"Current State","text":"<ul> <li>Phase 1 is working (basic deployment)</li> </ul>"},{"location":"02%20helm/question3/#what-you-add","title":"What You Add","text":"<ul> <li><code>templates/configmap.yaml</code> - application configuration file</li> <li>Update <code>values.yaml</code> with <code>config</code> section containing app settings</li> <li>Update <code>templates/deployment.yaml</code> to mount the ConfigMap as a volume</li> <li>Create <code>_helpers.tpl</code> for reusable template snippets</li> </ul>"},{"location":"02%20helm/question3/#requirements_1","title":"Requirements","text":"<ul> <li>ConfigMap contains application configuration (key-value pairs)</li> <li>Deployment mounts ConfigMap as a volume at <code>/etc/config</code></li> <li>Application reads configuration from mounted file</li> <li>Configuration changes update ConfigMap without redeploying pod</li> <li>Helm values override default configuration</li> </ul>"},{"location":"02%20helm/question3/#validation_1","title":"Validation","text":"<pre><code># Check ConfigMap is created\nkubectl get configmap\n\n# Verify mount\nkubectl exec &lt;pod-name&gt; -- cat /etc/config\n\n# Update config and reinstall\nhelm upgrade scalable-web-app .\nkubectl get configmap -o yaml\n</code></pre>"},{"location":"02%20helm/question3/#phase-3-multi-environment-support","title":"Phase 3: Multi-Environment Support","text":"<p>Objective: Support different configurations for dev, staging, prod.</p>"},{"location":"02%20helm/question3/#current-state_1","title":"Current State","text":"<ul> <li>Phase 2 is working (configuration management)</li> </ul>"},{"location":"02%20helm/question3/#what-you-add_1","title":"What You Add","text":"<ul> <li>Create <code>values-dev.yaml</code>, <code>values-staging.yaml</code>, <code>values-prod.yaml</code></li> <li>Each file overrides resource limits, replicas, etc.</li> <li>Update <code>values.yaml</code> with environment-specific sections</li> <li>Add conditional logic in templates to validate correct environment selected</li> </ul>"},{"location":"02%20helm/question3/#requirements_2","title":"Requirements","text":"<ul> <li><code>helm install -f values-dev.yaml</code> deploys dev version (1 replica, small resources)</li> <li><code>helm install -f values-prod.yaml</code> deploys prod version (3+ replicas, large resources)</li> <li>Cannot accidentally mix configurations</li> <li>Different storage classes per environment</li> <li>Different ingress rules per environment</li> <li>Chart validates that only one environment is selected</li> </ul>"},{"location":"02%20helm/question3/#validation_2","title":"Validation","text":"<pre><code># Deploy to dev\nhelm install scalable-web-app . -f values-dev.yaml\nkubectl get deployment,configmap\n\n# Verify: 1 replica, small resources\nkubectl get deployment -o yaml | grep replicas\n\n# Deploy to prod (different release)\nhelm install scalable-web-app-prod . -f values-prod.yaml\n\n# Verify: 3+ replicas, large resources\nkubectl get deployment scalable-web-app-prod -o yaml | grep replicas\n</code></pre>"},{"location":"02%20helm/question3/#phase-4-scaling-performance-hpa-pdb","title":"Phase 4: Scaling &amp; Performance (HPA &amp; PDB)","text":"<p>Objective: Auto-scale based on metrics and protect disruptions.</p>"},{"location":"02%20helm/question3/#current-state_2","title":"Current State","text":"<ul> <li>Phase 3 is working (multi-environment support)</li> </ul>"},{"location":"02%20helm/question3/#what-you-add_2","title":"What You Add","text":"<ul> <li><code>templates/hpa.yaml</code> - HorizontalPodAutoscaler (scale based on CPU/memory)</li> <li><code>templates/pdb.yaml</code> - PodDisruptionBudget (maintain availability during node maintenance)</li> <li>Update <code>values.yaml</code> with autoscaling config (min/max replicas, metrics)</li> <li>Metrics Server must be installed in cluster for HPA to work</li> </ul>"},{"location":"02%20helm/question3/#requirements_3","title":"Requirements","text":"<ul> <li>HPA scales between min (2) and max (10) replicas based on CPU usage</li> <li>PDB maintains at least 1 Pod available always</li> <li>Pods have resource requests/limits defined (required for HPA)</li> <li>HPA triggers when CPU exceeds threshold</li> <li>Scaling down respects graceful termination period</li> </ul>"},{"location":"02%20helm/question3/#validation_3","title":"Validation","text":"<pre><code># Check HPA status\nkubectl get hpa\nkubectl describe hpa scalable-web-app\n\n# Check PDB\nkubectl get pdb\n\n# Simulate load (in separate terminal)\nkubectl run -it --rm load-generator --image=busybox -- /bin/sh\n# Inside pod: while sleep 0.01; do wget -q -O- http://scalable-web-app:8080; done\n\n# Watch scaling in progress\nkubectl get hpa -w\nkubectl get pods -w\n</code></pre>"},{"location":"02%20helm/question3/#phase-5-security-hardening","title":"Phase 5: Security Hardening","text":"<p>Objective: Apply security best practices (RBAC, SecurityContext, NetworkPolicy).</p>"},{"location":"02%20helm/question3/#current-state_3","title":"Current State","text":"<ul> <li>Phase 4 is working (HPA &amp; PDB)</li> </ul>"},{"location":"02%20helm/question3/#what-you-add_3","title":"What You Add","text":"<ul> <li><code>templates/serviceaccount.yaml</code> - ServiceAccount for the app</li> <li><code>templates/role.yaml</code> - RBAC Role with minimal permissions</li> <li><code>templates/rolebinding.yaml</code> - Bind role to ServiceAccount</li> <li>Update <code>templates/deployment.yaml</code> with SecurityContext:</li> <li>Non-root user (runAsUser: 1000)</li> <li>Read-only root filesystem</li> <li>No privileged container</li> <li>Drop unnecessary capabilities</li> <li><code>templates/networkpolicy.yaml</code> - Deny all ingress by default, allow from ingress controller only</li> </ul>"},{"location":"02%20helm/question3/#requirements_4","title":"Requirements","text":"<ul> <li>Pod runs as non-root user</li> <li>Pod cannot write to root filesystem</li> <li>Pod has minimal RBAC permissions (only needed APIs)</li> <li>NetworkPolicy blocks unauthorized traffic</li> <li>No secrets in environment variables (will add later)</li> <li>Container cannot escalate privileges</li> </ul>"},{"location":"02%20helm/question3/#validation_4","title":"Validation","text":"<pre><code># Verify SecurityContext\nkubectl get deployment -o yaml | grep -A 10 securityContext\n\n# Verify ServiceAccount\nkubectl get sa\n\n# Verify RBAC\nkubectl auth can-i list pods --as=system:serviceaccount:default:scalable-web-app\n\n# Verify NetworkPolicy\nkubectl get networkpolicy\nkubectl describe networkpolicy scalable-web-app\n\n# Test: Pod cannot execute as root\nkubectl exec &lt;pod-name&gt; -- id  # Should show uid=1000\n</code></pre>"},{"location":"02%20helm/question3/#phase-6-storage-persistence","title":"Phase 6: Storage &amp; Persistence","text":"<p>Objective: Add persistent data storage with PVC.</p>"},{"location":"02%20helm/question3/#current-state_4","title":"Current State","text":"<ul> <li>Phase 5 is working (security)</li> </ul>"},{"location":"02%20helm/question3/#what-you-add_4","title":"What You Add","text":"<ul> <li><code>templates/pvc.yaml</code> - PersistentVolumeClaim for application data</li> <li>Update <code>templates/deployment.yaml</code> to mount PVC as volume</li> <li>Update <code>values.yaml</code> with storage size and storage class name</li> <li>Add support for different storage classes per environment (dev: hostPath, prod: EBS/Azure Disk)</li> </ul>"},{"location":"02%20helm/question3/#requirements_5","title":"Requirements","text":"<ul> <li>PVC is created from appropriate storage class</li> <li>Deployment mounts PVC at <code>/data</code></li> <li>Application can write to <code>/data</code> persistently</li> <li>Pod restart doesn't lose data</li> <li>Storage size is configurable per environment</li> <li>StatefulSet (not Deployment) used if multiple replicas need separate PVCs</li> </ul>"},{"location":"02%20helm/question3/#validation_5","title":"Validation","text":"<pre><code># Check PVC\nkubectl get pvc\nkubectl describe pvc scalable-web-app-data\n\n# Verify mount\nkubectl exec &lt;pod-name&gt; -- ls -la /data\n\n# Write test file\nkubectl exec &lt;pod-name&gt; -- sh -c \"echo 'test' &gt; /data/test.txt\"\n\n# Delete pod and verify data persists\nkubectl delete pod &lt;pod-name&gt;\n# Wait for new pod\nkubectl exec &lt;new-pod-name&gt; -- cat /data/test.txt  # Should show \"test\"\n</code></pre>"},{"location":"02%20helm/question3/#phase-7-networking-exposure-service-ingress","title":"Phase 7: Networking &amp; Exposure (Service, Ingress)","text":"<p>Objective: Expose app to external traffic securely.</p>"},{"location":"02%20helm/question3/#current-state_5","title":"Current State","text":"<ul> <li>Phase 6 is working (storage)</li> </ul>"},{"location":"02%20helm/question3/#what-you-add_5","title":"What You Add","text":"<ul> <li>Update <code>templates/service.yaml</code> to type: LoadBalancer (or NodePort for dev)</li> <li><code>templates/ingress.yaml</code> - expose app via Ingress with:</li> <li>TLS/HTTPS configuration (self-signed cert for dev, real cert for prod)</li> <li>Path-based routing</li> <li>Host-based routing (environment-specific domains)</li> <li>Authentication/authorization (basic auth for dev, OAuth for prod)</li> <li>Update <code>values.yaml</code> with ingress domain, TLS cert config</li> </ul>"},{"location":"02%20helm/question3/#requirements_6","title":"Requirements","text":"<ul> <li>Service exposes pod on port 8080</li> <li>Ingress exposes app on domain (ingress-class: nginx)</li> <li>HTTPS/TLS works with proper certificate</li> <li>Traffic routing works to correct pod</li> <li>Different ingress configs for dev vs prod (TLS required only in prod)</li> </ul>"},{"location":"02%20helm/question3/#validation_6","title":"Validation","text":"<pre><code># Check Service\nkubectl get svc\nkubectl describe svc scalable-web-app\n\n# Check Ingress\nkubectl get ingress\nkubectl describe ingress scalable-web-app\n\n# Test routing (if minikube/local cluster)\ncurl http://scalable-web-app.example.com  # Should route to pod\ncurl -k https://scalable-web-app.example.com  # Should work with TLS\n\n# Verify certificate\nkubectl get ingress -o yaml | grep cert\n</code></pre>"},{"location":"02%20helm/question3/#phase-8-observability-prometheus-logging","title":"Phase 8: Observability (Prometheus &amp; Logging)","text":"<p>Objective: Collect metrics and logs for monitoring.</p>"},{"location":"02%20helm/question3/#current-state_6","title":"Current State","text":"<ul> <li>Phase 7 is working (networking)</li> </ul>"},{"location":"02%20helm/question3/#what-you-add_6","title":"What You Add","text":"<ul> <li><code>templates/servicemonitor.yaml</code> - Prometheus ServiceMonitor for metrics scraping</li> <li>Update <code>templates/deployment.yaml</code> to expose <code>/metrics</code> endpoint (port 9090)</li> <li>Structured logging configuration (JSON format logs)</li> <li>Update <code>values.yaml</code> with observability config (enable metrics, log level)</li> <li><code>templates/prometheusrule.yaml</code> - Alert rules for SLA violations</li> <li>Dashboard definition (as ConfigMap) for Grafana</li> </ul>"},{"location":"02%20helm/question3/#requirements_7","title":"Requirements","text":"<ul> <li>Pod exposes Prometheus metrics on <code>/metrics</code> port 9090</li> <li>ServiceMonitor tells Prometheus where to scrape metrics</li> <li>Application logs are in JSON format (structured)</li> <li>Logs include request ID for tracing</li> <li>Alert rules fire when:</li> <li>Pod restarts frequently (&gt;3 restarts/hour)</li> <li>Error rate exceeds 5%</li> <li>Response time exceeds SLA (e.g., 500ms p99)</li> <li>Metrics show: request count, latency, errors, resource usage</li> </ul>"},{"location":"02%20helm/question3/#validation_7","title":"Validation","text":"<pre><code># Check ServiceMonitor\nkubectl get servicemonitor\n\n# Verify metrics endpoint\nkubectl port-forward &lt;pod-name&gt; 9090:9090\ncurl http://localhost:9090/metrics | grep requests_total\n\n# Check logs\nkubectl logs &lt;pod-name&gt;  # Should be JSON formatted\n\n# If Prometheus running, check scrape config\nkubectl exec -it prometheus-pod -c prometheus -- cat /etc/prometheus/prometheus.yml\n\n# Check alerts are defined\nkubectl get prometheusrule\nkubectl describe prometheusrule scalable-web-app\n</code></pre>"},{"location":"02%20helm/question3/#phase-9-deployment-strategies","title":"Phase 9: Deployment Strategies","text":"<p>Objective: Safely roll out updates with zero downtime.</p>"},{"location":"02%20helm/question3/#current-state_7","title":"Current State","text":"<ul> <li>Phase 8 is working (observability)</li> </ul>"},{"location":"02%20helm/question3/#what-you-add_7","title":"What You Add","text":"<ul> <li>Multiple deployment strategies as values option:</li> <li>Rolling Update (default): gradual pod replacement with maxSurge/maxUnavailable</li> <li>Blue-Green: two deployments, instant traffic switch</li> <li>Canary: gradual traffic shift to new version (requires ServiceMesh or Ingress rules)</li> <li>Add pre-upgrade job to backup data</li> <li>Add post-upgrade job to verify deployment health</li> <li>Update <code>values.yaml</code> to select strategy</li> </ul>"},{"location":"02%20helm/question3/#requirements_8","title":"Requirements","text":"<ul> <li>Rolling update: maxUnavailable=1, maxSurge=1 (always 1-2 pods running)</li> <li>Blue-green: create both v1 and v2 deployments, switch service selector</li> <li>Canary: gradually route traffic (10% \u2192 50% \u2192 100%) to new version</li> <li>Old pods gracefully shutdown (preStop hook, 30s termination grace)</li> <li>Automatic rollback if health checks fail</li> <li>Zero-downtime upgrades (no traffic loss)</li> </ul>"},{"location":"02%20helm/question3/#validation_8","title":"Validation","text":"<pre><code># Simulate deployment update\nhelm upgrade scalable-web-app . --set image.tag=v2\n\n# Watch rolling update\nkubectl rollout status deployment/scalable-web-app\nkubectl get pods -w\n\n# Verify zero downtime (in separate terminal, hit endpoint)\nwhile true; do curl http://scalable-web-app:8080; sleep 1; done\n# Should see continuous responses, no connection errors\n\n# Rollback if needed\nhelm rollback scalable-web-app\n\n# Blue-green test\nhelm install scalable-web-app . --set deploymentStrategy=blue-green --set image.tag=v1\nhelm upgrade scalable-web-app . --set image.tag=v2\n# Verify both deployments exist, traffic switches to v2\n</code></pre>"},{"location":"02%20helm/question3/#phase-10-production-readiness","title":"Phase 10: Production Readiness","text":"<p>Objective: Validate chart quality, documentation, and operational readiness.</p>"},{"location":"02%20helm/question3/#current-state_8","title":"Current State","text":"<ul> <li>Phase 9 is working (deployment strategies)</li> </ul>"},{"location":"02%20helm/question3/#what-you-add_8","title":"What You Add","text":"<ul> <li><code>values.schema.json</code> - JSON Schema for values validation</li> <li><code>NOTES.txt</code> - deployment instructions and next steps</li> <li><code>README.md</code> - comprehensive documentation</li> <li>Architecture overview</li> <li>Installation instructions</li> <li>Configuration reference</li> <li>Troubleshooting guide</li> <li>Performance tuning recommendations</li> <li>Chart tests in <code>templates/tests/</code> directory</li> <li>Verify pod is running</li> <li>Verify service is accessible</li> <li>Verify ingress works</li> <li>Verify configmap was created</li> <li>Verify metrics endpoint responds</li> <li>Update Chart.yaml with proper metadata (description, keywords, maintainers)</li> </ul>"},{"location":"02%20helm/question3/#requirements_9","title":"Requirements","text":"<ul> <li><code>helm lint</code> passes without errors/warnings</li> <li><code>helm template</code> produces valid YAML for all environments</li> <li><code>helm install --dry-run --debug</code> succeeds</li> <li>Chart values validate against schema (invalid values rejected)</li> <li>All 10 phases are working together cohesively</li> <li>Documentation answers: What? How? Why? When to use?</li> <li>Tests verify critical functionality automatically</li> <li>Chart follows Helm best practices</li> </ul>"},{"location":"02%20helm/question3/#validation_9","title":"Validation","text":"<pre><code># Lint chart\nhelm lint .\n\n# Validate against schema\nhelm template . | kubectl apply -f - --dry-run=client\n\n# Run tests\nhelm test scalable-web-app\n\n# Verify all resources created\nkubectl get all\nkubectl get configmap,pvc,networkpolicy,ingress,servicemonitor\n\n# Check NOTES output\nhelm install scalable-web-app .\n# Should print helpful next steps\n\n# Verify documentation\n# README.md should be comprehensive and clear\n# values.schema.json should validate all settings\n\n# Final check: can delete and redeploy cleanly\nhelm uninstall scalable-web-app\nhelm install scalable-web-app .\n</code></pre>"},{"location":"02%20helm/question3/#progression-summary","title":"Progression Summary","text":"Phase Focus Key Concepts 1 Basics Deployment, Service, templates 2 Config ConfigMap, volumes, helper functions 3 Environments values files, conditionals, validation 4 Scaling HPA, PDB, resource requests/limits 5 Security RBAC, SecurityContext, NetworkPolicy 6 Storage PVC, persistent data, StatefulSet 7 Networking Ingress, TLS, routing 8 Observability Prometheus, ServiceMonitor, alerts 9 Deployments Rolling, Blue-Green, Canary strategies 10 Production Schema validation, tests, documentation"},{"location":"02%20helm/question3/#what-youre-building","title":"What You're Building","text":"<p>By end of Phase 10, you have:</p> <ul> <li>\u2705 Chart that deploys a complete application</li> <li>\u2705 Multi-environment support (dev/staging/prod)</li> <li>\u2705 Auto-scaling based on metrics</li> <li>\u2705 High availability (PDB maintains availability)</li> <li>\u2705 Persistent storage for application data</li> <li>\u2705 HTTPS/TLS secured external access</li> <li>\u2705 Security hardening (RBAC, SecurityContext, NetworkPolicy)</li> <li>\u2705 Full observability (metrics, alerts, logs)</li> <li>\u2705 Safe deployment strategies (zero downtime)</li> <li>\u2705 Production-quality documentation and validation</li> </ul> <p>Total Implementation Time: 25-35 hours</p>"},{"location":"02%20helm/question3/#key-helm-concepts-youll-learn","title":"Key Helm Concepts You'll Learn","text":"<ol> <li>Chart Structure - metadata, templates, values hierarchy</li> <li>Templating - conditionals, loops, range, variables, functions</li> <li>Helper Functions - DRY principle, _helpers.tpl, named templates</li> <li>Values Management - defaults, overrides, precedence, schema validation</li> <li>Reusability - composing complex configs from simple building blocks</li> <li>Kubernetes Patterns - RBAC, NetworkPolicy, Deployments, StatefulSets</li> <li>Observability - metrics, logging, alerting at chart level</li> <li>Deployment Strategies - safe rollouts, health checks, rollbacks</li> <li>Best Practices - documentation, testing, linting, security</li> <li>Advanced Templating - computed values, dynamic resource generation</li> </ol>"},{"location":"02%20helm/question3/#notes","title":"Notes","text":"<ul> <li>Each phase depends on previous phases</li> <li>You'll iterate and refine as you discover edge cases</li> <li>Real-world constraints matter (actual cluster limitations, quotas)</li> <li>Documentation is as important as code</li> <li>Chart should be idempotent (safe to run multiple times)</li> </ul> <p>Good luck! Build incrementally, test thoroughly, and focus on one feature at a time.</p>"},{"location":"02%20helm/question4/","title":"Question4","text":""},{"location":"02%20helm/question4/#meaningful-question-4-build-an-enterprise-helm-chart-library-real-helm-complexity","title":"Meaningful Question 4: Build an Enterprise Helm Chart Library - Real Helm Complexity","text":"<p>What You'll Build: A reusable Helm chart library that powers your entire organization. 50+ teams use it to deploy their applications without touching Kubernetes manifests.</p> <p>Chart Name: <code>app-platform</code> (base chart library)</p> <p>Philosophy: This is about Helm, not Kubernetes. You're solving real templating, composition, and reusability challenges.</p>"},{"location":"02%20helm/question4/#the-business-context","title":"The Business Context","text":"<p>Your organization has 50+ teams deploying applications to Kubernetes:</p> <p>Current State (The Problem): - Each team writes their own manifests (Deployment, Service, Ingress, etc.) - Inconsistent patterns across teams (no standards) - Security varies wildly (some pods run as root, some don't) - Upgrades are manual and error-prone - Different teams solve the same problem 50 different ways - Onboarding new teams takes weeks</p> <p>Your Goal: - Build ONE Helm chart that 50+ teams can use - Teams only write <code>values.yaml</code>, no template changes - Standardized patterns (security, monitoring, networking) - Self-service deployments - Automatic compliance checking</p>"},{"location":"02%20helm/question4/#the-challenge-extreme-helm-flexibility","title":"The Challenge: Extreme Helm Flexibility","text":"<p>Your chart must support:</p> Use Case Example Helm Challenge Stateless Web App Node.js API Basic Deployment Stateful Database PostgreSQL StatefulSet with PVC Batch Job Data processor CronJob, Job cleanup Worker Queue RabbitMQ consumer Deployment + custom config External Service Third-party API Service without pods Microservices 10 services in 1 chart Dependencies, shared values API Gateway Kong, Traefik Multiple replicas, plugins Lambda-like Function runner Pod per invocation Cache Layer Redis Optional, conditional Message Queue Kafka, RabbitMQ Optional, conditional <p>Problem: How do you make ONE chart flexible enough for all these, but simple enough that teams only specify values?</p>"},{"location":"02%20helm/question4/#part-1-chart-architecture-flexibility","title":"Part 1: Chart Architecture &amp; Flexibility","text":""},{"location":"02%20helm/question4/#the-design-challenge","title":"The Design Challenge","text":"<p>You need to support different workload types with minimal configuration:</p> <pre><code># Team 1: Simple web app\nworkloadType: deployment\nreplicas: 3\nimage: myapp:1.0\n\n# Team 2: Stateful database\nworkloadType: statefulset\nreplicas: 3\npersistence:\n  enabled: true\n  size: 100Gi\n\n# Team 3: Scheduled job\nworkloadType: cronjob\nschedule: \"0 2 * * *\"\n\n# Team 4: Multiple services in one chart\nservices:\n  api:\n    workloadType: deployment\n    replicas: 3\n  worker:\n    workloadType: deployment\n    replicas: 2\n  cache:\n    workloadType: deployment\n    replicas: 1\n</code></pre>"},{"location":"02%20helm/question4/#requirements","title":"Requirements","text":"<ol> <li>Template Structure</li> <li>Use conditional logic to include only needed manifests</li> <li>DRY principle: reuse pod specs across workload types</li> <li>Support 5+ workload types (Deployment, StatefulSet, DaemonSet, Job, CronJob)</li> <li> <p>Shared template helpers for common patterns</p> </li> <li> <p>Workload Types</p> </li> <li><code>deployment</code>: Stateless app (replicas, rolling update)</li> <li><code>statefulset</code>: Stateful app (stable identity, persistent storage)</li> <li><code>daemonset</code>: Node-local app (one per node)</li> <li><code>job</code>: Run once, complete</li> <li><code>cronjob</code>: Run on schedule</li> <li> <p><code>external</code>: No pods (external service proxy)</p> </li> <li> <p>Flexible Configuration</p> </li> <li>One values.yaml controls everything</li> <li>Support single service or multiple services</li> <li>Optional components (cache, database, monitoring)</li> <li> <p>Different security levels per component</p> </li> <li> <p>Validation</p> </li> <li>Invalid workloadType combinations fail validation</li> <li>Missing required values fail early</li> <li>StatefulSet must have persistence enabled</li> <li>CronJob must have valid cron schedule</li> </ol>"},{"location":"02%20helm/question4/#validation-criteria","title":"Validation Criteria","text":"<pre><code># Deploy web app (deployment)\nhelm install web-app . -f values-web.yaml\nkubectl get deployment\n\n# Deploy database (statefulset)\nhelm install database . -f values-statefulset.yaml\nkubectl get statefulset\n\n# Deploy cronjob\nhelm install scheduler . -f values-cronjob.yaml\nkubectl get cronjob\n\n# Deploy multi-service\nhelm install platform . -f values-multi.yaml\nkubectl get deployment,statefulset\n# Should show api deployment, worker deployment, cache deployment\n\n# Validation test: invalid workloadType\nhelm template . -f values-invalid.yaml 2&gt;&amp;1 | grep -i error\n# Should fail with clear error message\n</code></pre>"},{"location":"02%20helm/question4/#part-2-values-schema-auto-documentation","title":"Part 2: Values Schema &amp; Auto-Documentation","text":""},{"location":"02%20helm/question4/#the-problem","title":"The Problem","text":"<p>Teams ask: - \"What values can I set?\" - \"What are the defaults?\" - \"Which values are required?\" - \"Can I use image tags or only repo URLs?\"</p> <p>You need to answer without manually documenting 200+ values.</p>"},{"location":"02%20helm/question4/#requirements_1","title":"Requirements","text":"<ol> <li>JSON Schema (values.schema.json)</li> <li>Complete schema for all valid values</li> <li>Type checking (string, number, boolean, array, object)</li> <li>Required vs optional fields</li> <li>Enum validation (e.g., workloadType must be one of: deployment, statefulset, ...)</li> <li>Min/max validation (replicas: min 1, max 1000)</li> <li>Pattern validation (image: must match regex)</li> <li> <p>Descriptions for each value</p> </li> <li> <p>Auto-Generated Documentation</p> </li> <li>README generated from schema</li> <li>Example values.yaml files for common use cases</li> <li> <p>Inline comments in schema explaining each field</p> </li> <li> <p>Validation</p> </li> <li><code>helm template</code> fails if invalid values provided</li> <li>Clear error messages: \"replicas must be &gt;= 1, got -5\"</li> <li>Suggest fixes: \"workloadType must be one of: [deployment, statefulset, job, cronjob, daemonset]\"</li> </ol>"},{"location":"02%20helm/question4/#validation-criteria_1","title":"Validation Criteria","text":"<pre><code># Valid values should work\nhelm template . -f valid-values.yaml &gt; /dev/null\necho $?  # Should be 0\n\n# Invalid values should fail with clear error\nhelm template . -f invalid-values.yaml 2&gt;&amp;1\n# Should show: \"Error: replicas must be &gt;= 1, got 0\"\n# Should show: \"Error: workloadType must be one of: [deployment, statefulset, ...]\"\n\n# Schema should be complete\ncat values.schema.json | jq '.properties | keys | length'\n# Should be 30+ properties\n\n# Documentation should be auto-generated\ncat README.md | grep -c \"##\"\n# Should have multiple sections auto-generated\n</code></pre>"},{"location":"02%20helm/question4/#part-3-reusable-template-blocks-dry-principle","title":"Part 3: Reusable Template Blocks &amp; DRY Principle","text":""},{"location":"02%20helm/question4/#the-problem_1","title":"The Problem","text":"<p>You have 20+ manifest templates. Many are similar:</p> <pre><code># deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: {{ .Values.name }}\n  labels:\n    app: {{ .Values.name }}\n    version: {{ .Values.version }}\nspec:\n  replicas: {{ .Values.replicas }}\n  selector:\n    matchLabels:\n      app: {{ .Values.name }}\n  template:\n    metadata:\n      labels:\n        app: {{ .Values.name }}\n    spec:\n      containers:\n      - name: app\n        image: {{ .Values.image }}\n        ports:\n        - containerPort: {{ .Values.port }}\n\n# statefulset.yaml\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: {{ .Values.name }}\n  labels:\n    app: {{ .Values.name }}\n    version: {{ .Values.version }}\nspec:\n  replicas: {{ .Values.replicas }}\n  selector:\n    matchLabels:\n      app: {{ .Values.name }}\n  template:\n    metadata:\n      labels:\n        app: {{ .Values.name }}\n    spec:\n      containers:\n      - name: app\n        image: {{ .Values.image }}\n        ports:\n        - containerPort: {{ .Values.port }}\n</code></pre> <p>Problem: Labels, selectors, container specs are duplicated. Change one place, forgot 5 others.</p>"},{"location":"02%20helm/question4/#requirements_2","title":"Requirements","text":"<ol> <li>Template Helpers (_helpers.tpl)</li> <li>Reusable blocks for common patterns</li> <li>Labels helper: generate consistent labels</li> <li>Selectors helper: generate consistent selectors</li> <li>Container spec helper: generate container template</li> <li>Health checks helper: generate probes</li> <li>Security context helper: generate security settings</li> <li> <p>Environment variables helper: handle secrets vs configmaps</p> </li> <li> <p>DRY Principle</p> </li> <li>Pod spec defined once, reused in Deployment, StatefulSet, Job</li> <li>Labels defined once, used everywhere</li> <li>Probes defined once, applied to all workloads</li> <li> <p>Security context defined once, applied everywhere</p> </li> <li> <p>Consistency</p> </li> <li>All pods get same security standards</li> <li>All pods have same labels</li> <li>All pods have same monitoring sidecars</li> <li>Changing one helper updates everywhere</li> </ol>"},{"location":"02%20helm/question4/#validation-criteria_2","title":"Validation Criteria","text":"<pre><code># Generate manifests\nhelm template . -f values.yaml &gt; manifests.yaml\n\n# Check: All labels are consistent\ngrep -c \"app: myapp\" manifests.yaml\n# Should be 10+ (pod spec, deployment, service, etc.)\n\n# Check: Selector matches labels\ngrep -A 2 \"matchLabels:\" manifests.yaml\n# Should show \"app: myapp\" matching pod labels\n\n# Check: Security context applied\ngrep -c \"runAsNonRoot\" manifests.yaml\n# Should be 4+ (one per pod)\n\n# Verify DRY: Change label in helper\n# Rerun template, all manifests should update\n\n# Count lines of code\nwc -l templates/*.yaml templates/_helpers.tpl\n# _helpers.tpl should be significant (200+), reused across 5+ templates\n</code></pre>"},{"location":"02%20helm/question4/#part-4-conditional-rendering-feature-toggles","title":"Part 4: Conditional Rendering &amp; Feature Toggles","text":""},{"location":"02%20helm/question4/#the-problem_2","title":"The Problem","text":"<p>Teams ask: - \"Can I disable monitoring for dev environment?\" - \"Can I skip database backups for testing?\" - \"Can I disable security policies for local development?\"</p> <p>You need sophisticated conditional logic.</p>"},{"location":"02%20helm/question4/#requirements_3","title":"Requirements","text":"<ol> <li>Feature Toggles</li> <li><code>monitoring.enabled</code>: Include ServiceMonitor, PrometheusRule</li> <li><code>ingress.enabled</code>: Include Ingress, adjust Service type</li> <li><code>persistence.enabled</code>: Include PVC, use emptyDir if not</li> <li><code>rbac.enabled</code>: Include ServiceAccount, Role, RoleBinding</li> <li><code>networkPolicy.enabled</code>: Include NetworkPolicy</li> <li> <p><code>backup.enabled</code>: Include backup CronJob</p> </li> <li> <p>Conditional Dependencies</p> </li> <li>If <code>persistence.enabled</code>, must have <code>persistence.storageClass</code></li> <li>If <code>monitoring.enabled</code>, must have <code>monitoring.scrapeInterval</code></li> <li>If <code>ingress.enabled</code>, must have <code>ingress.host</code></li> <li> <p>If <code>rbac.enabled</code>, can't use <code>runAsUser: 0</code></p> </li> <li> <p>Environment-Specific Configs</p> </li> <li>Development: monitoring disabled, no ingress, local storage</li> <li>Staging: monitoring enabled, ingress + TLS, persistent storage</li> <li>Production: monitoring required, security hardened, HA</li> </ol>"},{"location":"02%20helm/question4/#validation-criteria_3","title":"Validation Criteria","text":"<pre><code># Deploy with minimal features (dev)\nhelm template . -f values-dev.yaml | kubectl apply -f - --dry-run=client\n# Should work, only pod+service\n\n# Deploy with all features (prod)\nhelm template . -f values-prod.yaml | kubectl apply -f - --dry-run=client\n# Should include monitoring, ingress, security, persistence\n\n# Verify conditional logic\nhelm template . -f values-monitoring-disabled.yaml | grep -i servicemonitor\n# Should return nothing\n\nhelm template . -f values-monitoring-enabled.yaml | grep -i servicemonitor\n# Should show ServiceMonitor resource\n\n# Verify dependency validation\nhelm template . -f values-persistence-enabled-but-no-storageclass.yaml 2&gt;&amp;1\n# Should fail with clear error\n</code></pre>"},{"location":"02%20helm/question4/#part-5-multi-environment-values-inheritance","title":"Part 5: Multi-Environment &amp; Values Inheritance","text":""},{"location":"02%20helm/question4/#the-problem_3","title":"The Problem","text":"<p>Teams deploy same app to dev, staging, production with different configs:</p> <pre><code># All similar structure, different values\nreplicas:\n  dev: 1\n  staging: 2\n  prod: 3\n\nresources:\n  dev: {cpu: 100m, memory: 128Mi}\n  staging: {cpu: 500m, memory: 512Mi}\n  prod: {cpu: 1000m, memory: 2Gi}\n\ningress:\n  dev: {tls: false, domain: dev.example.com}\n  staging: {tls: true, domain: staging.example.com}\n  prod: {tls: true, domain: app.example.com}\n</code></pre> <p>Problem: How do you manage all this without duplicating values 3 times?</p>"},{"location":"02%20helm/question4/#requirements_4","title":"Requirements","text":"<ol> <li>Values Hierarchy</li> <li><code>values.yaml</code>: defaults (works for most cases)</li> <li><code>values-dev.yaml</code>: override for dev (small replicas, low resources)</li> <li><code>values-staging.yaml</code>: override for staging (medium replicas, medium resources)</li> <li> <p><code>values-prod.yaml</code>: override for prod (high replicas, high resources, security hardened)</p> </li> <li> <p>Smart Defaults</p> </li> <li>Default values.yaml is functional for dev</li> <li>Staging only needs to override replicas and resources</li> <li>Prod only needs to override security and monitoring settings</li> <li> <p>No value specified in 3 places unnecessarily</p> </li> <li> <p>Values Composition</p> </li> <li>Parent chart passes values to child charts correctly</li> <li>Global values accessible to all children</li> <li>Component-specific values only affect that component</li> </ol>"},{"location":"02%20helm/question4/#validation-criteria_4","title":"Validation Criteria","text":"<pre><code># Dev deployment (minimal)\nhelm template . -f values-dev.yaml | grep replicas:\n# Should show replicas: 1\n\n# Staging deployment\nhelm template . -f values-staging.yaml | grep replicas:\n# Should show replicas: 2\n\n# Prod deployment\nhelm template . -f values-prod.yaml | grep replicas:\n# Should show replicas: 3\n\n# Values files should be small (no duplication)\nwc -l values.yaml values-*.yaml\n# values.yaml should be 100+ lines, each override should be 20-30 lines only\n</code></pre>"},{"location":"02%20helm/question4/#part-6-parent-charts-multi-service-deployments","title":"Part 6: Parent Charts &amp; Multi-Service Deployments","text":""},{"location":"02%20helm/question4/#the-problem_4","title":"The Problem","text":"<p>One team wants to deploy: - API server - Worker  - Cache - Database</p> <p>All as one unit, with shared configuration.</p>"},{"location":"02%20helm/question4/#requirements_5","title":"Requirements","text":"<ol> <li>Parent Chart (Umbrella)</li> <li>Includes child charts as dependencies</li> <li>Each child is a service (api, worker, cache, database)</li> <li>Shared values passed to relevant children</li> <li> <p>Version constraints on child charts</p> </li> <li> <p>Child Charts</p> </li> <li>Each service is a reusable chart</li> <li>Can be deployed standalone or as part of parent</li> <li>Accepts shared values (labels, monitoring config, security)</li> <li> <p>Can override specific settings</p> </li> <li> <p>Dependency Management</p> </li> <li><code>api</code> depends on <code>database</code> (waits for startup)</li> <li><code>worker</code> depends on <code>cache</code></li> <li>Ordering in templates ensures dependencies start first</li> <li>Health checks verify dependencies are ready</li> </ol>"},{"location":"02%20helm/question4/#validation-criteria_5","title":"Validation Criteria","text":"<pre><code># Deploy parent chart\nhelm install platform . -f values-multi.yaml\n\n# Verify all services deployed\nkubectl get deployment\n# Should show: api, worker\n\nkubectl get statefulset\n# Should show: cache, database\n\n# Verify dependency ordering\nkubectl get events | grep Created\n# Database created first, then api\n\n# Verify shared values applied\nkubectl get deployment -o yaml | grep -i monitoring\n# All deployments should have monitoring enabled\n</code></pre>"},{"location":"02%20helm/question4/#part-7-helm-hooks-lifecycle-events","title":"Part 7: Helm Hooks &amp; Lifecycle Events","text":""},{"location":"02%20helm/question4/#the-problem_5","title":"The Problem","text":"<p>Teams need to run tasks at specific lifecycle points:</p> <ul> <li>Pre-install: Create namespace, apply CRDs</li> <li>Post-install: Run migrations, seed data</li> <li>Pre-upgrade: Backup database</li> <li>Post-upgrade: Verify app is healthy</li> <li>Pre-delete: Export data, cleanup</li> <li>Post-delete: Remove PVCs, cleanup resources</li> </ul>"},{"location":"02%20helm/question4/#requirements_6","title":"Requirements","text":"<ol> <li>Pre-Install Hooks</li> <li>Install CRDs (must run before any resources reference them)</li> <li>Create namespaces with labels</li> <li>Apply RBAC cluster-wide (if needed)</li> <li> <p>Validate cluster prerequisites</p> </li> <li> <p>Post-Install Hooks</p> </li> <li>Run database migrations</li> <li>Seed initial data</li> <li>Run smoke tests</li> <li> <p>Generate initial config</p> </li> <li> <p>Pre-Upgrade Hooks</p> </li> <li>Backup database to S3</li> <li>Run pre-upgrade validation</li> <li>Check for breaking changes</li> <li> <p>Warn if downtime required</p> </li> <li> <p>Post-Upgrade Hooks</p> </li> <li>Run schema migrations</li> <li>Validate data integrity</li> <li>Run smoke tests</li> <li> <p>Notify team of upgrade completion</p> </li> <li> <p>Pre-Delete Hooks</p> </li> <li>Export data to S3</li> <li>Run cleanup tasks</li> <li>Collect logs for archival</li> </ol>"},{"location":"02%20helm/question4/#validation-criteria_6","title":"Validation Criteria","text":"<pre><code># Install and watch hooks run\nhelm install app . --debug\n# Should see: pre-install hook running, post-install hook running\n\n# Upgrade and watch hooks\nhelm upgrade app . --set version=2 --debug\n# Should see: pre-upgrade hook (backup), post-upgrade hook (migrate)\n\n# Verify hook ordering\nkubectl get events | grep hook\n# Should show: pre-install, post-install, pre-upgrade, post-upgrade in order\n\n# Verify hooks only run once\nhelm upgrade app . --debug\n# Should not re-run post-install hook\n# Should run pre-upgrade and post-upgrade hooks\n</code></pre>"},{"location":"02%20helm/question4/#part-8-values-validation-error-handling","title":"Part 8: Values Validation &amp; Error Handling","text":""},{"location":"02%20helm/question4/#the-problem_6","title":"The Problem","text":"<p>Teams sometimes provide invalid values:</p> <pre><code>replicas: -5  # negative!\nimage: \"\"  # empty!\nport: 99999  # invalid!\nworkloadType: lambda  # not supported!\n</code></pre> <p>Result: Chart installs but deployment fails mysteriously.</p>"},{"location":"02%20helm/question4/#requirements_7","title":"Requirements","text":"<ol> <li>Schema Validation</li> <li><code>replicas</code>: integer, &gt;= 1, &lt;= 100</li> <li><code>image</code>: string, required, matches regex <code>^[a-z0-9:/.]+$</code></li> <li><code>port</code>: integer, &gt;= 1, &lt;= 65535</li> <li><code>workloadType</code>: enum [deployment, statefulset, job, cronjob, daemonset]</li> <li> <p><code>resources.requests.cpu</code>: string, matches Kubernetes CPU regex</p> </li> <li> <p>Smart Error Messages</p> </li> <li>Show what went wrong</li> <li>Show what should be fixed</li> <li> <p>Suggest valid options</p> </li> <li> <p>Optional Pre-Deployment Validation</p> </li> <li>Custom template validation logic</li> <li>Check interdependencies</li> <li>Fail early with clear errors</li> </ol>"},{"location":"02%20helm/question4/#validation-criteria_7","title":"Validation Criteria","text":"<pre><code># Invalid values fail immediately\nhelm template . -f values-invalid.yaml 2&gt;&amp;1\n# Should show: \"Error: replicas must be &gt;= 1 and &lt;= 100, got -5\"\n# Should show: \"Did you mean: replicas: 1?\"\n\n# Missing required values fail\nhelm template . -f values-incomplete.yaml 2&gt;&amp;1\n# Should show: \"Error: image is required, got empty string\"\n\n# Invalid workloadType fails\nhelm template . --set workloadType=lambda 2&gt;&amp;1\n# Should show: \"Error: workloadType must be one of: [deployment, statefulset, job, ...]\"\n#             \"Got: lambda\"\n\n# Invalid port fails\nhelm template . --set port=99999 2&gt;&amp;1\n# Should show: \"Error: port must be between 1 and 65535, got 99999\"\n\n# Schema validation\nhelm template . -f values.yaml | kubectl apply -f - --dry-run=client\n# Should succeed\n</code></pre>"},{"location":"02%20helm/question4/#part-9-testing-validation","title":"Part 9: Testing &amp; Validation","text":""},{"location":"02%20helm/question4/#the-problem_7","title":"The Problem","text":"<p>How do you verify 50+ teams' deployments are correct?</p> <p>Teams might deploy: - Without required probes - Without resource limits - With security vulnerabilities - With incorrect monitoring</p>"},{"location":"02%20helm/question4/#requirements_8","title":"Requirements","text":"<ol> <li>Helm Chart Tests</li> <li>Test: Pod is running</li> <li>Test: Service is accessible</li> <li>Test: Health checks respond</li> <li>Test: Metrics are exported</li> <li>Test: Security policies enforced</li> <li> <p>Test: Ingress routing works</p> </li> <li> <p>Policy as Code</p> </li> <li>No root containers</li> <li>No privileged mode</li> <li>Resources limits required</li> <li>Health checks required</li> <li>Labels present on all resources</li> <li> <p>ServiceAccount bound to restricted role</p> </li> <li> <p>Pre-Install Validation</p> </li> <li>Check cluster prerequisites</li> <li>Check available storage classes</li> <li>Check RBAC permissions</li> <li>Check API server version compatibility</li> </ol>"},{"location":"02%20helm/question4/#validation-criteria_8","title":"Validation Criteria","text":"<pre><code># Run helm tests\nhelm test app\n\n# Tests should verify:\n\u2713 Pod is running\n\u2713 Service is accessible\n\u2713 Health checks respond\n\u2713 Metrics endpoint works\n\u2713 Security context applied\n\u2713 Resource limits set\n\n# Policy validation\nhelm template . | kubesec scan -\n# Should pass security checks\n\nhelm template . | kubeval -\n# Should produce valid Kubernetes manifests\n\nhelm lint .\n# Should pass without errors\n</code></pre>"},{"location":"02%20helm/question4/#part-10-documentation-user-experience","title":"Part 10: Documentation &amp; User Experience","text":""},{"location":"02%20helm/question4/#the-problem_8","title":"The Problem","text":"<p>50+ teams using your chart need to understand: - How to use it - What values are available - Examples for common scenarios - Troubleshooting help - Best practices</p>"},{"location":"02%20helm/question4/#requirements_9","title":"Requirements","text":"<ol> <li>README.md</li> <li>Quick start (3-step deployment)</li> <li>Values reference (all 100+ values documented)</li> <li>Examples (5+ common use cases)</li> <li>Troubleshooting (10+ common issues)</li> <li>Architecture diagram</li> <li> <p>Performance tuning guide</p> </li> <li> <p>Example Values Files</p> </li> <li><code>values-web-app.yaml</code>: Simple web app</li> <li><code>values-stateful-app.yaml</code>: App with database</li> <li><code>values-microservices.yaml</code>: Multiple services</li> <li><code>values-dev.yaml</code>: Development environment</li> <li> <p><code>values-prod.yaml</code>: Production environment</p> </li> <li> <p>Inline Documentation</p> </li> <li>Comments in values.yaml explaining each field</li> <li>Comments in templates explaining complex logic</li> <li> <p>Comments in _helpers.tpl explaining each function</p> </li> <li> <p>Generated Documentation</p> </li> <li>Values schema auto-generates reference docs</li> <li>Examples auto-generated from schema</li> <li>Architecture rendered from code</li> </ol>"},{"location":"02%20helm/question4/#validation-criteria_9","title":"Validation Criteria","text":"<pre><code># README quality checks\nwc -l README.md  # Should be 500+ lines\ngrep -c \"##\" README.md  # Should have 10+ sections\ngrep -c \"example\" README.md  # Should have examples\n\n# Values are documented\ngrep \"^# \" values.yaml | wc -l  # Should have 50+ comment lines\n\n# Examples are valid\nfor f in values-*.yaml; do\n  helm template . -f $f &gt; /dev/null || echo \"Invalid: $f\"\ndone\n# All should succeed\n\n# Schema is complete\ncat values.schema.json | jq '.properties | keys | length'\n# Should be 30+ documented fields\n</code></pre>"},{"location":"02%20helm/question4/#what-youre-building-summary","title":"What You're Building Summary","text":"<p>By completing all 10 parts, you have:</p> <ul> <li>\u2705 Flexible chart supporting 5+ workload types</li> <li>\u2705 Complete schema validation with auto-generated docs</li> <li>\u2705 DRY templates with maximum reusability</li> <li>\u2705 Feature toggles for optional components</li> <li>\u2705 Multi-environment support with value inheritance</li> <li>\u2705 Parent/child charts for multi-service deployments</li> <li>\u2705 Lifecycle hooks for pre/post install/upgrade/delete</li> <li>\u2705 Comprehensive validation and error handling</li> <li>\u2705 Testing and policy enforcement</li> <li>\u2705 Complete user documentation</li> </ul> <p>This is a library. 50+ teams depend on it. Teams deploy new services in 5 minutes using just values.yaml.</p> <p>Total Implementation Time: 35-50 hours</p>"},{"location":"02%20helm/question4/#key-real-world-helm-concepts","title":"Key Real-World Helm Concepts","text":"<ol> <li>Extreme Flexibility - Supporting diverse use cases with one template</li> <li>Helm Best Practices - Schema, hooks, testing, documentation</li> <li>Template Reusability - Helpers, DRY principle, avoiding duplication</li> <li>Composition Patterns - Parent charts, child charts, dependency management</li> <li>Conditional Rendering - Feature toggles, environment-specific configs</li> <li>Validation &amp; Safety - Schema, pre-install checks, policy enforcement</li> <li>User Experience - Documentation, error messages, examples</li> <li>Testing &amp; Verification - Helm tests, policy as code, integration tests</li> <li>Lifecycle Management - Hooks for install, upgrade, delete</li> <li>Enterprise Scale - Supporting 50+ teams with standardized patterns</li> </ol>"},{"location":"02%20helm/question4/#progression-approach","title":"Progression Approach","text":"<p>Start with: Part 1 (basic flexibility with workload types) - Then add: Part 2 (schema and documentation) - Then add: Part 3 (DRY templates with helpers) - Then add: Part 4 (feature toggles and conditionals) - Then add: Part 5 (multi-environment support) - Then add: Part 6 (parent/child charts) - Then add: Part 7 (lifecycle hooks) - Then add: Part 8 (validation) - Then add: Part 9 (testing) - Finally: Part 10 (documentation)</p> <p>Each part builds on previous ones. Don't skip steps.</p>"},{"location":"02%20helm/question4/#notes-for-success","title":"Notes for Success","text":"<ul> <li>This is Helm, not Kubernetes architecture</li> <li>Focus on templating, composition, and reusability</li> <li>Real organizations have this exact problem</li> <li>Real constraints: 50+ teams, diverse use cases, zero documentation errors</li> <li>Real testing: comprehensive validation, no surprises</li> <li>Real documentation: teams should understand without asking</li> </ul> <p>This chart will be used by hundreds of deployments. Quality matters.</p> <p>Good luck. Build something that empowers your organization. \ud83d\ude80</p>"},{"location":"02%20helm/question4/#what-youll-manage","title":"What You'll Manage","text":"<ol> <li>Cluster Provisioning</li> <li>Define cluster spec (size, region, version, node types)</li> <li>Automatically provision infrastructure (IaC)</li> <li>Install CNI, CSI, ingress controller</li> <li> <p>Bootstrap cluster with base applications</p> </li> <li> <p>Multi-Cloud Support</p> </li> <li>AWS (EKS) templates</li> <li>Azure (AKS) templates</li> <li>GCP (GKE) templates</li> <li> <p>Same chart, different cloud backend</p> </li> <li> <p>Add-Ons Management</p> </li> <li>Networking (Cilium, Calico, Flannel)</li> <li>Storage (Local, EBS, Azure Disk, GCP Disk)</li> <li>Ingress (NGINX, Traefik, AWS ALB)</li> <li>Monitoring (Prometheus, Datadog, New Relic)</li> <li>Logging (Loki, ELK, Splunk)</li> <li> <p>Service Mesh (Istio, Linkerd, optional)</p> </li> <li> <p>Cluster Lifecycle</p> </li> <li>Provision new cluster</li> <li>Upgrade cluster version</li> <li>Scale cluster up/down</li> <li>Decommission cluster</li> <li>Backup cluster state</li> <li> <p>Restore from backup</p> </li> <li> <p>Multi-Tenancy</p> </li> <li>Multiple teams on one cluster</li> <li>Resource quotas and isolation</li> <li>Network policies between teams</li> <li> <p>RBAC scoped by team</p> </li> <li> <p>Compliance &amp; Governance</p> </li> <li>Enforce security policies</li> <li>Audit all changes</li> <li>Policy validation (no unencrypted storage, no privileged pods, etc.)</li> <li> <p>Cost tracking per team</p> </li> <li> <p>GitOps Workflow</p> </li> <li>All changes described in Git</li> <li>Automatic sync via Flux/ArgoCD</li> <li>PR-based approval workflow</li> <li> <p>Audit trail of who changed what</p> </li> <li> <p>Self-Healing</p> </li> <li>Automatic recovery from node failures</li> <li>Replace failed nodes</li> <li>Rebalance pods when needed</li> <li>Alert on degradation</li> </ol>"},{"location":"02%20helm/question4/#architecture-youll-design","title":"Architecture You'll Design","text":"<pre><code>Helm Charts Repo (Git)\n\u251c\u2500\u2500 clusters/\n\u2502   \u251c\u2500\u2500 values-aws-prod-us-east-1.yaml\n\u2502   \u251c\u2500\u2500 values-aws-staging-us-west-2.yaml\n\u2502   \u251c\u2500\u2500 values-azure-prod-eastus.yaml\n\u2502   \u2514\u2500\u2500 values-gcp-dev-us-central1.yaml\n\u251c\u2500\u2500 base-chart/\n\u2502   \u251c\u2500\u2500 Chart.yaml\n\u2502   \u251c\u2500\u2500 values.yaml\n\u2502   \u2514\u2500\u2500 templates/\n\u2502       \u251c\u2500\u2500 infrastructure/\n\u2502       \u251c\u2500\u2500 cluster-addons/\n\u2502       \u251c\u2500\u2500 networking/\n\u2502       \u251c\u2500\u2500 storage/\n\u2502       \u251c\u2500\u2500 monitoring/\n\u2502       \u2514\u2500\u2500 governance/\n\u2514\u2500\u2500 README.md\n</code></pre>"},{"location":"02%20helm/question4/#your-challenges","title":"Your Challenges","text":"<ol> <li>Multi-Cloud Abstraction</li> <li>Write once, deploy to AWS/Azure/GCP</li> <li>Handle cloud-specific resources</li> <li> <p>Manage secret injection per cloud</p> </li> <li> <p>Complex Dependencies</p> </li> <li>CNI must be installed before workloads</li> <li>Storage CSI before PVCs</li> <li>Ingress after API server healthy</li> <li>Monitoring after clusters ready</li> <li> <p>How do you order this?</p> </li> <li> <p>Cluster Size Variations</p> </li> <li>Dev: 1 control plane, 2 workers</li> <li>Staging: 2 control planes, 5 workers</li> <li>Prod: 3 control planes, 20+ workers</li> <li> <p>Different resource classes</p> </li> <li> <p>Regional Considerations</p> </li> <li>Multi-region failover</li> <li>Data locality compliance</li> <li>Network latency optimization</li> <li> <p>Different pricing per region</p> </li> <li> <p>Upgrade Strategy</p> </li> <li>Upgrade Kubernetes version without downtime</li> <li>Upgrade add-ons without disruption</li> <li>Handle breaking changes in dependencies</li> <li> <p>Rollback capability</p> </li> <li> <p>Cost Management</p> </li> <li>Right-size clusters by workload</li> <li>Use spot instances where possible</li> <li>Shutdown dev clusters after hours</li> <li> <p>Track spending per team</p> </li> <li> <p>Observability Across Clusters</p> </li> <li>Centralized monitoring (see all clusters)</li> <li>Logs aggregated from all clusters</li> <li>Alerts fired from central monitoring</li> <li> <p>But don't expose one cluster's data to another team</p> </li> <li> <p>Security at Scale</p> </li> <li>Enforce Pod Security Standards</li> <li>Network policies prevent cluster escape</li> <li>RBAC prevents privilege escalation</li> <li>Audit logging immutable</li> <li>Secret management across regions</li> </ol>"},{"location":"02%20helm/question4/#success-criteria","title":"Success Criteria","text":"<ul> <li>New cluster provisioned from values.yaml without manual steps</li> <li>Cluster supports 3+ sizes (dev, staging, prod)</li> <li>Works across AWS, Azure, GCP</li> <li>Multi-tenancy with strong isolation</li> <li>Upgrades work without downtime</li> <li>Cost dashboard shows spending by team</li> <li>All changes tracked in Git</li> <li>Can provision/destroy cluster in &lt;1 hour</li> <li>Monitoring shows health across all clusters</li> <li>Security policies enforced automatically</li> </ul>"},{"location":"02%20helm/question4/#estimated-complexity","title":"Estimated Complexity","text":"<ul> <li>20+ resource types to manage</li> <li>3 cloud providers</li> <li>Multiple add-on combinations</li> <li>Complex dependency ordering</li> <li>Multi-tenancy isolation</li> <li>Observability across clusters</li> <li>Time: 25-35 hours</li> </ul>"},{"location":"02%20helm/question4/#project-3-cicd-pipeline-as-helm-chart","title":"Project 3: CI/CD Pipeline as Helm Chart","text":"<p>What You'll Build: A complete CI/CD platform (equivalent to Jenkins + GitLab CI + Argo Workflows) as a Helm chart.</p>"},{"location":"02%20helm/question4/#the-scenario","title":"The Scenario","text":"<p>Your organization's CI/CD setup is complex: - Multiple teams use different tools (Jenkins, GitLab CI, GitHub Actions) - Pipeline configs are scattered across repos - No standard way to define pipelines - Security is ad-hoc (secrets leaked in logs) - Integration between tools is manual</p> <p>You're building a unified CI/CD platform where: - Pipelines are defined in Git (PipelineResource CRDs) - Execution is automatic (Tekton Pipelines) - Security is built-in (secrets, RBAC, audit) - Observability is complete (logs, metrics, traces) - Teams can self-service without DevOps</p>"},{"location":"02%20helm/question4/#what-youll-build","title":"What You'll Build","text":"<ol> <li>Pipeline Execution Engine</li> <li>Tekton Pipelines (or similar)</li> <li>Define pipelines as code (YAML)</li> <li>Run multiple pipeline versions simultaneously</li> <li>Support for parallel stages</li> <li> <p>Conditional stage execution</p> </li> <li> <p>Code Repository Integration</p> </li> <li>GitHub webhooks trigger pipelines</li> <li>GitLab push \u2192 pipeline runs</li> <li>GitOps: changes in Git automatically apply to cluster</li> <li> <p>Pull request checks (CI gates)</p> </li> <li> <p>Build Orchestration</p> </li> <li>Docker image builds</li> <li>Artifact storage (Nexus, Artifactory)</li> <li>Container scanning (Trivy, Snyk)</li> <li>Image signing and verification</li> <li> <p>Multi-platform builds (ARM, x86)</p> </li> <li> <p>Testing Automation</p> </li> <li>Unit tests in CI</li> <li>Integration tests</li> <li>Smoke tests on staged version</li> <li>Performance tests</li> <li> <p>Security scanning (SAST, DAST)</p> </li> <li> <p>Deployment Pipeline</p> </li> <li>Deploy to dev after merge</li> <li>Deploy to staging after tag</li> <li>Canary to prod (5% traffic)</li> <li>Full rollout after validation</li> <li> <p>Automated rollback on failure</p> </li> <li> <p>Artifact Management</p> </li> <li>Build artifacts versioned</li> <li>Container images with tags</li> <li>Helm charts pushed to repo</li> <li>SBOM (software bill of materials)</li> <li> <p>Provenance tracking</p> </li> <li> <p>Security &amp; Compliance</p> </li> <li>No secrets in logs</li> <li>RBAC on pipeline execution</li> <li>Audit trail of all deployments</li> <li>Signed artifacts</li> <li> <p>Compliance checks</p> </li> <li> <p>Notifications &amp; Feedback</p> </li> <li>Slack on pipeline success/failure</li> <li>Email to team leads</li> <li>Dashboard showing pipeline status</li> <li>Metrics on success rate, speed</li> <li> <p>Alerts on broken builds</p> </li> <li> <p>Pipeline Templates</p> </li> <li>Standard Node.js pipeline</li> <li>Standard Java/Maven pipeline</li> <li>Standard Python pipeline</li> <li>Standard Go pipeline</li> <li> <p>Custom pipeline for special cases</p> </li> <li> <p>Resource Management</p> <ul> <li>Pipeline pods don't consume resources</li> <li>Auto-scaling for heavy pipelines</li> <li>Spot instances for build nodes</li> <li>Cache builds to speed up</li> </ul> </li> </ol>"},{"location":"02%20helm/question4/#your-challenges_1","title":"Your Challenges","text":"<ol> <li>Pipeline as Code</li> <li>How do you define pipelines declaratively?</li> <li>How do you support 10+ languages?</li> <li>How do you allow customization?</li> <li> <p>How do you validate pipeline configs?</p> </li> <li> <p>Secure Secrets</p> </li> <li>Secrets for GitHub, Docker, databases</li> <li>Secrets rotated automatically</li> <li>Secrets never logged</li> <li>Different secrets per environment</li> <li> <p>How do you inject them safely?</p> </li> <li> <p>Parallelization</p> </li> <li>Multiple tests run in parallel</li> <li>Multiple image builds in parallel</li> <li>Coordinate between stages</li> <li> <p>Manage resource contention</p> </li> <li> <p>Artifact Traceability</p> </li> <li>Track what code produced which artifact</li> <li>Track which artifact deployed where</li> <li>Rollback by artifact version</li> <li> <p>Reproducible builds</p> </li> <li> <p>Performance Optimization</p> </li> <li>Cache dependencies (maven, npm, pip)</li> <li>Cache Docker layers</li> <li>Parallel test execution</li> <li>Reuse build output</li> <li> <p>Target only affected services</p> </li> <li> <p>Multi-Version Testing</p> </li> <li>Test against multiple Node versions</li> <li>Test against multiple Python versions</li> <li>Test against multiple Java versions</li> <li> <p>Test on multiple OS (Linux, Windows)</p> </li> <li> <p>Deployment Strategies</p> </li> <li>Blue-green for zero-downtime</li> <li>Canary with automatic rollback</li> <li>Rolling update with health checks</li> <li>Shadow traffic testing</li> <li> <p>Dark launch capability</p> </li> <li> <p>Monitoring Deployments</p> </li> <li>Is new version healthy?</li> <li>Are error rates increasing?</li> <li>Are latencies acceptable?</li> <li>Is resource usage normal?</li> <li>Automatic rollback if degradation</li> </ol>"},{"location":"02%20helm/question4/#architecture-youll-design_1","title":"Architecture You'll Design","text":"<pre><code>Git Repo Structure:\n\u251c\u2500\u2500 .pipelines/\n\u2502   \u251c\u2500\u2500 nodejs.yaml      # Reusable pipeline template\n\u2502   \u251c\u2500\u2500 python.yaml\n\u2502   \u251c\u2500\u2500 java.yaml\n\u2502   \u2514\u2500\u2500 deploy.yaml      # Deployment pipeline\n\u251c\u2500\u2500 src/\n\u2514\u2500\u2500 helm-chart/          # Application Helm chart\n\nPipeline Execution:\nCode Push \u2192 GitHub Webhook \u2192 Tekton EventListener\n  \u2193\nCreate PipelineRun \u2192 Execute Tasks in Parallel\n  \u251c\u2500\u2500 Build Docker Image\n  \u251c\u2500\u2500 Run Unit Tests\n  \u251c\u2500\u2500 Run Integration Tests\n  \u2514\u2500\u2500 Security Scan\n  \u2193\nPush Image to Registry (if tests pass)\n  \u2193\nTrigger Deployment Pipeline\n  \u251c\u2500\u2500 Deploy to Dev\n  \u251c\u2500\u2500 Deploy to Staging\n  \u2514\u2500\u2500 Deploy to Prod (canary)\n  \u2193\nMonitor for Issues \u2192 Rollback if Needed\n</code></pre>"},{"location":"02%20helm/question4/#success-criteria_1","title":"Success Criteria","text":"<ul> <li>Define pipeline in Git, runs automatically on push</li> <li>Multiple stages execute in parallel</li> <li>Secure secret handling (never logged)</li> <li>Build artifacts traced to code</li> <li>Deploy with zero downtime</li> <li>Automatic rollback on failure</li> <li>All actions audited and logged</li> <li>Performance metrics tracked</li> <li>Scaling works for heavy builds</li> <li>Team can define custom pipelines</li> </ul>"},{"location":"02%20helm/question4/#estimated-complexity_1","title":"Estimated Complexity","text":"<ul> <li>Tekton + EventListeners</li> <li>Multiple task types</li> <li>Conditional execution</li> <li>Artifact management</li> <li>Secret handling</li> <li>Deployment orchestration</li> <li>Monitoring and observability</li> <li>Time: 20-25 hours</li> </ul>"},{"location":"02%20helm/question4/#project-4-machine-learning-operations-mlops-platform","title":"Project 4: Machine Learning Operations (MLOps) Platform","text":"<p>What You'll Build: A Helm-based platform for training, deploying, and monitoring ML models.</p>"},{"location":"02%20helm/question4/#the-scenario_1","title":"The Scenario","text":"<p>Your company has data scientists building ML models but: - Model training is manual (scripts on laptops) - Deployment to production is ad-hoc - No A/B testing framework - No retraining pipeline - No model monitoring - Different versions conflict</p> <p>You're building MLOps: a platform where scientists describe experiments in code, and Helm manages training jobs, model deployment, and monitoring.</p>"},{"location":"02%20helm/question4/#what-youll-build_1","title":"What You'll Build","text":"<ol> <li>Experiment Management</li> <li>Define training job parameters</li> <li>Track hyperparameters used</li> <li>Compare results across runs</li> <li> <p>Reproducible experiments</p> </li> <li> <p>Training Infrastructure</p> </li> <li>GPU/TPU provisioning</li> <li>Distributed training (multiple GPUs)</li> <li>Fault tolerance (resume from checkpoint)</li> <li>Notebook environments for exploration</li> <li> <p>Job scheduling and queuing</p> </li> <li> <p>Model Registry</p> </li> <li>Store trained models</li> <li>Version all models</li> <li>Track model lineage (which data, code)</li> <li>Quality metrics (accuracy, precision, recall)</li> <li> <p>Approval workflow for production</p> </li> <li> <p>Model Serving</p> </li> <li>Deploy models as HTTP endpoints</li> <li>Multiple model versions simultaneously</li> <li>A/B testing between versions</li> <li>Traffic shifting (canary deployments)</li> <li>Model scaling based on load</li> <li> <p>Model monitoring and alerts</p> </li> <li> <p>Feature Engineering</p> </li> <li>Shared feature store</li> <li>Consistent features in training and serving</li> <li>Feature versioning</li> <li>Feature pipeline (compute features)</li> <li> <p>Cache for performance</p> </li> <li> <p>Data Pipeline</p> </li> <li>Fetch data for training</li> <li>Data validation (detect drift)</li> <li>Data labeling workflow</li> <li>Data versioning</li> <li> <p>Privacy-preserving (redaction, anonymization)</p> </li> <li> <p>Monitoring &amp; Governance</p> </li> <li>Track model performance over time</li> <li>Detect data drift (input distribution changed)</li> <li>Detect model drift (predictions changing)</li> <li>Alert when retraining needed</li> <li>Explainability and model interpretability</li> <li> <p>Audit predictions for bias</p> </li> <li> <p>AutoML &amp; Tuning</p> </li> <li>Hyperparameter optimization</li> <li>Neural Architecture Search (optional)</li> <li>Model selection automation</li> <li>Ensemble methods</li> </ol>"},{"location":"02%20helm/question4/#your-challenges_2","title":"Your Challenges","text":"<ol> <li>Resource Management</li> <li>GPUs are expensive and shared</li> <li>How do you schedule training jobs fairly?</li> <li>How do you prevent one team from monopolizing resources?</li> <li> <p>How do you autoscale GPU nodes?</p> </li> <li> <p>Experiment Reproducibility</p> </li> <li>Same seed, same code \u2192 same results</li> <li>Track all dependencies (library versions)</li> <li>Containerize everything</li> <li> <p>Tag images immutably</p> </li> <li> <p>Model Versioning</p> </li> <li>Multiple models in production simultaneously</li> <li>Different model versions for different requests</li> <li>Gradual traffic shift from old to new</li> <li> <p>Quick rollback if new model fails</p> </li> <li> <p>Data Handling</p> </li> <li>Large datasets (TB scale)</li> <li>Efficient storage and access</li> <li>Privacy compliance (GDPR, HIPAA)</li> <li> <p>Data retention policies</p> </li> <li> <p>Model Serving Performance</p> </li> <li>Sub-100ms inference latency</li> <li>Handle traffic spikes</li> <li>GPU sharing between models</li> <li> <p>CPU inference when GPU not needed</p> </li> <li> <p>Monitoring Model Quality</p> </li> <li>Accuracy stays above threshold</li> <li>Latency stays below SLO</li> <li>Data drift detection</li> <li>Fairness/bias monitoring</li> <li> <p>Automatic retraining</p> </li> <li> <p>Collaboration</p> </li> <li>Scientists define experiments in notebooks</li> <li>Experiments become production pipelines</li> <li>Track who changed what</li> <li>Reproducible results</li> </ol>"},{"location":"02%20helm/question4/#architecture-youll-design_2","title":"Architecture You'll Design","text":"<pre><code>Training Phase:\nGit Push (new model code)\n  \u2193\nTrigger Training Job\n  \u251c\u2500\u2500 Pull training data\n  \u251c\u2500\u2500 Run hyperparameter tuning (parallel)\n  \u251c\u2500\u2500 Select best model\n  \u251c\u2500\u2500 Validate on test set\n  \u2514\u2500\u2500 Push to model registry\n\nServing Phase:\nModel Registry Push\n  \u2193\nApproval Gate (data scientist reviews)\n  \u2193\nDeploy to Staging\n  \u251c\u2500\u2500 Serve model\n  \u251c\u2500\u2500 Run smoke tests\n  \u2514\u2500\u2500 Collect metrics\n  \u2193\nDeploy to Production (canary, 5% traffic)\n  \u251c\u2500\u2500 Monitor error rates\n  \u251c\u2500\u2500 Monitor latency\n  \u251c\u2500\u2500 Monitor fairness metrics\n  \u2514\u2500\u2500 After 24h, shift to 100% (or rollback)\n\nMonitoring Phase:\nContinuous Monitoring\n  \u251c\u2500\u2500 Data drift detection\n  \u251c\u2500\u2500 Model drift detection\n  \u251c\u2500\u2500 Performance degradation\n  \u2514\u2500\u2500 Alert if retraining needed\n  \u2193\nTrigger Retraining Pipeline\n</code></pre>"},{"location":"02%20helm/question4/#success-criteria_2","title":"Success Criteria","text":"<ul> <li>Scientist defines experiment, training runs automatically</li> <li>Multiple models in production simultaneously</li> <li>Canary deployment for new models</li> <li>Automatic rollback if model performance drops</li> <li>Data drift detection triggers retraining</li> <li>All experiments reproducible</li> <li>Model lineage tracked (code, data, hyperparameters)</li> <li>Serving latency &lt; 100ms</li> <li>GPU resources scheduled fairly</li> <li>Full audit trail</li> </ul>"},{"location":"02%20helm/question4/#estimated-complexity_2","title":"Estimated Complexity","text":"<ul> <li>Model training orchestration</li> <li>Model serving (KServe or Seldon)</li> <li>Feature store integration</li> <li>Data pipeline management</li> <li>Monitoring and governance</li> <li>Resource management (GPUs)</li> <li>Time: 25-30 hours</li> </ul>"},{"location":"02%20helm/question4/#project-5-complete-observability-aiops-platform","title":"Project 5: Complete Observability &amp; AIOps Platform","text":"<p>What You'll Build: An end-to-end observability platform with intelligent alerting and automatic remediation.</p>"},{"location":"02%20helm/question4/#the-scenario_2","title":"The Scenario","text":"<p>Your organization has: - Metrics from Prometheus (500+ servers) - Logs from ELK (petabytes/day) - Traces from Jaeger (millions/minute) - Hundreds of alert rules</p> <p>But: - Alert noise (70% false positives) - Slow incident response (manual investigation) - No correlation between metrics/logs/traces - Difficult root cause analysis - Manual remediation steps</p> <p>You're building AIOps: AI-powered observability with: - Intelligent alerting (correlation, deduplication) - Automatic root cause analysis - Self-healing (auto-remediation) - Predictive alerts (issue before it happens)</p>"},{"location":"02%20helm/question4/#what-youll-build_2","title":"What You'll Build","text":"<ol> <li>Metrics Collection</li> <li>Prometheus for infrastructure</li> <li>Application metrics (custom)</li> <li>Business metrics (revenue, usage)</li> <li>Metric aggregation and long-term storage</li> <li> <p>Cardinality management</p> </li> <li> <p>Logging</p> </li> <li>Centralized log collection (Filebeat, Fluentd)</li> <li>Structured JSON logging</li> <li>Log parsing and enrichment</li> <li>Log-based alerting</li> <li> <p>Long-term log archival</p> </li> <li> <p>Distributed Tracing</p> </li> <li>Request tracing across services</li> <li>Latency analysis</li> <li>Dependency mapping</li> <li>Error tracing</li> <li> <p>Trace sampling (intelligent)</p> </li> <li> <p>Alerting</p> </li> <li>Rule engine (Prometheus rules, custom rules)</li> <li>Multi-condition alerts (AND, OR)</li> <li>Alert grouping (reduce noise)</li> <li>Alert deduplication</li> <li> <p>Alert escalation</p> </li> <li> <p>Incident Management</p> </li> <li>Create incidents from alerts</li> <li>Incident timeline (what happened when)</li> <li>Call incident commander</li> <li>Coordinate team response</li> <li> <p>Post-mortem automation</p> </li> <li> <p>AIOps Features</p> </li> <li>Anomaly detection (ML models)</li> <li>Correlation engine (alert A + alert B = root cause C)</li> <li>Root cause analysis (automatic)</li> <li>Predictive alerts (issue before it happens)</li> <li> <p>Auto-remediation (trigger actions)</p> </li> <li> <p>Dashboarding</p> </li> <li>KPI dashboards (SLI/SLO)</li> <li>Service health dashboard</li> <li>Dependency map visualization</li> <li>Trend analysis</li> <li> <p>Cost impact of incidents</p> </li> <li> <p>Automation &amp; Remediation</p> </li> <li>Auto-scale on high load</li> <li>Restart failing services</li> <li>Drain misbehaving nodes</li> <li>Trigger disaster recovery</li> <li>Rollback bad deployments</li> </ol>"},{"location":"02%20helm/question4/#your-challenges_3","title":"Your Challenges","text":"<ol> <li>Scaling to Massive Data</li> <li>Handle petabytes of logs</li> <li>Query billions of metrics</li> <li>Trace millions of requests</li> <li> <p>Keep response time &lt; 1 second</p> </li> <li> <p>Alert Quality</p> </li> <li>70% of alerts are noise</li> <li>How do you deduplicate?</li> <li>How do you correlate?</li> <li> <p>How do you reduce false positives?</p> </li> <li> <p>Root Cause Analysis</p> </li> <li>Request took 10s (slow)</li> <li>Which service is slow?</li> <li>Which database query is slow?</li> <li>Which code change caused it?</li> <li> <p>Automatically detect and report</p> </li> <li> <p>Multi-Tenancy</p> </li> <li>Team A shouldn't see Team B's metrics/logs/traces</li> <li>But correlation needs cross-team data</li> <li> <p>How do you handle this tension?</p> </li> <li> <p>Data Retention</p> </li> <li>Metrics for 2 years (expensive)</li> <li>Logs for 1 year</li> <li>Traces for 30 days</li> <li>Smart data tiering</li> <li> <p>Compression strategies</p> </li> <li> <p>Predictive Alerting</p> </li> <li>Predict failures before they happen</li> <li>How do you train models?</li> <li>How do you avoid false positives?</li> <li> <p>How do you explain predictions?</p> </li> <li> <p>Automated Remediation</p> </li> <li>Auto-restart services on failure</li> <li>Auto-scale on high load</li> <li>Auto-drain failing nodes</li> <li>Know which actions are safe</li> <li>Prevent cascading failures</li> </ol>"},{"location":"02%20helm/question4/#architecture-youll-design_3","title":"Architecture You'll Design","text":"<pre><code>Data Collection:\n\u251c\u2500\u2500 Metrics (Prometheus) \u2192 TSDB\n\u251c\u2500\u2500 Logs (Fluentd) \u2192 Search (Elasticsearch)\n\u2514\u2500\u2500 Traces (Jaeger) \u2192 Trace Storage\n\nProcessing Pipeline:\n\u251c\u2500\u2500 Alert Rules (Prometheus) \u2192 Create Alerts\n\u251c\u2500\u2500 Correlation Engine \u2192 Group Related Alerts\n\u251c\u2500\u2500 Anomaly Detection \u2192 Predict Issues\n\u251c\u2500\u2500 Root Cause Analysis \u2192 Identify Culprit\n\u2514\u2500\u2500 Recommendation Engine \u2192 Suggest Fix\n\nOutput:\n\u251c\u2500\u2500 Alert to PagerDuty\n\u251c\u2500\u2500 Incident Created\n\u251c\u2500\u2500 Auto-Remediation Triggered\n\u251c\u2500\u2500 Dashboards Updated\n\u2514\u2500\u2500 Post-Mortem Data Collected\n</code></pre>"},{"location":"02%20helm/question4/#success-criteria_3","title":"Success Criteria","text":"<ul> <li>Ingest petabytes of data</li> <li>Alert latency &lt; 10 seconds</li> <li>Alert accuracy &gt; 95% (few false positives)</li> <li>Root cause identified in &lt; 5 minutes</li> <li>Auto-remediation success &gt; 90%</li> <li>Reduce MTTR (mean time to recovery) by 70%</li> <li>Predictive alerts work (catch issues early)</li> <li>Full audit trail of all changes</li> <li>Multi-tenant with strong isolation</li> <li>Cost tracking per team</li> </ul>"},{"location":"02%20helm/question4/#estimated-complexity_3","title":"Estimated Complexity","text":"<ul> <li>Multiple data sources (metrics, logs, traces)</li> <li>Scaling and performance optimization</li> <li>ML models for anomaly detection</li> <li>Correlation and root cause logic</li> <li>Automated remediation</li> <li>Multi-tenancy</li> <li>Time: 30-40 hours</li> </ul>"},{"location":"02%20helm/question4/#project-6-data-platform-lakehouse","title":"Project 6: Data Platform (Lakehouse)","text":"<p>What You'll Build: A complete data platform (data lake + warehouse) for analytics and reporting.</p>"},{"location":"02%20helm/question4/#the-scenario_3","title":"The Scenario","text":"<p>Your company collects data from: - Web application (event data) - Mobile app (user behavior) - IoT devices (sensor data) - Third-party APIs (external data)</p> <p>Currently: - Data is siloed in different systems - No unified schema - Difficult to correlate data - Slow to answer questions - No data governance</p> <p>You're building a data lakehouse: unified data platform for analytics.</p>"},{"location":"02%20helm/question4/#what-youll-build_3","title":"What You'll Build","text":"<ol> <li>Data Ingestion</li> <li>Streaming (events in real-time)</li> <li>Batch (nightly imports)</li> <li>CDC (change data capture)</li> <li>API polling</li> <li> <p>File uploads</p> </li> <li> <p>Data Processing</p> </li> <li>ETL (extract, transform, load)</li> <li>Data validation</li> <li>Schema enforcement</li> <li>Deduplication</li> <li> <p>Aggregations</p> </li> <li> <p>Data Storage</p> </li> <li>Object storage (Parquet, Iceberg)</li> <li>Columnar format for analytics</li> <li>Time-series optimized</li> <li>Partitioning for performance</li> <li> <p>Data tiering (hot/warm/cold)</p> </li> <li> <p>Data Warehouse</p> </li> <li>SQL interface (ClickHouse, Snowflake)</li> <li>Dimensional modeling (facts, dimensions)</li> <li>Slowly changing dimensions</li> <li>Materialized views</li> <li> <p>Incremental updates</p> </li> <li> <p>Metadata &amp; Governance</p> </li> <li>Data catalog (what data exists)</li> <li>Data lineage (where did this come from)</li> <li>Data quality checks</li> <li>Privacy controls (PII masking)</li> <li> <p>Retention policies</p> </li> <li> <p>Analytics &amp; BI</p> </li> <li>Ad-hoc SQL queries</li> <li>Pre-built dashboards</li> <li>Self-service analytics</li> <li>Drill-down capability</li> <li> <p>Scheduled reports</p> </li> <li> <p>Machine Learning</p> </li> <li>Export data for training</li> <li>Features for models</li> <li>Model predictions back to warehouse</li> <li>A/B testing framework</li> </ol>"},{"location":"02%20helm/question4/#your-challenges_4","title":"Your Challenges","text":"<ol> <li>Volume at Scale</li> <li>Ingest 1TB/day</li> <li>Store 100TB total</li> <li>Query across years</li> <li> <p>Keep costs reasonable</p> </li> <li> <p>Schema Evolution</p> </li> <li>New fields added over time</li> <li>Old fields deprecated</li> <li>Type changes (string \u2192 integer)</li> <li> <p>How do you handle this?</p> </li> <li> <p>Data Quality</p> </li> <li>Validate as data arrives</li> <li>Detect anomalies</li> <li>Alert on schema violations</li> <li> <p>Quarantine bad data</p> </li> <li> <p>Privacy Compliance</p> </li> <li>GDPR: right to be forgotten</li> <li>HIPAA: audit access</li> <li>PII: redact or encrypt</li> <li> <p>Retention: delete after period</p> </li> <li> <p>Performance</p> </li> <li>Query 1TB of data in &lt; 5 seconds</li> <li>Aggregations very fast</li> <li>Joins across large tables</li> <li> <p>Incremental updates efficient</p> </li> <li> <p>Cost Optimization</p> </li> <li>S3 storage cheaper than database</li> <li>Compression reduces cost</li> <li>Tiering saves money</li> <li>Prune old data</li> </ol>"},{"location":"02%20helm/question4/#success-criteria_4","title":"Success Criteria","text":"<ul> <li>Ingest 1TB/day without issues</li> <li>Queries run in &lt; 5 seconds</li> <li>Data quality &gt; 99%</li> <li>Privacy controls enforced</li> <li>Full audit trail</li> <li>Self-service analytics works</li> <li>Metadata is complete and accurate</li> <li>Cost efficient</li> <li>Retention policies followed</li> </ul>"},{"location":"02%20helm/question4/#estimated-complexity_4","title":"Estimated Complexity","text":"<ul> <li>Multiple data sources</li> <li>ETL pipeline orchestration</li> <li>Schema management</li> <li>Query optimization</li> <li>Governance and compliance</li> <li>Time: 20-25 hours</li> </ul>"},{"location":"02%20helm/question4/#which-project-should-you-start-with","title":"Which Project Should You Start With?","text":"<p>If you want end-to-end experience: Start with Project 1 (SaaS Platform) - Teaches multi-component charts - Covers scaling, security, upgrades - Most useful for real-world apps - Most satisfying to deploy</p> <p>If you want infrastructure focus: Start with Project 2 (IaC Platform) - Teaches multi-cloud abstraction - Covers infrastructure orchestration - Complex dependency management - Valuable for DevOps/platform teams</p> <p>If you want automation focus: Start with Project 3 (CI/CD Platform) - Teaches workflow orchestration - Covers secret management - Automated deployment pipelines - Valuable for DevOps engineers</p> <p>If you want ML focus: Start with Project 4 (MLOps Platform) - Teaches specialized workload management - Covers GPU scheduling, model serving - Valuable for ML engineers - Unique challenges around experiments</p> <p>If you want observability focus: Start with Project 5 (Observability Platform) - Teaches time-series data handling - Covers monitoring and alerting - Valuable for SREs/platform teams - Complex aggregation logic</p> <p>If you want analytics focus: Start with Project 6 (Data Platform) - Teaches data pipeline orchestration - Covers large-scale data handling - Valuable for data engineers - Cost optimization important</p>"},{"location":"02%20helm/question4/#project-complexity-comparison","title":"Project Complexity Comparison","text":"Project Components Complexity Time Best For 1 (SaaS) 8+ services High 20-30h Full-stack Helm mastery 2 (IaC) Multi-cloud, add-ons Very High 25-35h Infrastructure teams 3 (CI/CD) Pipeline orchestration Very High 20-25h DevOps automation 4 (MLOps) ML workloads, serving High 25-30h ML infrastructure 5 (Observability) Multi-datasource Very High 30-40h SRE/Monitoring 6 (Data) Data pipelines High 20-25h Data engineers"},{"location":"02%20helm/question4/#learning-path","title":"Learning Path","text":"<ol> <li>Start with Project 1 or 3 (most applicable)</li> <li>Then pick a second project based on your domain</li> <li>Then tackle hardest project (usually Project 2 or 5)</li> <li>Combine learnings from all projects into your own system</li> </ol>"},{"location":"02%20helm/question4/#what-youll-learn-across-all-projects","title":"What You'll Learn Across All Projects","text":"<p>\u2705 Multi-component chart architecture \u2705 Dependency management and ordering \u2705 Multi-environment support \u2705 Scaling patterns (horizontal, vertical) \u2705 Stateful application management \u2705 Backup and recovery \u2705 Security hardening and RBAC \u2705 Observability integration \u2705 Upgrade strategies \u2705 Cost optimization \u2705 Production-grade operations \u2705 Git-based workflows \u2705 Automated testing and validation \u2705 Real-world complexity handling</p>"},{"location":"02%20helm/question4/#success-criteria-for-any-project","title":"Success Criteria for Any Project","text":"<p>When you finish, you should be able to:</p> <ol> <li>\u2705 Deploy the complete system with one Helm command</li> <li>\u2705 Scale up/down smoothly without downtime</li> <li>\u2705 Upgrade to new versions safely</li> <li>\u2705 Handle failures gracefully</li> <li>\u2705 Monitor and observe everything</li> <li>\u2705 Maintain all state (backups, recovery)</li> <li>\u2705 Enforce security policies</li> <li>\u2705 Track costs</li> <li>\u2705 Audit all actions</li> <li>\u2705 Explain your design to others</li> </ol> <p>If you can do all 10, you've mastered Helm at a professional level. \ud83c\udfaf</p> <p>Good luck! Pick one project and build something meaningful! \ud83d\ude80</p>"},{"location":"03-kustomize/notes/","title":"Kustomize Master Guide: Advanced Patterns &amp; Modern Practices","text":""},{"location":"03-kustomize/notes/#overview-philosophy","title":"\ud83d\ude80 Overview &amp; Philosophy","text":"<p>Kustomize is a declarative configuration overlay system for Kubernetes, not a templating engine. It follows the \"Everything as YAML\" philosophy with patch-based inheritance.</p> <pre><code># Modern installation (standalone)\nbrew install kustomize  # macOS\n# or\ncurl -s \"https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh\" | bash\n\n# Built-in (kubectl v1.14+)\nkubectl kustomize &lt;dir&gt;\n</code></pre>"},{"location":"03-kustomize/notes/#advanced-project-structure","title":"\ud83d\udcc1 Advanced Project Structure","text":"<pre><code>k8s/\n\u251c\u2500\u2500 base/\n\u2502   \u251c\u2500\u2500 kustomization.yaml          # Base resources\n\u2502   \u251c\u2500\u2500 namespace.yaml\n\u2502   \u251c\u2500\u2500 rbac/\n\u2502   \u2502   \u251c\u2500\u2500 kustomization.yaml      # Component composition\n\u2502   \u2502   \u251c\u2500\u2500 serviceaccount.yaml\n\u2502   \u2502   \u2514\u2500\u2500 rolebinding.yaml\n\u2502   \u251c\u2500\u2500 networking/\n\u2502   \u251c\u2500\u2500 deployments/\n\u2502   \u2514\u2500\u2500 monitoring/\n\u251c\u2500\u2500 components/                      # Reusable transformations\n\u2502   \u251c\u2500\u2500 ingress-ssl/\n\u2502   \u251c\u2500\u2500 pod-security/\n\u2502   \u2514\u2500\u2500 resource-limits/\n\u251c\u2500\u2500 overlays/\n\u2502   \u251c\u2500\u2500 region/                     # Multi-dimensional overlays\n\u2502   \u2502   \u251c\u2500\u2500 us-west/\n\u2502   \u2502   \u2514\u2500\u2500 eu-central/\n\u2502   \u251c\u2500\u2500 environment/\n\u2502   \u2502   \u251c\u2500\u2500 dev/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 kustomization.yaml  # Environment config\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 namespace.yaml\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 patch-deployment.yaml\n\u2502   \u2502   \u251c\u2500\u2500 staging/\n\u2502   \u2502   \u2514\u2500\u2500 production/\n\u2502   \u2514\u2500\u2500 tenant/                     # Multi-tenancy\n\u2502       \u251c\u2500\u2500 team-a/\n\u2502       \u2514\u2500\u2500 team-b/\n\u251c\u2500\u2500 clusters/                       # Cluster-specific configs\n\u2502   \u251c\u2500\u2500 cluster-01/\n\u2502   \u2514\u2500\u2500 cluster-02/\n\u2514\u2500\u2500 generators/                     # Dynamic resource generation\n    \u251c\u2500\u2500 helm-charts/\n    \u2514\u2500\u2500 jsonnet/\n</code></pre>"},{"location":"03-kustomize/notes/#modern-kustomizationyaml-features","title":"\u26a1 Modern kustomization.yaml Features","text":""},{"location":"03-kustomize/notes/#components-kustomize-v4","title":"Components (Kustomize v4+)","text":"<pre><code># components/pod-security/kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1alpha1\nkind: Component\n\npatches:\n  - target:\n      kind: Pod\n    patch: |\n      - op: add\n        path: /spec/securityContext\n        value:\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n  - target:\n      kind: Deployment\n    patch: |\n      - op: add\n        path: /spec/template/spec/containers/0/securityContext\n        value:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop: [\"ALL\"]\n</code></pre>"},{"location":"03-kustomize/notes/#replacements-kustomize-v45-advanced-variable-substitution","title":"Replacements (Kustomize v4.5+) - Advanced Variable Substitution","text":"<pre><code># Base configmap\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: app-config\ndata:\n  appVersion: \"2.1.0\"\n  logLevel: \"INFO\"\n  replicas: \"3\"\n\n# Overlay using replacements\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\n\nresources:\n  - deployment.yaml\n  - configmap.yaml\n\nreplacements:\n  # Simple value copy\n  - source:\n      kind: ConfigMap\n      name: app-config\n      fieldPath: data.appVersion\n    targets:\n      - select:\n          kind: Deployment\n          name: app-deployment\n        fieldPaths:\n          - spec.template.metadata.labels.version\n\n  # Multiple field mapping\n  - source:\n      kind: ConfigMap\n      name: app-config\n      fieldPath: data.replicas\n    targets:\n      - select:\n          kind: Deployment\n        fieldPaths:\n          - spec.replicas\n\n  # Complex transformations\n  - source:\n      kind: ConfigMap\n      name: app-config\n      fieldPath: data.logLevel\n    targets:\n      - select:\n          kind: Deployment\n        fieldPaths:\n          - spec.template.spec.containers.[name=app].env.[name=LOG_LEVEL].value\n        options:\n          create: true  # Create if doesn't exist\n</code></pre>"},{"location":"03-kustomize/notes/#generators-dynamic-resource-creation","title":"Generators - Dynamic Resource Creation","text":"<pre><code># generators/secrets/kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\n\nsecretGenerator:\n  - name: tls-certs\n    type: \"kubernetes.io/tls\"\n    files:\n      - tls.crt=./certs/server.crt\n      - tls.key=./certs/server.key\n    options:\n      annotations:\n        sealedsecrets.bitnami.com/managed: \"true\"\n      labels:\n        cert-manager.io/certificate-name: \"app-tls\"\n\n  - name: dynamic-secret\n    type: Opaque\n    literals:\n      - DB_PASSWORD=$(cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 32 | head -n 1)\n      - API_KEY=$(openssl rand -base64 32)\n    behavior: create  # create, merge, or replace\n\nconfigMapGenerator:\n  - name: app-config\n    envs:\n      - .env.production\n      - .env.secrets\n    files:\n      - config.yaml\n      - \"*.conf\"  # Wildcard support\n    options:\n      immutable: true  # Kubernetes 1.19+ feature\n\ngenerators:\n  - |  # Inline generators\n    apiVersion: batch/v1\n    kind: Job\n    metadata:\n      name: db-migration\n    spec:\n      template:\n        spec:\n          containers:\n          - name: migrator\n            image: alpine:latest\n          restartPolicy: Never\n</code></pre>"},{"location":"03-kustomize/notes/#advanced-patching-techniques","title":"\ud83c\udfaf Advanced Patching Techniques","text":""},{"location":"03-kustomize/notes/#json-patch-rfc-6902-most-powerful","title":"JSON Patch (RFC 6902) - Most Powerful","text":"<pre><code>patches:\n  - target:\n      kind: Deployment\n      name: \".*\"  # Regex support\n      labelSelector: \"app=frontend\"\n    patch: |\n      [\n        {\n          \"op\": \"add\",\n          \"path\": \"/spec/template/spec/tolerations\",\n          \"value\": [\n            {\n              \"key\": \"gpu\",\n              \"operator\": \"Equal\",\n              \"value\": \"nvidia\",\n              \"effect\": \"NoSchedule\"\n            }\n          ]\n        },\n        {\n          \"op\": \"replace\",\n          \"path\": \"/spec/template/spec/containers/0/resources/limits/memory\",\n          \"value\": \"2Gi\"\n        },\n        {\n          \"op\": \"copy\",\n          \"from\": \"/spec/replicas\",\n          \"path\": \"/spec/template/metadata/annotations/initialReplicas\"\n        },\n        {\n          \"op\": \"test\",  # Conditional - only apply if condition matches\n          \"path\": \"/metadata/namespace\",\n          \"value\": \"production\"\n        }\n      ]\n\n  # Inline YAML patch\n  - patch: |\n      apiVersion: apps/v1\n      kind: Deployment\n      metadata:\n        name: app\n      spec:\n        template:\n          spec:\n            containers:\n            - name: main\n              envFrom:\n              - configMapRef:\n                  name: dynamic-config-$(CONFIG_HASH)\n    target:\n      name: app\n</code></pre>"},{"location":"03-kustomize/notes/#strategic-merge-patch-kubernetes-specific","title":"Strategic Merge Patch - Kubernetes-Specific","text":"<pre><code>patchesStrategicMerge:\n  # Add sidecar container (K8s knows how to merge arrays)\n  - |\n    apiVersion: apps/v1\n    kind: Deployment\n    metadata:\n      name: app\n    spec:\n      template:\n        spec:\n          containers:\n          - name: istio-proxy\n            image: istio/proxyv2:1.16\n            resources:\n              requests:\n                cpu: 100m\n                memory: 128Mi\n\n  # Add volume (merged by name)\n  - |\n    apiVersion: apps/v1\n    kind: Deployment\n    metadata:\n      name: app\n    spec:\n      template:\n        spec:\n          volumes:\n          - name: nginx-config\n            configMap:\n              name: nginx-config\n</code></pre>"},{"location":"03-kustomize/notes/#multi-base-composition-cross-cutting","title":"\ud83d\udd17 Multi-Base Composition &amp; Cross-Cutting","text":""},{"location":"03-kustomize/notes/#cross-cutting-concerns-as-components","title":"Cross-Cutting Concerns as Components","text":"<pre><code># components/istio-sidecar-injection/kustomization.yaml (Component)\napiVersion: kustomize.config.k8s.io/v1alpha1\nkind: Component\n\nnamespace: istio-system\n\npatches:\n  - target:\n      kind: Namespace\n    patch: |\n      - op: add\n        path: /metadata/labels/istio-injection\n        value: enabled\n\n# components/linkerd-proxy/kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\n\nresources:\n  - https://raw.githubusercontent.com/linkerd/linkerd2/main/manifests/linkerd-crds.yaml\n  - https://raw.githubusercontent.com/linkerd/linkerd2/main/manifests/linkerd-control-plane.yaml\n\ntransformers:\n  - |  # Inline transformer\n    apiVersion: builtin\n    kind: LabelTransformer\n    metadata:\n      name: linkerd-injection\n    labels:\n      linkerd.io/inject: enabled\n    fieldSpecs:\n    - path: metadata/labels\n      create: true\n</code></pre>"},{"location":"03-kustomize/notes/#dynamic-resource-inclusion","title":"Dynamic Resource Inclusion","text":"<pre><code>apiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\n\n# Include remote bases\nresources:\n  - github.com/kubernetes-sigs/cluster-api//config/default?ref=v1.4.0\n  - https://raw.githubusercontent.com/prometheus-operator/kube-prometheus/main/manifests/setup/prometheus-operator-0servicemonitorCustomResourceDefinition.yaml\n\n# Include local generated manifests\nresources:\n  - ../manifests-generated/  # Helm output\n  - ../jsonnet-output/\n  - ./dynamic/  # Generated by scripts\n\n# Conditional inclusion using build args\nvars:\n  - name: FEATURE_FLAG\n    objref:\n      kind: ConfigMap\n      name: feature-flags\n    fieldref:\n      fieldpath: data.enableMonitoring\n\nresources:\n  - name: monitoring\n    resource: ../monitoring/\n    if: $(FEATURE_FLAG) == \"true\"\n</code></pre>"},{"location":"03-kustomize/notes/#cicd-integration-patterns","title":"\ud83d\udd04 CI/CD Integration Patterns","text":""},{"location":"03-kustomize/notes/#github-actions-workflow","title":"GitHub Actions Workflow","text":"<pre><code>name: Kustomize CI/CD\non: [push, pull_request]\n\njobs:\n  kustomize-validation:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Setup Kustomize\n        uses: imranismail/setup-kustomize@v2\n        with:\n          kustomize-version: 'v5.0.0'\n\n      - name: Validate All Overlays\n        run: |\n          for overlay in overlays/*/; do\n            echo \"Validating $overlay\"\n            kustomize build $overlay --load-restrictor=LoadRestrictionsNone | \\\n              kubectl apply --dry-run=server --validate=true -f -\n          done\n\n  kustomize-diff:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n        with:\n          fetch-depth: 0\n\n      - name: Generate Kustomize Diff\n        run: |\n          git diff --name-only HEAD^ HEAD | grep -E '(kustomization\\.yaml|\\.yaml$|\\.yml$)' | \\\n            while read file; do\n              dir=$(dirname \"$file\")\n              if [ -f \"$dir/kustomization.yaml\" ]; then\n                echo \"Changes detected in $dir\"\n                kustomize build \"$dir\" --load-restrictor=LoadRestrictionsNone &gt; /tmp/new.yaml\n                git checkout HEAD^ -- \"$dir\"\n                kustomize build \"$dir\" --load-restrictor=LoadRestrictionsNone &gt; /tmp/old.yaml\n                diff -u /tmp/old.yaml /tmp/new.yaml || true\n              fi\n            done\n</code></pre>"},{"location":"03-kustomize/notes/#argocd-integration","title":"ArgoCD Integration","text":"<pre><code># argocd-application.yaml\napiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: production-app\nspec:\n  project: default\n  source:\n    repoURL: https://github.com/org/repo.git\n    targetRevision: main\n    path: k8s/overlays/production\n    kustomize:\n      # ArgoCD-specific kustomize options\n      namePrefix: prod-\n      nameSuffix: -v1\n      images:\n        - nginx:1.21.0\n      commonAnnotations:\n        deployed-by: argocd\n      commonLabels:\n        environment: production\n      # Force common labels/annotations on all resources\n      forceCommonLabels: true\n      forceCommonAnnotations: true\n      # Replicas override\n      replicas:\n        - name: app-deployment\n          count: 5\n  destination:\n    server: https://kubernetes.default.svc\n    namespace: production\n\n  syncPolicy:\n    automated:\n      prune: true\n      selfHeal: true\n    syncOptions:\n      - CreateNamespace=true\n      - PruneLast=true\n</code></pre>"},{"location":"03-kustomize/notes/#security-compliance-patterns","title":"\ud83d\udd10 Security &amp; Compliance Patterns","text":""},{"location":"03-kustomize/notes/#pod-security-standards-k8s-123","title":"Pod Security Standards (K8s 1.23+)","text":"<pre><code># components/psa-baseline/kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1alpha1\nkind: Component\n\npatches:\n  - target:\n      kind: Namespace\n    patch: |\n      - op: add\n        path: /metadata/labels\n        value:\n          pod-security.kubernetes.io/enforce: baseline\n          pod-security.kubernetes.io/enforce-version: latest\n          pod-security.kubernetes.io/audit: restricted\n\n  - target:\n      kind: Pod\n    patch: |\n      - op: add\n        path: /spec/securityContext\n        value:\n          seccompProfile:\n            type: RuntimeDefault\n          runAsNonRoot: true\n</code></pre>"},{"location":"03-kustomize/notes/#secrets-management-with-external-secrets","title":"Secrets Management with External Secrets","text":"<pre><code># overlays/production/secrets/kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\n\ngenerators:\n  - |  # ExternalSecret (external-secrets.io)\n    apiVersion: external-secrets.io/v1beta1\n    kind: ExternalSecret\n    metadata:\n      name: database-credentials\n    spec:\n      refreshInterval: 1h\n      secretStoreRef:\n        name: vault-backend\n        kind: SecretStore\n      target:\n        name: database-secret\n        creationPolicy: Owner\n      data:\n        - secretKey: password\n          remoteRef:\n            key: /secrets/production/db\n            property: password\n        - secretKey: username\n          remoteRef:\n            key: /secrets/production/db\n            property: username\n\nconfigMapGenerator:\n  - name: sealed-secret-params\n    literals:\n      - namespace=production\n      - scope=strict\n</code></pre>"},{"location":"03-kustomize/notes/#production-grade-patterns","title":"\ud83d\udea8 Production-Grade Patterns","text":""},{"location":"03-kustomize/notes/#multi-cluster-management-with-kustomize","title":"Multi-Cluster Management with Kustomize","text":"<pre><code># clusters/aws-us-west-2/production/kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\n\n# Import environment overlay\nresources:\n  - ../../../overlays/production\n\n# Cluster-specific patches\npatches:\n  - target:\n      kind: Ingress\n    patch: |\n      - op: replace\n        path: /metadata/annotations/nginx.ingress.kubernetes.io~1load-balancer-id\n        value: alb-1234567890\n\n  - target:\n      kind: PersistentVolumeClaim\n    patch: |\n      - op: replace\n        path: /spec/storageClassName\n        value: gp3\n\n# Cluster-specific generators\nconfigMapGenerator:\n  - name: cluster-info\n    literals:\n      - CLUSTER_NAME=aws-us-west-2\n      - REGION=us-west-2\n      - PROVIDER=aws\n      - VPC_ID=vpc-123456\n</code></pre>"},{"location":"03-kustomize/notes/#feature-flag-management","title":"Feature Flag Management","text":"<pre><code># kustomization.yaml with feature flags\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\n\n# Feature flag ConfigMap\nconfigMapGenerator:\n  - name: feature-flags\n    literals:\n      - enableNewUI=true\n      - enableBetaFeatures=false\n      - maintenanceMode=false\n\n# Conditional resources based on feature flags\ntransformers:\n  - target:\n      kind: Deployment\n      name: app\n    patch: |\n      - op: add\n        path: /spec/template/spec/containers/0/env\n        value:\n          - name: FEATURE_NEW_UI\n            valueFrom:\n              configMapKeyRef:\n                name: feature-flags\n                key: enableNewUI\n          - name: BETA_FEATURES\n            valueFrom:\n              configMapKeyRef:\n                name: feature-flags\n                key: enableBetaFeatures\n\n# Remove resources if feature is disabled\npatches:\n  - target:\n      kind: Deployment\n      name: beta-service\n    patch: |\n      - op: remove\n        path: /spec\n    options:\n      allowMissingTarget: true\n</code></pre>"},{"location":"03-kustomize/notes/#testing-validation","title":"\ud83e\uddea Testing &amp; Validation","text":""},{"location":"03-kustomize/notes/#kustomize-test-framework","title":"Kustomize Test Framework","text":"<pre><code># kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\n\nresources:\n  - deployment.yaml\n  - service.yaml\n\n# tests/test.yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: KustomizationTest\n\ntests:\n  - name: deployment-has-correct-replicas\n    resources:\n      - ../deployment.yaml\n    asserts:\n      - equals:\n          fieldPath: spec.replicas\n          value: 3\n\n  - name: service-selector-matches-deployment\n    resources:\n      - ../deployment.yaml\n      - ../service.yaml\n    asserts:\n      - and:\n        - equals:\n            fieldPath: spec.selector.matchLabels.app\n            value: myapp\n        - exists:\n            fieldPath: metadata.labels.app\n</code></pre>"},{"location":"03-kustomize/notes/#conftest-integration-open-policy-agent","title":"Conftest Integration (Open Policy Agent)","text":"<pre><code># conftest-policy.rego\npackage main\n\ndeny[msg] {\n  input.kind == \"Deployment\"\n  not input.spec.template.spec.securityContext.runAsNonRoot\n  msg := \"Deployment must set runAsNonRoot\"\n}\n\n# Test with\nkustomize build overlays/production | conftest test -\n</code></pre>"},{"location":"03-kustomize/notes/#performance-optimizations","title":"\u2699\ufe0f Performance Optimizations","text":"<pre><code># .kustomizerc (global config)\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: KustomizationConfig\n\nbuildMetadata: [originAnnotations, transformerAnnotations]\nloadRestrictor: LoadRestrictionsNone\nenableExec: true\nenableStar: true\n\n# Use caching for remote resources\nreorder: legacy\n\n# In production builds\nkustomize build \\\n  --enable-alpha-plugins \\\n  --load-restrictor=LoadRestrictionsNone \\\n  --reorder=legacy \\\n  --enable-exec \\\n  overlays/production\n</code></pre>"},{"location":"03-kustomize/notes/#custom-transformers-generators-go-plugins","title":"\ud83d\udd27 Custom Transformers &amp; Generators (Go Plugins)","text":"<pre><code>// main.go\npackage main\n\nimport (\n    \"sigs.k8s.io/kustomize/api/types\"\n    \"sigs.k8s.io/kustomize/kyaml/fn/framework\"\n    \"sigs.k8s.io/kustomize/kyaml/yaml\"\n)\n\ntype AnnotationTransformer struct {\n    Annotations map[string]string `yaml:\"annotations,omitempty\"`\n}\n\nfunc (at *AnnotationTransformer) Filter(objects []*yaml.RNode) ([]*yaml.RNode, error) {\n    for _, obj := range objects {\n        meta, err := obj.GetMeta()\n        if err != nil {\n            return nil, err\n        }\n        if meta.Annotations == nil {\n            meta.Annotations = make(map[string]string)\n        }\n        for k, v := range at.Annotations {\n            meta.Annotations[k] = v\n        }\n        err = obj.SetAnnotations(meta.Annotations)\n        if err != nil {\n            return nil, err\n        }\n    }\n    return objects, nil\n}\n\nfunc main() {\n    resource := &amp;framework.ResourceList{\n        FunctionConfig: &amp;AnnotationTransformer{},\n    }\n    framework.Command(resource, func() error {\n        return resource.Filter(&amp;AnnotationTransformer{})\n    }).Execute()\n}\n</code></pre> <pre><code># Use custom transformer\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\n\ntransformers:\n  - kind: AnnotationTransformer\n    annotations:\n      managed-by: kustomize\n      git-sha: $(git rev-parse HEAD)\n</code></pre>"},{"location":"03-kustomize/notes/#comparison-with-modern-alternatives","title":"\ud83d\udcca Comparison with Modern Alternatives","text":"Feature Kustomize Helm Carvel ytt CUE Paradigm Overlay/Patch Templating Templating Configuration Language Learning Curve Low Medium High Very High GitOps Ready \u2705 Excellent \u26a0\ufe0f Needs Tillerless \u2705 Good \u2705 Good Multi-Environment \u2705 Native \u26a0\ufe0f Values files \u2705 Good \u2705 Excellent Type Safety \u274c \u274c \u26a0\ufe0f Limited \u2705 Excellent Secret Management \u26a0\ufe0f Basic \u26a0\ufe0f Basic \u2705 Good \u2705 Good Community Adoption \u2705 High \u2705 Very High \u26a0\ufe0f Moderate \u26a0\ufe0f Moderate"},{"location":"03-kustomize/notes/#best-practices-summary","title":"\ud83c\udfaf Best Practices Summary","text":"<ol> <li>Use components for cross-cutting concerns (v4+)</li> <li>Leverage replacements over vars (deprecated)</li> <li>Structure overlays by concern: environment \u00d7 region \u00d7 tenant</li> <li>Always validate with <code>kustomize build --load-restrictor=LoadRestrictionsNone</code></li> <li>Use generators for dynamic content</li> <li>Implement testing with kustomize test framework</li> <li>Cache remote resources in CI/CD</li> <li>Seal secrets before committing</li> <li>Use JSON patches for complex transformations</li> <li>Monitor kustomize releases - rapid evolution</li> </ol> <p>This master guide covers advanced modern patterns for enterprise-grade Kubernetes management with Kustomize. The key is embracing its declarative, patch-based philosophy while leveraging new features like components, replacements, and generators for maximum power and flexibility.</p> <p>When you use:</p> <p>yaml secretGenerator:   - name: app-secret     behavior: merge     envs:       - prod-secret.txt The behavior field controls how Kustomize handles the existing secret in your manifests, NOT the source file. Here's what happens:</p> <ol> <li>If the secret DOES NOT exist in base manifests: With behavior: merge \u2192 Will create the secret</li> </ol> <p>With behavior: create \u2192 Will create the secret</p> <p>With behavior: replace \u2192 Will fail (no secret to replace)</p> <ol> <li>If the secret DOES exist in base manifests: With behavior: merge \u2192 Merge values from generator with base secret</li> </ol> <p>With behavior: create \u2192 Error (can't create, already exists)</p> <p>With behavior: replace \u2192 Replace base secret completely</p>"},{"location":"03-kustomize/question1/","title":"Question1","text":"<p>I'll provide a complete Kustomize exercise with step-by-step problem statements first, then the full solutions with YAML.</p>"},{"location":"03-kustomize/question1/#kustomize-learning-path-from-zero-to-production","title":"Kustomize Learning Path: From Zero to Production","text":""},{"location":"03-kustomize/question1/#application-overview","title":"Application Overview","text":"<p>We'll deploy a Node.js API with Redis cache and monitoring sidecar. Components: - <code>node:18-alpine</code> for API (port 3000) - <code>redis:7-alpine</code> for caching - <code>busybox:latest</code> for monitoring sidecar - <code>nginx:1.25</code> for ingress</p>"},{"location":"03-kustomize/question1/#problem-statements-only","title":"PROBLEM STATEMENTS ONLY","text":""},{"location":"03-kustomize/question1/#step-1-base-application","title":"Step 1 \u2013 Base Application","text":"<p>Create a minimal deployable base: 1. Create <code>base/</code> directory 2. Create <code>deployment.yaml</code> for <code>node:18-alpine</code> with 1 replica 3. Create <code>service.yaml</code> ClusterIP on port 80\u21923000 4. Create <code>kustomization.yaml</code> with these 2 resources</p>"},{"location":"03-kustomize/question1/#step-2-add-configmap-and-redis","title":"Step 2 \u2013 Add ConfigMap and Redis","text":"<p>Extend base with configuration: 1. Add <code>configmap.yaml</code> with <code>APP_NAME</code>, <code>LOG_LEVEL</code>, <code>REDIS_HOST</code> 2. Add Redis deployment and service (port 6379) 3. Mount ConfigMap as env vars in API deployment 4. Update base kustomization to include all resources</p>"},{"location":"03-kustomize/question1/#step-3-environment-overlays","title":"Step 3 \u2013 Environment Overlays","text":"<p>Create dev/prod overlays: 1. Create <code>overlays/dev/</code> and <code>overlays/prod/</code> 2. Each overlay references <code>../../base</code> 3. In dev: add <code>namePrefix: dev-</code> and label <code>env: dev</code> 4. In prod: add <code>namePrefix: prod-</code> and label <code>env: prod</code> 5. Test both overlays</p>"},{"location":"03-kustomize/question1/#step-4-replica-scaling","title":"Step 4 \u2013 Replica Scaling","text":"<p>Set different replica counts: 1. In dev overlay: set replicas to 2 2. In prod overlay: set replicas to 4 3. Use patchesStrategicMerge</p>"},{"location":"03-kustomize/question1/#step-5-image-tag-management","title":"Step 5 \u2013 Image Tag Management","text":"<p>Use different image tags: 1. In dev: use <code>node:18-alpine</code> 2. In prod: use specific version <code>node:18.20.0-alpine</code> 3. Use <code>images</code> transformer</p>"},{"location":"03-kustomize/question1/#step-6-environment-specific-configmaps","title":"Step 6 \u2013 Environment-Specific ConfigMaps","text":"<p>Override ConfigMap values: 1. In dev: set <code>LOG_LEVEL: \"debug\"</code>, <code>REDIS_HOST: \"dev-redis\"</code> 2. In prod: set <code>LOG_LEVEL: \"warn\"</code>, add <code>ENABLE_CACHE: \"true\"</code> 3. Use ConfigMapGenerator with behavior: merge</p>"},{"location":"03-kustomize/question1/#step-7-add-secrets","title":"Step 7 \u2013 Add Secrets","text":"<p>Add database secrets: 1. Add secret to base with placeholder values 2. In dev: use literal secrets (plain text) 3. In prod: use file-based secrets (simulate secure) 4. Mount secrets as env vars</p>"},{"location":"03-kustomize/question1/#step-8-resource-limits","title":"Step 8 \u2013 Resource Limits","text":"<p>Set CPU/memory limits: 1. Base: requests 100m CPU, 128Mi RAM; limits 200m CPU, 256Mi RAM 2. Dev: reduce to 50m/64Mi requests, 100m/128Mi limits 3. Prod: increase to 200m/256Mi requests, 500m/512Mi limits 4. Use JSON patches</p>"},{"location":"03-kustomize/question1/#step-9-add-ingress","title":"Step 9 \u2013 Add Ingress","text":"<p>Add ingress only in overlays: 1. In dev: ingress with host <code>dev-api.example.com</code> 2. In prod: ingress with host <code>api.example.com</code>, TLS, and annotations 3. Use different ingress classes</p>"},{"location":"03-kustomize/question1/#step-10-monitoring-sidecar-prod-only","title":"Step 10 \u2013 Monitoring Sidecar (Prod Only)","text":"<p>Add sidecar container: 1. Only in prod overlay 2. Add <code>busybox:latest</code> sidecar that runs <code>[\"sh\", \"-c\", \"while true; do echo 'Monitoring...'; sleep 30; done\"]</code> 3. Use JSON patch to add container</p>"},{"location":"03-kustomize/question1/#step-11-affinity-and-tolerations","title":"Step 11 \u2013 Affinity and Tolerations","text":"<p>Add production-specific scheduling: 1. In prod: add <code>podAntiAffinity</code> to spread across nodes 2. Add toleration for <code>dedicated=prod:NoSchedule</code> 3. Use strategic merge patch</p>"},{"location":"03-kustomize/question1/#step-12-common-labels-and-annotations","title":"Step 12 \u2013 Common Labels and Annotations","text":"<p>Add metadata across all resources: 1. Base: label <code>app: node-api</code>, <code>managed-by: kustomize</code> 2. Dev: annotation <code>environment: development</code>, <code>team: devops</code> 3. Prod: annotation <code>environment: production</code>, <code>team: platform</code> 4. Use <code>commonLabels</code> and <code>commonAnnotations</code></p>"},{"location":"03-kustomize/question1/#step-13-namespace-isolation","title":"Step 13 \u2013 Namespace Isolation","text":"<p>Deploy to different namespaces: 1. Create <code>overlays/prod-us</code> and <code>overlays/prod-eu</code> 2. Each sets different namespace 3. Add region-specific ConfigMap patches 4. Use <code>namespace</code> transformer</p>"},{"location":"03-kustomize/question1/#step-14-volume-mounts","title":"Step 14 \u2013 Volume Mounts","text":"<p>Add persistent configuration: 1. Base: add emptyDir volume 2. Dev: mount as <code>/tmp/logs</code> 3. Prod: mount as <code>/var/log/app</code> with read-only 4. Use volume mounts and patches</p>"},{"location":"03-kustomize/question1/#step-15-job-for-database-migration","title":"Step 15 \u2013 Job for Database Migration","text":"<p>Add initialization job: 1. Only in prod overlay 2. Create Job that runs before deployment 3. Use <code>kustomize.config.k8s.io/behavior: create</code> annotation 4. Job should simulate DB migration</p>"},{"location":"03-kustomize/question1/#step-16-multiple-environments-with-bases","title":"Step 16 \u2013 Multiple Environments with Bases","text":"<p>Create staging from prod: 1. Create <code>overlays/staging/</code> that uses prod as base 2. Override only specific values (replicas=3, different hostname) 3. Demonstrate inheritance chain</p>"},{"location":"03-kustomize/question1/#step-17-variables-replacement","title":"Step 17 \u2013 Variables Replacement","text":"<p>Use Kustomize variables: 1. Define variables for image tags 2. Use in deployment 3. Override in overlays</p>"},{"location":"03-kustomize/question1/#step-18-crd-patching","title":"Step 18 \u2013 CRD Patching","text":"<p>Add custom resource (simulate): 1. Add CustomResourceDefinition to base 2. Create instance of CRD 3. Patch fields in overlays</p> <p>Now here are the SOLUTIONS WITH YAML:</p>"},{"location":"03-kustomize/question1/#solutions","title":"SOLUTIONS","text":""},{"location":"03-kustomize/question1/#step-1-base-application_1","title":"Step 1 \u2013 Base Application","text":"<pre><code># base/deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: api\n  template:\n    metadata:\n      labels:\n        app: api\n    spec:\n      containers:\n      - name: api\n        image: node:18-alpine\n        command: [\"node\", \"-e\", \"console.log('API starting'); require('http').createServer((req, res) =&gt; { res.end('Hello from Base API') }).listen(3000)\"]\n        ports:\n        - containerPort: 3000\n</code></pre> <pre><code># base/service.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: api-service\nspec:\n  selector:\n    app: api\n  ports:\n  - port: 80\n    targetPort: 3000\n</code></pre> <pre><code># base/kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nresources:\n- deployment.yaml\n- service.yaml\n</code></pre>"},{"location":"03-kustomize/question1/#step-2-add-configmap-and-redis_1","title":"Step 2 \u2013 Add ConfigMap and Redis","text":"<pre><code># base/configmap.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: app-config\ndata:\n  APP_NAME: \"node-api\"\n  LOG_LEVEL: \"info\"\n  REDIS_HOST: \"redis-service\"\n</code></pre> <pre><code># base/redis-deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis\n  template:\n    metadata:\n      labels:\n        app: redis\n    spec:\n      containers:\n      - name: redis\n        image: redis:7-alpine\n        ports:\n        - containerPort: 6379\n</code></pre> <pre><code># base/redis-service.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: redis-service\nspec:\n  selector:\n    app: redis\n  ports:\n  - port: 6379\n    targetPort: 6379\n</code></pre> <p>Update deployment to use ConfigMap:</p> <pre><code># Update base/deployment.yaml (containers section)\ncontainers:\n- name: api\n  image: node:18-alpine\n  command: [\"node\", \"-e\", \"console.log('API starting'); require('http').createServer((req, res) =&gt; { res.end('Hello from Base API') }).listen(3000)\"]\n  ports:\n  - containerPort: 3000\n  env:\n  - name: APP_NAME\n    valueFrom:\n      configMapKeyRef:\n        name: app-config\n        key: APP_NAME\n  - name: LOG_LEVEL\n    valueFrom:\n      configMapKeyRef:\n        name: app-config\n        key: LOG_LEVEL\n  - name: REDIS_HOST\n    valueFrom:\n      configMapKeyRef:\n        name: app-config\n        key: REDIS_HOST\n</code></pre> <pre><code># Update base/kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nresources:\n- deployment.yaml\n- service.yaml\n- configmap.yaml\n- redis-deployment.yaml\n- redis-service.yaml\n</code></pre>"},{"location":"03-kustomize/question1/#step-3-environment-overlays_1","title":"Step 3 \u2013 Environment Overlays","text":"<pre><code># overlays/dev/kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nnamePrefix: dev-\ncommonLabels:\n  env: dev\nresources:\n- ../../base\n</code></pre> <pre><code># overlays/prod/kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nnamePrefix: prod-\ncommonLabels:\n  env: prod\nresources:\n- ../../base\n</code></pre>"},{"location":"03-kustomize/question1/#step-4-replica-scaling_1","title":"Step 4 \u2013 Replica Scaling","text":"<pre><code># overlays/dev/patch-replicas.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api-deployment\nspec:\n  replicas: 2\n</code></pre> <pre><code># overlays/prod/patch-replicas.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api-deployment\nspec:\n  replicas: 4\n</code></pre> <pre><code># Update overlays/dev/kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nnamePrefix: dev-\ncommonLabels:\n  env: dev\nresources:\n- ../../base\npatchesStrategicMerge:\n- patch-replicas.yaml\n</code></pre> <pre><code># Update overlays/prod/kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nnamePrefix: prod-\ncommonLabels:\n  env: prod\nresources:\n- ../../base\npatchesStrategicMerge:\n- patch-replicas.yaml\n</code></pre>"},{"location":"03-kustomize/question1/#step-5-image-tag-management_1","title":"Step 5 \u2013 Image Tag Management","text":"<pre><code># Update overlays/dev/kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nnamePrefix: dev-\ncommonLabels:\n  env: dev\nresources:\n- ../../base\npatchesStrategicMerge:\n- patch-replicas.yaml\nimages:\n- name: node:18-alpine\n  newTag: 18-alpine\n</code></pre> <pre><code># Update overlays/prod/kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nnamePrefix: prod-\ncommonLabels:\n  env: prod\nresources:\n- ../../base\npatchesStrategicMerge:\n- patch-replicas.yaml\nimages:\n- name: node:18-alpine\n  newTag: 18.20.0-alpine\n- name: redis:7-alpine\n  newTag: 7.2.4-alpine\n</code></pre>"},{"location":"03-kustomize/question1/#step-6-environment-specific-configmaps_1","title":"Step 6 \u2013 Environment-Specific ConfigMaps","text":"<pre><code># overlays/dev/configmap-patch.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: app-config\ndata:\n  LOG_LEVEL: \"debug\"\n  REDIS_HOST: \"dev-redis-service\"\n  ENV_TYPE: \"development\"\n</code></pre> <pre><code># overlays/prod/configmap-patch.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: app-config\ndata:\n  LOG_LEVEL: \"warn\"\n  ENV_TYPE: \"production\"\n  ENABLE_CACHE: \"true\"\n  MAX_CONNECTIONS: \"1000\"\n</code></pre> <pre><code># Update overlays/dev/kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nnamePrefix: dev-\ncommonLabels:\n  env: dev\nresources:\n- ../../base\npatchesStrategicMerge:\n- patch-replicas.yaml\n- configmap-patch.yaml\nimages:\n- name: node:18-alpine\n  newTag: 18-alpine\n</code></pre> <pre><code># Update overlays/prod/kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nnamePrefix: prod-\ncommonLabels:\n  env: prod\nresources:\n- ../../base\npatchesStrategicMerge:\n- patch-replicas.yaml\n- configmap-patch.yaml\nimages:\n- name: node:18-alpine\n  newTag: 18.20.0-alpine\n- name: redis:7-alpine\n  newTag: 7.2.4-alpine\n</code></pre>"},{"location":"03-kustomize/question1/#step-7-add-secrets_1","title":"Step 7 \u2013 Add Secrets","text":"<pre><code># base/secret.yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: app-secret\ntype: Opaque\ndata:\n  # Base64 encoded placeholder values\n  DB_PASSWORD: cGxhY2Vob2xkZXI=  # \"placeholder\"\n  API_KEY: cGxhY2Vob2xkZXI=      # \"placeholder\"\n</code></pre> <p>Update deployment to use secrets:</p> <pre><code># Add to base/deployment.yaml containers.env section\nenv:\n# ... existing env vars ...\n- name: DB_PASSWORD\n  valueFrom:\n    secretKeyRef:\n      name: app-secret\n      key: DB_PASSWORD\n- name: API_KEY\n  valueFrom:\n    secretKeyRef:\n      name: app-secret\n      key: API_KEY\n</code></pre> <pre><code># Update base/kustomization.yaml to include secret.yaml\nresources:\n- deployment.yaml\n- service.yaml\n- configmap.yaml\n- redis-deployment.yaml\n- redis-service.yaml\n- secret.yaml\n</code></pre> <pre><code># overlays/dev/kustomization.yaml with secretGenerator\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nnamePrefix: dev-\ncommonLabels:\n  env: dev\nresources:\n- ../../base\npatchesStrategicMerge:\n- patch-replicas.yaml\n- configmap-patch.yaml\nimages:\n- name: node:18-alpine\n  newTag: 18-alpine\nsecretGenerator:\n- name: app-secret\n  behavior: merge\n  literals:\n  - DB_PASSWORD=dev-password-123\n  - API_KEY=dev-api-key-abc\n</code></pre> <pre><code># Create prod-secret.txt\nDB_PASSWORD=prod-strong-password-!@#456\nAPI_KEY=prod-secure-api-key-xyz789\n</code></pre> <pre><code># overlays/prod/kustomization.yaml with file-based secret\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nnamePrefix: prod-\ncommonLabels:\n  env: prod\nresources:\n- ../../base\npatchesStrategicMerge:\n- patch-replicas.yaml\n- configmap-patch.yaml\nimages:\n- name: node:18-alpine\n  newTag: 18.20.0-alpine\n- name: redis:7-alpine\n  newTag: 7.2.4-alpine\nsecretGenerator:\n- name: app-secret\n  behavior: merge\n  envs:\n  - prod-secret.txt\n</code></pre>"},{"location":"03-kustomize/question1/#step-8-resource-limits_1","title":"Step 8 \u2013 Resource Limits","text":"<pre><code># base/deployment.yaml - add resources section\ncontainers:\n- name: api\n  image: node:18-alpine\n  # ... existing config ...\n  resources:\n    requests:\n      memory: \"128Mi\"\n      cpu: \"100m\"\n    limits:\n      memory: \"256Mi\"\n      cpu: \"200m\"\n- name: redis\n  image: redis:7-alpine\n  resources:\n    requests:\n      memory: \"64Mi\"\n      cpu: \"50m\"\n    limits:\n      memory: \"128Mi\"\n      cpu: \"100m\"\n</code></pre> <pre><code># overlays/dev/patch-resources.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api-deployment\nspec:\n  template:\n    spec:\n      containers:\n      - name: api\n        resources:\n          requests:\n            memory: \"64Mi\"\n            cpu: \"50m\"\n          limits:\n            memory: \"128Mi\"\n            cpu: \"100m\"\n      - name: redis\n        resources:\n          requests:\n            memory: \"32Mi\"\n            cpu: \"25m\"\n          limits:\n            memory: \"64Mi\"\n            cpu: \"50m\"\n</code></pre> <pre><code># overlays/prod/patch-resources.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api-deployment\nspec:\n  template:\n    spec:\n      containers:\n      - name: api\n        resources:\n          requests:\n            memory: \"256Mi\"\n            cpu: \"200m\"\n          limits:\n            memory: \"512Mi\"\n            cpu: \"500m\"\n      - name: redis\n        resources:\n          requests:\n            memory: \"128Mi\"\n            cpu: \"100m\"\n          limits:\n            memory: \"256Mi\"\n            cpu: \"200m\"\n</code></pre> <p>Update kustomization files to include these patches.</p>"},{"location":"03-kustomize/question1/#step-9-add-ingress_1","title":"Step 9 \u2013 Add Ingress","text":"<pre><code># overlays/dev/ingress.yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: api-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /\nspec:\n  ingressClassName: nginx\n  rules:\n  - host: dev-api.example.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: api-service\n            port:\n              number: 80\n</code></pre> <pre><code># overlays/prod/ingress.yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: api-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /\n    nginx.ingress.kubernetes.io/ssl-redirect: \"true\"\n    cert-manager.io/cluster-issuer: \"letsencrypt-prod\"\nspec:\n  ingressClassName: nginx\n  tls:\n  - hosts:\n    - api.example.com\n    secretName: api-tls-secret\n  rules:\n  - host: api.example.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: api-service\n            port:\n              number: 80\n</code></pre> <pre><code># Update overlays/dev/kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nnamePrefix: dev-\ncommonLabels:\n  env: dev\nresources:\n- ../../base\n- ingress.yaml\n# ... rest remains ...\n</code></pre> <pre><code># Update overlays/prod/kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nnamePrefix: prod-\ncommonLabels:\n  env: prod\nresources:\n- ../../base\n- ingress.yaml\n# ... rest remains ...\n</code></pre>"},{"location":"03-kustomize/question1/#step-10-monitoring-sidecar-prod-only_1","title":"Step 10 \u2013 Monitoring Sidecar (Prod Only)","text":"<pre><code># overlays/prod/patch-sidecar.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api-deployment\nspec:\n  template:\n    spec:\n      containers:\n      - name: monitor\n        image: busybox:latest\n        command: [\"sh\", \"-c\", \"while true; do echo 'Monitoring API...'; sleep 30; done\"]\n        resources:\n          requests:\n            memory: \"16Mi\"\n            cpu: \"10m\"\n          limits:\n            memory: \"32Mi\"\n            cpu: \"20m\"\n</code></pre> <p>Add to prod kustomization patches.</p>"},{"location":"03-kustomize/question1/#step-11-affinity-and-tolerations_1","title":"Step 11 \u2013 Affinity and Tolerations","text":"<pre><code># overlays/prod/patch-affinity.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api-deployment\nspec:\n  template:\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - api\n              topologyKey: kubernetes.io/hostname\n      tolerations:\n      - key: \"dedicated\"\n        operator: \"Equal\"\n        value: \"prod\"\n        effect: \"NoSchedule\"\n</code></pre>"},{"location":"03-kustomize/question1/#step-12-common-labels-and-annotations_1","title":"Step 12 \u2013 Common Labels and Annotations","text":"<pre><code># Update base/kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nresources:\n- deployment.yaml\n- service.yaml\n- configmap.yaml\n- redis-deployment.yaml\n- redis-service.yaml\n- secret.yaml\ncommonLabels:\n  app: node-api\n  managed-by: kustomize\n  version: v1.0.0\n</code></pre> <pre><code># Update overlays/dev/kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nnamePrefix: dev-\ncommonLabels:\n  env: dev\ncommonAnnotations:\n  environment: development\n  team: devops\n  deploy-timestamp: \"2024-01-15\"\n# ... rest remains ...\n</code></pre> <pre><code># Update overlays/prod/kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nnamePrefix: prod-\ncommonLabels:\n  env: prod\ncommonAnnotations:\n  environment: production\n  team: platform\n  owner: platform-team@company.com\n  sla-tier: \"gold\"\n# ... rest remains ...\n</code></pre>"},{"location":"03-kustomize/question1/#step-13-namespace-isolation_1","title":"Step 13 \u2013 Namespace Isolation","text":"<pre><code># overlays/prod-us/kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nnamespace: prod-us\nnamePrefix: us-\ncommonLabels:\n  region: us-east\nresources:\n- ../prod\npatchesStrategicMerge:\n- configmap-region.yaml\nconfigMapGenerator:\n- name: app-config\n  behavior: merge\n  literals:\n  - REGION=us-east\n  - DATACENTER=dc1\n</code></pre> <pre><code># overlays/prod-eu/kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nnamespace: prod-eu\nnamePrefix: eu-\ncommonLabels:\n  region: eu-west\nresources:\n- ../prod\npatchesStrategicMerge:\n- configmap-region.yaml\nconfigMapGenerator:\n- name: app-config\n  behavior: merge\n  literals:\n  - REGION=eu-west\n  - DATACENTER=dc2\n</code></pre>"},{"location":"03-kustomize/question1/#step-14-volume-mounts_1","title":"Step 14 \u2013 Volume Mounts","text":"<pre><code># Update base/deployment.yaml - add volumes\nspec:\n  template:\n    spec:\n      volumes:\n      - name: app-logs\n        emptyDir: {}\n      containers:\n      - name: api\n        # ... existing config ...\n        volumeMounts:\n        - name: app-logs\n          mountPath: /tmp/logs\n</code></pre> <pre><code># overlays/prod/patch-volumes.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api-deployment\nspec:\n  template:\n    spec:\n      containers:\n      - name: api\n        volumeMounts:\n        - name: app-logs\n          mountPath: /var/log/app\n          readOnly: false\n</code></pre>"},{"location":"03-kustomize/question1/#step-15-job-for-database-migration_1","title":"Step 15 \u2013 Job for Database Migration","text":"<pre><code># overlays/prod/migration-job.yaml\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: db-migration\n  annotations:\n    kustomize.config.k8s.io/behavior: create\nspec:\n  template:\n    spec:\n      restartPolicy: Never\n      containers:\n      - name: migration\n        image: node:18-alpine\n        command: [\"sh\", \"-c\", \"echo 'Running database migrations...'; sleep 5; echo 'Migrations completed successfully'\"]\n        env:\n        - name: DB_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: app-config\n              key: REDIS_HOST\n</code></pre>"},{"location":"03-kustomize/question1/#step-16-multiple-environments-with-bases_1","title":"Step 16 \u2013 Multiple Environments with Bases","text":"<pre><code># overlays/staging/kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nnamePrefix: staging-\ncommonLabels:\n  env: staging\ncommonAnnotations:\n  environment: staging\n  team: qa\nresources:\n- ../prod\npatchesStrategicMerge:\n- patch-staging.yaml\nimages:\n- name: node:18-alpine\n  newTag: 18-alpine\n</code></pre> <pre><code># overlays/staging/patch-staging.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api-deployment\nspec:\n  replicas: 3\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: api-ingress\nspec:\n  rules:\n  - host: staging-api.example.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: api-service\n            port:\n              number: 80\n</code></pre>"},{"location":"03-kustomize/question1/#step-17-variables-replacement_1","title":"Step 17 \u2013 Variables Replacement","text":"<pre><code># Update base/kustomization.yaml with vars\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nresources:\n- deployment.yaml\n- service.yaml\n- configmap.yaml\n- redis-deployment.yaml\n- redis-service.yaml\n- secret.yaml\ncommonLabels:\n  app: node-api\n  managed-by: kustomize\n  version: v1.0.0\nvars:\n- name: API_IMAGE\n  objref:\n    kind: Deployment\n    name: api-deployment\n    apiVersion: apps/v1\n  fieldref:\n    fieldpath: spec.template.spec.containers[0].image\n- name: REDIS_IMAGE\n  objref:\n    kind: Deployment\n    name: redis-deployment\n    apiVersion: apps/v1\n  fieldref:\n    fieldpath: spec.template.spec.containers[0].image\n</code></pre> <pre><code># Add to base/configmap.yaml\ndata:\n  # ... existing data ...\n  API_IMAGE: $(API_IMAGE)\n  REDIS_IMAGE: $(REDIS_IMAGE)\n</code></pre>"},{"location":"03-kustomize/question1/#step-18-crd-patching_1","title":"Step 18 \u2013 CRD Patching","text":"<pre><code># base/custom-resource.yaml\napiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  name: monitors.monitoring.example.com\nspec:\n  group: monitoring.example.com\n  versions:\n  - name: v1\n    served: true\n    storage: true\n    schema:\n      openAPIV3Schema:\n        type: object\n        properties:\n          spec:\n            type: object\n            properties:\n              interval:\n                type: string\n              endpoints:\n                type: array\n                items:\n                  type: string\n  scope: Namespaced\n  names:\n    plural: monitors\n    singular: monitor\n    kind: Monitor\n---\napiVersion: monitoring.example.com/v1\nkind: Monitor\nmetadata:\n  name: api-monitor\nspec:\n  interval: \"30s\"\n  endpoints:\n  - \"/health\"\n  - \"/metrics\"\n</code></pre> <pre><code># overlays/prod/patch-crd.yaml\napiVersion: monitoring.example.com/v1\nkind: Monitor\nmetadata:\n  name: api-monitor\nspec:\n  interval: \"15s\"\n  endpoints:\n  - \"/health\"\n  - \"/metrics\"\n  - \"/debug\"\n  alertThreshold: \"95\"\n</code></pre>"},{"location":"03-kustomize/question1/#testing-commands","title":"Testing Commands","text":"<pre><code># Test each step\nkubectl kustomize ./base\nkubectl kustomize ./overlays/dev\nkubectl kustomize ./overlays/prod\nkubectl kustomize ./overlays/prod-us\nkubectl kustomize ./overlays/prod-eu\nkubectl kustomize ./overlays/staging\n\n# Apply to cluster\nkubectl apply -k ./overlays/dev\nkubectl apply -k ./overlays/prod\n</code></pre> <p>This complete solution covers 15+ Kustomize transformers including: - <code>resources</code>, <code>patchesStrategicMerge</code>, <code>patchesJson6902</code> - <code>namePrefix</code>, <code>nameSuffix</code>, <code>namespace</code> - <code>commonLabels</code>, <code>commonAnnotations</code> - <code>images</code> transformer - <code>configMapGenerator</code>, <code>secretGenerator</code> - <code>vars</code> for variable substitution - <code>replicas</code> field in patches - <code>affinity</code>, <code>tolerations</code>, <code>volumes</code> - CRD support - Job creation with annotations - Multi-environment inheritance</p>"},{"location":"04-admission-webhook/commands/","title":"Commands","text":""},{"location":"04-admission-webhook/commands/#bootstrap-a-kind-cluster","title":"Bootstrap a kind cluster","text":"<p>kind create cluster --name webhook --image kindest/node:v1.29.2</p>"},{"location":"04-admission-webhook/commands/#generating-certificates-and-shit","title":"Generating Certificates and Shit!!","text":"<p>cd C:\\Users\\VikashKumar\\Desktop\\dev3\\ops\\kubequest\\compose\\04-admission-webhook\\controllers cd /mnt/c/Users/VikashKumar/Desktop/dev3/ops/kubequest/compose/04-admission-webhook/controllers</p> <p>mkdir -p tls</p> <p>docker run -it --rm -v ${PWD}:/work -w /work debian bash</p> <p>apt-get update &amp;&amp; apt-get install -y curl &amp;&amp; curl -L https://github.com/cloudflare/cfssl/releases/download/v1.5.0/cfssl_1.5.0_linux_amd64 -o /usr/local/bin/cfssl &amp;&amp; \\ curl -L https://github.com/cloudflare/cfssl/releases/download/v1.5.0/cfssljson_1.5.0_linux_amd64 -o /usr/local/bin/cfssljson &amp;&amp; \\ chmod +x /usr/local/bin/cfssl &amp;&amp; \\ chmod +x /usr/local/bin/cfssljson</p> <p>cat &lt;./tls/ca-csr.json {   \"hosts\": [     \"cluster.local\"   ],   \"key\": {     \"algo\": \"rsa\",     \"size\": 2048   },   \"names\": [     {       \"C\": \"AU\",       \"L\": \"Melbourne\",       \"O\": \"Example\",       \"OU\": \"CA\",       \"ST\": \"Example\"     }   ] } EOF <p>cat &lt; tls/ca-config.json {   \"signing\": {     \"default\": {       \"expiry\": \"175200h\"     },     \"profiles\": {       \"default\": {         \"usages\": [\"signing\", \"key encipherment\", \"server auth\", \"client auth\"],         \"expiry\": \"175200h\"       }     }   } } EOF"},{"location":"04-admission-webhook/commands/#generate-ca-in-tmp","title":"generate ca in /tmp","text":"<p>cfssl gencert -initca ./tls/ca-csr.json | cfssljson -bare /tmp/ca</p>"},{"location":"04-admission-webhook/commands/#generate-certificate-in-tmp","title":"generate certificate in /tmp","text":"<p>cfssl gencert \\   -ca=/tmp/ca.pem \\   -ca-key=/tmp/ca-key.pem \\   -config=./tls/ca-config.json \\   -hostname=\"example-webhook,example-webhook.default.svc.cluster.local,example-webhook.default.svc,localhost,127.0.0.1\" \\   -profile=default \\   ./tls/ca-csr.json | cfssljson -bare /tmp/example-webhook</p>"},{"location":"04-admission-webhook/commands/#make-a-secret","title":"make a secret","text":"<p>cat &lt; ./tls/example-webhook-tls.yaml apiVersion: v1 kind: Secret metadata:   name: example-webhook-tls type: Opaque data:   tls.crt: $(cat /tmp/example-webhook.pem | base64 | tr -d '\\n')   tls.key: $(cat /tmp/example-webhook-key.pem | base64 | tr -d '\\n')  EOF"},{"location":"04-admission-webhook/commands/#generate-ca-bundle-inject-into-template","title":"generate CA Bundle + inject into template","text":"<p>ca_pem_b64=\"$(openssl base64 -A &lt;\"/tmp/ca.pem\")\"</p> <p>sed -e 's@${CA_PEM_B64}@'\"$ca_pem_b64\"'@g' &lt;\"webhook-template.yaml\" \\     &gt; webhook.yaml</p> <p>mkdir -p tls/ca</p> <p>cp /tmp/ca.pem tls/ca/ca.pem cp /tmp/ca-key.pem tls/ca/ca-key.pem</p> <p>mkdir -p tls/webhook</p> <p>cp /tmp/example-webhook.pem     tls/webhook/tls.crt cp /tmp/example-webhook-key.pem tls/webhook/tls.key</p>"},{"location":"04-admission-webhook/commands/#go-coding-starting-maaeen","title":"Go coding starting maaeen","text":"<p>cd C:\\Users\\VikashKumar\\Desktop\\dev3\\ops\\kubequest\\compose\\04-admission-webhook\\controllers\\src cd /mnt/c/Users/VikashKumar/Desktop/dev3/ops/kubequest/compose/04-admission-webhook/controllers/src</p> <p>docker build . -t webhook docker run -it --rm -p 8081:80 -v ${PWD}:/app webhook sh go mod init example-webhook export CGO_ENABLED=0 go build -o webhook ./webhook</p> <p>docker run -it --rm --net host -v ${HOME}/.kube/:/root/.kube/ -v ${PWD}:/app webhook sh</p> <p>apk add --no-cache curl curl -LO https://storage.googleapis.com/kubernetes-release/release/<code>curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt</code>/bin/linux/amd64/kubectl chmod +x ./kubectl mv ./kubectl /usr/local/bin/kubectl apk add --no-cache make</p> <p>cat &lt;&lt;'EOF' &gt; Makefile .PHONY: build run docker-build docker-push clean help</p> <p>BINARY_NAME=webhook GO_BUILD_FLAGS=CGO_ENABLED=0 GOOS=linux DOCKER_IMAGE=webhook DOCKER_TAG=latest REGISTRY=your-registry.example.com</p> <p>build:     $(GO_BUILD_FLAGS) go build -o $(BINARY_NAME) .</p> <p>run: build     ./$(BINARY_NAME)</p> <p>docker-build:     docker build . -t $(DOCKER_IMAGE):$(DOCKER_TAG)     docker tag $(DOCKER_IMAGE):$(DOCKER_TAG) $(REGISTRY)/$(DOCKER_IMAGE):$(DOCKER_TAG)</p> <p>docker-push: docker-build     docker push $(REGISTRY)/$(DOCKER_IMAGE):$(DOCKER_TAG)</p> <p>clean:     rm -f $(BINARY_NAME)</p> <p>help:     @echo \"make build\"     @echo \"make run\"     @echo \"make docker-build\"     @echo \"make docker-push\"     @echo \"make clean\" EOF</p> <p>go get k8s.io/apimachinery@v0.29.0 go get k8s.io/client-go@v0.29.0 go get k8s.io/api@v0.29.0</p>"},{"location":"04-admission-webhook/notes/","title":"Notes","text":"<p>*</p> <p></p> <p></p>"},{"location":"04-admission-webhook/notes/#admission-webhooks-kubernetes-practical-notes","title":"Admission Webhooks (Kubernetes) \u2014 Practical Notes","text":"<p>Admission webhooks sit between authentication/authorization and object persistence. They are API-server extensions, not runtime components, and they execute on every matching API request (create, update, delete, connect).</p>"},{"location":"04-admission-webhook/notes/#1-where-admission-webhooks-execute-critical-mental-model","title":"1. Where Admission Webhooks Execute (Critical Mental Model)","text":"<pre><code>kubectl / client\n   \u2193\nAuthentication\n   \u2193\nAuthorization (RBAC)\n   \u2193\nAdmission Controllers\n   \u251c\u2500 MutatingAdmissionWebhook\n   \u2514\u2500 ValidatingAdmissionWebhook\n   \u2193\netcd (object persisted)\n</code></pre> <p>Key implication If an admission webhook fails or times out (depending on failurePolicy), the API request may be rejected even if RBAC allows it.</p>"},{"location":"04-admission-webhook/notes/#2-mutating-vs-validating-operational-difference","title":"2. Mutating vs Validating (Operational Difference)","text":""},{"location":"04-admission-webhook/notes/#mutatingadmissionwebhook","title":"MutatingAdmissionWebhook","text":"<ul> <li>Can modify the incoming object</li> <li>Runs first</li> <li> <p>Typical use:</p> </li> <li> <p>Inject sidecars</p> </li> <li>Add labels/annotations</li> <li>Set defaults not handled by API schema</li> </ul>"},{"location":"04-admission-webhook/notes/#validatingadmissionwebhook","title":"ValidatingAdmissionWebhook","text":"<ul> <li>Cannot modify the object</li> <li>Runs after mutation</li> <li> <p>Typical use:</p> </li> <li> <p>Enforce policies</p> </li> <li>Block non-compliant resources</li> </ul> <p>Golden rule Mutation makes objects convenient. Validation makes clusters safe.</p>"},{"location":"04-admission-webhook/notes/#3-what-actually-triggers-a-webhook","title":"3. What Actually Triggers a Webhook","text":"<p>A webhook triggers when ALL of these match:</p> <ul> <li>API group (e.g., <code>apps</code>, <code>\"\"</code> for core)</li> <li>API version (e.g., <code>v1</code>)</li> <li>Resource (e.g., <code>pods</code>, <code>deployments</code>)</li> <li>Operation (<code>CREATE</code>, <code>UPDATE</code>, <code>DELETE</code>, <code>CONNECT</code>)</li> <li>Namespace selector (optional)</li> <li>Object selector (optional)</li> </ul> <p>This is why webhooks can be surgically precise or dangerously broad.</p>"},{"location":"04-admission-webhook/notes/#4-real-world-use-cases-practical-not-theoretical","title":"4. Real-World Use Cases (Practical, Not Theoretical)","text":""},{"location":"04-admission-webhook/notes/#41-enforce-no-latest-image","title":"4.1 Enforce \u201cNo :latest Image\u201d","text":"<p>Type: Validating Why: Prevents non-reproducible deployments</p> <ul> <li>Trigger: <code>CREATE</code>, <code>UPDATE</code> on <code>pods</code></li> <li>Logic: Reject if any container image ends with <code>:latest</code></li> </ul> <p>Outcome: <code>kubectl apply</code> fails immediately with a validation error.</p>"},{"location":"04-admission-webhook/notes/#42-auto-inject-sidecar","title":"4.2 Auto-Inject Sidecar","text":"<p>Type: Mutating Why: Avoid developers manually adding boilerplate</p> <ul> <li>Trigger: <code>CREATE</code> on <code>pods</code></li> <li>Condition: Namespace has label <code>mesh=enabled</code></li> <li>Mutation: Append sidecar container spec</li> </ul> <p>Outcome: Pod spec stored in etcd already includes the sidecar.</p>"},{"location":"04-admission-webhook/notes/#43-enforce-required-labels","title":"4.3 Enforce Required Labels","text":"<p>Type: Validating Why: Governance, cost allocation, ownership</p> <ul> <li> <p>Require labels like:</p> </li> <li> <p><code>app</code></p> </li> <li><code>owner</code></li> <li><code>environment</code></li> </ul> <p>Outcome: Objects without labels never reach the cluster state.</p>"},{"location":"04-admission-webhook/notes/#44-block-privileged-containers","title":"4.4 Block Privileged Containers","text":"<p>Type: Validating Why: Security hardening</p> <ul> <li> <p>Reject if:</p> </li> <li> <p><code>securityContext.privileged: true</code></p> </li> <li><code>hostPath</code> volumes used</li> </ul> <p>Outcome: Security enforced before workload starts.</p>"},{"location":"04-admission-webhook/notes/#5-failure-modes-you-must-understand","title":"5. Failure Modes You Must Understand","text":""},{"location":"04-admission-webhook/notes/#failurepolicy","title":"failurePolicy","text":"<ul> <li><code>Fail</code>   API request fails if webhook is unreachable   Use for security-critical controls</li> <li><code>Ignore</code>   API request continues if webhook fails   Use for non-critical mutation</li> </ul> <p>Production guidance Validating webhooks enforcing security should almost always be <code>Fail</code>.</p>"},{"location":"04-admission-webhook/notes/#timeoutseconds","title":"timeoutSeconds","text":"<ul> <li>Default: 10 seconds</li> <li>API server blocks waiting for response</li> </ul> <p>Anti-pattern Slow webhook = cluster-wide deployment slowdown.</p>"},{"location":"04-admission-webhook/notes/#6-operational-risks-exam-real-world","title":"6. Operational Risks (Exam + Real World)","text":""},{"location":"04-admission-webhook/notes/#61-self-inflicted-cluster-outage","title":"6.1 Self-Inflicted Cluster Outage","text":"<ul> <li> <p>Webhook applies to:</p> </li> <li> <p><code>pods</code></p> </li> <li><code>deployments</code></li> <li>Webhook service is down</li> <li>failurePolicy = <code>Fail</code></li> </ul> <p>Result: No pods can be created, including webhook itself.</p> <p>Mitigation</p> <ul> <li>Narrow selectors</li> <li>Use namespace exclusion</li> <li>Careful bootstrapping order</li> </ul>"},{"location":"04-admission-webhook/notes/#62-infinite-mutation-loops","title":"6.2 Infinite Mutation Loops","text":"<ul> <li>Webhook mutates object</li> <li>Mutation changes something webhook re-triggers on</li> </ul> <p>Mitigation</p> <ul> <li>Add idempotent markers (e.g., annotation <code>mutated=true</code>)</li> <li>Exit early if already processed</li> </ul>"},{"location":"04-admission-webhook/notes/#7-how-admission-webhooks-are-deployed-practically","title":"7. How Admission Webhooks Are Deployed (Practically)","text":""},{"location":"04-admission-webhook/notes/#components","title":"Components","text":"<ol> <li> <p>Webhook server</p> </li> <li> <p>Runs as a Pod</p> </li> <li>Exposes HTTPS endpoint</li> <li> <p>Service</p> </li> <li> <p>Stable DNS for API server</p> </li> <li> <p>TLS</p> </li> <li> <p>Mandatory (API server only talks HTTPS)</p> </li> <li> <p>WebhookConfiguration</p> </li> <li> <p>MutatingWebhookConfiguration or ValidatingWebhookConfiguration</p> </li> </ol>"},{"location":"04-admission-webhook/notes/#8-admissionreview-request-response-conceptual","title":"8. AdmissionReview Request / Response (Conceptual)","text":"<p>Webhook receives:</p> <ul> <li>Full object (<code>oldObject</code> for updates)</li> <li>User info</li> <li>Namespace</li> <li>Operation</li> </ul> <p>Webhook returns:</p> <ul> <li><code>allowed: true | false</code></li> <li>Optional status message</li> <li>Optional JSON patch (mutation only)</li> </ul> <p>This is synchronous and blocking.</p>"},{"location":"04-admission-webhook/notes/#9-admission-webhooks-vs-other-controls-exam-relevant","title":"9. Admission Webhooks vs Other Controls (Exam-Relevant)","text":"Mechanism When it Acts Can Modify Can Block RBAC Before admission No Yes Admission Webhook Before persistence Mutating: Yes Yes Pod Security Admission Admission No Yes OPA / Gatekeeper Admission (via webhook) No Yes Runtime security After pod runs N/A No"},{"location":"04-admission-webhook/notes/#10-key-takeaways-to-remember","title":"10. Key Takeaways to Remember","text":"<ul> <li>Admission webhooks operate before objects exist</li> <li>They are synchronous and blocking</li> <li>Mutating runs before validating</li> <li>A broken webhook can break the cluster</li> <li>Always scope selectors tightly</li> <li>Prefer validation for safety, mutation for convenience</li> </ul>"},{"location":"04-admission-webhook/notes/#11-one-line-exam-summary","title":"11. One-Line Exam Summary","text":"<p>Admission webhooks intercept API requests after RBAC but before persistence, allowing mutation or rejection of Kubernetes objects based on custom logic.</p> <p>If you want, the next extension can cover:</p> <ul> <li>Step-by-step webhook deployment</li> <li>Common CKA traps</li> <li>Admission vs Pod Security Admission</li> <li>Debugging webhook failures in production</li> </ul>"},{"location":"04-admission-webhook/notes/#12-minimal-practical-code-examples-incremental-add-on","title":"12. Minimal, Practical Code Examples (Incremental Add-on)","text":"<p>The sections below extend the existing notes by adding working YAML and logic fragments that correspond directly to the use cases already described. Nothing earlier is replaced.</p>"},{"location":"04-admission-webhook/notes/#121-validating-webhook-block-latest-images","title":"12.1 Validating Webhook \u2014 Block <code>:latest</code> Images","text":""},{"location":"04-admission-webhook/notes/#webhook-configuration-validating","title":"Webhook Configuration (Validating)","text":"<pre><code>apiVersion: admissionregistration.k8s.io/v1\nkind: ValidatingWebhookConfiguration\nmetadata:\n  name: deny-latest-tag\nwebhooks:\n  - name: deny-latest.images.example.com\n    admissionReviewVersions: [\"v1\"]\n    sideEffects: None\n    failurePolicy: Fail\n    timeoutSeconds: 5\n    rules:\n      - apiGroups: [\"\"]\n        apiVersions: [\"v1\"]\n        operations: [\"CREATE\", \"UPDATE\"]\n        resources: [\"pods\"]\n    clientConfig:\n      service:\n        name: image-policy-webhook\n        namespace: admission\n        path: /validate-images\n      caBundle: &lt;BASE64_CA_CERT&gt;\n</code></pre>"},{"location":"04-admission-webhook/notes/#admission-logic-pseudo-code","title":"Admission Logic (Pseudo-code)","text":"<pre><code>for each container in pod.spec.containers:\n  if image ends with \":latest\":\n    deny(\"image tag ':latest' is not allowed\")\nallow()\n</code></pre> <p>Practical effect <code>kubectl apply</code> fails immediately if any container uses <code>:latest</code>.</p>"},{"location":"04-admission-webhook/notes/#122-mutating-webhook-sidecar-injection","title":"12.2 Mutating Webhook \u2014 Sidecar Injection","text":""},{"location":"04-admission-webhook/notes/#webhook-configuration-mutating","title":"Webhook Configuration (Mutating)","text":"<pre><code>apiVersion: admissionregistration.k8s.io/v1\nkind: MutatingWebhookConfiguration\nmetadata:\n  name: inject-sidecar\nwebhooks:\n  - name: sidecar.inject.example.com\n    admissionReviewVersions: [\"v1\"]\n    sideEffects: None\n    failurePolicy: Ignore\n    rules:\n      - apiGroups: [\"\"]\n        apiVersions: [\"v1\"]\n        operations: [\"CREATE\"]\n        resources: [\"pods\"]\n    namespaceSelector:\n      matchLabels:\n        mesh: enabled\n    clientConfig:\n      service:\n        name: sidecar-webhook\n        namespace: admission\n        path: /mutate-pod\n      caBundle: &lt;BASE64_CA_CERT&gt;\n</code></pre>"},{"location":"04-admission-webhook/notes/#json-patch-returned-by-webhook","title":"JSON Patch Returned by Webhook","text":"<pre><code>[\n  {\n    \"op\": \"add\",\n    \"path\": \"/spec/containers/-\",\n    \"value\": {\n      \"name\": \"mesh-proxy\",\n      \"image\": \"envoyproxy/envoy:v1.30.0\"\n    }\n  }\n]\n</code></pre> <p>Practical effect Developers deploy normal pods; sidecars appear automatically in stored objects.</p>"},{"location":"04-admission-webhook/notes/#123-validating-webhook-required-labels","title":"12.3 Validating Webhook \u2014 Required Labels","text":""},{"location":"04-admission-webhook/notes/#webhook-rule","title":"Webhook Rule","text":"<pre><code>rules:\n  - apiGroups: [\"apps\"]\n    apiVersions: [\"v1\"]\n    operations: [\"CREATE\", \"UPDATE\"]\n    resources: [\"deployments\"]\n</code></pre>"},{"location":"04-admission-webhook/notes/#validation-logic","title":"Validation Logic","text":"<pre><code>required = [\"app\", \"owner\", \"environment\"]\n\nfor key in required:\n  if key not in metadata.labels:\n    deny(\"missing required label: \" + key)\nallow()\n</code></pre> <p>Practical effect Governance enforced before workloads exist.</p>"},{"location":"04-admission-webhook/notes/#124-validating-webhook-block-privileged-pods","title":"12.4 Validating Webhook \u2014 Block Privileged Pods","text":""},{"location":"04-admission-webhook/notes/#validation-logic_1","title":"Validation Logic","text":"<pre><code>for container in pod.spec.containers:\n  if container.securityContext.privileged == true:\n    deny(\"privileged containers are not allowed\")\n\nfor volume in pod.spec.volumes:\n  if volume.hostPath exists:\n    deny(\"hostPath volumes are not allowed\")\nallow()\n</code></pre> <p>Practical effect Security policy enforced independently of RBAC.</p>"},{"location":"04-admission-webhook/notes/#125-webhook-server-skeleton","title":"12.5 Webhook Server (Skeleton)","text":""},{"location":"04-admission-webhook/notes/#https-server-conceptual","title":"HTTPS Server (Conceptual)","text":"<pre><code>POST /validate\n  decode AdmissionReview\n  inspect request.object\n  build AdmissionReview response:\n    allowed: true | false\n    status.message (if denied)\n  return response\n</code></pre> <p>Key requirement</p> <ul> <li>HTTPS only</li> <li>Valid certificate trusted by API server</li> </ul>"},{"location":"04-admission-webhook/notes/#126-bootstrap-safe-selector-example-outage-prevention","title":"12.6 Bootstrap-Safe Selector Example (Outage Prevention)","text":"<pre><code>namespaceSelector:\n  matchExpressions:\n    - key: kubernetes.io/metadata.name\n      operator: NotIn\n      values:\n        - kube-system\n        - admission\n</code></pre> <p>Why this matters Prevents the webhook from blocking its own Pods or system components.</p>"},{"location":"04-admission-webhook/notes/#127-debugging-a-failing-webhook-operational","title":"12.7 Debugging a Failing Webhook (Operational)","text":"<pre><code>kubectl get validatingwebhookconfigurations\nkubectl describe validatingwebhookconfiguration deny-latest-tag\n\nkubectl logs -n admission deploy/image-policy-webhook\n</code></pre> <p>Typical symptoms:</p> <ul> <li><code>context deadline exceeded</code></li> <li><code>no endpoints available for service</code></li> <li>TLS trust errors</li> </ul>"},{"location":"04-admission-webhook/notes/#128-exam-focused-code-recognition-checklist","title":"12.8 Exam-Focused Code Recognition Checklist","text":"<ul> <li><code>MutatingWebhookConfiguration</code> \u2192 JSON Patch present</li> <li><code>ValidatingWebhookConfiguration</code> \u2192 allow/deny only</li> <li><code>failurePolicy: Fail</code> \u2192 cluster safety risk if misused</li> <li><code>namespaceSelector</code> \u2192 blast-radius control</li> <li>HTTPS + CA bundle mandatory</li> </ul>"},{"location":"04-admission-webhook/notes/#129-one-line-extension-summary","title":"12.9 One-Line Extension Summary","text":"<p>Admission webhooks are implemented as HTTPS services registered via webhook configurations that synchronously mutate or reject Kubernetes API objects before persistence, with selectors and failure policies determining their operational safety.</p>"},{"location":"05-kubeconfig/how-to-setup/","title":"How to setup","text":"<p>kubectl config set-cluster kubernetes --server=https://172.30.1.2:6443 --certificate-authority=/etc/kubernetes/pki/ca.crt kubectl config set-credentials kubelet --token=m3qmx4.qa9c83ju82ru6njq kubectl config set-context default --cluster=kubernetes --user=kubelet kubectl config use-context default</p>"},{"location":"05-kubeconfig/notes/","title":"Notes","text":"<p>~/.kube                                                                                                                      14:23:22 \u276f kind get cluster Gets one of [clusters, nodes, kubeconfig]</p> <p>Usage:   kind get [flags]   kind get [command]</p> <p>Available Commands:   clusters    Lists existing kind clusters by their name   kubeconfig  Prints cluster kubeconfig   nodes       Lists existing kind nodes by their name</p> <p>Flags:   -h, --help   help for get</p> <p>Global Flags:   -q, --quiet             silence all stderr output   -v, --verbosity int32   info log verbosity, higher value produces more output</p> <p>Use \"kind get [command] --help\" for more information about a command. ERROR: Subcommand is required</p> <p>~/.kube                                                                                                                      14:24:45 \u276f kind get clusters gatewayapi</p> <p>~/.kube                                                                                                                      14:24:53 \u276f # Get kubeconfig for gatewayapi cluster kind get kubeconfig --name=gatewayapi &gt; /tmp/gatewayapi-kubeconfig.yaml</p>"},{"location":"05-kubeconfig/notes/#show-the-structure","title":"Show the structure","text":"<p>cat /tmp/gatewayapi-kubeconfig.yaml zsh: command not found: # zsh: command not found: # apiVersion: v1 clusters: - cluster:     certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURCVENDQWUyZ0F3SUJBZ0lJS0RYandhOW1KNll3RFFZSktvWklodmNOQVFFTEJRQXdGVEVUTUJFR0ExVUUKQXhNS2EzVmlaWEp1WlhSbGN6QWVGdzB5TmpBeE1ERXdOekE0TlRoYUZ3MHpOVEV5TXpBd056RXpOVGhhTUJVeApFekFSQmdOVkJBTVRDbXQxWW1WeWJtVjBaWE13Z2dFaU1BMEdDU3FHU0liM0RRRUJBUVVBQTRJQkR3QXdnZ0VLCkFvSUJBUUM1OHpsZnJGRUJRZHZuRzJ6MzBvQ09zODJnTldnUlNsWW5ZNGgwUkVtSE5DOVVYWmFNUm1hVHFCQ1kKT2pDbW9nMVhaYlFQcTE5MHFGTkV1ZDk2MDRjS3FhTFhHbGhuR20wZEQ1dWFtN3NsQngrQURmMzErbEhXYTNsSgpGU3A1WFlIMGxpb3dWMHpQVmN5RlpDZjk0UTIxejVMS1lvRmdXWHdTMG5oZUZ5WWZPdjFXWUV3cVExZVFlS0RGCjljbFNiQ0VpbEtUbDgzZng1MXZ1YThLcGExTmt3TG1tdzFBSHZkZXRBUnZBUDk2Zm5zTm94SFY0YVM3YVNTM1kKQkJUeC9SWExPaFVJRXFjbFZSSzNyeEF4T2NUYnVqaCtZRzh3Nmwrc3VBY296QmtNZmNzcGY4UjdxMm4wTkdkcAp4eGYvZG9WeUQxK200N3dKVzIyTDcxalN6OXNUQWdNQkFBR2pXVEJYTUE0R0ExVWREd0VCL3dRRUF3SUNwREFQCkJnTlZIUk1CQWY4RUJUQURBUUgvTUIwR0ExVWREZ1FXQkJTRXByTko1dmVOQU1kb1BWUlJ4cjJvUUpqUytqQVYKQmdOVkhSRUVEakFNZ2dwcmRXSmxjbTVsZEdWek1BMEdDU3FHU0liM0RRRUJDd1VBQTRJQkFRQmM5RnpOSEhTaAoyZnhJZ0l0azZEUVZRc3BpN0ZZcURKd3pKdFczZmx6ZHBnckF0STM4VTdmc0NhcnpFaFpQVmY3ODR2OTdFWHBDCmZabHRGVXlXL2xvWVlDZ1hnSGo1Sjh6MjNQSGxVVklJOUU0c1BYM1dKWlBjWmtsSWpyNFdDOFY1dzNpSms2aHoKZS9xdDhpb291QkFGNVlHM24ycTlSSUFuN0h3N3M0UCs5ZXJMOGlKbC80T0VOQWpNemRYMzJvZmJUenNzQlcyVwppelU4YWpVVnE0RkVRZW9zT3JmcWQvNURwcEJqb3lpZlJ1SXNLaDJNdG1vcFBtV3VLcTVLemI3M3BNcTJSeE5OClVWRDdXZWFzdTAxWVQrdEZTMlRqTFZyNGZxL1dYRjhDM0JUUHltZ3lUQWVxcXk4SzhVQlVFTnVBMG1FS1VONEcKcnVqbFZ1dEM1M0VXCi0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K     server: https://127.0.0.1:39619   name: kind-gatewayapi contexts: - context:     cluster: kind-gatewayapi     user: kind-gatewayapi   name: kind-gatewayapi current-context: kind-gatewayapi kind: Config users: - name: kind-gatewayapi   user:     client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURLVENDQWhHZ0F3SUJBZ0lJTGh0ajRISmR5Nkl3RFFZSktvWklodmNOQVFFTEJRQXdGVEVUTUJFR0ExVUUKQXhNS2EzVmlaWEp1WlhSbGN6QWVGdzB5TmpBeE1ERXdOekE0TlRoYUZ3MHlOekF4TURFd056RXpOVGhhTUR3eApIekFkQmdOVkJBb1RGbXQxWW1WaFpHMDZZMngxYzNSbGNpMWhaRzFwYm5NeEdUQVhCZ05WQkFNVEVHdDFZbVZ5CmJtVjBaWE10WVdSdGFXNHdnZ0VpTUEwR0NTcUdTSWIzRFFFQkFRVUFBNElCRHdBd2dnRUtBb0lCQVFDcWtwcEMKbndWQnNmMjFId2dGQ1Rod0dQR2RpcUtSVk42RVF3Z0Z3WnkzWjlNb0llRWZNa1MvYjlQOGxEY0d2Vk0vR3d3Rwp5RG4rcFJMK3ZYT2JObVNnblMveHNRK3NVamZnTERLb2V3ZHFBYXJCUG0vaHIxTXpPSXErdUdvMWRBeHZpcHNvCjNPQjc0Y0VsQmIxd2pxb3U1akwxRzNjNW9pbnRUSmFjcjJPcjZ3Z2JSaUsxaEF0ZjlPL2tFbndNRkQvdnV2NmMKSXZwVVB0aUtwUUJGZnhLRWdQYjZYOVZhc2J5d1Jab3JsS2pvSWlpVTNZM3ZBQ1FqSlExYVlEUm1JYW12bVNURQoyKzl4RDUweGY5MDNRSnp5UG45Umx0RzhPYnlHRTkvVW5IL1g2VER3UGRydFpwMjlXWHAxZDIrd3VyMld4a01PCnREZDZUbGNuV01kMStKL3JBZ01CQUFHalZqQlVNQTRHQTFVZER3RUIvd1FFQXdJRm9EQVRCZ05WSFNVRUREQUsKQmdnckJnRUZCUWNEQWpBTUJnTlZIUk1CQWY4RUFqQUFNQjhHQTFVZEl3UVlNQmFBRklTbXMwbm05NDBBeDJnOQpWRkhHdmFoQW1OTDZNQTBHQ1NxR1NJYjNEUUVCQ3dVQUE0SUJBUUJlUXk2N3YvMVdFUVB5bXV6L2V4NHlNdHgwCjY3S3BWQzkxUVZ1aEFjNUpZMWwwSnp3bWxjRzJrUk1lK2VFM3Erd1pEMTRmYVhKcjdrbGFjQkcxMXRsZUZKRXoKelZCVVArU3Jpbk9yN05wS3VlMGdmZ2FzTTJ4UkFRVWkyOXQvem5FTjZ3a1BzRzkvbUFBSGZZWTAzVjlxSmovMwp2N3NFcElrR3lxNzMzek5qRC81bHEvTFgvUWppeHc4MzE1b0h1WHMzekhmM3pONGkrZFhRQnAxTlk0TGVCQzFYCnVUa3lhbXJRR3ZQRTBsT2llTUlLQ3pkQ1k0cXlpb04yL2Jqd21KZStXYTBpNEorQ3VKQ0MyWjVKYVJpVUM1QUQKdW94Z1lYSXFSazlkekZ2SkU2WnRvWG9VMDVlS0oyNHFBaEd4UEViV2ptMDZJVEtwc3J3MCs0aUVLY09mCi0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K     client-key-data: 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</p> <p>~/.kube                                                                                                                      14:25:21 \u276f sed -n '/certificate-authority-data:/,/^[[:space:]]*server:/p' /tmp/gatewayapi-kubeconfig.yaml | \\   grep 'certificate-authority-data:' | \\   cut -d':' -f2- | \\   tr -d ' ' | \\   base64 -d &gt; /tmp/decoded-ca.crt</p> <p>~/.kube                                                                                                                      14:28:18 \u276f cat /tmp/decoded-ca.crt -----BEGIN CERTIFICATE----- MIIDBTCCAe2gAwIBAgIIKDXjwa9mJ6YwDQYJKoZIhvcNAQELBQAwFTETMBEGA1UE AxMKa3ViZXJuZXRlczAeFw0yNjAxMDEwNzA4NThaFw0zNTEyMzAwNzEzNThaMBUx EzARBgNVBAMTCmt1YmVybmV0ZXMwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEK AoIBAQC58zlfrFEBQdvnG2z30oCOs82gNWgRSlYnY4h0REmHNC9UXZaMRmaTqBCY OjCmog1XZbQPq190qFNEud9604cKqaLXGlhnGm0dD5uam7slBx+ADf31+lHWa3lJ FSp5XYH0liowV0zPVcyFZCf94Q21z5LKYoFgWXwS0nheFyYfOv1WYEwqQ1eQeKDF 9clSbCEilKTl83fx51vua8Kpa1NkwLmmw1AHvdetARvAP96fnsNoxHV4aS7aSS3Y BBTx/RXLOhUIEqclVRK3rxAxOcTbujh+YG8w6l+suAcozBkMfcspf8R7q2n0NGdp xxf/doVyD1+m47wJW22L71jSz9sTAgMBAAGjWTBXMA4GA1UdDwEB/wQEAwICpDAP BgNVHRMBAf8EBTADAQH/MB0GA1UdDgQWBBSEprNJ5veNAMdoPVRRxr2oQJjS+jAV BgNVHREEDjAMggprdWJlcm5ldGVzMA0GCSqGSIb3DQEBCwUAA4IBAQBc9FzNHHSh 2fxIgItk6DQVQspi7FYqDJwzJtW3flzdpgrAtI38U7fsCarzEhZPVf784v97EXpC fZltFUyW/loYYCgXgHj5J8z23PHlUVII9E4sPX3WJZPcZklIjr4WC8V5w3iJk6hz e/qt8ioouBAF5YG3n2q9RIAn7Hw7s4P+9erL8iJl/4OENAjMzdX32ofbTzssBW2W izU8ajUVq4FEQeosOrfqd/5DppBjoyifRuIsKh2MtmopPmWuKq5Kzb73pMq2RxNN UVD7Weasu01YT+tFS2TjLVr4fq/WXF8C3BTPymgyTAeqqy8K8UBUENuA0mEKUN4G rujlVutC53EW -----END CERTIFICATE-----</p> <p>~/.kube                                                                                                                      14:28:22 \u276f openssl x509 -in /tmp/decoded-ca.crt -text -noout | head -30 Certificate:     Data:         Version: 3 (0x2)         Serial Number: 2897472356293683110 (0x2835e3c1af6627a6)         Signature Algorithm: sha256WithRSAEncryption         Issuer: CN = kubernetes         Validity             Not Before: Jan  1 07:08:58 2026 GMT             Not After : Dec 30 07:13:58 2035 GMT         Subject: CN = kubernetes         Subject Public Key Info:             Public Key Algorithm: rsaEncryption                 Public-Key: (2048 bit)                 Modulus:                     00:b9:f3:39:5f:ac:51:01:41:db:e7:1b:6c:f7:d2:                     80:8e:b3:cd:a0:35:68:11:4a:56:27:63:88:74:44:                     49:87:34:2f:54:5d:96:8c:46:66:93:a8:10:98:3a:                     30:a6:a2:0d:57:65:b4:0f:ab:5f:74:a8:53:44:b9:                     df:7a:d3:87:0a:a9:a2:d7:1a:58:67:1a:6d:1d:0f:                     9b:9a:9b:bb:25:07:1f:80:0d:fd:f5:fa:51:d6:6b:                     79:49:15:2a:79:5d:81:f4:96:2a:30:57:4c:cf:55:                     cc:85:64:27:fd:e1:0d:b5:cf:92:ca:62:81:60:59:                     7c:12:d2:78:5e:17:26:1f:3a:fd:56:60:4c:2a:43:                     57:90:78:a0:c5:f5:c9:52:6c:21:22:94:a4:e5:f3:                     77:f1:e7:5b:ee:6b:c2:a9:6b:53:64:c0:b9:a6:c3:                     50:07:bd:d7:ad:01:1b:c0:3f:de:9f:9e:c3:68:c4:                     75:78:69:2e:da:49:2d:d8:04:14:f1:fd:15:cb:3a:                     15:08:12:a7:25:55:12:b7:af:10:31:39:c4:db:ba:                     38:7e:60:6f:30:ea:5f:ac:b8:07:28:cc:19:0c:7d:                     cb:29:7f:c4:7b:ab:69:f4:34:67:69:c7:17:ff:76:</p> <p>~/.kube                                                                                                                      14:28:36 \u276f awk '/client-certificate-data:/{flag=1} flag &amp;&amp; /^[[:space:]]client-key-data:/{flag=0} flag' /tmp/gatewayapi-kubeconfig.yaml | \\   grep 'client-certificate-data:' | \\   cut -d':' -f2- | \\   sed 's/^[[:space:]]//' | \\   base64 -d &gt; /tmp/decoded-client.crt</p> <p>~/.kube                                                                                                                      14:28:57 \u276f openssl x509 -in /tmp/decoded-client.crt -text -noout | head -30 Certificate:     Data:         Version: 3 (0x2)         Serial Number: 3322358965758446498 (0x2e1b63e0725dcba2)         Signature Algorithm: sha256WithRSAEncryption         Issuer: CN = kubernetes         Validity             Not Before: Jan  1 07:08:58 2026 GMT             Not After : Jan  1 07:13:58 2027 GMT         Subject: O = kubeadm:cluster-admins, CN = kubernetes-admin         Subject Public Key Info:             Public Key Algorithm: rsaEncryption                 Public-Key: (2048 bit)                 Modulus:                     00:aa:92:9a:42:9f:05:41:b1:fd:b5:1f:08:05:09:                     38:70:18:f1:9d:8a:a2:91:54:de:84:43:08:05:c1:                     9c:b7:67:d3:28:21:e1:1f:32:44:bf:6f:d3:fc:94:                     37:06:bd:53:3f:1b:0c:06:c8:39:fe:a5:12:fe:bd:                     73:9b:36:64:a0:9d:2f:f1:b1:0f:ac:52:37:e0:2c:                     32:a8:7b:07:6a:01:aa:c1:3e:6f:e1:af:53:33:38:                     8a:be:b8:6a:35:74:0c:6f:8a:9b:28:dc:e0:7b:e1:                     c1:25:05:bd:70:8e:aa:2e:e6:32:f5:1b:77:39:a2:                     29:ed:4c:96:9c:af:63:ab:eb:08:1b:46:22:b5:84:                     0b:5f:f4:ef:e4:12:7c:0c:14:3f:ef:ba:fe:9c:22:                     fa:54:3e:d8:8a:a5:00:45:7f:12:84:80:f6:fa:5f:                     d5:5a:b1:bc:b0:45:9a:2b:94:a8:e8:22:28:94:dd:                     8d:ef:00:24:23:25:0d:5a:60:34:66:21:a9:af:99:                     24:c4:db:ef:71:0f:9d:31:7f:dd:37:40:9c:f2:3e:                     7f:51:96:d1:bc:39:bc:86:13:df:d4:9c:7f:d7:e9:                     30:f0:3d:da:ed:66:9d:bd:59:7a:75:77:6f:b0:ba:</p> <p>~/.kube                                                                                                                      14:29:02 \u276f awk '/client-key-data:/{flag=1} flag &amp;&amp; /^[[:space:]]token:/{flag=0} flag' /tmp/gatewayapi-kubeconfig.yaml | \\   grep 'client-key-data:' | \\   cut -d':' -f2- | \\   sed 's/^[[:space:]]//' | \\   base64 -d &gt; /tmp/decoded-client.key</p> <p>~/.kube                                                                                                                      14:29:18 \u276f file /tmp/decoded-client.key /tmp/decoded-client.key: PEM RSA private key</p> <p>~/.kube                                                                                                                      14:29:22 \u276f # Using yq (if installed) cat /tmp/gatewayapi-kubeconfig.yaml | yq '.clusters[0].cluster.certificate-authority-data' | base64 -d &gt; /tmp/ca.crt cat /tmp/gatewayapi-kubeconfig.yaml | yq '.users[0].user.client-certificate-data' | base64 -d &gt; /tmp/client.crt cat /tmp/gatewayapi-kubeconfig.yaml | yq '.users[0].user.client-key-data' | base64 -d &gt; /tmp/client.key</p>"},{"location":"05-kubeconfig/notes/#using-grepsed-only","title":"Using grep/sed only","text":"<p>cat /tmp/gatewayapi-kubeconfig.yaml | grep -A1 -B1 'certificate-authority-data:' | tail -1 | base64 -d &gt; /tmp/ca2.crt zsh: unknown file attribute: i jq: error: authority/0 is not defined at , line 1: .clusters[0].cluster.certificate-authority-data jq: error: data/0 is not defined at , line 1: .clusters[0].cluster.certificate-authority-data jq: 2 compile errors jq: error: certificate/0 is not defined at , line 1: .users[0].user.client-certificate-data jq: error: data/0 is not defined at , line 1: .users[0].user.client-certificate-data jq: 2 compile errors jq: error: key/0 is not defined at , line 1: .users[0].user.client-key-data jq: error: data/0 is not defined at , line 1: .users[0].user.client-key-data jq: 2 compile errors zsh: command not found: # base64: invalid input <p>~/.kube                                                                                                                      14:29:36 \u276f</p>"},{"location":"06-cluster-certificates-and-kube-system%20%5Bimportant%5D/kubeadm-certs/","title":"kubeadm Certificate Checks &amp; Renewal","text":""},{"location":"06-cluster-certificates-and-kube-system%20%5Bimportant%5D/kubeadm-certs/#check-certificate-expiration","title":"Check Certificate Expiration","text":"<pre><code>kubeadm certs check-expiration\nkubeadm certs check-expiration -v=5\n</code></pre>"},{"location":"06-cluster-certificates-and-kube-system%20%5Bimportant%5D/kubeadm-certs/#renew-certificates","title":"Renew Certificates","text":"<pre><code>kubeadm certs renew all\nkubeadm certs renew all --dry-run\n</code></pre>"},{"location":"06-cluster-certificates-and-kube-system%20%5Bimportant%5D/kubeadm-certs/#renew-specific-certificate","title":"Renew Specific Certificate","text":"<pre><code>kubeadm certs renew apiserver\n</code></pre>"},{"location":"07-crd%27s/crd/","title":"Custom Resource Definitions (CRDs) and Custom Resources (CRs)","text":""},{"location":"07-crd%27s/crd/#what-crds-and-crs-are","title":"What CRDs and CRs are","text":"<ul> <li>CustomResourceDefinition (CRD) extends the Kubernetes API by defining a new resource type</li> <li>Custom Resource (CR) is an instance of that resource type</li> <li>CRDs are created once</li> <li>CRs are created many times</li> <li>A CR cannot exist unless its CRD already exists</li> <li>CRDs are always cluster-scoped</li> <li>CRs can be namespaced or cluster-scoped, depending on the CRD</li> </ul> <p>Analogy:</p> <ul> <li>Deployment API \u2192 Deployment objects</li> <li>CRD \u2192 Custom API</li> <li>CR \u2192 Objects using that API</li> </ul>"},{"location":"07-crd%27s/crd/#core-identity-of-any-kubernetes-resource","title":"Core identity of any Kubernetes resource","text":"<p>Every Kubernetes resource is uniquely identified by:</p> <pre><code>group + version + kind\n</code></pre> <p>Examples:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\n</code></pre> <pre><code>apiVersion: storage.example.com/v1\nkind: Backup\n</code></pre>"},{"location":"07-crd%27s/crd/#minimal-crd-structure-exam-ready-v1","title":"Minimal CRD structure (exam-ready, v1)","text":"<p>With <code>apiextensions.k8s.io/v1</code>, a schema is mandatory.</p> <pre><code>apiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  name: &lt;plural&gt;.&lt;group&gt;\nspec:\n  group: &lt;group&gt;\n  scope: Namespaced | Cluster\n  names:\n    plural: &lt;plural&gt;\n    singular: &lt;singular&gt;\n    kind: &lt;Kind&gt;\n  versions:\n    - name: v1\n      served: true\n      storage: true\n      schema:\n        openAPIV3Schema:\n          type: object\n</code></pre> <p>Critical rules:</p> <pre><code>metadata.name = plural.group\nExactly one version must have storage: true\nAt least one version must have served: true\n</code></pre>"},{"location":"07-crd%27s/crd/#meaning-of-crd-fields","title":"Meaning of CRD fields","text":"<ol> <li>group</li> <li>Logical API grouping</li> <li> <p>Used in CRD and CR <code>apiVersion</code></p> </li> <li> <p>version</p> </li> <li>API version</li> <li> <p>Appears after <code>/</code> in <code>apiVersion</code></p> </li> <li> <p>kind</p> </li> <li>Singular, capitalized resource name</li> <li> <p>Used in CR YAML</p> </li> <li> <p>plural</p> </li> <li> <p>Used in <code>kubectl</code> commands</p> </li> <li> <p>singular</p> </li> <li> <p>Optional <code>kubectl</code> usage</p> </li> <li> <p>scope</p> </li> <li>Determines whether CRs are namespaced or cluster-scoped</li> </ol>"},{"location":"07-crd%27s/crd/#served-and-storage-exam-critical","title":"<code>served</code> and <code>storage</code> (exam-critical)","text":""},{"location":"07-crd%27s/crd/#served","title":"served","text":"<pre><code>served: true\n</code></pre> <ul> <li>Determines whether this version is accessible via the API</li> <li>If <code>false</code>, users cannot create or read CRs using this version</li> </ul> <p>Mental model:</p> <pre><code>served = can users use this version?\n</code></pre>"},{"location":"07-crd%27s/crd/#storage","title":"storage","text":"<pre><code>storage: true\n</code></pre> <ul> <li>Determines which version is used to store objects in etcd</li> <li>Exactly one version must be <code>storage: true</code></li> <li>Other served versions are automatically converted to this version</li> </ul> <p>Mental model:</p> <pre><code>storage = how Kubernetes stores the object internally\n</code></pre>"},{"location":"07-crd%27s/crd/#namespaced-vs-cluster-scope","title":"Namespaced vs Cluster scope","text":""},{"location":"07-crd%27s/crd/#namespaced-crd","title":"Namespaced CRD","text":"<ul> <li>CR must include <code>metadata.namespace</code></li> <li>kubectl supports <code>-n</code> and <code>-A</code></li> </ul> <p>Example CR:</p> <pre><code>metadata:\n  name: daily-backup\n  namespace: default\n</code></pre> <p>Commands:</p> <pre><code>kubectl get backups -n default\nkubectl get backups -A\n</code></pre>"},{"location":"07-crd%27s/crd/#cluster-scoped-crd","title":"Cluster-scoped CRD","text":"<ul> <li>CR must NOT include <code>metadata.namespace</code></li> <li>kubectl does NOT support <code>-n</code> or <code>-A</code></li> </ul> <p>Example CR:</p> <pre><code>metadata:\n  name: sunday-window\n</code></pre>"},{"location":"07-crd%27s/crd/#example-1-namespaced-crd-backup","title":"Example 1: Namespaced CRD (Backup)","text":""},{"location":"07-crd%27s/crd/#customresourcedefinition","title":"CustomResourceDefinition","text":"<pre><code>apiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  name: backups.storage.example.com\nspec:\n  group: storage.example.com\n  scope: Namespaced\n  names:\n    plural: backups\n    singular: backup\n    kind: Backup\n  versions:\n    - name: v1\n      served: true\n      storage: true\n      schema:\n        openAPIV3Schema:\n          type: object\n</code></pre> <p>Create and verify:</p> <pre><code>kubectl apply -f backup-crd.yaml\nkubectl get crd backups.storage.example.com\n</code></pre> <p>Custom Resource</p> <pre><code>apiVersion: storage.example.com/v1\nkind: Backup\nmetadata:\n  name: daily-backup\n  namespace: default\n</code></pre>"},{"location":"07-crd%27s/crd/#example-2-cluster-scoped-crd-maintenancewindow","title":"Example 2: Cluster-scoped CRD (MaintenanceWindow)","text":""},{"location":"07-crd%27s/crd/#customresourcedefinition_1","title":"CustomResourceDefinition","text":"<p><pre><code>apiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  name: maintenancewindows.ops.example.com\nspec:\n  group: ops.example.com\n  scope: Cluster\n  names:\n    plural: maintenancewindows\n    singular: maintenancewindow\n    kind: MaintenanceWindow\n  versions:\n    - name: v1\n      served: true\n      storage: true\n      schema:\n        openAPIV3Schema:\n          type: object\n</code></pre> Custom Resource</p>"},{"location":"07-crd%27s/crd/#apiversion-opsexamplecomv1-kind-maintenancewindow-metadata-name-example-maintenancewindow","title":"<pre><code>apiVersion: ops.example.com/v1\nkind: MaintenanceWindow\nmetadata:\n  name: example-maintenancewindow\n</code></pre>","text":""},{"location":"07-crd%27s/crd/#example-3-namespaced-crd-with-operational-intent-scheduledbackup","title":"Example 3: Namespaced CRD with operational intent (ScheduledBackup)","text":"<p>This example models scheduled backups with retention and demonstrates validation.</p>"},{"location":"07-crd%27s/crd/#customresourcedefinition_2","title":"CustomResourceDefinition","text":"<pre><code>apiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  name: scheduledbackups.storage.example.com\nspec:\n  group: storage.example.com\n  scope: Namespaced\n  names:\n    plural: scheduledbackups\n    singular: scheduledbackup\n    kind: ScheduledBackup\n  versions:\n    - name: v1\n      served: true\n      storage: true\n      schema:\n        openAPIV3Schema:\n          type: object\n          properties:\n            spec:\n              type: object\n              required:\n                - schedule\n                - retention\n              properties:\n                schedule:\n                  type: string\n                retention:\n                  type: object\n                  required:\n                    - days\n                  properties:\n                    days:\n                      type: integer\n                      minimum: 1\n</code></pre> <p>Custom Resource</p>"},{"location":"07-crd%27s/crd/#apiversion-storageexamplecomv1-kind-scheduledbackup-metadata-name-daily-backup-namespace-default-spec-schedule-0-2-retention-days-7","title":"<pre><code>apiVersion: storage.example.com/v1\nkind: ScheduledBackup\nmetadata:\n  name: daily-backup\n  namespace: default\nspec:\n  schedule: \"0 2 * * *\"\n  retention:\n    days: 7\n</code></pre>","text":""},{"location":"07-crd%27s/crd/#adding-shortcuts-with-shortnames","title":"Adding shortcuts with <code>shortNames</code>","text":"<p>Shortcuts make kubectl usage faster.</p>"},{"location":"07-crd%27s/crd/#how-to-add","title":"How to add","text":"<p>CR <pre><code>names:\n  plural: featuretoggles\n  singular: featuretoggle\n  kind: FeatureToggle\n  shortNames:\n    - ft\n</code></pre></p>"},{"location":"07-crd%27s/crd/#usage","title":"Usage","text":"<pre><code>kubectl get ft\nkubectl describe ft generic -n default\nkubectl delete ft generic -n default\n</code></pre> <p>Short names are:</p> <ul> <li>Defined only in the CRD</li> <li>Optional</li> <li>Very useful in the exam</li> </ul>"},{"location":"07-crd%27s/crd/#creating-custom-resources-crs","title":"Creating Custom Resources (CRs)","text":"<p>Rules:</p> <ul> <li>CRD must exist first</li> <li>CR <code>apiVersion</code> must match CRD group/version</li> <li>CR <code>kind</code> must match CRD kind exactly</li> <li>Namespace must match scope</li> </ul> <p>Command:</p> <pre><code>kubectl apply -f cr.yaml\n</code></pre>"},{"location":"07-crd%27s/crd/#listing-describing-deleting-crs","title":"Listing, describing, deleting CRs","text":"<pre><code>kubectl get &lt;plural&gt;\nkubectl describe &lt;singular&gt; &lt;name&gt; -n &lt;namespace&gt;\nkubectl delete &lt;singular&gt; &lt;name&gt; -n &lt;namespace&gt;\n</code></pre>"},{"location":"07-crd%27s/crd/#order-of-operations-exam-critical","title":"Order of operations (exam critical)","text":"<p>Correct:</p> <ol> <li>Create CRD</li> <li>Verify CRD exists</li> <li>Create CR</li> </ol> <p>Incorrect:</p> <ul> <li>Creating CR before CRD</li> </ul> <p>Error:</p> <pre><code>no matches for kind \"&lt;Kind&gt;\" in version \"&lt;group&gt;/&lt;version&gt;\"\n</code></pre>"},{"location":"07-crd%27s/crd/#kubectl-mental-model","title":"kubectl mental model","text":"<ul> <li><code>kubectl get</code> \u2192 plural</li> <li>YAML <code>kind</code> \u2192 Kind</li> <li>YAML <code>apiVersion</code> \u2192 group/version</li> <li>CRD name \u2192 plural.group</li> </ul>"},{"location":"07-crd%27s/crd/#common-cka-mistakes","title":"Common CKA mistakes","text":"<ul> <li>Missing schema in v1 CRDs</li> <li>Wrong CRD name (not plural.group)</li> <li>apiVersion mismatch in CR</li> <li>Namespace used on cluster-scoped CR</li> <li>Missing namespace on namespaced CR</li> <li>Using singular with <code>kubectl get</code></li> <li>Creating CR before CRD</li> </ul>"},{"location":"07-crd%27s/crd/#final-exam-checklist","title":"Final exam checklist","text":"<ul> <li>CRD applied successfully</li> <li>Schema present for v1 CRDs</li> <li>metadata.name = plural.group</li> <li>Exactly one <code>storage: true</code></li> <li>At least one <code>served: true</code></li> <li>Scope respected</li> <li>Correct plural used in kubectl commands</li> </ul>"},{"location":"07-crd%27s/question/","title":"Question","text":""},{"location":"07-crd%27s/question/#question-1-reading-and-reasoning","title":"Question 1 \u2014 Reading and reasoning","text":"<p>You are given the following CRD:</p> <pre><code>apiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  name: accesspolicies.security.example.com\nspec:\n  group: security.example.com\n  scope: Namespaced\n  names:\n    plural: accesspolicies\n    singular: accesspolicy\n    kind: AccessPolicy\n  versions:\n    - name: v1\n      served: true\n      storage: true\n</code></pre> <p>Tasks:</p> <ol> <li>Write the correct <code>apiVersion</code> for a Custom Resource</li> <li>Write the correct <code>kubectl get</code> command</li> <li>State whether a namespace is required when creating a CR</li> <li>State the correct CRD name format</li> </ol>"},{"location":"07-crd%27s/question/#question-2-fix-the-broken-crd","title":"Question 2 \u2014 Fix the broken CRD","text":"<p>The following CRD fails to apply:</p> <pre><code>apiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  name: policies.security.example.com\nspec:\n  group: security.example.com\n  scope: Namespaced\n  names:\n    plural: accesspolicies\n    singular: accesspolicy\n    kind: AccessPolicy\n  versions:\n    - name: v1\n      served: true\n      storage: true\n</code></pre> <p>Tasks:</p> <ol> <li>Identify what is wrong</li> <li>Fix the CRD</li> <li>Explain why Kubernetes rejects the original</li> </ol>"},{"location":"07-crd%27s/question/#question-3-featuretoggle-crd","title":"Question 3 \u2014 FeatureToggle CRD","text":"<p>Create a namespaced CRD with:</p> <ul> <li>Kind: <code>FeatureToggle</code></li> <li>Group: <code>config.example.com</code></li> <li>Version: <code>v1</code></li> <li>Plural: <code>featuretoggles</code></li> <li> <p>Spec fields:</p> </li> <li> <p><code>enabled</code> (boolean)</p> </li> <li><code>description</code> (string)</li> </ul> <p>Then create a Custom Resource:</p> <ul> <li>Name: <code>new-ui</code></li> <li>Namespace: <code>default</code></li> <li>enabled: <code>true</code></li> </ul>"},{"location":"07-crd%27s/question/#question-4-schema-validation-failure","title":"Question 4 \u2014 Schema validation failure","text":"<p>You apply the following CR:</p> <pre><code>apiVersion: config.example.com/v1\nkind: FeatureToggle\nmetadata:\n  name: broken-toggle\n  namespace: default\nspec:\n  enabled: \"true\"\n</code></pre> <p>Tasks:</p> <ol> <li>Explain why this CR is rejected</li> <li>Identify the exact field causing the issue</li> <li>State the correct value type</li> </ol>"},{"location":"07-crd%27s/question/#question-5-kubectl-explain","title":"Question 5 \u2014 kubectl explain","text":"<p>Assume the CRD <code>featuretoggles.config.example.com</code> exists.</p> <p>Tasks:</p> <ol> <li>Inspect the Custom Resource using <code>kubectl explain</code></li> <li>Inspect the <code>spec.enabled</code> field</li> <li>Explain why this is useful in the exam</li> </ol>"},{"location":"07-crd%27s/question/#question-6-cluster-vs-namespaced-trap","title":"Question 6 \u2014 Cluster vs Namespaced trap","text":"<p>You are given a CRD with:</p> <pre><code>spec:\n  scope: Cluster\n</code></pre> <p>But the CR YAML contains:</p> <pre><code>metadata:\n  namespace: default\n</code></pre> <p>Tasks:</p> <ol> <li>State what will go wrong</li> <li>Explain how to fix it</li> <li>Identify which object is incorrect</li> </ol>"},{"location":"07-crd%27s/question/#question-7-fast-recognition-drill","title":"Question 7 \u2014 Fast recognition drill","text":"<p>Given:</p> <pre><code>CRD name: backups.storage.example.com\n</code></pre> <p>Answer:</p> <ol> <li>Group</li> <li>Plural</li> <li>Kind</li> <li><code>kubectl get</code> command</li> </ol>"},{"location":"07-crd%27s/solutions/","title":"Solutions","text":""},{"location":"07-crd%27s/solutions/#solutions","title":"Solutions","text":""},{"location":"07-crd%27s/solutions/#solution-1-reading-and-reasoning","title":"Solution 1 \u2014 Reading and reasoning","text":"<ol> <li>Custom Resource <code>apiVersion</code>:</li> </ol> <pre><code>apiVersion: security.example.com/v1\n</code></pre> <ol> <li>kubectl get command:</li> </ol> <pre><code>kubectl get accesspolicies\n</code></pre> <ol> <li> <p>Namespace requirement:</p> </li> <li> <p>Yes. The CRD scope is <code>Namespaced</code>, so <code>metadata.namespace</code> is required.</p> </li> <li> <p>Correct CRD name format:</p> </li> </ol> <pre><code>&lt;plural&gt;.&lt;group&gt; \u2192 accesspolicies.security.example.com\n</code></pre>"},{"location":"07-crd%27s/solutions/#solution-2-fix-the-broken-crd","title":"Solution 2 \u2014 Fix the broken CRD","text":"<ol> <li> <p>What is wrong:</p> </li> <li> <p><code>metadata.name</code> does not match <code>spec.names.plural + \".\" + spec.group</code>.</p> </li> <li> <p>Fixed CRD:</p> </li> </ol> <pre><code>metadata:\n  name: accesspolicies.security.example.com\n</code></pre> <ol> <li> <p>Why Kubernetes rejects the original:</p> </li> <li> <p>Kubernetes requires the CRD name to be exactly <code>plural.group</code>; otherwise the API cannot be registered.</p> </li> </ol>"},{"location":"07-crd%27s/solutions/#solution-3-featuretoggle-crd-and-cr","title":"Solution 3 \u2014 FeatureToggle CRD and CR","text":""},{"location":"07-crd%27s/solutions/#customresourcedefinition","title":"CustomResourceDefinition","text":"<pre><code>apiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  name: featuretoggles.config.example.com\nspec:\n  group: config.example.com\n  scope: Namespaced\n  names:\n    plural: featuretoggles\n    singular: featuretoggle\n    kind: FeatureToggle\n  versions:\n    - name: v1\n      served: true\n      storage: true\n      schema:\n        openAPIV3Schema:\n          type: object\n          properties:\n            spec:\n              type: object\n              properties:\n                enabled:\n                  type: boolean\n                description:\n                  type: string\n</code></pre>"},{"location":"07-crd%27s/solutions/#custom-resource","title":"Custom Resource","text":"<pre><code>apiVersion: config.example.com/v1\nkind: FeatureToggle\nmetadata:\n  name: new-ui\n  namespace: default\nspec:\n  enabled: true\n</code></pre>"},{"location":"07-crd%27s/solutions/#solution-4-schema-validation-failure","title":"Solution 4 \u2014 Schema validation failure","text":"<ol> <li> <p>Why the CR is rejected:</p> </li> <li> <p>The value type does not match the CRD schema.</p> </li> <li> <p>Field causing the issue:</p> </li> </ol> <pre><code>spec.enabled\n</code></pre> <ol> <li>Correct value type:</li> </ol> <pre><code>boolean (true or false)\n</code></pre>"},{"location":"07-crd%27s/solutions/#solution-5-kubectl-explain","title":"Solution 5 \u2014 kubectl explain","text":"<ol> <li>Inspect the Custom Resource:</li> </ol> <pre><code>kubectl explain featuretoggles\n</code></pre> <ol> <li>Inspect the <code>spec.enabled</code> field:</li> </ol> <pre><code>kubectl explain featuretoggles.spec.enabled\n</code></pre> <ol> <li> <p>Why this is useful in the exam:</p> </li> <li> <p>It shows field names, types, and descriptions without opening YAML files.</p> </li> </ol>"},{"location":"07-crd%27s/solutions/#solution-6-cluster-vs-namespaced-trap","title":"Solution 6 \u2014 Cluster vs Namespaced trap","text":"<ol> <li> <p>What will go wrong:</p> </li> <li> <p>The CR will be rejected because cluster-scoped resources cannot have a namespace.</p> </li> <li> <p>How to fix it:</p> </li> <li> <p>Remove <code>metadata.namespace</code> from the CR.</p> </li> <li> <p>Which object is incorrect:</p> </li> <li> <p>The Custom Resource (CR), not the CRD.</p> </li> </ol>"},{"location":"07-crd%27s/solutions/#solution-7-fast-recognition-drill","title":"Solution 7 \u2014 Fast recognition drill","text":"<p>Given:</p> <pre><code>CRD name: backups.storage.example.com\n</code></pre> <ol> <li>Group:     <pre><code>storage.example.com\n</code></pre></li> <li> <p>Plural:     <pre><code>backups\n</code></pre></p> </li> <li> <p>Kind:     <pre><code>Backup\n</code></pre></p> </li> <li> <p>kubectl get command:     <pre><code>kubectl get backups\n</code></pre></p> </li> </ol>"},{"location":"08-gateway-api/gateway-api/","title":"Kubernetes Gateway API","text":""},{"location":"08-gateway-api/gateway-api/#core-architecture","title":"Core Architecture","text":""},{"location":"08-gateway-api/gateway-api/#three-tier-model","title":"Three-Tier Model:","text":"<ol> <li>GatewayClass \u2192 Which controller implementation to use</li> <li>Gateway \u2192 Actual load balancer with listeners  </li> <li>HTTPRoute/TCPRoute/TLSRoute \u2192 Routing rules and logic</li> </ol>"},{"location":"08-gateway-api/gateway-api/#complete-yaml-examples-all-fields-explained","title":"Complete YAML Examples (All Fields Explained)","text":""},{"location":"08-gateway-api/gateway-api/#gatewayclass-all-possible-fields","title":"GatewayClass - All Possible Fields:","text":"<pre><code>apiVersion: gateway.networking.k8s.io/v1\nkind: GatewayClass\nmetadata:\n  name: traefik\nspec:\n  # REQUIRED: Which controller implements this\n  controllerName: \"traefik.io/gateway-controller\"\n\n  # OPTIONAL: Description\n  description: \"Traefik GatewayClass for production\"\n\n  # OPTIONAL: Controller-specific parameters\n  parametersRef:\n    name: traefik-config\n    group: traefik.io\n    kind: GatewayClassConfig\n</code></pre>"},{"location":"08-gateway-api/gateway-api/#gateway-complete-with-all-listeners","title":"Gateway - Complete with All Listeners:","text":"<pre><code>apiVersion: gateway.networking.k8s.io/v1\nkind: Gateway\nmetadata:\n  name: traefik\nspec:\n  # REQUIRED: Which GatewayClass to use\n  gatewayClassName: traefik\n\n  # REQUIRED: At least one listener\n  listeners:\n  - name: http-public\n    port: 80\n    protocol: HTTP\n\n    # OPTIONAL: Restrict hostnames\n    hostname: \"*.example-app.com\"\n\n    # OPTIONAL: TLS (for HTTPS/TLS listeners only)\n    # tls:\n    #   mode: Terminate\n    #   certificateRefs: []\n\n    # OPTIONAL: Which namespaces can attach routes\n    allowedRoutes:\n      namespaces:\n        from: Same  # Same, All, or Selector\n        # selector:  # When from: Selector\n        #   matchLabels:\n        #     shared-gateway: \"true\"\n\n  # HTTPS Listener Example  \n  - name: https-secure\n    port: 443\n    protocol: HTTPS\n\n    hostname: \"example-app.com\"\n\n    # REQUIRED for HTTPS: TLS configuration\n    tls:\n      mode: Terminate  # or Passthrough\n      certificateRefs:\n      - name: secret-tls\n        kind: Secret\n        group: \"\"\n      # options:  # Optional TLS options\n      #   cipherSuites: []\n      #   minVersion: \"TLSv1.2\"\n\n    allowedRoutes:\n      namespaces:\n        from: All\n\n  # TCP Listener Example (Experimental)\n  - name: tcp-database\n    port: 5432\n    protocol: TCP\n\n    allowedRoutes:\n      namespaces:\n        from: Same\n</code></pre> <p>Listener Protocol Types: - <code>HTTP</code> \u2192 For HTTPRoute - <code>HTTPS</code> \u2192 For HTTPRoute with TLS - <code>TLS</code> \u2192 For TLSRoute (passthrough) - <code>TCP</code> \u2192 For TCPRoute - <code>UDP</code> \u2192 For UDPRoute (experimental)</p> <p>TLS Modes: - <code>Terminate</code> \u2192 TLS ends at Gateway (decrypts) - <code>Passthrough</code> \u2192 TLS continues to backend</p>"},{"location":"08-gateway-api/gateway-api/#httproute-complete-with-all-features","title":"HTTPRoute - Complete with All Features","text":""},{"location":"08-gateway-api/gateway-api/#route-by-hostname-from-readme","title":"Route by Hostname (From README):","text":"<pre><code>apiVersion: gateway.networking.k8s.io/v1\nkind: HTTPRoute\nmetadata:\n  name: go\nspec:\n  # REQUIRED: Attach to which Gateway\n  parentRefs:\n  - name: traefik\n    # namespace: default  # If Gateway in different namespace\n    # sectionName: web    # Target specific listener\n    # port: 80           # Target specific port\n\n  # OPTIONAL: Which hostnames to match\n  hostnames:\n  - \"example-app-go.com\"\n  - \"*.test.example-app-go.com\"  # Wildcard subdomains\n\n  # REQUIRED: At least one rule\n  rules:\n  - matches:\n    # Path matching (choose one type)\n    - path:\n        type: PathPrefix  # Exact, PathPrefix, or RegularExpression\n        value: \"/\"\n\n    # OPTIONAL: Header matching\n    # headers:\n    # - type: Exact  # or RegularExpression\n    #   name: \"X-API-Version\"\n    #   value: \"v2\"\n\n    # OPTIONAL: Query parameter matching  \n    # queryParams:\n    # - type: Exact\n    #   name: \"debug\"\n    #   value: \"true\"\n\n    # OPTIONAL: HTTP method matching\n    # method: \"GET\"  # GET, POST, PUT, DELETE, etc.\n\n    # OPTIONAL: Request filters (applied in order)\n    filters:\n    # - type: RequestHeaderModifier\n    #   requestHeaderModifier:\n    #     set:    # Overwrite if exists\n    #     - name: \"X-Request-ID\"\n    #       value: \"{{uuid}}\"\n    #     add:    # Add if not exists\n    #     - name: \"X-Forwarded-For\"\n    #       value: \"$remote_addr\"\n    #     remove: # Remove headers\n    #     - \"X-Secret-Header\"\n\n    # - type: ResponseHeaderModifier\n    #   responseHeaderModifier:\n    #     add:\n    #     - name: \"Cache-Control\"\n    #       value: \"max-age=3600\"\n\n    # - type: RequestRedirect\n    #   requestRedirect:\n    #     scheme: \"https\"\n    #     hostname: \"secure.example.com\"\n    #     port: 443\n    #     statusCode: 301\n\n    # - type: URLRewrite\n    #   urlRewrite:\n    #     path:\n    #       type: ReplacePrefixMatch  # or ReplaceFullPath\n    #       replacePrefixMatch: \"/v2\"\n\n    # - type: RequestMirror\n    #   requestMirror:\n    #     backendRef:\n    #       name: audit-service\n    #       port: 8080\n\n    # - type: ExtensionRef\n    #   extensionRef:\n    #     name: custom-filter\n    #     kind: CustomFilter\n    #     group: example.com\n\n    # REQUIRED: Where to send traffic\n    backendRefs:\n    - name: go-svc\n      port: 5000\n      # weight: 100  # For traffic splitting (0-100)\n      # filters: []  # Backend-specific filters\n\n    # Multiple backends for traffic splitting\n    # - name: go-svc-v2\n    #   port: 5000\n    #   weight: 20\n</code></pre>"},{"location":"08-gateway-api/gateway-api/#route-by-path-exact-match","title":"Route by Path - Exact Match:","text":"<pre><code>apiVersion: gateway.networking.k8s.io/v1\nkind: HTTPRoute\nmetadata:\n  name: python-exact\nspec:\n  parentRefs:\n  - name: traefik\n  hostnames:\n  - \"example-app-python.com\"\n  rules:\n  - matches:\n    - path:\n        type: Exact  # Only \"/\" matches\n        value: \"/\"\n    backendRefs:\n    - name: python-svc\n      port: 5000\n</code></pre>"},{"location":"08-gateway-api/gateway-api/#route-by-path-prefix-match","title":"Route by Path - Prefix Match:","text":"<pre><code>apiVersion: gateway.networking.k8s.io/v1\nkind: HTTPRoute\nmetadata:\n  name: python-prefix\nspec:\n  parentRefs:\n  - name: traefik\n  hostnames:\n  - \"example-app-python.com\"\n  rules:\n  - matches:\n    - path:\n        type: PathPrefix  # \"/\", \"/api\", \"/api/users\" all match\n        value: \"/\"\n    backendRefs:\n    - name: python-svc\n      port: 5000\n</code></pre>"},{"location":"08-gateway-api/gateway-api/#url-rewrite-pattern-api-gateway","title":"URL Rewrite Pattern (API Gateway):","text":"<pre><code>apiVersion: gateway.networking.k8s.io/v1\nkind: HTTPRoute\nmetadata:\n  name: api-gateway\nspec:\n  parentRefs:\n  - name: traefik\n  hostnames:\n  - \"example-app.com\"\n  rules:\n  # Python service\n  - matches:\n    - path:\n        type: PathPrefix\n        value: \"/api/python\"\n    filters:\n    - type: URLRewrite\n      urlRewrite:\n        path:\n          type: ReplacePrefixMatch\n          replacePrefixMatch: \"/\"\n    backendRefs:\n    - name: python-svc\n      port: 5000\n\n  # Go service\n  - matches:\n    - path:\n        type: PathPrefix\n        value: \"/api/go\"\n    filters:\n    - type: URLRewrite\n      urlRewrite:\n        path:\n          type: ReplacePrefixMatch\n          replacePrefixMatch: \"/\"\n    backendRefs:\n    - name: go-svc\n      port: 5000\n</code></pre> <p>URL Rewrite Types: - <code>ReplacePrefixMatch</code> \u2192 Replace matched prefix only - <code>ReplaceFullPath</code> \u2192 Replace entire path</p>"},{"location":"08-gateway-api/gateway-api/#header-modification-cors-example","title":"Header Modification (CORS Example):","text":"<pre><code>apiVersion: gateway.networking.k8s.io/v1\nkind: HTTPRoute\nmetadata:\n  name: go-cors\nspec:\n  parentRefs:\n  - name: traefik\n  hostnames:\n  - \"example-app.com\"\n  rules:\n  - matches:\n    - path:\n        type: PathPrefix\n        value: \"/api/go\"\n    filters:\n    - type: ResponseHeaderModifier\n      responseHeaderModifier:\n        add:\n        - name: \"Access-Control-Allow-Origin\"\n          value: \"*\"\n        - name: \"Access-Control-Allow-Methods\"\n          value: \"GET, POST, PUT, DELETE, OPTIONS\"\n        - name: \"Access-Control-Allow-Headers\"\n          value: \"Content-Type, Authorization\"\n        - name: \"Access-Control-Max-Age\"\n          value: \"86400\"\n    - type: URLRewrite\n      urlRewrite:\n        path:\n          type: ReplacePrefixMatch\n          replacePrefixMatch: \"/\"\n    backendRefs:\n    - name: go-svc\n      port: 5000\n</code></pre>"},{"location":"08-gateway-api/gateway-api/#httpstls-configuration","title":"HTTPS/TLS Configuration:","text":"<pre><code># Updated Gateway with TLS\napiVersion: gateway.networking.k8s.io/v1\nkind: Gateway\nmetadata:\n  name: traefik\nspec:\n  gatewayClassName: traefik\n  listeners:\n  - name: web\n    port: 80\n    protocol: HTTP\n    hostname: \"*.example-app.com\"\n\n  - name: web-secure\n    port: 443\n    protocol: HTTPS\n    hostname: \"*.example-app.com\"\n    tls:\n      mode: Terminate\n      certificateRefs:\n      - name: secret-tls\n\n# HTTPRoute targeting TLS listener\napiVersion: gateway.networking.k8s.io/v1\nkind: HTTPRoute\nmetadata:\n  name: go-tls\nspec:\n  parentRefs:\n  - name: traefik\n    sectionName: web-secure  # Target HTTPS listener\n  hostnames:\n  - \"example-app.com\"\n  rules:\n  - matches:\n    - path:\n        type: PathPrefix\n        value: \"/api/go\"\n    filters:\n    - type: URLRewrite\n      urlRewrite:\n        path:\n          type: ReplacePrefixMatch\n          replacePrefixMatch: \"/\"\n    backendRefs:\n    - name: go-svc\n      port: 5000\n</code></pre>"},{"location":"08-gateway-api/gateway-api/#other-route-types-experimental","title":"Other Route Types (Experimental)","text":""},{"location":"08-gateway-api/gateway-api/#tcproute","title":"TCPRoute:","text":"<pre><code>apiVersion: gateway.networking.k8s.io/v1alpha2\nkind: TCPRoute\nmetadata:\n  name: postgres-route\nspec:\n  parentRefs:\n  - name: traefik\n  rules:\n  - backendRefs:\n    - name: postgres-service\n      port: 5432\n</code></pre>"},{"location":"08-gateway-api/gateway-api/#tlsroute","title":"TLSRoute:","text":"<pre><code>apiVersion: gateway.networking.k8s.io/v1alpha2\nkind: TLSRoute\nmetadata:\n  name: tls-passthrough\nspec:\n  parentRefs:\n  - name: traefik\n  hostnames:\n  - \"secure.example-app.com\"\n  rules:\n  - backendRefs:\n    - name: backend-service\n      port: 8443\n</code></pre>"},{"location":"08-gateway-api/gateway-api/#supporting-resources","title":"Supporting Resources","text":""},{"location":"08-gateway-api/gateway-api/#tls-secret","title":"TLS Secret:","text":"<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: secret-tls\ntype: kubernetes.io/tls\ndata:\n  tls.crt: BASE64_ENCODED_CERT\n  tls.key: BASE64_ENCODED_KEY\n</code></pre>"},{"location":"08-gateway-api/gateway-api/#referencegrant-cross-namespace","title":"ReferenceGrant (Cross-Namespace):","text":"<pre><code>apiVersion: gateway.networking.k8s.io/v1beta1\nkind: ReferenceGrant\nmetadata:\n  name: allow-routes\n  namespace: gateway-namespace\nspec:\n  from:\n  - group: gateway.networking.k8s.io\n    kind: HTTPRoute\n    namespace: app-namespace\n\n  to:\n  - group: gateway.networking.k8s.io\n    kind: Gateway\n    name: shared-gateway\n</code></pre>"},{"location":"08-gateway-api/gateway-api/#testing-commands","title":"Testing Commands","text":""},{"location":"08-gateway-api/gateway-api/#local-testing-with-kind","title":"Local Testing with kind:","text":"<pre><code># Update /etc/hosts\n127.0.0.1 example-app.com\n127.0.0.1 example-app-go.com\n127.0.0.1 example-app-python.com\n\n# Port forward\nkubectl -n traefik port-forward svc/traefik 80\nkubectl -n traefik port-forward svc/traefik 443\n\n# Test HTTP\ncurl -H \"Host: example-app-go.com\" http://localhost\ncurl -H \"Host: example-app-python.com\" http://localhost\ncurl -H \"Host: example-app.com\" http://localhost/api/go\n\n# Test HTTPS\ncurl -k -H \"Host: example-app.com\" https://localhost:443/api/go\n</code></pre>"},{"location":"08-gateway-api/gateway-api/#key-concepts","title":"Key Concepts","text":""},{"location":"08-gateway-api/gateway-api/#traffic-flow","title":"Traffic Flow:","text":"<pre><code>Request \u2192 Gateway (listener) \u2192 HTTPRoute (match) \u2192 Filters \u2192 Backend Service\n</code></pre>"},{"location":"08-gateway-api/gateway-api/#required-fields","title":"Required Fields:","text":"<ul> <li>GatewayClass: <code>controllerName</code></li> <li>Gateway: <code>gatewayClassName</code>, <code>listeners</code> (with <code>name</code>, <code>port</code>, <code>protocol</code>)</li> <li>HTTPRoute: <code>parentRefs</code>, <code>rules</code> (with <code>backendRefs</code>)</li> </ul>"},{"location":"08-gateway-api/gateway-api/#optional-but-important","title":"Optional But Important:","text":"<ul> <li><code>hostnames</code> \u2192 Virtual hosting</li> <li><code>tls</code> \u2192 HTTPS/TLS</li> <li><code>allowedRoutes</code> \u2192 Security boundaries</li> <li><code>filters</code> \u2192 Transformations</li> <li><code>sectionName</code> \u2192 Target specific listener</li> </ul>"},{"location":"08-gateway-api/gateway-api/#path-matching-types","title":"Path Matching Types:","text":"<ul> <li><code>Exact</code> \u2192 Only exact path</li> <li><code>PathPrefix</code> \u2192 Path and everything under it</li> <li><code>RegularExpression</code> \u2192 Regex pattern</li> </ul>"},{"location":"08-gateway-api/gateway-api/#filter-types","title":"Filter Types:","text":"<ol> <li><code>RequestHeaderModifier</code> \u2192 Modify request headers</li> <li><code>ResponseHeaderModifier</code> \u2192 Modify response headers</li> <li><code>URLRewrite</code> \u2192 Rewrite URL path</li> <li><code>RequestRedirect</code> \u2192 Redirect to different URL</li> <li><code>RequestMirror</code> \u2192 Mirror traffic</li> <li><code>ExtensionRef</code> \u2192 Custom filters</li> </ol>"},{"location":"08-gateway-api/gateway-api/#infrastructure-labels","title":"Infrastructure Labels","text":"<pre><code>apiVersion: gateway.networking.k8s.io/v1\nkind: Gateway\nmetadata:\n  name: cloud-gateway\n  labels:\n    gateway.networking.k8s.io/infrastructure: \"aws-nlb\"\n    environment: \"production\"\n  annotations:\n    service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\"\n    service.beta.kubernetes.io/aws-load-balancer-scheme: \"internet-facing\"\n</code></pre> <p>Purpose: Propagate labels/annotations to cloud infrastructure.</p>"},{"location":"08-gateway-api/gateway-api/#available-controllers","title":"Available Controllers","text":"<ul> <li>Traefik \u2192 Used in README, simple setup</li> <li>Envoy \u2192 High-performance proxy</li> <li>Istio \u2192 Service mesh integration</li> <li>NGINX Fabric \u2192 NGINX-based implementation</li> </ul>"},{"location":"08-gateway-api/question/","title":"Gateway API Practice Questions","text":""},{"location":"08-gateway-api/question/#1-simple-host-based-routing","title":"1. Simple Host-Based Routing","text":"<p>You need to route traffic based on hostnames to different services: - <code>app.example.com</code> \u2192 <code>frontend-service:8080</code> - <code>api.example.com</code> \u2192 <code>backend-service:3000</code></p> <p>Both should use the existing Gateway named <code>main-gateway</code> on port 80.</p> <p>Task: Write the HTTPRoute YAML(s) to implement this.</p>"},{"location":"08-gateway-api/question/#2-path-rewriting-api-gateway-pattern","title":"2. Path Rewriting (API Gateway Pattern)","text":"<p>You have a single domain <code>services.company.com</code> with these requirements: - <code>/shop/products</code> \u2192 <code>product-service:8080</code> (should receive <code>/products</code>) - <code>/shop/cart</code> \u2192 <code>cart-service:9090</code> (should receive <code>/cart</code>) - <code>/auth/login</code> \u2192 <code>auth-service:7070</code> (should receive <code>/login</code>)</p> <p>Task: Create a single HTTPRoute that rewrites paths to remove the <code>/shop</code> and <code>/auth</code> prefixes.</p>"},{"location":"08-gateway-api/question/#3-path-redirects-migration","title":"3. Path Redirects (Migration)","text":"<p>You're migrating from an old path structure to a new one: - <code>/old/dashboard</code> \u2192 redirect to <code>/new/dashboard</code> (301 permanent) - <code>/old/api/v1</code> \u2192 redirect to <code>/api/v2</code> (301 permanent) - <code>/temp/redirect</code> \u2192 redirect to <code>/new/temp</code> (302 temporary)</p> <p>Task: Write an HTTPRoute that handles these redirects.</p>"},{"location":"08-gateway-api/question/#4-tls-configuration-with-multiple-hostnames","title":"4. TLS Configuration with Multiple Hostnames","text":"<p>You have a Gateway <code>secure-gateway</code> with an HTTPS listener on port 443. You need to: 1. Configure TLS with a certificate secret named <code>wildcard-tls</code> 2. Route traffic for <code>app.company.com</code> and <code>api.company.com</code> to their respective services 3. Ensure only HTTPS traffic is accepted (no HTTP)</p> <p>Task: Write both the Gateway update and HTTPRoute(s) for TLS configuration.</p>"},{"location":"08-gateway-api/question/#5-exact-vs-prefix-path-matching","title":"5. Exact vs Prefix Path Matching","text":"<p>You have these specific routing requirements: - <code>/api/health</code> (exact) \u2192 <code>health-check:8080</code> - <code>/api/users</code> (prefix) \u2192 <code>user-service:3000</code> (matches <code>/api/users</code>, <code>/api/users/123</code>, etc.) - <code>/admin</code> (exact) \u2192 <code>admin-panel:9000</code> - <code>/admin/config</code> (exact) \u2192 <code>config-service:4000</code> (higher priority than <code>/admin</code> prefix)</p> <p>Task: Create an HTTPRoute with proper path matching types and rule ordering.</p>"},{"location":"10-scheduling/nodename/","title":"NodeName","text":""},{"location":"10-scheduling/nodename/#notes-on-nodename","title":"Notes on <code>nodeName</code>","text":"<p>What it does: - Forces pod to specific node by name - Skips normal scheduler logic - Direct assignment only</p> <p>Example: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: example-pod\nspec:\n  nodeName: target-node-1\n  containers:\n  - name: main\n    image: nginx\n</code></pre></p> <p>Use cases: - Hardware-specific nodes - Local storage access - Testing purposes</p> <p>Limitations: - No failover if node fails - Pod stays Pending if node missing - No resource validation</p> <p>Check placement: <pre><code>kubectl get pod node-monitor -o wide\n</code></pre></p>"},{"location":"10-scheduling/nodename/#question","title":"Question","text":"<p>You have a 3-node Kubernetes cluster: - <code>control-plane</code> - <code>node-01</code> - <code>node-02</code></p> <p>You need to deploy a monitoring pod that must run on <code>node-01</code> because it has special monitoring tools installed.</p> <p>Write a Pod YAML manifest that: 1. Pod name: <code>node-monitor</code> 2. Image: <code>ubuntu:latest</code> 3. Command: <code>[\"sleep\", \"infinity\"]</code> 4. Label: <code>monitor: system</code> 5. Force to node: <code>node-01</code></p>"},{"location":"10-scheduling/nodeselector/","title":"NodeSelector","text":""},{"location":"10-scheduling/nodeselector/#what-is-nodeselector","title":"What is nodeSelector?","text":"<p><code>nodeSelector</code> is a field in a Pod spec that selects nodes based on their labels. Unlike <code>nodeName</code> which bypasses the scheduler, <code>nodeSelector</code> works with the scheduler to match pods to nodes with specific labels.</p>"},{"location":"10-scheduling/nodeselector/#how-it-works","title":"How It Works","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: example-pod\nspec:\n  nodeSelector:          # \u2190 This is the key field\n    disktype: ssd        # Node must have label disktype=ssd\n    region: us-east      # Node must have label region=us-east\n  containers:\n  - name: main\n    image: nginx\n</code></pre> <p>The scheduler will only place this pod on nodes that have ALL the labels specified in <code>nodeSelector</code>.</p>"},{"location":"10-scheduling/nodeselector/#key-differences-nodeselector-vs-nodename","title":"Key Differences: nodeSelector vs nodeName","text":"Feature <code>nodeName</code> <code>nodeSelector</code> Scheduler Bypasses scheduler Works with scheduler Flexibility Hardcoded node name Uses labels (more flexible) Failover No failover Can reschedule to other matching nodes Validation No resource checking Normal scheduler validation Production Use Rarely used Commonly used"},{"location":"10-scheduling/nodeselector/#step-by-step-usage","title":"Step-by-Step Usage","text":""},{"location":"10-scheduling/nodeselector/#1-label-your-nodes","title":"1. Label Your Nodes","text":"<pre><code># Add labels to nodes\nkubectl label nodes node-01 disktype=ssd\nkubectl label nodes node-01 region=us-east\n\nkubectl label nodes node-02 disktype=hdd\nkubectl label nodes node-02 region=us-west\n\n# Verify labels\nkubectl get nodes --show-labels\n</code></pre>"},{"location":"10-scheduling/nodeselector/#2-create-pod-with-nodeselector","title":"2. Create Pod with nodeSelector","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: web-app\nspec:\n  nodeSelector:\n    disktype: ssd\n    region: us-east\n  containers:\n  - name: app\n    image: nginx:alpine\n</code></pre>"},{"location":"10-scheduling/nodeselector/#3-verify-scheduling","title":"3. Verify Scheduling","text":"<pre><code># Check which node the pod landed on\nkubectl get pod web-app -o wide\n\n# Describe the pod to see scheduling details\nkubectl describe pod web-app\n</code></pre>"},{"location":"10-scheduling/nodeselector/#common-use-cases","title":"Common Use Cases","text":"<ol> <li> <p>Hardware Requirements <pre><code>nodeSelector:\n  gpu: \"true\"\n  memory: \"high\"\n</code></pre></p> </li> <li> <p>Environment/Region <pre><code>nodeSelector:\n  environment: production\n  zone: zone-a\n</code></pre></p> </li> <li> <p>Team/Project Isolation <pre><code>nodeSelector:\n  team: data-science\n  project: ml-training\n</code></pre></p> </li> </ol>"},{"location":"10-scheduling/nodeselector/#limitations","title":"Limitations","text":"<ol> <li>AND Logic Only: All labels must match (cannot do OR logic)</li> <li>No Preferences: Cannot say \"prefer SSD, but HDD is okay\" </li> <li>No Complex Rules: Cannot specify \"not equal to\" or \"exists\" operators</li> </ol>"},{"location":"10-scheduling/nodeselector/#best-practices","title":"Best Practices","text":"<ol> <li>Use Meaningful Labels: <code>disktype:ssd</code> instead of <code>type:1</code></li> <li>Document Labels: Keep a list of node labels your team uses</li> <li>Combine with Resources: Use with resource requests/limits</li> <li>Test Label Matching: Verify labels exist before deploying</li> </ol>"},{"location":"10-scheduling/nodeselector/#troubleshooting","title":"Troubleshooting","text":"<p>Pod Stays Pending? <pre><code># Check if any nodes have the required labels\nkubectl get nodes -l disktype=ssd,region=us-east\n\n# Check pod events\nkubectl describe pod [pod-name]\n\n# Check node capacity\nkubectl describe node [node-name]\n</code></pre></p> <p>Remove Labels: <pre><code>kubectl label node node-01 disktype-\n</code></pre></p>"},{"location":"10-scheduling/nodeselector/#practice-question-2","title":"Practice Question 2","text":"<p>You have a Kubernetes cluster with the following nodes and labels:</p> <pre><code>Node: worker-1\n  Labels: environment=production, storage=ssd, zone=us-east-1a\n\nNode: worker-2  \n  Labels: environment=staging, storage=hdd, zone=us-east-1b\n\nNode: worker-3\n  Labels: environment=production, storage=ssd, zone=us-east-1b\n</code></pre> <p>Create a Pod YAML for a database application that: 1. Pod name: <code>production-db</code> 2. Image: <code>postgres:14</code> 3. Label the pod: <code>app: database</code> 4. Environment variable: <code>POSTGRES_PASSWORD: secret123</code> 5. Must run only on nodes with:    - <code>environment=production</code>    - <code>storage=ssd</code> 6. Container port: <code>5432</code></p> <p>Write your YAML solution below:</p>"},{"location":"10-scheduling/podAffinity-nodeAffinity-deployments/notes/","title":"Kubernetes Scheduling: Pod &amp; Node Affinity - Theory &amp; Practice","text":""},{"location":"10-scheduling/podAffinity-nodeAffinity-deployments/notes/#1-theoretical-foundation-how-affinity-works","title":"1. Theoretical Foundation: How Affinity Works","text":""},{"location":"10-scheduling/podAffinity-nodeAffinity-deployments/notes/#key-concepts","title":"Key Concepts:","text":"<ul> <li>Labels: Key-value pairs attached to Kubernetes objects (nodes, pods)</li> <li>Node Affinity: Rules based on NODE LABELS</li> <li>Pod Affinity: Rules based on EXISTING POD LABELS</li> </ul>"},{"location":"10-scheduling/podAffinity-nodeAffinity-deployments/notes/#node-affinity-operators-explained","title":"Node Affinity Operators Explained:","text":"Operator What It Does Example When to Use <code>In</code> Value must be in list <code>zone</code> in [\"us-east-1a\", \"us-east-1b\"] Specific values allowed <code>NotIn</code> Value must NOT be in list <code>node-type</code> not in [\"spot\", \"preemptible\"] Exclude certain node types <code>Exists</code> Label key must exist <code>gpu</code> exists Check for feature presence <code>DoesNotExist</code> Label key must NOT exist <code>maintenance</code> does not exist Avoid problematic nodes <code>Gt</code>, <code>Lt</code> Numeric comparison <code>memory-gb</code> &gt; 16 Resource-based selection"},{"location":"10-scheduling/podAffinity-nodeAffinity-deployments/notes/#important-theory-points","title":"Important Theory Points:","text":"<ol> <li>NodeSelectorTerms Logic:</li> <li>Within a term: All conditions must be true (AND logic)</li> <li> <p>Between terms: Any term can be true (OR logic)</p> </li> <li> <p>Required vs Preferred:</p> </li> <li><code>requiredDuringScheduling</code>: MUST be satisfied (hard rule)</li> <li> <p><code>preferredDuringScheduling</code>: SHOULD be satisfied (soft rule, weighted 1-100)</p> </li> <li> <p><code>IgnoredDuringExecution</code>: Once scheduled, rules aren't re-evaluated if labels change</p> </li> </ol>"},{"location":"10-scheduling/podAffinity-nodeAffinity-deployments/notes/#2-pod-affinity-theory-relationships-between-pods","title":"2. Pod Affinity Theory: Relationships Between Pods","text":""},{"location":"10-scheduling/podAffinity-nodeAffinity-deployments/notes/#three-types-of-pod-relationships","title":"Three Types of Pod Relationships:","text":"<ol> <li>Pod Affinity: \"Run near these pods\" (attraction)</li> <li>Pod Anti-Affinity: \"Don't run near these pods\" (repulsion)</li> <li>Topology: Defines what \"near\" means</li> </ol>"},{"location":"10-scheduling/podAffinity-nodeAffinity-deployments/notes/#topology-keys-theory","title":"Topology Keys Theory:","text":"<p>A topology key is a node label that defines a grouping domain:</p> <ul> <li><code>kubernetes.io/hostname</code> = Different physical/virtual machines</li> <li><code>topology.kubernetes.io/zone</code> = Different availability zones</li> <li><code>topology.kubernetes.io/region</code> = Different geographic regions</li> <li>Custom keys = Your own grouping logic (e.g., <code>rack</code>, <code>row</code>, <code>datacenter</code>)</li> </ul>"},{"location":"10-scheduling/podAffinity-nodeAffinity-deployments/notes/#how-pod-affinity-works","title":"How Pod Affinity Works:","text":"<p><pre><code>podAntiAffinity:\n  requiredDuringSchedulingIgnoredDuringExecution:\n  - labelSelector:\n      matchLabels:\n        app: web  # Look for pods with this label\n    topologyKey: kubernetes.io/hostname  # Group by node\n</code></pre> Translation: \"Don't schedule this pod on any node that already has a pod with label <code>app=web</code>\"</p>"},{"location":"10-scheduling/podAffinity-nodeAffinity-deployments/notes/#3-complete-practical-example-with-theory-applied","title":"3. Complete Practical Example with Theory Applied","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: example-app\nspec:\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: example\n        tier: backend\n    spec:\n      affinity:\n        # NODE AFFINITY: Where can pods go?\n        nodeAffinity:\n          # HARD REQUIREMENT: Must be satisfied\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            # Term 1: Production environment\n            - matchExpressions:\n              - key: environment\n                operator: In\n                values: [\"production\"]\n              - key: available\n                operator: Exists\n            # Term 2: OR staging with SSD\n            - matchExpressions:\n              - key: environment\n                operator: In\n                values: [\"staging\"]\n              - key: storage-type\n                operator: In\n                values: [\"ssd\"]\n\n          # SOFT PREFERENCE: Try to satisfy\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 90  # Higher weight = more important\n            preference:\n              matchExpressions:\n              - key: zone\n                operator: In\n                values: [\"us-east-1a\"]\n\n        # POD ANTI-AFFINITY: How pods relate to each other\n        podAntiAffinity:\n          # HARD: Don't put pods on same node\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                app: example  # Look for our own pods\n            topologyKey: kubernetes.io/hostname  # Different nodes\n\n          # SOFT: Try to spread across zones\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 70\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app: example\n              topologyKey: topology.kubernetes.io/zone  # Different zones\n\n      containers:\n      - name: app\n        image: nginx\n</code></pre> <p>Theory Applied: 1. Node Selection: Pods can run in production (any storage) OR staging (only SSD) 2. Preference: Prefer zone us-east-1a if possible 3. Pod Placement: Never on same node, try for different zones 4. Result: High availability across failure domains</p>"},{"location":"10-scheduling/podAffinity-nodeAffinity-deployments/notes/#4-when-to-use-each-feature-decision-guide","title":"4. When to Use Each Feature - Decision Guide","text":""},{"location":"10-scheduling/podAffinity-nodeAffinity-deployments/notes/#use-node-affinity-when","title":"Use Node Affinity When:","text":"<ul> <li>Pod needs specific hardware (GPU, SSD, memory)</li> <li>Pod needs to run in specific region/zone</li> <li>You want to dedicate nodes to certain workloads</li> <li>You need to exclude certain node types</li> </ul>"},{"location":"10-scheduling/podAffinity-nodeAffinity-deployments/notes/#use-pod-affinity-when","title":"Use Pod Affinity When:","text":"<ul> <li>Related pods should be co-located (app + cache)</li> <li>You need data locality (pod near its data)</li> <li>Communication latency matters</li> </ul>"},{"location":"10-scheduling/podAffinity-nodeAffinity-deployments/notes/#use-pod-anti-affinity-when","title":"Use Pod Anti-Affinity When:","text":"<ul> <li>You need high availability (spread across nodes/zones)</li> <li>You want to avoid resource contention</li> <li>You have stateful applications that shouldn't share nodes</li> </ul>"},{"location":"10-scheduling/podAffinity-nodeAffinity-deployments/notes/#use-topology-spread-constraints-alternative","title":"Use Topology Spread Constraints (Alternative):","text":"<ul> <li>For simpler \"spread evenly\" requirements</li> <li>When you want built-in balancing</li> <li>For zone/region spreading without complex rules</li> </ul>"},{"location":"10-scheduling/podAffinity-nodeAffinity-deployments/notes/#5-practice-questions","title":"5. Practice Questions","text":""},{"location":"10-scheduling/podAffinity-nodeAffinity-deployments/notes/#question-1-basic-node-affinity","title":"Question 1: Basic Node Affinity","text":"<p>Create a Deployment that: 1. Name: <code>cache-app</code> 2. Replicas: 2 3. Image: <code>redis:alpine</code> 4. Must run on nodes with label <code>memory-type=high</code> 5. Should prefer nodes with label <code>storage=ssd</code> (weight: 80) 6. Pods should not be on the same node</p>"},{"location":"10-scheduling/podAffinity-nodeAffinity-deployments/notes/#question-2-complex-node-selection","title":"Question 2: Complex Node Selection","text":"<p>Create a StatefulSet that: 1. Name: <code>postgres-cluster</code> 2. Replicas: 3 3. Image: <code>postgres:15</code> 4. Requirements:    - Must be in production environment    - Must have fast storage (any label with \"fast\" in key)    - Must NOT be on spot instances    - Must NOT be under maintenance 5. Each pod should have 10Gi persistent storage</p>"},{"location":"10-scheduling/podAffinity-nodeAffinity-deployments/notes/#question-3-pod-anti-affinity-for-ha","title":"Question 3: Pod Anti-Affinity for HA","text":"<p>Create a Deployment that: 1. Name: <code>web-frontend</code> 2. Replicas: 5 3. Image: <code>nginx:latest</code> 4. Requirements:    - Hard rule: Pods must be on different nodes    - Soft rule: Try to spread across zones (weight: 100)    - Pods should prefer to run near cache pods (weight: 60) 5. Add readiness and liveness probes</p>"},{"location":"10-scheduling/podAffinity-nodeAffinity-deployments/notes/#question-4-complete-production-setup","title":"Question 4: Complete Production Setup","text":"<p>Create a complete application with: 1. Deployment: <code>api-server</code> (3 replicas) 2. Deployment: <code>redis-cache</code> (2 replicas) 3. Requirements:    - Both must run in <code>environment=production</code>    - API pods should be on same nodes as cache pods    - API pods should be spread across zones    - Cache pods should be on different nodes    - Prefer nodes with label <code>instance-type=large</code> 4. Include all necessary: resources, probes, environment variables</p>"},{"location":"11-coredns/","title":"Index","text":"<p>it's creazy there is none quetion in 70 question worth doing!!</p>"},{"location":"12-deployments/probing/probing-psotgres/","title":"Probing psotgres","text":"<p>apiVersion: v1 kind: Pod metadata:   name: postgres-pod spec:   containers:     - name: postgres       image: postgres:latest       env:         - name: POSTGRES_PASSWORD           value: dbpassword         - name: POSTGRES_DB           value: database       ports:         - containerPort: 5432       livenessProbe:         tcpSocket:           port: 5432         initialDelaySeconds: 30         periodSeconds: 10       readinessProbe:         exec:           command:             - psql             - -h             - localhost             - -U             - postgres             - -c             - SELECT 1         initialDelaySeconds: 5         periodSeconds: 5</p>"},{"location":"13-manage-cluster-via-kubeadm/cluster-create/","title":"kubeadm Cluster Create &amp; Upgrade","text":""},{"location":"13-manage-cluster-via-kubeadm/cluster-create/#upgrade-order-must-follow","title":"Upgrade Order (Must Follow)","text":"<ul> <li>Always upgrade kubeadm first</li> <li>Upgrade control plane nodes before worker nodes</li> <li>kubelet must be upgraded manually on every node</li> </ul>"},{"location":"13-manage-cluster-via-kubeadm/cluster-create/#upgrade-kubeadm","title":"Upgrade kubeadm","text":""},{"location":"13-manage-cluster-via-kubeadm/cluster-create/#check-available-versions","title":"Check Available Versions","text":"<pre><code>apt-cache show kubeadm\n</code></pre>"},{"location":"13-manage-cluster-via-kubeadm/cluster-create/#install-specific-kubeadm-version","title":"Install Specific kubeadm Version","text":"<pre><code>apt-get install kubeadm=1.34.3-1.1\nkubeadm version\n</code></pre>"},{"location":"13-manage-cluster-via-kubeadm/cluster-create/#upgrade-the-control-plane","title":"Upgrade the Control Plane","text":""},{"location":"13-manage-cluster-via-kubeadm/cluster-create/#see-upgrade-plan","title":"See Upgrade Plan","text":"<pre><code>kubeadm upgrade plan\n</code></pre>"},{"location":"13-manage-cluster-via-kubeadm/cluster-create/#apply-upgrade-example-version","title":"Apply Upgrade (Example Version)","text":"<pre><code>kubeadm upgrade apply v1.34.1\n</code></pre>"},{"location":"13-manage-cluster-via-kubeadm/cluster-create/#upgrade-kubelet-and-kubectl","title":"Upgrade kubelet and kubectl","text":"<pre><code>apt-get install kubelet=1.34.1-1.1 kubectl=1.34.1-1.1\nsystemctl restart kubelet\n</code></pre>"},{"location":"13-manage-cluster-via-kubeadm/cluster-create/#create-a-cluster-kubeadm-init","title":"Create a Cluster (kubeadm init)","text":""},{"location":"13-manage-cluster-via-kubeadm/cluster-create/#basic-init","title":"Basic Init","text":"<pre><code>kubeadm init --kubernetes-version=1.34.1\n</code></pre>"},{"location":"13-manage-cluster-via-kubeadm/cluster-create/#init-with-pod-network-cidr","title":"Init with Pod Network CIDR","text":"<pre><code>kubeadm init \\\n  --kubernetes-version=1.34.1 \\\n  --pod-network-cidr=192.168.0.0/16 \\\n  --ignore-preflight-errors=NumCPU,Mem\n</code></pre>"},{"location":"13-manage-cluster-via-kubeadm/cluster-create/#pod-network-cidr-notes","title":"Pod Network CIDR Notes","text":"<ul> <li>Required for some CNIs (Calico, Flannel)</li> <li>Must be valid CIDR (0\u2013255 per octet)</li> <li>Must not conflict with host or service CIDR</li> </ul>"},{"location":"13-manage-cluster-via-kubeadm/cluster-create/#configure-kubectl-access","title":"Configure kubectl Access","text":"<pre><code>mkdir -p /root/.kube\ncp /etc/kubernetes/admin.conf /root/.kube/config\n</code></pre> <p>If copying to another node:</p> <pre><code>scp /etc/kubernetes/admin.conf node-summer:/root/.kube/config\n</code></pre>"},{"location":"13-manage-cluster-via-kubeadm/cluster-create/#join-worker-nodes","title":"Join Worker Nodes","text":""},{"location":"13-manage-cluster-via-kubeadm/cluster-create/#generate-join-command","title":"Generate Join Command","text":"<pre><code>kubeadm token create --print-join-command\n</code></pre>"},{"location":"13-manage-cluster-via-kubeadm/cluster-create/#example-join","title":"Example Join","text":"<pre><code>kubeadm join 172.30.1.2:6443 \\\n  --token m3qmx4.qa9c83ju82ru6njq \\\n  --discovery-token-ca-cert-hash sha256:4b0e1b1109e852de2f92bbb4bf0bdeae7616dd94ec1a35da21ba7ba83c0bb441\n</code></pre>"},{"location":"14-ingress-controller-and-resources/north-south-traffic-management/","title":"Kubernetes Ingress","text":""},{"location":"14-ingress-controller-and-resources/north-south-traffic-management/#core-mental-model","title":"Core Mental Model","text":"<p>Ingress does only two things, always in this order:</p> <pre><code>MATCH  \u2192  ACTION\n</code></pre> <ul> <li>MATCH decides whether a rule applies to a request</li> <li>ACTION decides what happens after the rule matches</li> </ul> <p>Ingress never:</p> <ul> <li>Creates traffic</li> <li>Modifies traffic implicitly</li> <li>Chooses backends randomly</li> </ul> <p>Everything you see in Ingress YAML is either:</p> <ul> <li>A matching constraint, or</li> <li>An explicit instruction</li> </ul> <p>If something changes (path, protocol, destination), there is always a line in YAML causing it.</p>"},{"location":"14-ingress-controller-and-resources/north-south-traffic-management/#1-matching-theory-when-a-rule-applies","title":"1. MATCHING THEORY (WHEN A RULE APPLIES)","text":"<p>Ingress matching happens in two layers:</p> <ol> <li>Host match</li> <li>Path match</li> </ol> <p>If either fails, the rule is ignored.</p>"},{"location":"14-ingress-controller-and-resources/north-south-traffic-management/#11-host-matching-http-host-header","title":"1.1 Host Matching (HTTP Host Header)","text":"<p>Ingress matches against the HTTP <code>Host</code> header.</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: host-match\nspec:\n  rules:\n  - host: api.example.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: app\n            port:\n              number: 80\n</code></pre> <p>Theory</p> <ul> <li>Host matching is exact</li> <li>Case-insensitive</li> <li>No wildcards unless controller-specific (not tested in CKA)</li> </ul> <p>Implications</p> <ul> <li><code>api.example.com</code> \u2260 <code>example.com</code></li> <li>Requests without the correct Host header skip this rule entirely</li> <li>On the exam, missing Host is a common silent failure</li> </ul>"},{"location":"14-ingress-controller-and-resources/north-south-traffic-management/#2-path-matching-theory-cka-critical","title":"2. PATH MATCHING THEORY (CKA CRITICAL)","text":"<p>After host match succeeds, Ingress evaluates path rules.</p> <p>Ingress supports three pathTypes, each with different semantics and priority.</p>"},{"location":"14-ingress-controller-and-resources/north-south-traffic-management/#21-exact-path-match-highest-priority","title":"2.1 Exact Path Match (Highest Priority)","text":"<pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: exact-path\nspec:\n  rules:\n  - host: example.com\n    http:\n      paths:\n      - path: /login\n        pathType: Exact\n        backend:\n          service:\n            name: auth-service\n            port:\n              number: 80\n</code></pre> <p>Theory</p> <ul> <li>Exact means byte-for-byte equality</li> <li>No normalization</li> <li>No implicit trailing slash handling</li> </ul> <p>Matches</p> <ul> <li><code>/login</code></li> </ul> <p>Does NOT match</p> <ul> <li><code>/login/</code></li> <li><code>/login/admin</code></li> </ul> <p>Why it exists</p> <ul> <li>Guarantees deterministic routing</li> <li>Used for security-sensitive or fixed endpoints</li> </ul> <p>Exam Insight Exact paths always override Prefix paths with the same value.</p>"},{"location":"14-ingress-controller-and-resources/north-south-traffic-management/#22-prefix-path-match-most-used","title":"2.2 Prefix Path Match (MOST USED)","text":"<pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: prefix-path\nspec:\n  rules:\n  - host: example.com\n    http:\n      paths:\n      - path: /api\n        pathType: Prefix\n        backend:\n          service:\n            name: api-service\n            port:\n              number: 80\n</code></pre> <p>Theory</p> <ul> <li>Prefix matches if the request path starts with the prefix</li> <li>Path is not modified</li> <li>This is hierarchical routing</li> </ul> <p>Matches</p> <ul> <li><code>/api</code></li> <li><code>/api/users</code></li> <li><code>/api/v1/orders</code></li> </ul> <p>Does NOT match</p> <ul> <li><code>/ap</code></li> <li><code>/myapi</code></li> </ul> <p>Why it is dominant</p> <ul> <li>Natural fit for APIs and microservices</li> <li>Minimal ambiguity</li> <li>Best performance</li> </ul> <p>Exam Insight If unsure which pathType to use \u2192 Prefix is almost always correct.</p>"},{"location":"14-ingress-controller-and-resources/north-south-traffic-management/#23-implementationspecific-regex","title":"2.3 ImplementationSpecific (Regex)","text":"<pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: regex-path\n  annotations:\n    nginx.ingress.kubernetes.io/use-regex: \"true\"\nspec:\n  rules:\n  - host: example.com\n    http:\n      paths:\n      - path: ^/users/[0-9]+$\n        pathType: ImplementationSpecific\n        backend:\n          service:\n            name: user-service\n            port:\n              number: 80\n</code></pre> <p>Theory</p> <ul> <li>Meaning depends on Ingress controller</li> <li>NGINX interprets this as a regular expression</li> <li>Kubernetes does not validate regex correctness</li> </ul> <p>Matches</p> <ul> <li><code>/users/123</code></li> </ul> <p>Does NOT match</p> <ul> <li><code>/users/abc</code></li> <li><code>/users/123/profile</code></li> </ul> <p>Mandatory Requirements</p> <ul> <li><code>use-regex: \"true\"</code></li> <li><code>pathType: ImplementationSpecific</code></li> </ul> <p>Exam Insight Regex is tested only at a basic level:</p> <ul> <li>Numeric IDs</li> <li>Simple anchors (<code>^</code>, <code>$</code>)</li> <li>Single capture groups</li> </ul>"},{"location":"14-ingress-controller-and-resources/north-south-traffic-management/#3-action-theory-what-happens-after-match","title":"3. ACTION THEORY (WHAT HAPPENS AFTER MATCH)","text":"<p>Once a rule matches, exactly one action occurs.</p>"},{"location":"14-ingress-controller-and-resources/north-south-traffic-management/#31-forward-default-behavior","title":"3.1 Forward (DEFAULT BEHAVIOR)","text":"<pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: forward-only\nspec:\n  rules:\n  - host: example.com\n    http:\n      paths:\n      - path: /app\n        pathType: Prefix\n        backend:\n          service:\n            name: app-service\n            port:\n              number: 80\n</code></pre> <p>Theory</p> <ul> <li>Forwarding is implicit</li> <li>No annotation required</li> <li>Request is proxied as-is</li> </ul> <p>What does NOT change</p> <ul> <li>URL in browser</li> <li>Path</li> <li>HTTP method</li> </ul> <p>Exam Insight If no annotation is present, the action is always forward.</p>"},{"location":"14-ingress-controller-and-resources/north-south-traffic-management/#32-rewrite-server-side-path-transformation","title":"3.2 Rewrite (SERVER-SIDE PATH TRANSFORMATION)","text":"<p>Rewrite changes the path sent to the backend, not what the client sees.</p>"},{"location":"14-ingress-controller-and-resources/north-south-traffic-management/#strip-prefix-most-important-rewrite","title":"Strip Prefix (MOST IMPORTANT REWRITE)","text":"<pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: rewrite-strip\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /$2\nspec:\n  rules:\n  - host: example.com\n    http:\n      paths:\n      - path: /api(/|$)(.*)\n        pathType: Prefix\n        backend:\n          service:\n            name: backend\n            port:\n              number: 80\n</code></pre> <p>Theory</p> <ul> <li>Regex captures parts of the path</li> <li>Rewrite replaces the matched path before proxying</li> </ul> <p>Flow</p> <ul> <li>Client \u2192 <code>/api/users</code></li> <li>Match \u2192 <code>/api(/|$)(.*)</code></li> <li><code>$2 = users</code></li> <li>Backend receives <code>/users</code></li> </ul> <p>Why this pattern exists</p> <ul> <li>Backends often do not know public URL structure</li> <li>API gateway behavior without changing services</li> </ul>"},{"location":"14-ingress-controller-and-resources/north-south-traffic-management/#add-prefix-rewrite","title":"Add Prefix Rewrite","text":"<pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: rewrite-add\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /v1/$1\nspec:\n  rules:\n  - host: example.com\n    http:\n      paths:\n      - path: /(.*)\n        pathType: Prefix\n        backend:\n          service:\n            name: api\n            port:\n              number: 80\n</code></pre> <p>Theory</p> <ul> <li>Entire path is captured</li> <li>New prefix is injected</li> </ul> <p>Flow</p> <ul> <li><code>/users</code> \u2192 backend sees <code>/v1/users</code></li> </ul>"},{"location":"14-ingress-controller-and-resources/north-south-traffic-management/#33-redirect-client-side-action","title":"3.3 Redirect (CLIENT-SIDE ACTION)","text":"<p>Redirect tells the client to issue a new request.</p>"},{"location":"14-ingress-controller-and-resources/north-south-traffic-management/#permanent-redirect-301","title":"Permanent Redirect (301)","text":"<pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: permanent-redirect\n  annotations:\n    nginx.ingress.kubernetes.io/permanent-redirect: https://new.example.com\nspec:\n  rules:\n  - host: old.example.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: dummy\n            port:\n              number: 80\n</code></pre> <p>Theory</p> <ul> <li>Client receives <code>301</code></li> <li>Browser updates address bar</li> <li>Search engines cache this</li> </ul>"},{"location":"14-ingress-controller-and-resources/north-south-traffic-management/#temporary-redirect-302","title":"Temporary Redirect (302)","text":"<pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: temporary-redirect\n  annotations:\n    nginx.ingress.kubernetes.io/temporary-redirect: /maintenance\nspec:\n  rules:\n  - host: example.com\n    http:\n      paths:\n      - path: /app\n        pathType: Prefix\n        backend:\n          service:\n            name: dummy\n            port:\n              number: 80\n</code></pre> <p>Theory</p> <ul> <li>Client retries at new location</li> <li>No caching assumption</li> </ul>"},{"location":"14-ingress-controller-and-resources/north-south-traffic-management/#4-priority-theory-exam-favorite","title":"4. PRIORITY THEORY (EXAM FAVORITE)","text":"<p>Ingress chooses one rule only, based on specificity.</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: priority\nspec:\n  rules:\n  - host: example.com\n    http:\n      paths:\n      - path: /api\n        pathType: Exact\n        backend:\n          service:\n            name: exact-api\n            port:\n              number: 80\n      - path: /api\n        pathType: Prefix\n        backend:\n          service:\n            name: prefix-api\n            port:\n              number: 80\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: default\n            port:\n              number: 80\n</code></pre> <p>Priority Order</p> <ol> <li>Exact</li> <li>Longer Prefix</li> <li>Shorter Prefix</li> <li><code>/</code> catch-all</li> </ol> <p>Exam Insight YAML order does not override specificity.</p>"},{"location":"14-ingress-controller-and-resources/north-south-traffic-management/#5-multi-path-theory","title":"5. MULTI-PATH THEORY","text":"<pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: multi-path\nspec:\n  rules:\n  - host: example.com\n    http:\n      paths:\n      - path: /web\n        pathType: Prefix\n        backend:\n          service:\n            name: frontend\n            port:\n              number: 80\n      - path: /app\n        pathType: Prefix\n        backend:\n          service:\n            name: frontend\n            port:\n              number: 80\n</code></pre> <p>Theory</p> <ul> <li>Multiple public entry points</li> <li>Single backend</li> <li>No conflict because prefixes differ</li> </ul>"},{"location":"14-ingress-controller-and-resources/north-south-traffic-management/#6-catch-all-theory","title":"6. CATCH-ALL THEORY","text":"<pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: catch-all\nspec:\n  rules:\n  - host: example.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: default-app\n            port:\n              number: 80\n</code></pre> <p>Theory</p> <ul> <li>Matches everything</li> <li>Used as fallback</li> <li>Lowest priority possible</li> </ul>"},{"location":"14-ingress-controller-and-resources/north-south-traffic-management/#7-tls-theory-cka-level","title":"7. TLS THEORY (CKA LEVEL)","text":""},{"location":"14-ingress-controller-and-resources/north-south-traffic-management/#tls-termination","title":"TLS Termination","text":"<pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: tls-ingress\nspec:\n  tls:\n  - hosts:\n    - example.com\n    secretName: example-tls\n  rules:\n  - host: example.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: web\n            port:\n              number: 80\n</code></pre> <p>Theory</p> <ul> <li>TLS ends at Ingress</li> <li>Backend sees plain HTTP</li> <li>Secret must be in same namespace</li> </ul>"},{"location":"14-ingress-controller-and-resources/north-south-traffic-management/#http-https-redirect","title":"HTTP \u2192 HTTPS Redirect","text":"<pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: ssl-redirect\n  annotations:\n    nginx.ingress.kubernetes.io/ssl-redirect: \"true\"\nspec:\n  rules:\n  - host: example.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: web\n            port:\n              number: 80\n</code></pre> <p>Theory</p> <ul> <li>HTTP requests receive redirect</li> <li>HTTPS requests are forwarded</li> </ul>"},{"location":"14-ingress-controller-and-resources/north-south-traffic-management/#8-cka-failure-modes-why-things-break","title":"8. CKA FAILURE MODES (WHY THINGS BREAK)","text":"<ul> <li>Host mismatch \u2192 rule ignored</li> <li>Exact vs Prefix confusion \u2192 wrong backend</li> <li>Regex without <code>use-regex</code> \u2192 no match</li> <li>Wrong capture group \u2192 broken rewrite</li> <li>TLS secret missing \u2192 HTTPS fails</li> </ul>"},{"location":"14-ingress-controller-and-resources/north-south-traffic-management/#final-cka-mental-model-memorize","title":"FINAL CKA MENTAL MODEL (MEMORIZE)","text":"<ul> <li>Ingress = MATCH \u2192 ACTION</li> <li>Match = Host + Path</li> <li>PathTypes = Exact | Prefix | ImplementationSpecific</li> <li>Priority = Exact &gt; Longer Prefix &gt; Shorter Prefix &gt; /</li> <li>Forward is default</li> <li>Rewrite changes backend path only</li> <li>Redirect changes client behavior</li> <li>Regex requires annotation + ImplementationSpecific</li> </ul> <p>This is the entire CKA Ingress surface area.</p>"},{"location":"14-ingress-controller-and-resources/questions/","title":"NGINX Ingress Controller Practice Questions","text":""},{"location":"14-ingress-controller-and-resources/questions/#1-simple-host-based-routing-with-nginx-ingress","title":"1. Simple Host-Based Routing with NGINX Ingress","text":"<p>You need to route traffic based on hostnames to different services using NGINX Ingress Controller: - <code>app.example.com</code> \u2192 <code>frontend-service:8080</code>  - <code>api.example.com</code> \u2192 <code>backend-service:3000</code> - Both should be served on port 80 - Use the NGINX Ingress Controller (already installed)</p>"},{"location":"14-ingress-controller-and-resources/questions/#2-path-rewriting-with-nginx-annotations","title":"2. Path Rewriting with NGINX Annotations","text":"<p>You have a single domain <code>services.company.com</code> with these requirements using NGINX Ingress: - <code>/shop/products</code> \u2192 <code>product-service:8080</code> (should receive <code>/products</code>) - <code>/shop/cart</code> \u2192 <code>cart-service:9090</code> (should receive <code>/cart</code>) - <code>/auth/login</code> \u2192 <code>auth-service:7070</code> (should receive <code>/login</code>) - You must use NGINX Ingress annotations for path rewriting</p>"},{"location":"14-ingress-controller-and-resources/questions/#3-path-redirects-with-nginx-configuration","title":"3. Path Redirects with NGINX Configuration","text":"<p>You're migrating from an old path structure to a new one using NGINX Ingress: - <code>/old/dashboard</code> \u2192 redirect to <code>/new/dashboard</code> (301 permanent) - <code>/old/api/v1</code> \u2192 redirect to <code>/api/v2</code> (301 permanent) - <code>/temp/redirect</code> \u2192 redirect to <code>/new/temp</code> (302 temporary) - Domain: <code>redirect.example.com</code> - Use NGINX Ingress annotations for redirects</p>"},{"location":"14-ingress-controller-and-resources/questions/#4-tls-configuration-with-nginx-ingress","title":"4. TLS Configuration with NGINX Ingress","text":"<p>You need to configure TLS for NGINX Ingress with: 1. TLS certificate stored in secret named <code>wildcard-tls</code> 2. Serve <code>app.company.com</code> and <code>api.company.com</code> over HTTPS 3. Redirect HTTP to HTTPS automatically 4. Use appropriate NGINX Ingress annotations</p>"},{"location":"14-ingress-controller-and-resources/questions/#5-exact-vs-prefix-path-matching-with-nginx","title":"5. Exact vs Prefix Path Matching with NGINX","text":"<p>You have these specific routing requirements using NGINX Ingress: - <code>/api/health</code> (exact match only) \u2192 <code>health-check:8080</code> - <code>/api/users</code> (prefix match) \u2192 <code>user-service:3000</code> (matches <code>/api/users</code>, <code>/api/users/123</code>, etc.) - <code>/admin</code> (exact match) \u2192 <code>admin-panel:9000</code> - <code>/admin/config</code> (exact match, higher priority than <code>/admin</code> prefix) \u2192 <code>config-service:4000</code> - Implement proper path matching using NGINX Ingress path types and annotations if needed</p>"},{"location":"15-troubleshoot-controlplane/prerequisite-and-approach/","title":"How to Approach Control Plane Issues","text":""},{"location":"15-troubleshoot-controlplane/prerequisite-and-approach/#prerequisite-knowledge","title":"Prerequisite Knowledge","text":"<ul> <li><code>/etc/kubernetes/manifests</code>   Location of control plane static pod manifests.</li> </ul>"},{"location":"15-troubleshoot-controlplane/prerequisite-and-approach/#places-to-look-for-logs","title":"Places to Look for Logs","text":""},{"location":"15-troubleshoot-controlplane/prerequisite-and-approach/#kubelet-logs","title":"Kubelet Logs","text":"<p>Use these when the kubelet itself is failing due to syntax errors, invalid flags, or startup issues.</p> <pre><code>journalctl -u kubelet\njournalctl -u kubelet -n 100\njournalctl -u kubelet -f\njournalctl -u kubelet -b\njournalctl -u kubelet -p err\njournalctl -u kubelet --since \"10 minutes ago\"\n</code></pre> <p>To inspect kubelet service flags and configuration location:</p> <pre><code>systemctl cat kubelet\n</code></pre> <p>System logs (if applicable):</p> <pre><code>/var/log/syslog | grep &lt;keyword&gt;\n</code></pre>"},{"location":"15-troubleshoot-controlplane/prerequisite-and-approach/#static-pod-logs","title":"Static Pod Logs","text":"<p>Use these when the kubelet is running and able to create static pods, but the pods are crashing.</p> <ul> <li> <p>Pod logs:</p> </li> <li> <p><code>/var/log/pods/</code></p> <ul> <li><code>0.log</code> \u2192 current log</li> <li><code>1.log</code> \u2192 rotated log</li> <li>Container logs:</li> </ul> </li> <li> <p><code>/var/log/containers/</code></p> </li> <li>This directory contains symlinks to pod logs.</li> </ul>"},{"location":"15-troubleshoot-controlplane/prerequisite-and-approach/#container-runtime","title":"Container Runtime","text":"<p>Use these when the kubelet created the static pod but the container is failing.</p> <pre><code>crictl ps\ncrictl ps -a\ncrictl logs &lt;container-id&gt;\n</code></pre>"},{"location":"15-troubleshoot-controlplane/prerequisite-and-approach/#health-and-probe-endpoints","title":"Health and Probe Endpoints","text":""},{"location":"15-troubleshoot-controlplane/prerequisite-and-approach/#kube-apiserver","title":"kube-apiserver","text":"Probe Type Endpoint Port Startup <code>/livez</code> 6443 Liveness <code>/livez</code> 6443 Readiness <code>/readyz</code> 6443"},{"location":"15-troubleshoot-controlplane/prerequisite-and-approach/#etcd","title":"etcd","text":"Probe Type Endpoint Port Liveness <code>/health</code> 2379 Readiness <code>/health</code> 2379"},{"location":"15-troubleshoot-controlplane/prerequisite-and-approach/#kube-controller-manager","title":"kube-controller-manager","text":"Probe Type Endpoint Port Liveness <code>/healthz</code> 10257 Readiness <code>/healthz</code> 10257"},{"location":"15-troubleshoot-controlplane/prerequisite-and-approach/#kube-scheduler","title":"kube-scheduler","text":"Probe Type Endpoint Port Liveness <code>/healthz</code> 10259 Readiness <code>/healthz</code> 10259"},{"location":"15-troubleshoot-controlplane/prerequisite-and-approach/#kubelet","title":"kubelet","text":"Endpoint Address <code>/healthz</code> <code>127.0.0.1:10248</code>"},{"location":"15-troubleshoot-controlplane/prerequisite-and-approach/#key-component-ports","title":"Key Component Ports","text":"Component Purpose Port kube-apiserver API + health 6443 etcd Client API 2379 kubelet Local health 10248"},{"location":"15-troubleshoot-controlplane/prerequisite-and-approach/#kube-apiserver-minimal-required-flags","title":"kube-apiserver Minimal Required Flags","text":"<ul> <li><code>--advertise-address=&lt;node-ip&gt;</code></li> <li><code>--secure-port=6443</code></li> <li><code>--etcd-servers=https://127.0.0.1:2379</code></li> </ul>"},{"location":"15-troubleshoot-controlplane/prerequisite-and-approach/#tls-certificate-paths-do-not-invent","title":"TLS Certificate Paths (Do Not Invent)","text":""},{"location":"15-troubleshoot-controlplane/prerequisite-and-approach/#api-server","title":"API Server","text":"<pre><code>/etc/kubernetes/pki/apiserver.crt\n/etc/kubernetes/pki/apiserver.key\n/etc/kubernetes/pki/ca.crt\n</code></pre>"},{"location":"15-troubleshoot-controlplane/prerequisite-and-approach/#etcd_1","title":"etcd","text":"<pre><code>/etc/kubernetes/pki/etcd/ca.crt\n/etc/kubernetes/pki/etcd/server.crt\n/etc/kubernetes/pki/etcd/server.key\n</code></pre> <p>If these files exist, do not modify them.</p>"},{"location":"17-authorization/cani/","title":"Cani","text":"<p>Below is SECTION 5 only. Focused on <code>kubectl auth can-i</code> \u2014 complete, correct usage with full examples and explanations. This is an exam-critical debugging tool.</p>"},{"location":"17-authorization/cani/#section-5-kubectl-auth-can-i-deep-correct-practical","title":"Section 5 \u2014 <code>kubectl auth can-i</code> (Deep, Correct, Practical)","text":""},{"location":"17-authorization/cani/#what-kubectl-auth-can-i-actually-does","title":"What <code>kubectl auth can-i</code> Actually Does","text":"<p><code>kubectl auth can-i</code> answers one question only:</p> <p>Does RBAC allow this subject to perform this action?</p> <p>It checks:</p> <ul> <li>Roles</li> <li>ClusterRoles</li> <li>RoleBindings</li> <li>ClusterRoleBindings</li> </ul> <p>It does NOT check:</p> <ul> <li>Admission controllers</li> <li>PodSecurity</li> <li>Resource existence</li> <li>Runtime errors</li> </ul> <p>If <code>can-i</code> says yes and action fails \u2192 RBAC is not the issue.</p>"},{"location":"17-authorization/cani/#51-basic-usage-current-user","title":"5.1 Basic Usage (Current User)","text":""},{"location":"17-authorization/cani/#example-1-can-i-read-pods","title":"Example 1 \u2014 Can I read pods?","text":"<pre><code>kubectl auth can-i get pods\n</code></pre> <p>Explanation:</p> <ul> <li>Uses current kubeconfig identity</li> <li>Checks default namespace</li> </ul>"},{"location":"17-authorization/cani/#example-2-explicit-namespace","title":"Example 2 \u2014 Explicit namespace","text":"<pre><code>kubectl auth can-i list pods -n dev\n</code></pre> <p>Explanation:</p> <ul> <li>RBAC evaluation is namespace-aware</li> <li>Missing <code>-n</code> often causes confusion</li> </ul>"},{"location":"17-authorization/cani/#52-acting-as-another-identity-as","title":"5.2 Acting As Another Identity (<code>--as</code>)","text":""},{"location":"17-authorization/cani/#example-3-acting-as-a-user","title":"Example 3 \u2014 Acting as a User","text":"<pre><code>kubectl auth can-i delete deployments \\\n  --as alice \\\n  -n prod\n</code></pre> <p>Explanation:</p> <ul> <li><code>alice</code> must match exact user name from auth provider</li> <li>Namespace is mandatory for namespaced resources</li> </ul>"},{"location":"17-authorization/cani/#example-4-acting-as-a-serviceaccount-correct-format","title":"Example 4 \u2014 Acting as a ServiceAccount (CORRECT FORMAT)","text":"<pre><code>kubectl auth can-i get pods \\\n  --as system:serviceaccount:default:my-sa \\\n  -n default\n</code></pre> <p>Explanation:</p> <ul> <li>ServiceAccount identity is always expanded</li> <li>Short names (<code>--as my-sa</code>) will not match</li> </ul>"},{"location":"17-authorization/cani/#53-common-mistake-wrong-serviceaccount-identity","title":"5.3 Common Mistake \u2014 Wrong ServiceAccount Identity","text":""},{"location":"17-authorization/cani/#wrong","title":"\u274c Wrong","text":"<pre><code>kubectl auth can-i get pods --as prometheus\n</code></pre>"},{"location":"17-authorization/cani/#correct","title":"\u2705 Correct","text":"<pre><code>kubectl auth can-i get pods \\\n  --as system:serviceaccount:monitoring:prometheus \\\n  -n prod\n</code></pre> <p>Explanation:</p> <ul> <li>RBAC matches full identity string</li> <li>Namespace is part of identity</li> </ul>"},{"location":"17-authorization/cani/#54-checking-cluster-scoped-resources","title":"5.4 Checking Cluster-Scoped Resources","text":""},{"location":"17-authorization/cani/#example-5-nodes-cluster-scoped","title":"Example 5 \u2014 Nodes (cluster-scoped)","text":"<pre><code>kubectl auth can-i list nodes\n</code></pre> <p>Explanation:</p> <ul> <li>No namespace flag</li> <li>Requires ClusterRole + ClusterRoleBinding</li> </ul>"},{"location":"17-authorization/cani/#example-6-namespaces","title":"Example 6 \u2014 Namespaces","text":"<pre><code>kubectl auth can-i get namespaces\n</code></pre> <p>Explanation:</p> <ul> <li>Namespaces are cluster-scoped</li> <li>RoleBinding can never grant this</li> </ul>"},{"location":"17-authorization/cani/#55-verb-resource-accuracy-exam-trap","title":"5.5 Verb + Resource Accuracy (Exam Trap)","text":""},{"location":"17-authorization/cani/#example-7-wrong-resource-name","title":"Example 7 \u2014 Wrong resource name","text":"<pre><code>kubectl auth can-i get deployment\n</code></pre> <p>Result:</p> <pre><code>no\n</code></pre>"},{"location":"17-authorization/cani/#correct_1","title":"Correct","text":"<pre><code>kubectl auth can-i get deployments.apps\n</code></pre> <p>Explanation:</p> <ul> <li>CLI uses fully qualified resource</li> <li>Same rule as <code>kubectl create role</code></li> </ul>"},{"location":"17-authorization/cani/#56-subresource-checks","title":"5.6 Subresource Checks","text":""},{"location":"17-authorization/cani/#example-8-logs","title":"Example 8 \u2014 Logs","text":"<pre><code>kubectl auth can-i get pods/log -n debug\n</code></pre> <p>Explanation:</p> <ul> <li><code>pods</code> permission \u2260 <code>pods/log</code></li> <li>This explains <code>kubectl logs</code> failures</li> </ul>"},{"location":"17-authorization/cani/#example-9-exec","title":"Example 9 \u2014 Exec","text":"<pre><code>kubectl auth can-i create pods/exec -n dev\n</code></pre> <p>Explanation:</p> <ul> <li><code>exec</code> uses <code>create</code> verb</li> <li>Very common exam trap</li> </ul>"},{"location":"17-authorization/cani/#57-listing-effective-permissions","title":"5.7 Listing Effective Permissions","text":""},{"location":"17-authorization/cani/#example-10-what-can-i-do","title":"Example 10 \u2014 What can I do?","text":"<pre><code>kubectl auth can-i --list\n</code></pre> <p>Explanation:</p> <ul> <li>Shows all RBAC-allowed actions</li> <li>Only for current user</li> <li>Namespaced output if <code>-n</code> is provided</li> </ul>"},{"location":"17-authorization/cani/#example-11-namespace-specific-listing","title":"Example 11 \u2014 Namespace-specific listing","text":"<pre><code>kubectl auth can-i --list -n prod\n</code></pre> <p>Explanation:</p> <ul> <li>Extremely useful to debug RoleBindings</li> </ul>"},{"location":"17-authorization/cani/#58-wildcard-checks-debug-only","title":"5.8 Wildcard Checks (Debug Only)","text":""},{"location":"17-authorization/cani/#example-12-full-access-check","title":"Example 12 \u2014 Full access check","text":"<pre><code>kubectl auth can-i '*' '*'\n</code></pre> <p>Explanation:</p> <ul> <li>Indicates cluster-admin-like access</li> <li>Never use as justification for least privilege</li> </ul>"},{"location":"17-authorization/cani/#59-resourcenames-behavior","title":"5.9 ResourceNames Behavior","text":""},{"location":"17-authorization/cani/#example-13-specific-object-access","title":"Example 13 \u2014 Specific object access","text":"<pre><code>kubectl auth can-i get secrets/db-creds -n finance\n</code></pre> <p>Explanation:</p> <ul> <li>Tests <code>resourceNames</code></li> <li><code>list</code> will still fail</li> </ul>"},{"location":"17-authorization/cani/#example-14-list-still-denied","title":"Example 14 \u2014 List still denied","text":"<pre><code>kubectl auth can-i list secrets -n finance\n</code></pre> <p>Explanation:</p> <ul> <li><code>resourceNames</code> blocks list</li> <li>Very common misunderstanding</li> </ul>"},{"location":"17-authorization/cani/#510-debug-flow-using-can-i-exam-pattern","title":"5.10 Debug Flow Using <code>can-i</code> (Exam Pattern)","text":""},{"location":"17-authorization/cani/#typical-exam-debug-steps","title":"Typical Exam Debug Steps","text":"<ol> <li>Test action:</li> </ol> <pre><code>kubectl auth can-i create pods -n dev\n</code></pre> <ol> <li>Test identity explicitly:</li> </ol> <pre><code>kubectl auth can-i create pods \\\n  --as system:serviceaccount:ci:ci-bot \\\n  -n dev\n</code></pre> <ol> <li>Test subresource if needed:</li> </ol> <pre><code>kubectl auth can-i create pods/exec -n dev\n</code></pre> <p>If all return yes \u2192 RBAC is correct.</p>"},{"location":"17-authorization/cani/#section-5-hard-rules-to-remember","title":"Section 5 \u2014 Hard Rules to Remember","text":"<ul> <li><code>can-i</code> checks RBAC only</li> <li>Always specify <code>-n</code> for namespaced resources</li> <li>ServiceAccount must use full identity string</li> <li>Subresources must be checked explicitly</li> <li>Cluster-scoped resources never use namespace</li> <li><code>--list</code> is your RBAC truth view</li> </ul> <p>Say \u201csection 6\u201d for system users, nodes, and control-plane components or \u201cfinal cram\u201d for a one-page RBAC exam summary.</p>"},{"location":"17-authorization/examples/","title":"Examples","text":"<p>Here are 20 YAML examples with detailed explanations and 20 CLI examples with explanations, properly formatted.</p>"},{"location":"17-authorization/examples/#yaml-examples-with-explanations","title":"YAML Examples with Explanations","text":""},{"location":"17-authorization/examples/#example-1-basic-role-for-pod-reading","title":"Example 1: Basic Role for Pod Reading","text":"<p>This creates a Role that allows reading pods in the default namespace. <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: pod-reader\n  namespace: default\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n</code></pre> The Role defines permissions but does not grant access. It specifies get, list, and watch verbs on pods in the core API group.</p>"},{"location":"17-authorization/examples/#example-2-rolebinding-for-user","title":"Example 2: RoleBinding for User","text":"<p>This binds the pod-reader Role to a user named alice. <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: pod-reader-binding\n  namespace: default\nsubjects:\n- kind: User\n  name: alice\nroleRef:\n  kind: Role\n  name: pod-reader\n  apiGroup: rbac.authorization.k8s.io\n</code></pre> The RoleBinding grants alice the permissions defined in the pod-reader Role, but only in the default namespace.</p>"},{"location":"17-authorization/examples/#example-3-clusterrole-for-node-reading","title":"Example 3: ClusterRole for Node Reading","text":"<p>This creates a ClusterRole that allows reading nodes cluster-wide. <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: node-reader\nrules:\n- apiGroups: [\"\"]\n  resources: [\"nodes\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n</code></pre> ClusterRoles exist at cluster scope and can reference cluster-scoped resources like nodes.</p>"},{"location":"17-authorization/examples/#example-4-clusterrolebinding-for-group","title":"Example 4: ClusterRoleBinding for Group","text":"<p>This grants node-reader permissions to all users in the monitoring group. <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: node-reader-binding\nsubjects:\n- kind: Group\n  name: monitoring\nroleRef:\n  kind: ClusterRole\n  name: node-reader\n  apiGroup: rbac.authorization.k8s.io\n</code></pre> The binding is cluster-wide, so group members can read nodes in any namespace.</p>"},{"location":"17-authorization/examples/#example-5-role-with-multiple-resources","title":"Example 5: Role with Multiple Resources","text":"<p>This Role allows management of both pods and services. <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: resource-manager\n  namespace: production\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\", \"services\"]\n  verbs: [\"create\", \"get\", \"update\", \"delete\", \"list\"]\n</code></pre> A single Role can contain rules for multiple resources in the same API group.</p>"},{"location":"17-authorization/examples/#example-6-role-with-specific-resource-names","title":"Example 6: Role with Specific Resource Names","text":"<p>This Role allows access only to specific ConfigMaps by name. <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: config-reader\n  namespace: default\nrules:\n- apiGroups: [\"\"]\n  resources: [\"configmaps\"]\n  resourceNames: [\"app-config\", \"database-config\"]\n  verbs: [\"get\"]\n</code></pre> The resourceNames field restricts access to only those named resources, not all ConfigMaps.</p>"},{"location":"17-authorization/examples/#example-7-role-for-subresources","title":"Example 7: Role for Subresources","text":"<p>This Role allows accessing pod logs and execution. <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: pod-debugger\n  namespace: default\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods/log\", \"pods/exec\"]\n  verbs: [\"get\", \"create\"]\n</code></pre> Subresources like logs and exec require explicit permissions separate from the parent resource.</p>"},{"location":"17-authorization/examples/#example-8-serviceaccount-binding","title":"Example 8: ServiceAccount Binding","text":"<p>This binds a ServiceAccount to a Role in its namespace. <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: sa-binding\n  namespace: default\nsubjects:\n- kind: ServiceAccount\n  name: my-app-sa\n  namespace: default\nroleRef:\n  kind: Role\n  name: pod-reader\n  apiGroup: rbac.authorization.k8s.io\n</code></pre> ServiceAccounts must specify their namespace when referenced in bindings.</p>"},{"location":"17-authorization/examples/#example-9-cross-namespace-serviceaccount-access","title":"Example 9: Cross-Namespace ServiceAccount Access","text":"<p>This allows a ServiceAccount from monitoring namespace to read pods in default namespace. <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: cross-ns-binding\n  namespace: default\nsubjects:\n- kind: ServiceAccount\n  name: prometheus\n  namespace: monitoring\nroleRef:\n  kind: Role\n  name: pod-reader\n  apiGroup: rbac.authorization.k8s.io\n</code></pre> ServiceAccounts can be granted permissions in namespaces other than where they exist.</p>"},{"location":"17-authorization/examples/#example-10-clusterrole-bound-to-namespace","title":"Example 10: ClusterRole Bound to Namespace","text":"<p>This uses a ClusterRole but limits it to a specific namespace via RoleBinding. <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: limited-clusterrole\n  namespace: staging\nsubjects:\n- kind: User\n  name: tester\nroleRef:\n  kind: ClusterRole\n  name: admin\n  apiGroup: rbac.authorization.k8s.io\n</code></pre> Even though admin is a ClusterRole, the RoleBinding limits its permissions to the staging namespace only.</p>"},{"location":"17-authorization/examples/#example-11-role-with-api-group-specific-resources","title":"Example 11: Role with API Group Specific Resources","text":"<p>This Role manages resources from the apps API group. <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: apps-manager\n  namespace: default\nrules:\n- apiGroups: [\"apps\"]\n  resources: [\"deployments\", \"statefulsets\", \"daemonsets\"]\n  verbs: [\"get\", \"list\", \"create\", \"update\", \"delete\"]\n</code></pre> Different API groups require separate rules or comma-separated entries in the apiGroups array.</p>"},{"location":"17-authorization/examples/#example-12-role-with-multiple-api-groups","title":"Example 12: Role with Multiple API Groups","text":"<p>This Role covers resources from multiple API groups. <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: multi-group-role\n  namespace: default\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\", \"services\"]\n  verbs: [\"get\", \"list\"]\n- apiGroups: [\"apps\"]\n  resources: [\"deployments\"]\n  verbs: [\"get\", \"list\"]\n</code></pre> Each distinct API group requires a separate rule entry in the rules array.</p>"},{"location":"17-authorization/examples/#example-13-clusterrole-for-non-resource-urls","title":"Example 13: ClusterRole for Non-Resource URLs","text":"<p>This ClusterRole allows access to health check endpoints. <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: health-checker\nrules:\n- nonResourceURLs: [\"/healthz\", \"/readyz\", \"/livez\"]\n  verbs: [\"get\"]\n</code></pre> Non-resource URLs represent API endpoints that aren't tied to specific resources.</p>"},{"location":"17-authorization/examples/#example-14-role-with-all-verbs","title":"Example 14: Role with All Verbs","text":"<p>This Role grants all actions on pods within a namespace. <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: pod-admin\n  namespace: default\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\"]\n  verbs: [\"*\"]\n</code></pre> The asterisk wildcard grants all available verbs for the specified resource.</p>"},{"location":"17-authorization/examples/#example-15-role-with-all-resources","title":"Example 15: Role with All Resources","text":"<p>This Role grants read access to all resources in a namespace. <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: namespace-reader\n  namespace: default\nrules:\n- apiGroups: [\"*\"]\n  resources: [\"*\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n</code></pre> Wildcards in both apiGroups and resources grant access across all API groups and resources.</p>"},{"location":"17-authorization/examples/#example-16-binding-multiple-subjects","title":"Example 16: Binding Multiple Subjects","text":"<p>This RoleBinding grants access to both a user and a ServiceAccount. <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: multi-subject-binding\n  namespace: default\nsubjects:\n- kind: User\n  name: developer\n- kind: ServiceAccount\n  name: automation\n  namespace: default\nroleRef:\n  kind: Role\n  name: pod-reader\n  apiGroup: rbac.authorization.k8s.io\n</code></pre> A single binding can reference multiple subjects of different kinds.</p>"},{"location":"17-authorization/examples/#example-17-role-for-persistentvolumeclaims","title":"Example 17: Role for PersistentVolumeClaims","text":"<p>This Role manages PersistentVolumeClaims in a namespace. <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: pvc-manager\n  namespace: storage\nrules:\n- apiGroups: [\"\"]\n  resources: [\"persistentvolumeclaims\"]\n  verbs: [\"create\", \"delete\", \"get\", \"list\"]\n</code></pre> PersistentVolumeClaims are namespaced resources in the core API group.</p>"},{"location":"17-authorization/examples/#example-18-role-for-secrets-management","title":"Example 18: Role for Secrets Management","text":"<p>This Role allows reading and creating secrets. <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: secret-manager\n  namespace: default\nrules:\n- apiGroups: [\"\"]\n  resources: [\"secrets\"]\n  verbs: [\"get\", \"list\", \"create\"]\n</code></pre> Secret management requires explicit permissions as secrets are sensitive resources.</p>"},{"location":"17-authorization/examples/#example-19-clusterrole-for-namespace-operations","title":"Example 19: ClusterRole for Namespace Operations","text":"<p>This ClusterRole allows creating and listing namespaces. <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: namespace-admin\nrules:\n- apiGroups: [\"\"]\n  resources: [\"namespaces\"]\n  verbs: [\"create\", \"get\", \"list\"]\n</code></pre> Namespace operations require cluster-wide permissions since namespaces are cluster-scoped.</p>"},{"location":"17-authorization/examples/#example-20-aggregated-clusterrole","title":"Example 20: Aggregated ClusterRole","text":"<p>This ClusterRole uses aggregation to combine rules from other ClusterRoles. <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: monitoring-aggregated\naggregationRule:\n  clusterRoleSelectors:\n  - matchLabels:\n      rbac.monitoring.io/aggregate-to-monitoring: \"true\"\nrules: []\n</code></pre> Aggregated ClusterRoles dynamically collect rules from other ClusterRoles with matching labels.</p>"},{"location":"17-authorization/examples/#cli-examples-with-explanations","title":"CLI Examples with Explanations","text":""},{"location":"17-authorization/examples/#example-1-create-basic-role","title":"Example 1: Create Basic Role","text":"<p><pre><code>kubectl create role pod-reader --verb=get,list,watch --resource=pods --namespace=default\n</code></pre> Creates a Role named pod-reader in the default namespace with read-only pod permissions.</p>"},{"location":"17-authorization/examples/#example-2-create-rolebinding-for-user","title":"Example 2: Create RoleBinding for User","text":"<p><pre><code>kubectl create rolebinding pod-reader-binding --role=pod-reader --user=alice --namespace=default\n</code></pre> Binds the pod-reader Role to user alice in the default namespace.</p>"},{"location":"17-authorization/examples/#example-3-create-clusterrole","title":"Example 3: Create ClusterRole","text":"<p><pre><code>kubectl create clusterrole node-reader --verb=get,list,watch --resource=nodes\n</code></pre> Creates a ClusterRole that allows reading nodes across the entire cluster.</p>"},{"location":"17-authorization/examples/#example-4-create-clusterrolebinding","title":"Example 4: Create ClusterRoleBinding","text":"<p><pre><code>kubectl create clusterrolebinding node-reader-binding --clusterrole=node-reader --group=monitoring\n</code></pre> Grants node-reader permissions to all users in the monitoring group cluster-wide.</p>"},{"location":"17-authorization/examples/#example-5-create-role-with-multiple-resources","title":"Example 5: Create Role with Multiple Resources","text":"<p><pre><code>kubectl create role resource-manager --verb=create,get,update,delete,list --resource=pods,services --namespace=production\n</code></pre> Creates a Role that manages both pods and services in the production namespace.</p>"},{"location":"17-authorization/examples/#example-6-create-role-with-specific-resource-names","title":"Example 6: Create Role with Specific Resource Names","text":"<p><pre><code>kubectl create role config-reader --verb=get --resource=configmaps --resource-name=app-config,database-config --namespace=default\n</code></pre> Creates a Role that only allows accessing specific ConfigMaps by name.</p>"},{"location":"17-authorization/examples/#example-7-create-role-for-subresources","title":"Example 7: Create Role for Subresources","text":"<p><pre><code>kubectl create role pod-debugger --verb=get,create --resource=pods/log,pods/exec --namespace=default\n</code></pre> Creates a Role for accessing pod logs and exec subresources.</p>"},{"location":"17-authorization/examples/#example-8-create-serviceaccount-binding","title":"Example 8: Create ServiceAccount Binding","text":"<p><pre><code>kubectl create rolebinding sa-binding --role=pod-reader --serviceaccount=default:my-app-sa --namespace=default\n</code></pre> Binds a ServiceAccount to a Role using the serviceaccount:namespace:name format.</p>"},{"location":"17-authorization/examples/#example-9-test-permissions","title":"Example 9: Test Permissions","text":"<p><pre><code>kubectl auth can-i create pods --namespace=default\n</code></pre> Checks if the current user has permission to create pods in the default namespace.</p>"},{"location":"17-authorization/examples/#example-10-test-permissions-as-different-user","title":"Example 10: Test Permissions as Different User","text":"<p><pre><code>kubectl auth can-i delete deployments --as=system:serviceaccount:default:my-app-sa --namespace=default\n</code></pre> Tests permissions for a specific ServiceAccount rather than the current user.</p>"},{"location":"17-authorization/examples/#example-11-list-all-permissions","title":"Example 11: List All Permissions","text":"<p><pre><code>kubectl auth can-i --list --namespace=default\n</code></pre> Lists all permissions the current user has in the specified namespace.</p>"},{"location":"17-authorization/examples/#example-12-create-role-for-all-resources","title":"Example 12: Create Role for All Resources","text":"<p><pre><code>kubectl create role namespace-reader --verb=get,list,watch --resource='*' --namespace=default\n</code></pre> Creates a Role with read access to all resources in the namespace.</p>"},{"location":"17-authorization/examples/#example-13-create-clusterrole-for-non-resource-urls","title":"Example 13: Create ClusterRole for Non-Resource URLs","text":"<p><pre><code>kubectl create clusterrole health-checker --verb=get --non-resource-url=/healthz,/readyz,/livez\n</code></pre> Creates a ClusterRole for accessing health check endpoints.</p>"},{"location":"17-authorization/examples/#example-14-bind-clusterrole-to-namespace","title":"Example 14: Bind ClusterRole to Namespace","text":"<p><pre><code>kubectl create rolebinding admin-in-staging --clusterrole=admin --user=admin --namespace=staging\n</code></pre> Binds a ClusterRole to a user but limits it to a specific namespace via RoleBinding.</p>"},{"location":"17-authorization/examples/#example-15-create-role-with-api-group","title":"Example 15: Create Role with API Group","text":"<p><pre><code>kubectl create role deployment-manager --verb=get,list,create,update,delete --resource=deployments.apps --namespace=production\n</code></pre> Specifies the API group (apps) for deployments resource.</p>"},{"location":"17-authorization/examples/#example-16-check-cross-namespace-permissions","title":"Example 16: Check Cross-Namespace Permissions","text":"<p><pre><code>kubectl auth can-i get pods --namespace=production --as=system:serviceaccount:monitoring:prometheus\n</code></pre> Checks if a ServiceAccount from one namespace has permissions in another namespace.</p>"},{"location":"17-authorization/examples/#example-17-create-role-for-specific-verbs-only","title":"Example 17: Create Role for Specific Verbs Only","text":"<p><pre><code>kubectl create role pod-lister --verb=list --resource=pods --namespace=default\n</code></pre> Creates a Role with only list permission, not get or watch.</p>"},{"location":"17-authorization/examples/#example-18-update-role-rules","title":"Example 18: Update Role Rules","text":"<p><pre><code>kubectl edit role pod-reader --namespace=default\n</code></pre> Opens the Role definition in an editor for modification.</p>"},{"location":"17-authorization/examples/#example-19-describe-rbac-resources","title":"Example 19: Describe RBAC Resources","text":"<p><pre><code>kubectl describe role pod-reader --namespace=default\n</code></pre> Shows detailed information about a Role including its rules.</p>"},{"location":"17-authorization/examples/#example-20-delete-rbac-resources","title":"Example 20: Delete RBAC Resources","text":"<p><pre><code>kubectl delete rolebinding pod-reader-binding --namespace=default\n</code></pre> Removes a RoleBinding, which revokes the permissions it was granting.</p> <p>Each CLI command corresponds to creating, testing, or managing RBAC resources with specific parameters and options. The explanations detail what each command does and when to use it.</p>"},{"location":"17-authorization/gotchas/","title":"RBAC Gotchas and Common Mistakes","text":""},{"location":"17-authorization/gotchas/#41-core-api-group-empty-string-requirement","title":"4.1 Core API Group Empty String Requirement","text":""},{"location":"17-authorization/gotchas/#incorrect-implementation","title":"Incorrect Implementation","text":"<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: pod-access\n  namespace: default\nrules:\n- apiGroups: [\"v1\"]\n  resources: [\"pods\"]\n  verbs: [\"get\", \"list\"]\n</code></pre>"},{"location":"17-authorization/gotchas/#correct-implementation","title":"Correct Implementation","text":"<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: pod-access\n  namespace: default\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\"]\n  verbs: [\"get\", \"list\"]\n</code></pre> <p>Explanation: The empty string <code>\"\"</code> represents the core API group, which contains fundamental resources like pods, services, and configmaps. Using <code>\"v1\"</code> instead of <code>\"\"</code> will cause the RBAC rule to be ignored, resulting in access denial without clear error messages.</p>"},{"location":"17-authorization/gotchas/#42-serviceaccount-subject-api-group-omission","title":"4.2 ServiceAccount Subject API Group Omission","text":""},{"location":"17-authorization/gotchas/#incorrect-implementation_1","title":"Incorrect Implementation","text":"<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: sa-binding\n  namespace: default\nsubjects:\n- kind: ServiceAccount\n  name: my-app\n  namespace: default\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: Role\n  name: pod-reader\n  apiGroup: rbac.authorization.k8s.io\n</code></pre>"},{"location":"17-authorization/gotchas/#correct-implementation_1","title":"Correct Implementation","text":"<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: sa-binding\n  namespace: default\nsubjects:\n- kind: ServiceAccount\n  name: my-app\n  namespace: default\nroleRef:\n  kind: Role\n  name: pod-reader\n  apiGroup: rbac.authorization.k8s.io\n</code></pre> <p>Explanation: ServiceAccount resources exist in the core API group, not the RBAC API group. The apiGroup field must be omitted when referencing ServiceAccounts as subjects. For Users and Groups, the apiGroup must be <code>rbac.authorization.k8s.io</code>.</p>"},{"location":"17-authorization/gotchas/#43-rolebinding-scope-limitation","title":"4.3 RoleBinding Scope Limitation","text":""},{"location":"17-authorization/gotchas/#incorrect-expectation","title":"Incorrect Expectation","text":"<p>User creates a RoleBinding in the development namespace and expects access to production namespace.</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: pod-access\n  namespace: development\nsubjects:\n- kind: User\n  name: developer\nroleRef:\n  kind: Role\n  name: pod-reader\n  apiGroup: rbac.authorization.k8s.io\n</code></pre> <p>Result: The developer only has access in the development namespace, not in production or any other namespace.</p>"},{"location":"17-authorization/gotchas/#correct-approach-for-multi-namespace-access","title":"Correct Approach for Multi-Namespace Access","text":"<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: pod-reader\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\"]\n  verbs: [\"get\", \"list\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: pod-access-dev\n  namespace: development\nsubjects:\n- kind: User\n  name: developer\nroleRef:\n  kind: ClusterRole\n  name: pod-reader\n  apiGroup: rbac.authorization.k8s.io\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: pod-access-prod\n  namespace: production\nsubjects:\n- kind: User\n  name: developer\nroleRef:\n  kind: ClusterRole\n  name: pod-reader\n  apiGroup: rbac.authorization.k8s.io\n</code></pre> <p>Explanation: RoleBindings are always namespace-scoped. A single RoleBinding cannot grant access across multiple namespaces. To provide access in multiple namespaces, either create multiple RoleBindings or use a ClusterRoleBinding for cluster-wide access.</p>"},{"location":"17-authorization/gotchas/#44-clusterrole-with-rolebinding-scope-limitation","title":"4.4 ClusterRole with RoleBinding Scope Limitation","text":""},{"location":"17-authorization/gotchas/#misunderstanding","title":"Misunderstanding","text":"<p>Assuming ClusterRole automatically provides cluster-wide access.</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: pod-manager\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\"]\n  verbs: [\"get\", \"list\", \"create\", \"delete\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: pod-manager-binding\n  namespace: staging\nsubjects:\n- kind: User\n  name: operator\nroleRef:\n  kind: ClusterRole\n  name: pod-manager\n  apiGroup: rbac.authorization.k8s.io\n</code></pre> <p>Result: The operator only has pod management permissions in the staging namespace, not cluster-wide.</p>"},{"location":"17-authorization/gotchas/#correct-implementation-for-cluster-wide-access","title":"Correct Implementation for Cluster-Wide Access","text":"<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: pod-manager-global\nsubjects:\n- kind: User\n  name: operator\nroleRef:\n  kind: ClusterRole\n  name: pod-manager\n  apiGroup: rbac.authorization.k8s.io\n</code></pre> <p>Explanation: The scope of permissions is determined by the binding type, not the role type. A ClusterRole bound with a RoleBinding becomes namespace-scoped. For cluster-wide access, a ClusterRoleBinding must be used.</p>"},{"location":"17-authorization/gotchas/#45-subresource-permission-requirements","title":"4.5 Subresource Permission Requirements","text":""},{"location":"17-authorization/gotchas/#incorrect-implementation_2","title":"Incorrect Implementation","text":"<p>Assuming pod access includes subresource access.</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: pod-access\n  namespace: default\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\"]\n  verbs: [\"get\", \"list\"]\n</code></pre> <p>Result: Users can see pods but cannot view logs or execute commands inside containers.</p>"},{"location":"17-authorization/gotchas/#correct-implementation-with-subresources","title":"Correct Implementation with Subresources","text":"<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: pod-access\n  namespace: default\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\"]\n  verbs: [\"get\", \"list\"]\n- apiGroups: [\"\"]\n  resources: [\"pods/log\"]\n  verbs: [\"get\"]\n- apiGroups: [\"\"]\n  resources: [\"pods/exec\"]\n  verbs: [\"create\"]\n</code></pre> <p>Explanation: Subresources like logs, exec, port-forward, and attach require explicit permissions separate from the parent resource. Each subresource must be listed individually with appropriate verbs.</p>"},{"location":"17-authorization/gotchas/#46-resourcenames-limitation-with-list-verb","title":"4.6 ResourceNames Limitation with List Verb","text":""},{"location":"17-authorization/gotchas/#incorrect-implementation_3","title":"Incorrect Implementation","text":"<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: secret-access\n  namespace: default\nrules:\n- apiGroups: [\"\"]\n  resources: [\"secrets\"]\n  resourceNames: [\"db-credentials\", \"api-key\"]\n  verbs: [\"get\", \"list\"]\n</code></pre> <p>Result: Users can get specific secrets by name but cannot list all secrets.</p>"},{"location":"17-authorization/gotchas/#correct-implementation-separating-actions","title":"Correct Implementation Separating Actions","text":"<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: secret-access\n  namespace: default\nrules:\n- apiGroups: [\"\"]\n  resources: [\"secrets\"]\n  verbs: [\"list\"]\n- apiGroups: [\"\"]\n  resources: [\"secrets\"]\n  resourceNames: [\"db-credentials\", \"api-key\"]\n  verbs: [\"get\"]\n</code></pre> <p>Explanation: The list verb is incompatible with resourceNames. When resourceNames is specified, users can only access the named resources individually, not list all resources of that type. For list access, a separate rule without resourceNames is required.</p>"},{"location":"17-authorization/gotchas/#47-cross-namespace-role-reference-prohibition","title":"4.7 Cross-Namespace Role Reference Prohibition","text":""},{"location":"17-authorization/gotchas/#incorrect-setup","title":"Incorrect Setup","text":"<p>Attempting to reference a Role from another namespace.</p> <pre><code># Role exists in development namespace\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: pod-reader\n  namespace: development\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\"]\n  verbs: [\"get\", \"list\"]\n</code></pre> <pre><code># RoleBinding attempts to reference it from production namespace\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: pod-reader-binding\n  namespace: production\nsubjects:\n- kind: User\n  name: operator\nroleRef:\n  kind: Role\n  name: pod-reader\n  apiGroup: rbac.authorization.k8s.io\n</code></pre> <p>Result: The binding fails validation or becomes invalid.</p>"},{"location":"17-authorization/gotchas/#correct-solution","title":"Correct Solution","text":"<pre><code># Option 1: Create duplicate Role in production namespace\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: pod-reader\n  namespace: production\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\"]\n  verbs: [\"get\", \"list\"]\n\n# Option 2: Use ClusterRole and RoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: pod-reader\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\"]\n  verbs: [\"get\", \"list\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: pod-reader-binding\n  namespace: production\nsubjects:\n- kind: User\n  name: operator\nroleRef:\n  kind: ClusterRole\n  name: pod-reader\n  apiGroup: rbac.authorization.k8s.io\n</code></pre> <p>Explanation: Role resources are namespace-scoped and cannot be referenced across namespace boundaries. A RoleBinding can only reference a Role in the same namespace. For cross-namespace permission reuse, ClusterRoles should be used.</p>"},{"location":"17-authorization/gotchas/#48-cluster-scoped-resource-access-with-rolebinding","title":"4.8 Cluster-Scoped Resource Access with RoleBinding","text":""},{"location":"17-authorization/gotchas/#incorrect-expectation_1","title":"Incorrect Expectation","text":"<p>Attempting to grant node access through a RoleBinding.</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: node-viewer\nrules:\n- apiGroups: [\"\"]\n  resources: [\"nodes\"]\n  verbs: [\"get\", \"list\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: node-access\n  namespace: default\nsubjects:\n- kind: User\n  name: viewer\nroleRef:\n  kind: ClusterRole\n  name: node-viewer\n  apiGroup: rbac.authorization.k8s.io\n</code></pre> <p>Result: The user cannot access nodes despite the binding.</p>"},{"location":"17-authorization/gotchas/#correct-implementation_2","title":"Correct Implementation","text":"<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: node-access-global\nsubjects:\n- kind: User\n  name: viewer\nroleRef:\n  kind: ClusterRole\n  name: node-viewer\n  apiGroup: rbac.authorization.k8s.io\n</code></pre> <p>Explanation: Cluster-scoped resources like nodes, persistentvolumes, and namespaces cannot be accessed through namespace-scoped bindings. RoleBindings can only grant access to namespace-scoped resources, even when referencing a ClusterRole. For cluster-scoped resource access, ClusterRoleBinding must be used.</p>"},{"location":"17-authorization/gotchas/#49-missing-default-serviceaccount-permissions","title":"4.9 Missing Default ServiceAccount Permissions","text":""},{"location":"17-authorization/gotchas/#common-misunderstanding","title":"Common Misunderstanding","text":"<p>Assuming ServiceAccounts have default permissions.</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-app\n  namespace: default\nspec:\n  serviceAccountName: my-app-sa\n  containers:\n  - name: app\n    image: myapp:latest\n</code></pre> <p>Result: The pod cannot access any Kubernetes API resources.</p>"},{"location":"17-authorization/gotchas/#required-rbac-setup","title":"Required RBAC Setup","text":"<pre><code># First, create the ServiceAccount\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: my-app-sa\n  namespace: default\n---\n# Then, create permissions\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: my-app-role\n  namespace: default\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\"]\n  verbs: [\"get\", \"list\"]\n---\n# Finally, bind them\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: my-app-binding\n  namespace: default\nsubjects:\n- kind: ServiceAccount\n  name: my-app-sa\n  namespace: default\nroleRef:\n  kind: Role\n  name: my-app-role\n  apiGroup: rbac.authorization.k8s.io\n</code></pre> <p>Explanation: ServiceAccounts have no permissions by default. Unlike some cloud providers' IAM systems, Kubernetes RBAC requires explicit granting of all permissions. Each ServiceAccount starts with zero access and must be explicitly bound to Roles or ClusterRoles.</p>"},{"location":"17-authorization/gotchas/#410-verb-wildcard-incompatibility","title":"4.10 Verb Wildcard Incompatibility","text":""},{"location":"17-authorization/gotchas/#incorrect-implementation_4","title":"Incorrect Implementation","text":"<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: wildcard-role\n  namespace: default\nrules:\n- apiGroups: [\"\"]\n  resources: [\"configmaps\"]\n  resourceNames: [\"app-config\"]\n  verbs: [\"*\"]\n</code></pre> <p>Result: The wildcard verb may not work as expected with resourceNames.</p>"},{"location":"17-authorization/gotchas/#correct-implementation_3","title":"Correct Implementation","text":"<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: config-access\n  namespace: default\nrules:\n- apiGroups: [\"\"]\n  resources: [\"configmaps\"]\n  resourceNames: [\"app-config\"]\n  verbs: [\"get\", \"update\", \"patch\", \"delete\"]\n</code></pre> <p>Explanation: When using resourceNames, explicitly list the required verbs instead of using wildcards. Some verbs like list and watch may not be compatible with resourceNames restrictions.</p>"},{"location":"17-authorization/gotchas/#411-api-group-specification-for-custom-resources","title":"4.11 API Group Specification for Custom Resources","text":""},{"location":"17-authorization/gotchas/#incorrect-implementation_5","title":"Incorrect Implementation","text":"<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: crd-access\nrules:\n- apiGroups: [\"\"]\n  resources: [\"customresourcedefinitions\"]\n  verbs: [\"get\", \"list\"]\n</code></pre> <p>Result: Cannot access CRDs or custom resources.</p>"},{"location":"17-authorization/gotchas/#correct-implementation_4","title":"Correct Implementation","text":"<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: crd-access\nrules:\n- apiGroups: [\"apiextensions.k8s.io\"]\n  resources: [\"customresourcedefinitions\"]\n  verbs: [\"get\", \"list\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: custom-resource-access\nrules:\n- apiGroups: [\"mycompany.com\"]\n  resources: [\"myresources\"]\n  verbs: [\"get\", \"list\", \"create\"]\n</code></pre> <p>Explanation: CustomResourceDefinitions are in the apiextensions.k8s.io API group, not the core group. Custom resources themselves are in their own API groups defined by their CRD. Both the CRD access and the custom resource access require separate rules with correct API groups.</p>"},{"location":"17-authorization/gotchas/#412-role-binding-subject-namespace-requirement","title":"4.12 Role Binding Subject Namespace Requirement","text":""},{"location":"17-authorization/gotchas/#incorrect-implementation_6","title":"Incorrect Implementation","text":"<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: invalid-binding\n  namespace: app-ns\nsubjects:\n- kind: ServiceAccount\n  name: monitor-sa\nroleRef:\n  kind: Role\n  name: pod-reader\n  apiGroup: rbac.authorization.k8s.io\n</code></pre> <p>Result: Binding fails validation or ServiceAccount reference is invalid.</p>"},{"location":"17-authorization/gotchas/#correct-implementation_5","title":"Correct Implementation","text":"<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: valid-binding\n  namespace: app-ns\nsubjects:\n- kind: ServiceAccount\n  name: monitor-sa\n  namespace: monitoring-ns\nroleRef:\n  kind: Role\n  name: pod-reader\n  apiGroup: rbac.authorization.k8s.io\n</code></pre> <p>Explanation: When referencing a ServiceAccount in a RoleBinding, the namespace field is required in the subject definition. This is because ServiceAccounts are namespace-scoped resources, and the binding needs to know which namespace contains the ServiceAccount being referenced.</p>"},{"location":"17-authorization/gotchas/#413-non-resource-url-access-requirements","title":"4.13 Non-Resource URL Access Requirements","text":""},{"location":"17-authorization/gotchas/#incorrect-implementation_7","title":"Incorrect Implementation","text":"<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: health-checker\n  namespace: default\nrules:\n- apiGroups: [\"\"]\n  resources: [\"/healthz\"]\n  verbs: [\"get\"]\n</code></pre> <p>Result: Cannot access health check endpoints.</p>"},{"location":"17-authorization/gotchas/#correct-implementation_6","title":"Correct Implementation","text":"<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: health-checker\nrules:\n- nonResourceURLs: [\"/healthz\", \"/readyz\", \"/livez\"]\n  verbs: [\"get\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: health-check-binding\nsubjects:\n- kind: User\n  name: health-monitor\nroleRef:\n  kind: ClusterRole\n  name: health-checker\n  apiGroup: rbac.authorization.k8s.io\n</code></pre> <p>Explanation: Non-resource URLs like health check endpoints require ClusterRoles with nonResourceURLs field instead of resources field. They also require ClusterRoleBindings since these endpoints exist at the cluster level, not within namespaces.</p>"},{"location":"17-authorization/gotchas/#414-missing-api-group-for-apps-resources","title":"4.14 Missing API Group for Apps Resources","text":""},{"location":"17-authorization/gotchas/#incorrect-implementation_8","title":"Incorrect Implementation","text":"<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: deployment-access\n  namespace: default\nrules:\n- apiGroups: [\"\"]\n  resources: [\"deployments\"]\n  verbs: [\"get\", \"list\"]\n</code></pre> <p>Result: Cannot access deployments.</p>"},{"location":"17-authorization/gotchas/#correct-implementation_7","title":"Correct Implementation","text":"<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: deployment-access\n  namespace: default\nrules:\n- apiGroups: [\"apps\"]\n  resources: [\"deployments\"]\n  verbs: [\"get\", \"list\"]\n</code></pre> <p>Explanation: Deployments, StatefulSets, and DaemonSets are in the apps API group, not the core API group. Many resources that were originally in extensions/v1beta1 have moved to stable API groups like apps/v1.</p>"},{"location":"17-authorization/gotchas/#415-aggregated-clusterrole-rule-inheritance","title":"4.15 Aggregated ClusterRole Rule Inheritance","text":""},{"location":"17-authorization/gotchas/#incorrect-expectation_2","title":"Incorrect Expectation","text":"<p>Assuming aggregated ClusterRoles automatically update.</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: aggregated-role\n  labels:\n    rbac.example.com/aggregate-to-monitoring: \"true\"\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\"]\n  verbs: [\"get\", \"list\"]\n</code></pre> <p>Result: The rules are not aggregated to other ClusterRoles.</p>"},{"location":"17-authorization/gotchas/#correct-aggregation-setup","title":"Correct Aggregation Setup","text":"<pre><code># Base ClusterRole that will receive aggregated rules\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: monitoring-role\naggregationRule:\n  clusterRoleSelectors:\n  - matchLabels:\n      rbac.example.com/aggregate-to-monitoring: \"true\"\nrules: []  # Rules will be aggregated here\n---\n# ClusterRole that contributes rules\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: monitoring-pods\n  labels:\n    rbac.example.com/aggregate-to-monitoring: \"true\"\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\"]\n  verbs: [\"get\", \"list\"]\n---\n# Another contributing ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: monitoring-services\n  labels:\n    rbac.example.com/aggregate-to-monitoring: \"true\"\nrules:\n- apiGroups: [\"\"]\n  resources: [\"services\"]\n  verbs: [\"get\", \"list\"]\n</code></pre> <p>Explanation: Aggregated ClusterRoles use the aggregationRule field to dynamically collect rules from other ClusterRoles with matching labels. The base ClusterRole has empty rules initially, and contributing ClusterRoles must have the matching label. The aggregation is not automatic for regular ClusterRoles.</p>"},{"location":"17-authorization/gotchas/#summary-of-key-rbac-principles","title":"Summary of Key RBAC Principles","text":"<ol> <li>Empty string represents the core API group</li> <li>ServiceAccount subjects do not use apiGroup field</li> <li>RoleBindings are always namespace-scoped</li> <li>ClusterRoles become namespace-scoped when bound with RoleBindings</li> <li>Subresources require explicit permissions</li> <li>ResourceNames is incompatible with list verb</li> <li>Roles cannot be referenced across namespaces</li> <li>Cluster-scoped resources require ClusterRoleBindings</li> <li>ServiceAccounts have no default permissions</li> <li>Custom resources have specific API groups</li> <li>Non-resource URLs require ClusterRoles with nonResourceURLs field</li> <li>Apps resources are in the apps API group, not core</li> </ol> <p>Each example demonstrates a specific gotcha with both incorrect and correct implementations, showing exactly what fails and how to fix it.</p>"},{"location":"17-authorization/role-clusterrole/","title":"RBAC Core Objects","text":""},{"location":"17-authorization/role-clusterrole/#1-four-objects","title":"1. Four Objects","text":"<ul> <li>Role: Namespaced permissions definition</li> <li>ClusterRole: Cluster-wide permissions definition  </li> <li>RoleBinding: Grants namespace-scoped access</li> <li>ClusterRoleBinding: Grants cluster-wide access</li> </ul>"},{"location":"17-authorization/role-clusterrole/#2-responsibility-split","title":"2. Responsibility Split","text":"<p>Permissions (WHAT): Role/ClusterRole Scope (WHERE): Binding type determines namespace/global Access (WHO): Binding links subjects to permissions</p>"},{"location":"17-authorization/role-clusterrole/#3-key-rules","title":"3. Key Rules","text":"<ul> <li>No binding = no access</li> <li>Binding always decides scope</li> <li>RoleBinding always namespace-scoped</li> <li>ClusterRoleBinding always cluster-wide</li> </ul>"},{"location":"17-authorization/role-clusterrole/#clusterrole-rolebinding-critical-concept","title":"ClusterRole + RoleBinding - Critical Concept","text":""},{"location":"17-authorization/role-clusterrole/#core-principle","title":"Core Principle","text":"<p>Permissions come from Role/ClusterRole Scope comes from Binding type</p>"},{"location":"17-authorization/role-clusterrole/#scope-matrix","title":"Scope Matrix","text":"Role Type Binding Type Scope Role RoleBinding Single namespace ClusterRole RoleBinding Single namespace ClusterRole ClusterRoleBinding Entire cluster"},{"location":"17-authorization/role-clusterrole/#why-clusterrole-exists","title":"Why ClusterRole Exists","text":"<p>Avoid duplicating same Role across namespaces. ClusterRole can be: - Bound cluster-wide (ClusterRoleBinding) - Limited to namespace (RoleBinding)</p>"},{"location":"17-authorization/role-clusterrole/#gotcha-cluster-scoped-resources","title":"Gotcha: Cluster-Scoped Resources","text":"<p>If ClusterRole contains cluster-scoped resources (nodes, PVs): - RoleBinding \u2192 IGNORES cluster-scoped permissions - ClusterRoleBinding \u2192 Grants them globally</p>"},{"location":"17-authorization/role-clusterrole/#key-takeaway","title":"Key Takeaway","text":"<p>ClusterRole \u2260 cluster-wide access Binding type determines scope, not role type.</p> <p>Below is ONLY SECTION 2.1. Strictly about scoping, namespaces, and what can / cannot reference what.</p>"},{"location":"17-authorization/role-clusterrole/#21-scoping-rules-namespaced-vs-cluster-scoped-very-important","title":"2.1 Scoping Rules \u2014 Namespaced vs Cluster-Scoped (Very Important)","text":""},{"location":"17-authorization/role-clusterrole/#fundamental-rule","title":"Fundamental Rule","text":"<p>RBAC object scope is fixed and cannot be changed.</p> <p>Some objects are namespaced, some are cluster-scoped. You cannot bend this rule.</p>"},{"location":"17-authorization/role-clusterrole/#which-rbac-objects-are-namespaced","title":"Which RBAC Objects Are Namespaced","text":"<p>These belong to exactly one namespace:</p> <ul> <li><code>Role</code></li> <li><code>RoleBinding</code></li> <li><code>ServiceAccount</code></li> </ul> <p>You must specify <code>metadata.namespace</code> for them.</p>"},{"location":"17-authorization/role-clusterrole/#which-rbac-objects-are-cluster-scoped","title":"Which RBAC Objects Are Cluster-Scoped","text":"<p>These do not belong to any namespace:</p> <ul> <li><code>ClusterRole</code></li> <li><code>ClusterRoleBinding</code></li> </ul> <p>They have no namespace field.</p>"},{"location":"17-authorization/role-clusterrole/#what-this-means-practically","title":"What This Means Practically","text":""},{"location":"17-authorization/role-clusterrole/#role","title":"Role","text":"<ul> <li>Exists in one namespace</li> <li> <p>Can only define permissions for:</p> </li> <li> <p>Namespaced resources</p> </li> <li>Cannot reference cluster-scoped resources (nodes, namespaces)</li> </ul>"},{"location":"17-authorization/role-clusterrole/#rolebinding","title":"RoleBinding","text":"<ul> <li>Exists in one namespace</li> <li>Grants access only inside that namespace</li> <li> <p>Can reference:</p> </li> <li> <p>A Role (same namespace only)</p> </li> <li>A ClusterRole (global, but scope limited by RoleBinding namespace)</li> </ul>"},{"location":"17-authorization/role-clusterrole/#clusterrole","title":"ClusterRole","text":"<ul> <li>Exists at cluster level</li> <li> <p>Can define permissions for:</p> </li> <li> <p>Namespaced resources</p> </li> <li>Cluster-scoped resources</li> <li>Has no scope until bound</li> </ul>"},{"location":"17-authorization/role-clusterrole/#clusterrolebinding","title":"ClusterRoleBinding","text":"<ul> <li>Exists at cluster level</li> <li> <p>Grants access:</p> </li> <li> <p>Across all namespaces</p> </li> <li>To cluster-scoped resources</li> </ul>"},{"location":"17-authorization/role-clusterrole/#namespace-crossing-rules-critical","title":"Namespace Crossing Rules (Critical)","text":""},{"location":"17-authorization/role-clusterrole/#can-a-rolebinding-reference-a-role-from-another-namespace","title":"Can a RoleBinding reference a Role from another namespace?","text":"<p>\u274c No</p> <pre><code>roleRef:\n  kind: Role\n  name: some-role\n</code></pre> <p>That Role must exist in the same namespace as the RoleBinding.</p>"},{"location":"17-authorization/role-clusterrole/#can-a-rolebinding-reference-a-clusterrole","title":"Can a RoleBinding reference a ClusterRole?","text":"<p>\u2705 Yes</p> <pre><code>roleRef:\n  kind: ClusterRole\n  name: view\n</code></pre> <p>This is the intended reuse pattern.</p>"},{"location":"17-authorization/role-clusterrole/#can-a-rolebinding-grant-access-outside-its-namespace","title":"Can a RoleBinding grant access outside its namespace?","text":"<p>\u274c No</p> <p>Even if:</p> <ul> <li>Subject is a global User</li> <li>Subject is a Group</li> <li>Subject is a ServiceAccount from another namespace</li> </ul> <p>Access is still limited to the RoleBinding namespace.</p>"},{"location":"17-authorization/role-clusterrole/#serviceaccount-namespace-rule-important","title":"ServiceAccount Namespace Rule (Important)","text":"<p>ServiceAccounts are namespaced identities.</p> <p>This identity:</p> <pre><code>system:serviceaccount:monitoring:prometheus\n</code></pre> <ul> <li>Belongs to <code>monitoring</code></li> <li>Can be referenced in any RoleBinding</li> <li>Gets access only in the RoleBinding namespace</li> </ul>"},{"location":"17-authorization/role-clusterrole/#example-cross-namespace-serviceaccount-access","title":"Example: Cross-Namespace ServiceAccount Access","text":"<pre><code>kind: RoleBinding\nmetadata:\n  namespace: production\nsubjects:\n- kind: ServiceAccount\n  name: prometheus\n  namespace: monitoring\nroleRef:\n  kind: ClusterRole\n  name: pod-reader\n  apiGroup: rbac.authorization.k8s.io\n</code></pre> <p>Result:</p> <ul> <li>Prometheus SA can read pods in <code>production</code></li> <li>Identity remains in <code>monitoring</code></li> </ul>"},{"location":"17-authorization/role-clusterrole/#cluster-scoped-resources-vs-rolebinding-exam-trap","title":"Cluster-Scoped Resources vs RoleBinding (Exam Trap)","text":""},{"location":"17-authorization/role-clusterrole/#cluster-scoped-resources","title":"Cluster-scoped resources:","text":"<ul> <li><code>nodes</code></li> <li><code>namespaces</code></li> <li><code>persistentvolumes</code></li> <li><code>clusterroles</code></li> <li><code>clusterrolebindings</code></li> </ul> <p>These cannot be granted using a RoleBinding.</p> <p>This silently fails:</p> <pre><code>rules:\n- apiGroups: [\"\"]\n  resources: [\"nodes\"]\n  verbs: [\"get\"]\n</code></pre> <p>Bound via RoleBinding \u2192 ignored</p>"},{"location":"17-authorization/role-clusterrole/#correct-way","title":"Correct Way","text":"<p>You must use ClusterRoleBinding.</p>"},{"location":"17-authorization/role-clusterrole/#how-to-verify-scope-using-cli-source-of-truth","title":"How to Verify Scope Using CLI (Source of Truth)","text":""},{"location":"17-authorization/role-clusterrole/#list-all-resources-with-scope","title":"List All Resources With Scope","text":"<pre><code>kubectl api-resources\n</code></pre> <p>Look at:</p> <pre><code>NAMESPACED\n</code></pre>"},{"location":"17-authorization/role-clusterrole/#namespaced-only","title":"Namespaced Only","text":"<pre><code>kubectl api-resources --namespaced=true\n</code></pre>"},{"location":"17-authorization/role-clusterrole/#cluster-scoped-only","title":"Cluster-Scoped Only","text":"<pre><code>kubectl api-resources --namespaced=false\n</code></pre>"},{"location":"17-authorization/role-clusterrole/#rbac-specific-check","title":"RBAC-Specific Check","text":"<pre><code>kubectl api-resources | grep -i role\n</code></pre> <p>Expected:</p> <ul> <li>roles \u2192 namespaced</li> <li>rolebindings \u2192 namespaced</li> <li>clusterroles \u2192 cluster-scoped</li> <li>clusterrolebindings \u2192 cluster-scoped</li> </ul>"},{"location":"17-authorization/role-clusterrole/#mental-locks-for-section-21","title":"Mental Locks for Section 2.1","text":"<ul> <li>RoleBinding scope = its namespace</li> <li>Role must be in same namespace as RoleBinding</li> <li>ClusterRole can be reused anywhere</li> <li>RoleBinding cannot grant cluster-scoped resources</li> <li><code>kubectl api-resources</code> is the authority</li> </ul> <p>Say \u201cnext\u201d when you want Section 3 (examples: YAML + CLI mixes).</p>"},{"location":"18-basic-bash/bash/","title":"Bash","text":"<p>base64 \"$TV ... \" -&gt; can render only man!!</p>"},{"location":"20-services/cli/","title":"Cli","text":"<p>create services using cli is must learnable art</p>"},{"location":"21-storage-class-pv-pvc/example/","title":"Example","text":"<p>provisioner volumeBindingMode</p> <p>accessModes storageClassName capacity hostPath</p> <p>accessModes resources storageClassName</p> <p>https://killercoda.com/sachin/course/CKA/sc-pv-pvc-pod</p> <p>apiVersion: storage.k8s.io/v1 kind: StorageClass metadata:   name: blue-stc-cka provisioner: kubernetes.io/no-provisioner volumeBindingMode: WaitForFirstConsumer</p> <p>apiVersion: v1 kind: PersistentVolume metadata:   name: blue-pv-cka spec:   capacity:     storage: 100Mi   accessModes:   - ReadWriteOnce   persistentVolumeReclaimPolicy: Retain   storageClassName: blue-stc-cka   local:     path: /opt/blue-data-cka   nodeAffinity:     required:       nodeSelectorTerms:       - matchExpressions:         - key: kubernetes.io/hostname           operator: In           values:           - controlplane</p> <p>apiVersion: v1 kind: PersistentVolumeClaim metadata:   name: blue-pvc-cka spec:   accessModes:   - ReadWriteOnce   resources:     requests:       storage: 50Mi   storageClassName: blue-stc-cka   volumeName: blue-pv-cka</p> <p>controlplane:~$ k get sc -o yaml apiVersion: v1 items: - apiVersion: storage.k8s.io/v1   kind: StorageClass   metadata:     annotations:       kubectl.kubernetes.io/last-applied-configuration: |         {\"apiVersion\":\"storage.k8s.io/v1\",\"kind\":\"StorageClass\",\"metadata\":{\"annotations\":{},\"name\":\"local-path\"},\"provisioner\":\"rancher.io/local-path\",\"reclaimPolicy\":\"Delete\",\"volumeBindingMode\":\"WaitForFirstConsumer\"}       storageclass.kubernetes.io/is-default-class: \"true\"     creationTimestamp: \"2025-11-17T19:11:36Z\"     name: local-path     resourceVersion: \"821\"     uid: 2fa12284-994b-4e2c-811c-6efc5dd0c132   provisioner: rancher.io/local-path   reclaimPolicy: Delete   volumeBindingMode: WaitForFirstConsumer - allowVolumeExpansion: true   apiVersion: storage.k8s.io/v1   kind: StorageClass   metadata:     annotations:       kubectl.kubernetes.io/last-applied-configuration: |         {\"allowVolumeExpansion\":true,\"apiVersion\":\"storage.k8s.io/v1\",\"kind\":\"StorageClass\",\"metadata\":{\"annotations\":{},\"name\":\"nginx-stc-cka\"},\"provisioner\":\"kubernetes.io/no-provisioner\",\"volumeBindingMode\":\"WaitForFirstConsumer\"}     creationTimestamp: \"2025-12-16T20:50:53Z\"     name: nginx-stc-cka     resourceVersion: \"6400\"     uid: dc8fa5cb-bf0f-4abb-a10e-8c9517d9d299   provisioner: kubernetes.io/no-provisioner   reclaimPolicy: Delete   volumeBindingMode: WaitForFirstConsumer</p>"}]}