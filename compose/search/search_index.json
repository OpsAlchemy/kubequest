{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"CKA Always focus on practical tips only. Always use the command line - it is much faster than editing YAML manually. Example: create or modify resources directly using CLI flags instead of opening an editor. Always take a backup before making any changes. Example: save the current state of a resource before modifying it. Use kubectl explain to verify syntax and object structure. Example: check available fields before writing or updating a manifest. k api-resoruces is good thing","title":"CKA"},{"location":"#cka","text":"Always focus on practical tips only. Always use the command line - it is much faster than editing YAML manually. Example: create or modify resources directly using CLI flags instead of opening an editor. Always take a backup before making any changes. Example: save the current state of a resource before modifying it. Use kubectl explain to verify syntax and object structure. Example: check available fields before writing or updating a manifest. k api-resoruces is good thing","title":"CKA"},{"location":"01-autoscaling/hpa/","text":"Horizontal Pod Autoscaler (HPA) \ud83d\udcca Metrics Server Installation & Configuration Purpose The Metrics Server collects resource utilization data (CPU/Memory) from Kubernetes nodes and pods, enabling the kubectl top command and providing metrics to the Horizontal Pod Autoscaler (HPA). Core Issue in Labs Lab environments like KillerCoda often have self-signed or untrusted TLS certificates between components. Without bypassing TLS verification, the Metrics Server cannot scrape metrics from kubelets. Recommended Installation Method (Helm) Standard Installation # Add Helm repository helm repo add metrics-server https://kubernetes-sigs.github.io/metrics-server/ helm repo update # Install with lab-friendly configuration helm install metrics-server metrics-server/metrics-server \\ --namespace kube-system \\ --set 'args={--cert-dir=/tmp,--secure-port=4443,--kubelet-insecure-tls,--kubelet-preferred-address-types=InternalIP}' \\ --set containerPort = 4443 \\ --set 'livenessProbe.httpGet.path=/livez' \\ --set 'livenessProbe.httpGet.port=4443' \\ --set 'livenessProbe.httpGet.scheme=HTTPS' \\ --set 'readinessProbe.httpGet.path=/readyz' \\ --set 'readinessProbe.httpGet.port=4443' \\ --set 'readinessProbe.httpGet.scheme=HTTPS' \\ --set service.port = 4443 \\ --set service.targetPort = 4443 Key Configuration Parameters Explained Parameter Purpose Why It's Needed --kubelet-insecure-tls Disables TLS verification Lab environments have self-signed certs --kubelet-preferred-address-types=InternalIP Uses internal IPs Ensures correct network connectivity containerPort: 4443 Explicit port definition Avoids conflicts with default ports Port 4443 in probes Consistent port usage Health checks match actual listening port HTTPS scheme Secure connections Required for metrics server security Verification Steps # Wait 20-30 seconds for initialization sleep 20 # Check pod status kubectl get pods -n kube-system -l app.kubernetes.io/name = metrics-server # Test metrics collection kubectl top nodes kubectl top pods # Verify API service kubectl get apiservice v1beta1.metrics.k8s.io # Should show: AVAILABLE=True Troubleshooting Metrics Server # Check logs for connection issues kubectl logs -n kube-system deployment/metrics-server # If 'Failed to scrape node', verify network policies kubectl get networkpolicies -A # Test direct metrics API access kubectl get --raw \"/apis/metrics.k8s.io/v1beta1/nodes\" | head -20 \u2699\ufe0f Horizontal Pod Autoscaler (HPA) - Complete Guide What HPA Does HPA automatically adjusts the number of pod replicas in a deployment based on observed metrics, maintaining your defined target utilization. Core Formula : desiredReplicas = ceil(currentReplicas \u00d7 (currentMetricValue / desiredMetricValue)) HPA Architecture Diagram \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 HPA Controller \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 Get Metrics \u2502\u2500\u2500\u2500\u2500\u25b6\u2502 Calculate \u2502\u2500\u2500\u2500\u2500\u25b6\u2502 Update \u2502 \u2502 \u2502 \u2502 from API \u2502 \u2502 Desired \u2502 \u2502 Deployment \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 Replicas \u2502 \u2502 Replicas \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Metrics API \u2502 \u2502 Deployment \u2502 \u2502 (Metrics- \u2502 \u2502 Controller \u2502 \u2502 Server) \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Kubelet \u2502 \u2502 (Nodes/Pods) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Basic HPA Structure apiVersion : autoscaling/v2 kind : HorizontalPodAutoscaler metadata : name : app-hpa spec : scaleTargetRef : apiVersion : apps/v1 kind : Deployment name : your-app minReplicas : 1 # Minimum pods for availability maxReplicas : 10 # Maximum pods for cost control metrics : # What to monitor - type : Resource resource : name : cpu target : type : Utilization averageUtilization : 70 # Target 70% CPU usage behavior : # How to scale (optional but recommended) scaleUp : stabilizationWindowSeconds : 0 policies : [ ... ] scaleDown : stabilizationWindowSeconds : 300 policies : [ ... ] Prerequisite: Resource Requests Critical : Pods must have resource requests for HPA to calculate utilization percentages. # In your deployment template resources : requests : memory : \"256Mi\" # HPA uses: (actual usage / 256Mi) \u00d7 100% cpu : \"250m\" # Requested = 0.25 CPU cores limits : memory : \"512Mi\" cpu : \"500m\" Common Mistake : Using wrong units - \u274c CPU: \"256Mi\" (Mi is for memory!) - \u2705 CPU: \"250m\" (millicores) or \"0.25\" - \u2705 Memory: \"256Mi\" (mebibytes) or \"512Mi\" \ud83c\udf9b\ufe0f HPA Behavior Configuration - Complete Details Purpose of Behavior Settings Control the timing , speed , and magnitude of scaling operations to prevent rapid oscillations (\"flapping\") and ensure stable application performance. Core Components Explained 1. stabilizationWindowSeconds A waiting period after a metric change before taking scaling action. scaleDown : stabilizationWindowSeconds : 300 # Wait 5 minutes of low usage before scaling down Use Case Recommended Value Scale Up (responsive) 0-30 seconds Scale Down (conservative) 300-600 seconds Stateful applications 600+ seconds 2. policies - Scaling Rules Define how many pods can be added/removed in a time window. policies : - type : Pods # Fixed number of pods value : 2 # Add/remove 2 pods max periodSeconds : 60 # Every 60 seconds - type : Percent # Percentage of current pods value : 50 # Add/remove 50% of current pods periodSeconds : 30 # Every 30 seconds Policy Types Comparison : | Type | Best For | Example | Result (10 pods \u2192 ?) | |------|----------|---------|----------------------| | Pods | Predictable scaling | value: 3 | Add/remove exactly 3 pods | | Percent | Proportional scaling | value: 50 | Add/remove 5 pods (50% of 10) | 3. selectPolicy - Choosing Between Policies When multiple policies exist, which one to apply? selectPolicy : Max # Use the policy allowing the BIGGEST change Policy Behavior Use Case Max Uses policy allowing largest change Fast scaling response Min Uses policy allowing smallest change Conservative, safe scaling Disabled No scaling in this direction Disable scale-up/down Example with Max : policies : - type : Pods value : 2 # Can change 2 pods - type : Percent value : 100 # Can change 100% of pods (all of them!) # With selectPolicy: Max \u2192 Uses Percent: 100 (bigger change) # With selectPolicy: Min \u2192 Uses Pods: 2 (smaller change) Complete Behavior Configuration Examples 1. Web Application Pattern (Responsive) behavior : scaleUp : stabilizationWindowSeconds : 0 # React immediately to traffic spikes policies : - type : Pods value : 4 periodSeconds : 15 - type : Percent value : 100 periodSeconds : 15 selectPolicy : Max # Scale aggressively during spikes scaleDown : stabilizationWindowSeconds : 300 # Wait 5 minutes before scaling down policies : - type : Percent value : 50 periodSeconds : 60 selectPolicy : Max 2. Batch Processing Pattern (Conservative) behavior : scaleUp : stabilizationWindowSeconds : 60 # Wait 1 minute to confirm need policies : - type : Pods value : 1 periodSeconds : 90 selectPolicy : Max scaleDown : stabilizationWindowSeconds : 600 # Wait 10 minutes (long jobs) policies : - type : Pods value : 1 periodSeconds : 180 selectPolicy : Min # Scale down very slowly 3. Real-time/Streaming Pattern (Aggressive) behavior : scaleUp : stabilizationWindowSeconds : 0 policies : - type : Percent value : 200 # Can double pod count periodSeconds : 10 selectPolicy : Max scaleDown : stabilizationWindowSeconds : 60 policies : - type : Percent value : 100 # Can remove all extra pods periodSeconds : 30 selectPolicy : Max \ud83e\uddea Load Testing with stress Command Purpose of Load Testing Verify HPA triggers at correct thresholds Test scaling behavior under controlled conditions Identify optimal resource requests and limits Using the polinux/stress Image Basic Test Pod # Create a pod for testing kubectl run stress-test --image = polinux/stress --command -- sleep 3600 # Run stress commands inside the pod kubectl exec stress-test -- stress --vm 1 --vm-bytes 100M --timeout 60 Memory Stress Testing # 1. Memory Stress Only # Syntax: --vm N (workers) --vm-bytes X (memory per worker) --timeout T (seconds) kubectl exec <pod> -- stress --vm 2 --vm-bytes 150M --timeout 120 # 2. Heavy Memory Load kubectl exec <pod> -- stress --vm 4 --vm-bytes 500M --timeout 180 # 3. Continuous Memory Stress (background) kubectl exec <pod> -- stress --vm 1 --vm-bytes 100M --timeout 600 & CPU Stress Testing # 1. CPU Stress Only # Syntax: --cpu N (workers) --timeout T (seconds) kubectl exec <pod> -- stress --cpu 4 --timeout 90 # 2. Heavy CPU Load (all cores) kubectl exec <pod> -- stress --cpu 8 --timeout 120 # 3. Mixed CPU Load (varying intensity) kubectl exec <pod> -- stress --cpu 2 --timeout 60 & \\ kubectl exec <pod> -- stress --cpu 4 --timeout 60 Combined Stress Testing # 1. Memory + CPU Combined Stress kubectl exec <pod> -- stress --vm 1 --vm-bytes 200M --cpu 2 --timeout 180 # 2. Heavy Combined Load kubectl exec <pod> -- stress --vm 2 --vm-bytes 300M --cpu 4 --timeout 240 # 3. Sequential Stress Testing kubectl exec <pod> -- stress --cpu 4 --timeout 60 kubectl exec <pod> -- stress --vm 2 --vm-bytes 100M --timeout 60 kubectl exec <pod> -- stress --cpu 2 --vm 1 --vm-bytes 150M --timeout 90 Testing HPA with Memory Metrics # Given: HPA target is 70% memory utilization # Given: Pod requests 128Mi memory # Calculation: Need ~90Mi usage to trigger scaling (128Mi \u00d7 70% = 89.6Mi) # Trigger scaling kubectl exec deployment/your-app -- stress --vm 1 --vm-bytes 100M --timeout 300 # Monitor scaling watch -n 5 'kubectl get hpa,pods && echo \"---\" && kubectl top pods' Testing HPA with CPU Metrics # Given: HPA target is 50% CPU utilization # Given: Pod requests 500m CPU (0.5 cores) # Calculation: Need ~250m usage to trigger scaling (500m \u00d7 50% = 250m) # Trigger CPU scaling kubectl exec deployment/your-app -- stress --cpu 2 --timeout 300 # Monitor CPU-based scaling watch -n 3 'kubectl get hpa && echo \"CPU Usage:\" && kubectl top pods | grep your-app' Multi-Metric HPA Testing # Create HPA watching both CPU and Memory cat <<EOF | kubectl apply -f - apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: multi-hpa spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: your-app minReplicas: 1 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 60 - type: Resource resource: name: memory target: type: Utilization averageUtilization: 70 EOF # Test both resources kubectl exec deployment/your-app -- stress --cpu 4 --vm 2 --vm-bytes 200M --timeout 300 \ud83d\udd0d HPA Status Interpretation Checking HPA Status kubectl get hpa # Output shows: REFERENCE, TARGETS, MINPODS, MAXPODS, REPLICAS kubectl describe hpa your-hpa # Shows conditions, events, and detailed metrics Key HPA Conditions Condition Status Meaning Action Required AbleToScale True/False Can HPA modify replicas? Check permissions if False ScalingActive True/False Getting metrics successfully? Fix metrics server if False ScalingLimited True/False Hit min/max replica limits? Adjust min/max if True Common HPA Events kubectl describe hpa | grep -A5 \"Events:\" Event Meaning Typical Cause SuccessfulRescale HPA changed replica count Normal operation FailedGetResourceMetric Can't fetch metrics Metrics server down FailedComputeMetricsReplicas Can't calculate desired replicas Invalid metric configuration \ud83d\udea8 Troubleshooting Common Issues Issue 1: HPA shows <unknown> targets Cause : Metrics Server not working Solution : # Check metrics-server pod kubectl get pods -n kube-system | grep metrics-server # Check logs kubectl logs -n kube-system deployment/metrics-server # Reinstall if needed (with correct flags) helm upgrade metrics-server --reuse-values --set 'args={--kubelet-insecure-tls}' Issue 2: HPA not scaling when expected Checklist : 1. Verify pod has resource requests ( kubectl get pod -o yaml | grep requests ) 2. Check current vs target metrics ( kubectl describe hpa ) 3. Verify HPA is not limited ( ScalingLimited condition) 4. Ensure metrics are above target threshold Issue 3: Rapid scaling oscillations (\"flapping\") Solution : Adjust behavior settings behavior : scaleDown : stabilizationWindowSeconds : 600 # Increase from 300 to 600 policies : - type : Percent value : 20 # Reduce from 50% to 20% periodSeconds : 120 \ud83d\udccb Quick Reference Commands HPA Management # Create HPA with kubectl kubectl autoscale deployment/my-app --cpu-percent = 50 --min = 1 --max = 5 # Get HPA information kubectl get hpa kubectl describe hpa <name> kubectl get hpa <name> -o yaml # Edit HPA kubectl edit hpa <name> # Delete HPA kubectl delete hpa <name> Monitoring & Testing # Watch scaling in real-time watch -n 2 'kubectl get hpa,deploy,pods && echo \"---\" && kubectl top pods 2>/dev/null || echo \"Metrics loading...\"' # Generate memory load for testing kubectl exec deployment/<name> -- stress --vm 1 --vm-bytes 150M --timeout 120 # Generate CPU load for testing kubectl exec deployment/<name> -- stress --cpu 4 --timeout 120 # Check resource usage kubectl top pods kubectl top pods --containers # Show container-level usage Debugging # Check HPA events kubectl get events --field-selector involvedObject.kind = HorizontalPodAutoscaler --sort-by = .metadata.creationTimestamp # Verify API access kubectl get --raw \"/apis/metrics.k8s.io/v1beta1/nodes\" | jq '.items[0].usage' # Check pod details kubectl describe pod <pod-name> | grep -A10 \"Resources:\" \ud83d\udca1 Best Practices Summary Always set resource requests in deployments (required for HPA calculations) Start with conservative behavior and adjust based on observations Test both CPU and memory scaling before production deployment Monitor HPA events for failed scaling attempts Use appropriate min/max values based on application needs and cluster capacity Regularly review metrics to ensure targets are appropriate Consider combining with VPA (Vertical Pod Autoscaler) for right-sizing requests \u26a0\ufe0f Common Pitfalls to Avoid \u274c Forgetting to install/configure Metrics Server \u274c Not setting resource requests in pods \u274c Setting minReplicas to 0 for stateful applications \u274c Too aggressive scale-down causing application instability \u274c Incorrect metric units (using Mi for CPU, m for memory) \u274c Not monitoring HPA events for failures \u274c Testing only one resource type (CPU or Memory) when both matter","title":"Horizontal Pod Autoscaler (HPA)"},{"location":"01-autoscaling/hpa/#horizontal-pod-autoscaler-hpa","text":"","title":"Horizontal Pod Autoscaler (HPA)"},{"location":"01-autoscaling/hpa/#metrics-server-installation-configuration","text":"","title":"\ud83d\udcca Metrics Server Installation &amp; Configuration"},{"location":"01-autoscaling/hpa/#purpose","text":"The Metrics Server collects resource utilization data (CPU/Memory) from Kubernetes nodes and pods, enabling the kubectl top command and providing metrics to the Horizontal Pod Autoscaler (HPA).","title":"Purpose"},{"location":"01-autoscaling/hpa/#core-issue-in-labs","text":"Lab environments like KillerCoda often have self-signed or untrusted TLS certificates between components. Without bypassing TLS verification, the Metrics Server cannot scrape metrics from kubelets.","title":"Core Issue in Labs"},{"location":"01-autoscaling/hpa/#recommended-installation-method-helm","text":"","title":"Recommended Installation Method (Helm)"},{"location":"01-autoscaling/hpa/#standard-installation","text":"# Add Helm repository helm repo add metrics-server https://kubernetes-sigs.github.io/metrics-server/ helm repo update # Install with lab-friendly configuration helm install metrics-server metrics-server/metrics-server \\ --namespace kube-system \\ --set 'args={--cert-dir=/tmp,--secure-port=4443,--kubelet-insecure-tls,--kubelet-preferred-address-types=InternalIP}' \\ --set containerPort = 4443 \\ --set 'livenessProbe.httpGet.path=/livez' \\ --set 'livenessProbe.httpGet.port=4443' \\ --set 'livenessProbe.httpGet.scheme=HTTPS' \\ --set 'readinessProbe.httpGet.path=/readyz' \\ --set 'readinessProbe.httpGet.port=4443' \\ --set 'readinessProbe.httpGet.scheme=HTTPS' \\ --set service.port = 4443 \\ --set service.targetPort = 4443","title":"Standard Installation"},{"location":"01-autoscaling/hpa/#key-configuration-parameters-explained","text":"Parameter Purpose Why It's Needed --kubelet-insecure-tls Disables TLS verification Lab environments have self-signed certs --kubelet-preferred-address-types=InternalIP Uses internal IPs Ensures correct network connectivity containerPort: 4443 Explicit port definition Avoids conflicts with default ports Port 4443 in probes Consistent port usage Health checks match actual listening port HTTPS scheme Secure connections Required for metrics server security","title":"Key Configuration Parameters Explained"},{"location":"01-autoscaling/hpa/#verification-steps","text":"# Wait 20-30 seconds for initialization sleep 20 # Check pod status kubectl get pods -n kube-system -l app.kubernetes.io/name = metrics-server # Test metrics collection kubectl top nodes kubectl top pods # Verify API service kubectl get apiservice v1beta1.metrics.k8s.io # Should show: AVAILABLE=True","title":"Verification Steps"},{"location":"01-autoscaling/hpa/#troubleshooting-metrics-server","text":"# Check logs for connection issues kubectl logs -n kube-system deployment/metrics-server # If 'Failed to scrape node', verify network policies kubectl get networkpolicies -A # Test direct metrics API access kubectl get --raw \"/apis/metrics.k8s.io/v1beta1/nodes\" | head -20","title":"Troubleshooting Metrics Server"},{"location":"01-autoscaling/hpa/#horizontal-pod-autoscaler-hpa-complete-guide","text":"","title":"\u2699\ufe0f Horizontal Pod Autoscaler (HPA) - Complete Guide"},{"location":"01-autoscaling/hpa/#what-hpa-does","text":"HPA automatically adjusts the number of pod replicas in a deployment based on observed metrics, maintaining your defined target utilization. Core Formula : desiredReplicas = ceil(currentReplicas \u00d7 (currentMetricValue / desiredMetricValue))","title":"What HPA Does"},{"location":"01-autoscaling/hpa/#hpa-architecture-diagram","text":"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 HPA Controller \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 Get Metrics \u2502\u2500\u2500\u2500\u2500\u25b6\u2502 Calculate \u2502\u2500\u2500\u2500\u2500\u25b6\u2502 Update \u2502 \u2502 \u2502 \u2502 from API \u2502 \u2502 Desired \u2502 \u2502 Deployment \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 Replicas \u2502 \u2502 Replicas \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Metrics API \u2502 \u2502 Deployment \u2502 \u2502 (Metrics- \u2502 \u2502 Controller \u2502 \u2502 Server) \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Kubelet \u2502 \u2502 (Nodes/Pods) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518","title":"HPA Architecture Diagram"},{"location":"01-autoscaling/hpa/#basic-hpa-structure","text":"apiVersion : autoscaling/v2 kind : HorizontalPodAutoscaler metadata : name : app-hpa spec : scaleTargetRef : apiVersion : apps/v1 kind : Deployment name : your-app minReplicas : 1 # Minimum pods for availability maxReplicas : 10 # Maximum pods for cost control metrics : # What to monitor - type : Resource resource : name : cpu target : type : Utilization averageUtilization : 70 # Target 70% CPU usage behavior : # How to scale (optional but recommended) scaleUp : stabilizationWindowSeconds : 0 policies : [ ... ] scaleDown : stabilizationWindowSeconds : 300 policies : [ ... ]","title":"Basic HPA Structure"},{"location":"01-autoscaling/hpa/#prerequisite-resource-requests","text":"Critical : Pods must have resource requests for HPA to calculate utilization percentages. # In your deployment template resources : requests : memory : \"256Mi\" # HPA uses: (actual usage / 256Mi) \u00d7 100% cpu : \"250m\" # Requested = 0.25 CPU cores limits : memory : \"512Mi\" cpu : \"500m\" Common Mistake : Using wrong units - \u274c CPU: \"256Mi\" (Mi is for memory!) - \u2705 CPU: \"250m\" (millicores) or \"0.25\" - \u2705 Memory: \"256Mi\" (mebibytes) or \"512Mi\"","title":"Prerequisite: Resource Requests"},{"location":"01-autoscaling/hpa/#hpa-behavior-configuration-complete-details","text":"","title":"\ud83c\udf9b\ufe0f HPA Behavior Configuration - Complete Details"},{"location":"01-autoscaling/hpa/#purpose-of-behavior-settings","text":"Control the timing , speed , and magnitude of scaling operations to prevent rapid oscillations (\"flapping\") and ensure stable application performance.","title":"Purpose of Behavior Settings"},{"location":"01-autoscaling/hpa/#core-components-explained","text":"","title":"Core Components Explained"},{"location":"01-autoscaling/hpa/#1-stabilizationwindowseconds","text":"A waiting period after a metric change before taking scaling action. scaleDown : stabilizationWindowSeconds : 300 # Wait 5 minutes of low usage before scaling down Use Case Recommended Value Scale Up (responsive) 0-30 seconds Scale Down (conservative) 300-600 seconds Stateful applications 600+ seconds","title":"1. stabilizationWindowSeconds"},{"location":"01-autoscaling/hpa/#2-policies-scaling-rules","text":"Define how many pods can be added/removed in a time window. policies : - type : Pods # Fixed number of pods value : 2 # Add/remove 2 pods max periodSeconds : 60 # Every 60 seconds - type : Percent # Percentage of current pods value : 50 # Add/remove 50% of current pods periodSeconds : 30 # Every 30 seconds Policy Types Comparison : | Type | Best For | Example | Result (10 pods \u2192 ?) | |------|----------|---------|----------------------| | Pods | Predictable scaling | value: 3 | Add/remove exactly 3 pods | | Percent | Proportional scaling | value: 50 | Add/remove 5 pods (50% of 10) |","title":"2. policies - Scaling Rules"},{"location":"01-autoscaling/hpa/#3-selectpolicy-choosing-between-policies","text":"When multiple policies exist, which one to apply? selectPolicy : Max # Use the policy allowing the BIGGEST change Policy Behavior Use Case Max Uses policy allowing largest change Fast scaling response Min Uses policy allowing smallest change Conservative, safe scaling Disabled No scaling in this direction Disable scale-up/down Example with Max : policies : - type : Pods value : 2 # Can change 2 pods - type : Percent value : 100 # Can change 100% of pods (all of them!) # With selectPolicy: Max \u2192 Uses Percent: 100 (bigger change) # With selectPolicy: Min \u2192 Uses Pods: 2 (smaller change)","title":"3. selectPolicy - Choosing Between Policies"},{"location":"01-autoscaling/hpa/#complete-behavior-configuration-examples","text":"","title":"Complete Behavior Configuration Examples"},{"location":"01-autoscaling/hpa/#1-web-application-pattern-responsive","text":"behavior : scaleUp : stabilizationWindowSeconds : 0 # React immediately to traffic spikes policies : - type : Pods value : 4 periodSeconds : 15 - type : Percent value : 100 periodSeconds : 15 selectPolicy : Max # Scale aggressively during spikes scaleDown : stabilizationWindowSeconds : 300 # Wait 5 minutes before scaling down policies : - type : Percent value : 50 periodSeconds : 60 selectPolicy : Max","title":"1. Web Application Pattern (Responsive)"},{"location":"01-autoscaling/hpa/#2-batch-processing-pattern-conservative","text":"behavior : scaleUp : stabilizationWindowSeconds : 60 # Wait 1 minute to confirm need policies : - type : Pods value : 1 periodSeconds : 90 selectPolicy : Max scaleDown : stabilizationWindowSeconds : 600 # Wait 10 minutes (long jobs) policies : - type : Pods value : 1 periodSeconds : 180 selectPolicy : Min # Scale down very slowly","title":"2. Batch Processing Pattern (Conservative)"},{"location":"01-autoscaling/hpa/#3-real-timestreaming-pattern-aggressive","text":"behavior : scaleUp : stabilizationWindowSeconds : 0 policies : - type : Percent value : 200 # Can double pod count periodSeconds : 10 selectPolicy : Max scaleDown : stabilizationWindowSeconds : 60 policies : - type : Percent value : 100 # Can remove all extra pods periodSeconds : 30 selectPolicy : Max","title":"3. Real-time/Streaming Pattern (Aggressive)"},{"location":"01-autoscaling/hpa/#load-testing-with-stress-command","text":"","title":"\ud83e\uddea Load Testing with stress Command"},{"location":"01-autoscaling/hpa/#purpose-of-load-testing","text":"Verify HPA triggers at correct thresholds Test scaling behavior under controlled conditions Identify optimal resource requests and limits","title":"Purpose of Load Testing"},{"location":"01-autoscaling/hpa/#using-the-polinuxstress-image","text":"","title":"Using the polinux/stress Image"},{"location":"01-autoscaling/hpa/#basic-test-pod","text":"# Create a pod for testing kubectl run stress-test --image = polinux/stress --command -- sleep 3600 # Run stress commands inside the pod kubectl exec stress-test -- stress --vm 1 --vm-bytes 100M --timeout 60","title":"Basic Test Pod"},{"location":"01-autoscaling/hpa/#memory-stress-testing","text":"# 1. Memory Stress Only # Syntax: --vm N (workers) --vm-bytes X (memory per worker) --timeout T (seconds) kubectl exec <pod> -- stress --vm 2 --vm-bytes 150M --timeout 120 # 2. Heavy Memory Load kubectl exec <pod> -- stress --vm 4 --vm-bytes 500M --timeout 180 # 3. Continuous Memory Stress (background) kubectl exec <pod> -- stress --vm 1 --vm-bytes 100M --timeout 600 &","title":"Memory Stress Testing"},{"location":"01-autoscaling/hpa/#cpu-stress-testing","text":"# 1. CPU Stress Only # Syntax: --cpu N (workers) --timeout T (seconds) kubectl exec <pod> -- stress --cpu 4 --timeout 90 # 2. Heavy CPU Load (all cores) kubectl exec <pod> -- stress --cpu 8 --timeout 120 # 3. Mixed CPU Load (varying intensity) kubectl exec <pod> -- stress --cpu 2 --timeout 60 & \\ kubectl exec <pod> -- stress --cpu 4 --timeout 60","title":"CPU Stress Testing"},{"location":"01-autoscaling/hpa/#combined-stress-testing","text":"# 1. Memory + CPU Combined Stress kubectl exec <pod> -- stress --vm 1 --vm-bytes 200M --cpu 2 --timeout 180 # 2. Heavy Combined Load kubectl exec <pod> -- stress --vm 2 --vm-bytes 300M --cpu 4 --timeout 240 # 3. Sequential Stress Testing kubectl exec <pod> -- stress --cpu 4 --timeout 60 kubectl exec <pod> -- stress --vm 2 --vm-bytes 100M --timeout 60 kubectl exec <pod> -- stress --cpu 2 --vm 1 --vm-bytes 150M --timeout 90","title":"Combined Stress Testing"},{"location":"01-autoscaling/hpa/#testing-hpa-with-memory-metrics","text":"# Given: HPA target is 70% memory utilization # Given: Pod requests 128Mi memory # Calculation: Need ~90Mi usage to trigger scaling (128Mi \u00d7 70% = 89.6Mi) # Trigger scaling kubectl exec deployment/your-app -- stress --vm 1 --vm-bytes 100M --timeout 300 # Monitor scaling watch -n 5 'kubectl get hpa,pods && echo \"---\" && kubectl top pods'","title":"Testing HPA with Memory Metrics"},{"location":"01-autoscaling/hpa/#testing-hpa-with-cpu-metrics","text":"# Given: HPA target is 50% CPU utilization # Given: Pod requests 500m CPU (0.5 cores) # Calculation: Need ~250m usage to trigger scaling (500m \u00d7 50% = 250m) # Trigger CPU scaling kubectl exec deployment/your-app -- stress --cpu 2 --timeout 300 # Monitor CPU-based scaling watch -n 3 'kubectl get hpa && echo \"CPU Usage:\" && kubectl top pods | grep your-app'","title":"Testing HPA with CPU Metrics"},{"location":"01-autoscaling/hpa/#multi-metric-hpa-testing","text":"# Create HPA watching both CPU and Memory cat <<EOF | kubectl apply -f - apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: multi-hpa spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: your-app minReplicas: 1 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 60 - type: Resource resource: name: memory target: type: Utilization averageUtilization: 70 EOF # Test both resources kubectl exec deployment/your-app -- stress --cpu 4 --vm 2 --vm-bytes 200M --timeout 300","title":"Multi-Metric HPA Testing"},{"location":"01-autoscaling/hpa/#hpa-status-interpretation","text":"","title":"\ud83d\udd0d HPA Status Interpretation"},{"location":"01-autoscaling/hpa/#checking-hpa-status","text":"kubectl get hpa # Output shows: REFERENCE, TARGETS, MINPODS, MAXPODS, REPLICAS kubectl describe hpa your-hpa # Shows conditions, events, and detailed metrics","title":"Checking HPA Status"},{"location":"01-autoscaling/hpa/#key-hpa-conditions","text":"Condition Status Meaning Action Required AbleToScale True/False Can HPA modify replicas? Check permissions if False ScalingActive True/False Getting metrics successfully? Fix metrics server if False ScalingLimited True/False Hit min/max replica limits? Adjust min/max if True","title":"Key HPA Conditions"},{"location":"01-autoscaling/hpa/#common-hpa-events","text":"kubectl describe hpa | grep -A5 \"Events:\" Event Meaning Typical Cause SuccessfulRescale HPA changed replica count Normal operation FailedGetResourceMetric Can't fetch metrics Metrics server down FailedComputeMetricsReplicas Can't calculate desired replicas Invalid metric configuration","title":"Common HPA Events"},{"location":"01-autoscaling/hpa/#troubleshooting-common-issues","text":"","title":"\ud83d\udea8 Troubleshooting Common Issues"},{"location":"01-autoscaling/hpa/#issue-1-hpa-shows-unknown-targets","text":"Cause : Metrics Server not working Solution : # Check metrics-server pod kubectl get pods -n kube-system | grep metrics-server # Check logs kubectl logs -n kube-system deployment/metrics-server # Reinstall if needed (with correct flags) helm upgrade metrics-server --reuse-values --set 'args={--kubelet-insecure-tls}'","title":"Issue 1: HPA shows &lt;unknown&gt; targets"},{"location":"01-autoscaling/hpa/#issue-2-hpa-not-scaling-when-expected","text":"Checklist : 1. Verify pod has resource requests ( kubectl get pod -o yaml | grep requests ) 2. Check current vs target metrics ( kubectl describe hpa ) 3. Verify HPA is not limited ( ScalingLimited condition) 4. Ensure metrics are above target threshold","title":"Issue 2: HPA not scaling when expected"},{"location":"01-autoscaling/hpa/#issue-3-rapid-scaling-oscillations-flapping","text":"Solution : Adjust behavior settings behavior : scaleDown : stabilizationWindowSeconds : 600 # Increase from 300 to 600 policies : - type : Percent value : 20 # Reduce from 50% to 20% periodSeconds : 120","title":"Issue 3: Rapid scaling oscillations (\"flapping\")"},{"location":"01-autoscaling/hpa/#quick-reference-commands","text":"","title":"\ud83d\udccb Quick Reference Commands"},{"location":"01-autoscaling/hpa/#hpa-management","text":"# Create HPA with kubectl kubectl autoscale deployment/my-app --cpu-percent = 50 --min = 1 --max = 5 # Get HPA information kubectl get hpa kubectl describe hpa <name> kubectl get hpa <name> -o yaml # Edit HPA kubectl edit hpa <name> # Delete HPA kubectl delete hpa <name>","title":"HPA Management"},{"location":"01-autoscaling/hpa/#monitoring-testing","text":"# Watch scaling in real-time watch -n 2 'kubectl get hpa,deploy,pods && echo \"---\" && kubectl top pods 2>/dev/null || echo \"Metrics loading...\"' # Generate memory load for testing kubectl exec deployment/<name> -- stress --vm 1 --vm-bytes 150M --timeout 120 # Generate CPU load for testing kubectl exec deployment/<name> -- stress --cpu 4 --timeout 120 # Check resource usage kubectl top pods kubectl top pods --containers # Show container-level usage","title":"Monitoring &amp; Testing"},{"location":"01-autoscaling/hpa/#debugging","text":"# Check HPA events kubectl get events --field-selector involvedObject.kind = HorizontalPodAutoscaler --sort-by = .metadata.creationTimestamp # Verify API access kubectl get --raw \"/apis/metrics.k8s.io/v1beta1/nodes\" | jq '.items[0].usage' # Check pod details kubectl describe pod <pod-name> | grep -A10 \"Resources:\"","title":"Debugging"},{"location":"01-autoscaling/hpa/#best-practices-summary","text":"Always set resource requests in deployments (required for HPA calculations) Start with conservative behavior and adjust based on observations Test both CPU and memory scaling before production deployment Monitor HPA events for failed scaling attempts Use appropriate min/max values based on application needs and cluster capacity Regularly review metrics to ensure targets are appropriate Consider combining with VPA (Vertical Pod Autoscaler) for right-sizing requests","title":"\ud83d\udca1 Best Practices Summary"},{"location":"01-autoscaling/hpa/#common-pitfalls-to-avoid","text":"\u274c Forgetting to install/configure Metrics Server \u274c Not setting resource requests in pods \u274c Setting minReplicas to 0 for stateful applications \u274c Too aggressive scale-down causing application instability \u274c Incorrect metric units (using Mi for CPU, m for memory) \u274c Not monitoring HPA events for failures \u274c Testing only one resource type (CPU or Memory) when both matter","title":"\u26a0\ufe0f Common Pitfalls to Avoid"},{"location":"01-autoscaling/keywords/","text":"Search Keywords+ Keyword: hpa example Recommendation: Recommended https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/ Keyword: hpa Recommendation: Not recommended https://kubernetes.io/docs/concepts/workloads/autoscaling/horizontal-pod-autoscale/ Keyword: vpa Recommendation: Recommended https://kubernetes.io/docs/concepts/workloads/autoscaling/vertical-pod-autoscale/","title":"Search Keywords+"},{"location":"01-autoscaling/keywords/#search-keywords","text":"","title":"Search Keywords+"},{"location":"01-autoscaling/keywords/#keyword-hpa-example","text":"Recommendation: Recommended https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/","title":"Keyword: hpa example"},{"location":"01-autoscaling/keywords/#keyword-hpa","text":"Recommendation: Not recommended https://kubernetes.io/docs/concepts/workloads/autoscaling/horizontal-pod-autoscale/","title":"Keyword: hpa"},{"location":"01-autoscaling/keywords/#keyword-vpa","text":"Recommendation: Recommended https://kubernetes.io/docs/concepts/workloads/autoscaling/vertical-pod-autoscale/","title":"Keyword: vpa"},{"location":"01-autoscaling/question/","text":"Autoscaling Practice QUESTIONS \ud83d\udd25 Question 1 \u2014 Memory-based HPA with Custom Behavior Objective Configure a Horizontal Pod Autoscaler that scales only on memory utilization and uses custom scaling behavior . Requirements Deployment name: mem-hpa Image: polinux/stress Command: [ \"sleep\" , \"3600\" ] Resource requests: memory : 50Mi HPA configuration: Metric: memory utilization Target: 80% Min replicas: 1 Max replicas: 5 Behavior: Scale up: +2 pods per 30 seconds Scale down: \u22121 pod per 60 seconds Scale-down stabilization window: 300 seconds Load Generation POD = $( kubectl get pods -l app = mem-hpa -o name | shuf -n 1 ) && \\ echo \"Running stress in: $POD \" && \\ kubectl exec \" $POD \" -- sh -c \\ 'nohup stress --vm 1 --vm-bytes 200Mi --timeout 600 > /dev/null 2>&1 &' Observation watch -n 5 ' kubectl top po; echo \"-----\"; kubectl get po -l app=mem-hpa ' \ud83d\udd25 Question 2 \u2014 CPU + Memory HPA with Complex Custom Behavior Objective Configure a multi-metric HPA that scales on CPU OR memory , using asymmetric, production-style behavior rules . Requirements Deployment name: multi-hpa Image: polinux/stress Command: [ \"sleep\" , \"3600\" ] Resource requests: cpu : 40m memory : 60Mi HPA configuration: CPU target: 65% Memory target: 75% Min replicas: 2 Max replicas: 8 Behavior: Scale up: Max of 4 pods OR 50% per 30 seconds Select policy: Max Scale down: \u22121 pod per 90 seconds Stabilization window: 300 seconds Load Generation POD = $( kubectl get pods -l app = multi-hpa -o name | shuf -n 1 ) && \\ echo \"Running stress in: $POD \" && \\ kubectl exec \" $POD \" -- sh -c \\ 'nohup stress --cpu 2 --vm 1 --vm-bytes 180Mi --timeout 600 > /dev/null 2>&1 &' Observation watch -n 10 ' kubectl describe hpa multi-hpa | grep -A15 \"Metrics:\"; echo \"-----\"; kubectl get po -l app=multi-hpa ' \ud83d\ude80 Question 3 \u2014 VPA in Off Mode Objective Validate that VPA Off mode only observes usage and never mutates pods. Requirements Deployment name: vpa-off Image: polinux/stress Command: [ \"sleep\" , \"3600\" ] * Initial requests: cpu : 20m memory : 30Mi * VPA configuration: Update mode: Off Min allowed: 30m / 40Mi Max allowed: 200m / 200Mi Load Generation kubectl exec deploy/vpa-off -- sh -c \\ 'stress --cpu 2 --vm 1 --vm-bytes 100Mi --timeout 600' Observation kubectl describe vpa vpa-off \ud83d\ude80 Question 4 \u2014 VPA in Initial Mode Objective Understand how VPA Initial mode injects requests only at pod creation time . Requirements Deployment name: vpa-initial Image: polinux/stress Command: [ \"sleep\" , \"3600\" ] * Initial requests: cpu : 10m memory : 20Mi * VPA configuration: Update mode: Initial Min allowed: 20m / 30Mi Max allowed: 100m / 100Mi Load Generation kubectl exec deploy/vpa-initial -- sh -c \\ 'stress --cpu 1 --vm 1 --vm-bytes 60Mi --timeout 600' Observation watch -n 30 ' kubectl describe vpa vpa-initial | grep -A8 -B6 \"Recommendation\" ' \ud83d\ude80 Question 5 \u2014 VPA in Auto Mode (Fully Configured) Objective Observe automatic pod eviction and recreation when VPA actively enforces sizing. Requirements Deployment name: vpa-auto Image: polinux/stress Command: [ \"sleep\" , \"3600\" ] * Initial requests: cpu : 15m memory : 25Mi * VPA configuration: Update mode: Auto Min allowed: 20m / 30Mi Max allowed: 150m / 200Mi Controlled resources: CPU and memory Load Generation POD = $( kubectl get pods -l app = vpa-auto -o name | shuf -n 1 ) && \\ echo \"Running stress in: $POD \" && \\ kubectl exec \" $POD \" -- sh -c \\ 'nohup stress --cpu 3 --vm 1 --vm-bytes 150Mi --timeout 300 > /dev/null 2>&1 &' Observation watch -n 15 ' kubectl get po -l app=vpa-auto '","title":"Autoscaling Practice"},{"location":"01-autoscaling/question/#autoscaling-practice","text":"","title":"Autoscaling Practice"},{"location":"01-autoscaling/question/#questions","text":"","title":"QUESTIONS"},{"location":"01-autoscaling/question/#question-1-memory-based-hpa-with-custom-behavior","text":"","title":"\ud83d\udd25 Question 1 \u2014 Memory-based HPA with Custom Behavior"},{"location":"01-autoscaling/question/#objective","text":"Configure a Horizontal Pod Autoscaler that scales only on memory utilization and uses custom scaling behavior .","title":"Objective"},{"location":"01-autoscaling/question/#requirements","text":"Deployment name: mem-hpa Image: polinux/stress Command: [ \"sleep\" , \"3600\" ] Resource requests: memory : 50Mi HPA configuration: Metric: memory utilization Target: 80% Min replicas: 1 Max replicas: 5 Behavior: Scale up: +2 pods per 30 seconds Scale down: \u22121 pod per 60 seconds Scale-down stabilization window: 300 seconds","title":"Requirements"},{"location":"01-autoscaling/question/#load-generation","text":"POD = $( kubectl get pods -l app = mem-hpa -o name | shuf -n 1 ) && \\ echo \"Running stress in: $POD \" && \\ kubectl exec \" $POD \" -- sh -c \\ 'nohup stress --vm 1 --vm-bytes 200Mi --timeout 600 > /dev/null 2>&1 &'","title":"Load Generation"},{"location":"01-autoscaling/question/#observation","text":"watch -n 5 ' kubectl top po; echo \"-----\"; kubectl get po -l app=mem-hpa '","title":"Observation"},{"location":"01-autoscaling/question/#question-2-cpu-memory-hpa-with-complex-custom-behavior","text":"","title":"\ud83d\udd25 Question 2 \u2014 CPU + Memory HPA with Complex Custom Behavior"},{"location":"01-autoscaling/question/#objective_1","text":"Configure a multi-metric HPA that scales on CPU OR memory , using asymmetric, production-style behavior rules .","title":"Objective"},{"location":"01-autoscaling/question/#requirements_1","text":"Deployment name: multi-hpa Image: polinux/stress Command: [ \"sleep\" , \"3600\" ] Resource requests: cpu : 40m memory : 60Mi HPA configuration: CPU target: 65% Memory target: 75% Min replicas: 2 Max replicas: 8 Behavior: Scale up: Max of 4 pods OR 50% per 30 seconds Select policy: Max Scale down: \u22121 pod per 90 seconds Stabilization window: 300 seconds","title":"Requirements"},{"location":"01-autoscaling/question/#load-generation_1","text":"POD = $( kubectl get pods -l app = multi-hpa -o name | shuf -n 1 ) && \\ echo \"Running stress in: $POD \" && \\ kubectl exec \" $POD \" -- sh -c \\ 'nohup stress --cpu 2 --vm 1 --vm-bytes 180Mi --timeout 600 > /dev/null 2>&1 &'","title":"Load Generation"},{"location":"01-autoscaling/question/#observation_1","text":"watch -n 10 ' kubectl describe hpa multi-hpa | grep -A15 \"Metrics:\"; echo \"-----\"; kubectl get po -l app=multi-hpa '","title":"Observation"},{"location":"01-autoscaling/question/#question-3-vpa-in-off-mode","text":"","title":"\ud83d\ude80 Question 3 \u2014 VPA in Off Mode"},{"location":"01-autoscaling/question/#objective_2","text":"Validate that VPA Off mode only observes usage and never mutates pods.","title":"Objective"},{"location":"01-autoscaling/question/#requirements_2","text":"Deployment name: vpa-off Image: polinux/stress Command: [ \"sleep\" , \"3600\" ] * Initial requests: cpu : 20m memory : 30Mi * VPA configuration: Update mode: Off Min allowed: 30m / 40Mi Max allowed: 200m / 200Mi","title":"Requirements"},{"location":"01-autoscaling/question/#load-generation_2","text":"kubectl exec deploy/vpa-off -- sh -c \\ 'stress --cpu 2 --vm 1 --vm-bytes 100Mi --timeout 600'","title":"Load Generation"},{"location":"01-autoscaling/question/#observation_2","text":"kubectl describe vpa vpa-off","title":"Observation"},{"location":"01-autoscaling/question/#question-4-vpa-in-initial-mode","text":"","title":"\ud83d\ude80 Question 4 \u2014 VPA in Initial Mode"},{"location":"01-autoscaling/question/#objective_3","text":"Understand how VPA Initial mode injects requests only at pod creation time .","title":"Objective"},{"location":"01-autoscaling/question/#requirements_3","text":"Deployment name: vpa-initial Image: polinux/stress Command: [ \"sleep\" , \"3600\" ] * Initial requests: cpu : 10m memory : 20Mi * VPA configuration: Update mode: Initial Min allowed: 20m / 30Mi Max allowed: 100m / 100Mi","title":"Requirements"},{"location":"01-autoscaling/question/#load-generation_3","text":"kubectl exec deploy/vpa-initial -- sh -c \\ 'stress --cpu 1 --vm 1 --vm-bytes 60Mi --timeout 600'","title":"Load Generation"},{"location":"01-autoscaling/question/#observation_3","text":"watch -n 30 ' kubectl describe vpa vpa-initial | grep -A8 -B6 \"Recommendation\" '","title":"Observation"},{"location":"01-autoscaling/question/#question-5-vpa-in-auto-mode-fully-configured","text":"","title":"\ud83d\ude80 Question 5 \u2014 VPA in Auto Mode (Fully Configured)"},{"location":"01-autoscaling/question/#objective_4","text":"Observe automatic pod eviction and recreation when VPA actively enforces sizing.","title":"Objective"},{"location":"01-autoscaling/question/#requirements_4","text":"Deployment name: vpa-auto Image: polinux/stress Command: [ \"sleep\" , \"3600\" ] * Initial requests: cpu : 15m memory : 25Mi * VPA configuration: Update mode: Auto Min allowed: 20m / 30Mi Max allowed: 150m / 200Mi Controlled resources: CPU and memory","title":"Requirements"},{"location":"01-autoscaling/question/#load-generation_4","text":"POD = $( kubectl get pods -l app = vpa-auto -o name | shuf -n 1 ) && \\ echo \"Running stress in: $POD \" && \\ kubectl exec \" $POD \" -- sh -c \\ 'nohup stress --cpu 3 --vm 1 --vm-bytes 150Mi --timeout 300 > /dev/null 2>&1 &'","title":"Load Generation"},{"location":"01-autoscaling/question/#observation_4","text":"watch -n 15 ' kubectl get po -l app=vpa-auto '","title":"Observation"},{"location":"01-autoscaling/solution/","text":"Solution \u2014 Deployment Template (Base) apiVersion : apps/v1 kind : Deployment metadata : name : app labels : app : app spec : replicas : 1 selector : matchLabels : app : app template : metadata : labels : app : app spec : containers : - name : app image : polinux/stress command : [ \"sleep\" , \"3600\" ] resources : requests : cpu : 50m memory : 50Mi Solution \u2014 HPA (Memory with Custom Behavior) apiVersion : autoscaling/v2 kind : HorizontalPodAutoscaler metadata : name : mem-hpa spec : scaleTargetRef : apiVersion : apps/v1 kind : Deployment name : mem-hpa minReplicas : 1 maxReplicas : 5 metrics : - type : Resource resource : name : memory target : type : Utilization averageUtilization : 80 behavior : scaleUp : policies : - type : Pods value : 2 periodSeconds : 30 scaleDown : stabilizationWindowSeconds : 300 policies : - type : Pods value : 1 periodSeconds : 60 Solution \u2014 HPA (CPU + Memory with Complex Behavior) apiVersion : autoscaling/v2 kind : HorizontalPodAutoscaler metadata : name : multi-hpa spec : scaleTargetRef : apiVersion : apps/v1 kind : Deployment name : multi-hpa minReplicas : 2 maxReplicas : 8 metrics : - type : Resource resource : name : cpu target : type : Utilization averageUtilization : 65 - type : Resource resource : name : memory target : type : Utilization averageUtilization : 75 behavior : scaleUp : selectPolicy : Max policies : - type : Pods value : 4 periodSeconds : 30 - type : Percent value : 50 periodSeconds : 30 scaleDown : stabilizationWindowSeconds : 300 policies : - type : Pods value : 1 periodSeconds : 90 Solution \u2014 VPA Off Mode apiVersion : autoscaling.k8s.io/v1 kind : VerticalPodAutoscaler metadata : name : vpa-off spec : targetRef : apiVersion : apps/v1 kind : Deployment name : vpa-off updatePolicy : updateMode : Off resourcePolicy : containerPolicies : - containerName : app controlledResources : [ \"cpu\" , \"memory\" ] minAllowed : cpu : 30m memory : 40Mi maxAllowed : cpu : 200m memory : 200Mi Solution \u2014 VPA Initial Mode apiVersion : autoscaling.k8s.io/v1 kind : VerticalPodAutoscaler metadata : name : vpa-initial spec : targetRef : apiVersion : apps/v1 kind : Deployment name : vpa-initial updatePolicy : updateMode : Initial resourcePolicy : containerPolicies : - containerName : app controlledResources : [ \"cpu\" , \"memory\" ] minAllowed : cpu : 20m memory : 30Mi maxAllowed : cpu : 100m memory : 100Mi Solution \u2014 VPA Auto Mode apiVersion : autoscaling.k8s.io/v1 kind : VerticalPodAutoscaler metadata : name : vpa-auto spec : targetRef : apiVersion : apps/v1 kind : Deployment name : vpa-auto updatePolicy : updateMode : Auto resourcePolicy : containerPolicies : - containerName : app controlledResources : [ \"cpu\" , \"memory\" ] minAllowed : cpu : 20m memory : 30Mi maxAllowed : cpu : 150m memory : 200Mi HINTS HPA scales replicas using resource requests, never limits. Memory HPA requires usage well above the request to be observable. Multi-metric HPA chooses the highest desired replica count. HPA behavior controls rate and stability, not scaling triggers. VPA Off observes only. VPA Initial injects requests only on pod creation. VPA Auto evicts and recreates pods to enforce recommendations.","title":"Solution"},{"location":"01-autoscaling/solution/#solution-deployment-template-base","text":"apiVersion : apps/v1 kind : Deployment metadata : name : app labels : app : app spec : replicas : 1 selector : matchLabels : app : app template : metadata : labels : app : app spec : containers : - name : app image : polinux/stress command : [ \"sleep\" , \"3600\" ] resources : requests : cpu : 50m memory : 50Mi","title":"Solution \u2014 Deployment Template (Base)"},{"location":"01-autoscaling/solution/#solution-hpa-memory-with-custom-behavior","text":"apiVersion : autoscaling/v2 kind : HorizontalPodAutoscaler metadata : name : mem-hpa spec : scaleTargetRef : apiVersion : apps/v1 kind : Deployment name : mem-hpa minReplicas : 1 maxReplicas : 5 metrics : - type : Resource resource : name : memory target : type : Utilization averageUtilization : 80 behavior : scaleUp : policies : - type : Pods value : 2 periodSeconds : 30 scaleDown : stabilizationWindowSeconds : 300 policies : - type : Pods value : 1 periodSeconds : 60","title":"Solution \u2014 HPA (Memory with Custom Behavior)"},{"location":"01-autoscaling/solution/#solution-hpa-cpu-memory-with-complex-behavior","text":"apiVersion : autoscaling/v2 kind : HorizontalPodAutoscaler metadata : name : multi-hpa spec : scaleTargetRef : apiVersion : apps/v1 kind : Deployment name : multi-hpa minReplicas : 2 maxReplicas : 8 metrics : - type : Resource resource : name : cpu target : type : Utilization averageUtilization : 65 - type : Resource resource : name : memory target : type : Utilization averageUtilization : 75 behavior : scaleUp : selectPolicy : Max policies : - type : Pods value : 4 periodSeconds : 30 - type : Percent value : 50 periodSeconds : 30 scaleDown : stabilizationWindowSeconds : 300 policies : - type : Pods value : 1 periodSeconds : 90","title":"Solution \u2014 HPA (CPU + Memory with Complex Behavior)"},{"location":"01-autoscaling/solution/#solution-vpa-off-mode","text":"apiVersion : autoscaling.k8s.io/v1 kind : VerticalPodAutoscaler metadata : name : vpa-off spec : targetRef : apiVersion : apps/v1 kind : Deployment name : vpa-off updatePolicy : updateMode : Off resourcePolicy : containerPolicies : - containerName : app controlledResources : [ \"cpu\" , \"memory\" ] minAllowed : cpu : 30m memory : 40Mi maxAllowed : cpu : 200m memory : 200Mi","title":"Solution \u2014 VPA Off Mode"},{"location":"01-autoscaling/solution/#solution-vpa-initial-mode","text":"apiVersion : autoscaling.k8s.io/v1 kind : VerticalPodAutoscaler metadata : name : vpa-initial spec : targetRef : apiVersion : apps/v1 kind : Deployment name : vpa-initial updatePolicy : updateMode : Initial resourcePolicy : containerPolicies : - containerName : app controlledResources : [ \"cpu\" , \"memory\" ] minAllowed : cpu : 20m memory : 30Mi maxAllowed : cpu : 100m memory : 100Mi","title":"Solution \u2014 VPA Initial Mode"},{"location":"01-autoscaling/solution/#solution-vpa-auto-mode","text":"apiVersion : autoscaling.k8s.io/v1 kind : VerticalPodAutoscaler metadata : name : vpa-auto spec : targetRef : apiVersion : apps/v1 kind : Deployment name : vpa-auto updatePolicy : updateMode : Auto resourcePolicy : containerPolicies : - containerName : app controlledResources : [ \"cpu\" , \"memory\" ] minAllowed : cpu : 20m memory : 30Mi maxAllowed : cpu : 150m memory : 200Mi","title":"Solution \u2014 VPA Auto Mode"},{"location":"01-autoscaling/solution/#hints","text":"HPA scales replicas using resource requests, never limits. Memory HPA requires usage well above the request to be observable. Multi-metric HPA chooses the highest desired replica count. HPA behavior controls rate and stability, not scaling triggers. VPA Off observes only. VPA Initial injects requests only on pod creation. VPA Auto evicts and recreates pods to enforce recommendations.","title":"HINTS"},{"location":"01-autoscaling/vpa/","text":"Vertical Pod Autoscaler (VPA) \ud83c\udfaf What is VPA? Vertical Pod Autoscaler (VPA) automatically adjusts the CPU and memory requests/limits of your pods based on actual usage patterns. Unlike HPA (which adds/removes pods), VPA right-sizes individual pods by modifying their resource specifications. \ud83d\udca1 Explanation: Think of your pods wearing clothes. HPA buys more shirts when you have more people (adds pods). VPA measures each person and tailors their shirt to fit perfectly (adjusts CPU/memory per pod). If someone grows or shrinks, VPA remeasures and makes them a new shirt. Core Analogy: Tailoring Clothes HPA : Buys more shirts when you have more people VPA : Resizes each shirt to perfectly fit each person \ud83d\udcca VPA vs HPA Comparison Aspect Horizontal Pod Autoscaler (HPA) Vertical Pod Autoscaler (VPA) Scales Number of pod replicas CPU/Memory per pod Direction Horizontal (more/fewer pods) Vertical (more/fewer resources) Best for Variable traffic Predictable resource patterns Disruption None (adds/removes pods) Pod recreation required Typical Use Web apps, APIs Databases, memory-heavy apps \ud83d\udca1 Key Difference: HPA changes quantity (pod count), VPA changes quality (resources per pod). They can work together: VPA makes each pod the right size, HPA decides how many of those right-sized pods you need. \ud83c\udfd7\ufe0f VPA Architecture & Components VPA Component Diagram \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 VPA Controller \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 1. RECOMMENDER \u2502 \u2502 \u2502 \u2502 \u2022 Analyzes historical usage \u2502 \u2502 \u2502 \u2502 \u2022 Creates resource profiles \u2502 \u2502 \u2502 \u2502 \u2022 Suggests optimal requests/limits \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 2. UPDATER \u2502 \u2502 \u2502 \u2502 \u2022 Evicts pods needing updates \u2502 \u2502 \u2502 \u2502 \u2022 Only in \"Auto\" mode \u2502 \u2502 \u2502 \u2502 \u2022 Gracefully terminates pods \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 3. ADMISSION CONTROLLER \u2502 \u2502 \u2502 \u2502 \u2022 Intercepts pod creation \u2502 \u2502 \u2502 \u2502 \u2022 Injects recommended resources \u2502 \u2502 \u2502 \u2502 \u2022 Mutating webhook \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 METRICS HISTORY \u2502 \u2502 (Metrics Server / Prometheus) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \ud83d\udca1 How They Work Together: 1. Recommender = The \"brain\" that learns what resources pods actually need 2. Updater = The \"action taker\" that replaces pods when they need new sizes (only in Auto mode) 3. Admission Controller = The \"gatekeeper\" that gives new pods the right resources from the start 4. Metrics = The \"memory\" that stores what happened in the past \ud83d\ude80 VPA Installation Method 1: Official Release (Recommended) # Clone VPA repository git clone https://github.com/kubernetes/autoscaler.git cd autoscaler/vertical-pod-autoscaler # Install all components ./hack/vpa-up.sh # Verify installation kubectl get pods -n kube-system | grep vpa \ud83d\udca1 What This Installs: - vpa-recommender : Learns and suggests resource sizes - vpa-updater : Takes action to resize pods (in Auto mode) - vpa-admission-controller : Updates new pods as they're created - vpa-crd : Custom Resource Definition for VPA objects Method 2: Helm Installation # Add Helm repository helm repo add fairwinds-stable https://charts.fairwinds.com/stable helm repo update # Install VPA helm install vpa fairwinds-stable/vpa \\ --namespace kube-system \\ --set recommender.enabled = true \\ --set updater.enabled = true \\ --set admissionController.enabled = true \ud83d\udca1 Helm Benefits: Easier upgrades, configuration management, and clean uninstalls. Method 3: Manifest Files # Apply individual components kubectl apply -f https://github.com/kubernetes/autoscaler/releases/latest/download/vertical-pod-autoscaler-recommender.yaml kubectl apply -f https://github.com/kubernetes/autoscaler/releases/latest/download/vertical-pod-autoscaler-updater.yaml kubectl apply -f https://github.com/kubernetes/autoscaler/releases/latest/download/vertical-pod-autoscaler-admission-controller.yaml # Verify kubectl get pods -n kube-system -l app = vpa \u2699\ufe0f VPA Components Explained 1. VPA Recommender Purpose : Analyzes historical resource usage Output : Resource recommendations stored in VPA object Data Source : Metrics Server (last 8 days by default) Algorithm : Uses histogram of usage patterns \ud83d\udca1 How It Learns: Imagine you have a pod that uses between 200-400m CPU over time. Recommender watches for 8 days, builds a histogram, and says: \"This pod usually needs 300m CPU, but sometimes hits 400m. Let's recommend 350m to be safe.\" 2. VPA Updater Purpose : Evicts pods that need resource adjustments Action : Deletes pods with incorrect allocations Trigger : When recommendations differ significantly from current Mode : Only active in Auto mode \ud83d\udca1 The \"Pod Killer\": Updater is like a tailor who says \"Your shirt doesn't fit anymore!\" and makes you take it off so they can give you a new one. It doesn't resize the shirt while you're wearing it\u2014it makes you change shirts entirely. 3. VPA Admission Controller Purpose : Modifies pod specs during creation How : Mutating admission webhook When : Intercepts all pod creation requests What : Replaces resource requests/limits with recommendations \ud83d\udca1 The \"Birth Certificate\": When a pod is born, Admission Controller checks the Recommender's notes and says: \"This pod should have 350m CPU, not the 100m written in its DNA (YAML file).\" It secretly changes the birth certificate before anyone notices. \ud83d\udcdd VPA Configuration Modes Four Update Modes 1. Initial Mode (Most Common) updatePolicy : updateMode : \"Initial\" - Behavior : Only sets resources on pod creation - Existing pods : Never updated - New pods : Get recommended resources - Best for : Safe production use, testing \ud83d\udca1 Analogy: Like a hospital that measures every newborn baby and gives them the right size diaper, but doesn't change diapers on babies who've already left the hospital. 2. Auto Mode (Aggressive) updatePolicy : updateMode : \"Auto\" - Behavior : Updates resources and recreates pods - Existing pods : Evicted and recreated with new resources - Risk : Pod disruptions, potential downtime - Best for : Non-critical workloads, dev environments \ud83d\udca1 Analogy: Like a strict parent who makes you change clothes immediately if your shirt doesn't fit perfectly, even if you're in the middle of dinner. 3. Recreate Mode (Rarely Used) updatePolicy : updateMode : \"Recreate\" - Behavior : Only recreates pods (no resource updates) - Use case : When pod recreation needed without resource changes \ud83d\udca1 When to Use: If you want VPA to restart pods periodically but not change their resources. Rare case. 4. Off Mode (Monitor Only) updatePolicy : updateMode : \"Off\" - Behavior : Only provides recommendations - Action : No automatic changes - Best for : Learning patterns, manual optimization \ud83d\udca1 Analogy: Like a nutritionist who tells you \"You should eat 2000 calories per day\" but doesn't stop you from eating a whole pizza. \ud83d\udd27 Basic VPA Configuration Minimal VPA Example apiVersion : autoscaling.k8s.io/v1 kind : VerticalPodAutoscaler metadata : name : myapp-vpa spec : targetRef : apiVersion : \"apps/v1\" kind : Deployment name : myapp-deployment updatePolicy : updateMode : \"Initial\" # Safe mode resourcePolicy : containerPolicies : - containerName : \"*\" # Apply to all containers minAllowed : cpu : \"100m\" memory : \"128Mi\" maxAllowed : cpu : \"2\" memory : \"2Gi\" controlledResources : [ \"cpu\" , \"memory\" ] \ud83d\udca1 Breaking It Down: - targetRef : Which deployment/statefulset to watch (like tagging a person for measurement) - updateMode: \"Initial\" : Safe mode\u2014only fix new pods - containerName: \"*\" : Apply to ALL containers in the pod (asterisk = wildcard) - minAllowed/maxAllowed : Safety rails so VPA doesn't recommend crazy sizes - controlledResources : Which resources to adjust (CPU, memory, or both) Complete VPA with All Options apiVersion : autoscaling.k8s.io/v1 kind : VerticalPodAutoscaler metadata : name : complete-vpa spec : targetRef : apiVersion : \"apps/v1\" kind : Deployment # Can be: Deployment, StatefulSet, DaemonSet name : my-application updatePolicy : updateMode : \"Auto\" # Initial, Auto, Recreate, or Off minReplicas : 2 # Optional: Minimum pods to consider resourcePolicy : containerPolicies : - containerName : \"app\" mode : \"Auto\" # Auto, Off, or Initial minAllowed : cpu : \"100m\" memory : \"128Mi\" maxAllowed : cpu : \"4\" memory : \"8Gi\" controlledResources : [ \"cpu\" , \"memory\" ] controlledValues : \"RequestsAndLimits\" # or \"RequestsOnly\" - containerName : \"sidecar\" mode : \"Off\" # Don't autoscale this container \ud83d\udca1 Advanced Options Explained: - minReplicas : Only start recommending if you have at least this many pods (more data = better recommendations) - container-specific mode : Different rules per container in same pod - controlledValues : \"RequestsOnly\" = only adjust requests, not limits (limits stay as you set them) \ud83d\udcca Resource Policy Explained Container Policies resourcePolicy : containerPolicies : - containerName : \"webapp\" # Specific container minAllowed : # Minimum VPA can recommend cpu : \"100m\" memory : \"256Mi\" maxAllowed : # Maximum VPA can recommend cpu : \"2\" memory : \"4Gi\" controlledResources : # Which resources to adjust - \"cpu\" - \"memory\" controlledValues : \"RequestsAndLimits\" # Options: # - RequestsAndLimits (default) # - RequestsOnly \ud83d\udca1 Why Min/Max Matters: Without these, VPA might recommend 0.001m CPU (too small to run) or 1000 CPU cores (breaks your cluster). These are guardrails. Wildcard vs Specific Containers # Option 1: All containers - containerName : \"*\" # Apply to EVERY container # Option 2: Specific containers - containerName : \"app-server\" # Only this container - containerName : \"cache\" # Different policy for cache - containerName : \"logger\" # Don't set mode: \"Off\" \ud83d\udca1 Real Example: Your pod has 3 containers: app (needs lots of CPU), redis (needs lots of memory), logger (tiny, fixed needs). Use specific names to treat each differently. \ud83d\udee0\ufe0f Practical VPA Examples Example 1: Web Application apiVersion : autoscaling.k8s.io/v1 kind : VerticalPodAutoscaler metadata : name : webapp-vpa spec : targetRef : apiVersion : \"apps/v1\" kind : Deployment name : webapp updatePolicy : updateMode : \"Initial\" # Safe for production resourcePolicy : containerPolicies : - containerName : \"nginx\" minAllowed : cpu : \"100m\" memory : \"128Mi\" maxAllowed : cpu : \"1\" memory : \"1Gi\" - containerName : \"app\" minAllowed : cpu : \"200m\" memory : \"256Mi\" maxAllowed : cpu : \"2\" memory : \"2Gi\" \ud83d\udca1 Two Containers, Different Needs: Nginx (web server) vs App (application logic) have different resource patterns. VPA handles each separately. Example 2: Database with StatefulSet apiVersion : autoscaling.k8s.io/v1 kind : VerticalPodAutoscaler metadata : name : postgres-vpa spec : targetRef : apiVersion : \"apps/v1\" kind : StatefulSet # Works with StatefulSets! name : postgres updatePolicy : updateMode : \"Initial\" # CRITICAL: Databases hate pod restarts! resourcePolicy : containerPolicies : - containerName : \"postgres\" minAllowed : cpu : \"500m\" memory : \"1Gi\" maxAllowed : cpu : \"4\" memory : \"16Gi\" \ud83d\udca1 StatefulSet Warning: Databases store data on disk. If VPA kills the pod in Auto mode, database might get confused. Initial mode is safer\u2014only fixes new pods when they're created during maintenance. Example 3: Multi-Container Microservice apiVersion : autoscaling.k8s.io/v1 kind : VerticalPodAutoscaler metadata : name : microservice-vpa spec : targetRef : apiVersion : \"apps/v1\" kind : Deployment name : order-service updatePolicy : updateMode : \"Auto\" # Will recreate pods resourcePolicy : containerPolicies : - containerName : \"order-processor\" minAllowed : cpu : \"200m\" memory : \"512Mi\" maxAllowed : cpu : \"2\" memory : \"4Gi\" controlledValues : \"RequestsAndLimits\" - containerName : \"redis-sidecar\" mode : \"Off\" # Don't autoscale sidecar # No min/max needed when mode is Off \ud83d\udca1 Sidecar Pattern: Redis runs alongside your app as a cache. You might not want VPA changing it because Redis has known memory patterns. mode: \"Off\" tells VPA to leave it alone. \ud83d\udd0d How VPA Makes Recommendations Recommendation Algorithm VPA Recommendation Process: 1. Collect 8 days of usage data (default) 2. Build histogram of CPU/Memory usage 3. Calculate: - Target: 90th percentile of usage + safety margin - Lower Bound: 50th percentile - Upper Bound: 95th percentile + safety margin 4. Store in VPA object status \ud83d\udca1 What This Means: If your pod uses 100m, 200m, 300m, 400m CPU over time: - 50th percentile (Lower Bound) : 250m (half the time it uses less than this) - 90th percentile (Target) : 380m (90% of the time it uses less than this) - 95th percentile (Upper Bound) : 390m (almost never uses more than this) VPA recommends Target (380m) as the \"right size\" with safety margins. Viewing Recommendations # Check VPA status kubectl get vpa # Detailed view with recommendations kubectl describe vpa myapp-vpa # Raw YAML with recommendations kubectl get vpa myapp-vpa -o yaml Sample VPA Output $ kubectl describe vpa/webapp-vpa Status: Conditions: Status: True Type: RecommendationProvided Recommendation: Container Recommendations: Container Name: nginx Lower Bound: # Minimum safe Cpu: 100m Memory: 128Mi Target: # \u2b50 RECOMMENDED VALUE \u2b50 Cpu: 350m Memory: 512Mi Uncapped Target: # Without min/max constraints Cpu: 420m Memory: 600Mi Upper Bound: # Maximum safe Cpu: 500m Memory: 1Gi \ud83d\udca1 Reading This Output: - Target (350m CPU) : What VPA wants to set your pod to - Lower Bound (100m) : Below this is dangerously small - Upper Bound (500m) : Above this is wastefully large - Uncapped Target (420m) : What VPA would recommend if you had no min/max limits \ud83e\uddea Testing VPA Step by Step Step 1: Create Test Deployment # test-deployment.yaml apiVersion : apps/v1 kind : Deployment metadata : name : vpa-test spec : replicas : 2 selector : matchLabels : app : vpa-test template : metadata : labels : app : vpa-test spec : containers : - name : test-app image : polinux/stress command : [ \"sleep\" , \"3600\" ] resources : requests : cpu : \"50m\" # Intentionally TOO LOW memory : \"64Mi\" # Intentionally TOO LOW \ud83d\udca1 Setting Up the Test: We create pods with obviously wrong resources (50m CPU) so VPA has something to fix. Step 2: Create VPA # test-vpa.yaml apiVersion : autoscaling.k8s.io/v1 kind : VerticalPodAutoscaler metadata : name : vpa-test-vpa spec : targetRef : apiVersion : \"apps/v1\" kind : Deployment name : vpa-test updatePolicy : updateMode : \"Auto\" # Will show immediate effect resourcePolicy : containerPolicies : - containerName : \"*\" minAllowed : cpu : \"100m\" memory : \"128Mi\" maxAllowed : cpu : \"2\" memory : \"2Gi\" \ud83d\udca1 Using Auto Mode for Testing: In real life, start with Initial . For testing, Auto shows VPA in action immediately. Step 3: Apply and Generate Load # Apply configurations kubectl apply -f test-deployment.yaml kubectl apply -f test-vpa.yaml # Generate load to help VPA learn kubectl exec deployment/vpa-test -- stress --cpu 2 --vm 1 --vm-bytes 200M --timeout 300 # Wait for VPA to analyze (5-10 minutes) sleep 300 # Check recommendations kubectl describe vpa vpa-test-vpa # Watch pods get recreated (Auto mode) kubectl get pods -l app = vpa-test -w \ud83d\udca1 The Learning Process: VPA needs to see actual usage. stress command simulates load so VPA can say \"Hey, this pod needs more than 50m CPU!\" \ud83d\udea8 VPA Troubleshooting Common Issues & Solutions Issue 1: VPA shows no recommendations # Check VPA components are running kubectl get pods -n kube-system | grep vpa # Check logs kubectl logs -n kube-system deployment/vpa-recommender # Verify metrics are available kubectl top pods # Check VPA object kubectl describe vpa <name> \ud83d\udca1 Likely Causes: Metrics server not running, VPA pods crashed, or not enough time has passed (VPA needs hours of data). Issue 2: Pods not being updated in Auto mode # Check update mode kubectl get vpa <name> -o yaml | grep updateMode # Check if recommendations exist kubectl describe vpa <name> | grep -A5 \"Recommendation\" # Check updater logs kubectl logs -n kube-system deployment/vpa-updater # Check events kubectl get events | grep -i vpa \ud83d\udca1 Common Reason: Recommendations don't differ enough from current resources. VPA won't restart pods for tiny changes. Issue 3: Admission controller not working # Check mutating webhook kubectl get mutatingwebhookconfigurations # Check webhook logs kubectl logs -n kube-system deployment/vpa-admission-controller # Test pod creation kubectl run test --image = nginx --dry-run = client -o yaml | kubectl apply -f - \ud83d\udca1 Webhook Issues: Admission controller is a webhook that intercepts pod creation. If it's down, new pods won't get VPA recommendations. Diagnostic Commands # Get all VPA resources kubectl get vpa --all-namespaces # Check VPA system status kubectl get pods -n kube-system -l app = vpa kubectl get deployments -n kube-system -l app = vpa kubectl get services -n kube-system -l app = vpa # Check events kubectl get events --field-selector involvedObject.kind = VerticalPodAutoscaler kubectl get events --sort-by = .metadata.creationTimestamp | tail -20 # Check resource usage history kubectl get --raw \"/apis/metrics.k8s.io/v1beta1/pods\" | jq '.items[] | select(.metadata.name | contains(\"your-pod\"))' \u26a1 VPA Best Practices 1. Start with Initial Mode updatePolicy : updateMode : \"Initial\" # Always start here \ud83d\udca1 Why: Zero risk. Existing pods keep running, only new pods get changes. Like dipping your toe in water before jumping in. 2. Set Conservative Bounds minAllowed : cpu : \"100m\" # Prevent too-small pods memory : \"128Mi\" maxAllowed : cpu : \"4\" # Prevent runaway growth memory : \"8Gi\" \ud83d\udca1 Safety Rails: Without bounds, VPA might recommend 0.1m CPU (pod won't start) or 100 CPU cores (bankrupts your cloud bill). 3. Monitor Before Switching to Auto # Run in Initial/Off mode for 1-2 weeks # Check recommendations are stable kubectl describe vpa <name> | grep -A10 \"Recommendation\" # Only switch to Auto when: # 1. Recommendations are stable for 7+ days # 2. You understand the impact of pod recreation # 3. Your app can handle pod restarts \ud83d\udca1 The 7-Day Rule: VPA needs to see weekly patterns (weekday vs weekend, business hours vs night). Don't trust day 1 recommendations. 4. Combine with HPA # VPA optimizes resource per pod # HPA adjusts number of pods # Perfect combination for variable workloads \ud83d\udca1 Dream Team: VPA makes each pod the perfect size. HPA decides how many perfect-sized pods you need based on traffic. 5. Regular Reviews Weekly: Check VPA recommendations Monthly: Adjust min/max bounds if needed Quarterly: Review if VPA still needed \ud83d\udca1 VPA Isn't Fire-and-Forget: Like a garden, it needs occasional checking. Applications change, usage patterns shift. \ud83d\udcc8 VPA with HPA: Combined Strategy Why Combine Both? VPA : Gets resource requests right for each pod HPA : Scales pod count based on those correct resources Result : Optimal scaling in both dimensions Implementation Example # 1. Deployment with placeholder resources apiVersion : apps/v1 kind : Deployment metadata : name : combined-app spec : replicas : 2 template : spec : containers : - name : app image : nginx resources : requests : cpu : \"100m\" # Placeholder - VPA will fix memory : \"256Mi\" # Placeholder - VPA will fix --- # 2. VPA for resource optimization apiVersion : autoscaling.k8s.io/v1 kind : VerticalPodAutoscaler metadata : name : combined-vpa spec : targetRef : apiVersion : \"apps/v1\" kind : Deployment name : combined-app updatePolicy : updateMode : \"Initial\" # Safe mode --- # 3. HPA for replica scaling apiVersion : autoscaling/v2 kind : HorizontalPodAutoscaler metadata : name : combined-hpa spec : scaleTargetRef : apiVersion : apps/v1 kind : Deployment name : combined-app minReplicas : 2 maxReplicas : 10 metrics : - type : Resource resource : name : cpu target : type : Utilization averageUtilization : 70 # Scale based on VPA-optimized resources Important Interaction HPA uses: (actual usage / requested resources) \u00d7 100% Since VPA optimizes requested resources, HPA makes better scaling decisions! \ud83d\udca1 Example: Your pod actually uses 300m CPU. - Without VPA: You guessed 500m request \u2192 HPA sees 60% usage (300/500) \u2192 No scaling - With VPA: VPA sets 350m request \u2192 HPA sees 85% usage (300/350) \u2192 Scales up! VPA makes HPA's math more accurate. \u26a0\ufe0f VPA Limitations & Gotchas 1. Pod Recreation Required VPA cannot update running pods' resources Pods must be recreated (causes brief downtime) Not suitable for stateful applications that hate restarts \ud83d\udca1 The Shirt Problem: You can't resize a shirt while someone's wearing it. You must give them a new shirt (recreate pod). 2. Learning Period Required Needs 8+ hours of metrics for good recommendations Longer (24-48 hours) for stable patterns Initial recommendations may be inaccurate \ud83d\udca1 Like a New Doctor: A doctor needs to examine you multiple times before understanding your health patterns. Day 1 diagnosis might be wrong. 3. Container Name Dependency # VPA tracks by CONTAINER NAME containers : - name : \"app\" # Profile tied to THIS name - name : \"app-v2\" # Different name = different profile! \ud83d\udca1 Name Change = New Person: If you rename container from \"app\" to \"app-v2\", VPA thinks it's a completely different container and starts learning from scratch. 4. Not for All Workloads Avoid VPA for: - \u274c Short-lived Jobs (< 5 minutes) - \u274c StatefulSets with persistent data - \u274c Applications with bursty, unpredictable patterns - \u274c When pod recreation causes issues \ud83d\udca1 Wrong Tool for the Job: VPA is great for steady workloads. For spiky, unpredictable workloads, HPA is better. 5. Resource Quota Conflicts VPA might recommend resources exceeding namespace quotas Can cause pod creation failures Monitor quotas when using VPA \ud83d\udca1 Quota Jail: If your namespace has 2 CPU quota and VPA wants to give one pod 3 CPU, that pod can't be created. \ud83d\udd27 Advanced VPA Configuration Custom Metrics Integration # VPA can use Prometheus metrics apiVersion : autoscaling.k8s.io/v1 kind : VerticalPodAutoscaler metadata : name : custom-vpa annotations : vpa.custom.metrics.prometheus.io/cpu : | rate(container_cpu_usage_seconds_total{container=\"myapp\"}[5m]) vpa.custom.metrics.prometheus.io/memory : | container_memory_working_set_bytes{container=\"myapp\"} \ud83d\udca1 Beyond CPU/Memory: In theory, VPA could use any metric (requests per second, queue length), but CPU/memory are the standard ones. Resource-Specific Controls controlledValues : \"RequestsOnly\" # Only adjust requests, not limits # or controlledValues : \"RequestsAndLimits\" # Adjust both (default) controlledResources : [ \"cpu\" ] # Only adjust CPU, not memory # or controlledResources : [ \"cpu\" , \"memory\" ] # Adjust both (default) \ud83d\udca1 Requests vs Limits: - Requests : What Kubernetes guarantees you - Limits : Maximum you can use - Most people let VPA adjust both, but RequestsOnly is safer. Target CPU/Memory Percentiles # Adjust safety margins (advanced) # These are VPA recommender flags, not in VPA spec # Set as command-line args to recommender spec : containers : - name : recommender args : - --cpu-histogram-decay-half-life=24h - --memory-histogram-decay-half-life=48h - --target-cpu-utilization=0.9 # 90th percentile - --target-memory-utilization=0.9 \ud83d\udca1 Tuning Knobs: - decay-half-life : How quickly VPA \"forgets\" old data (24h = yesterday's data matters half as much as today's) - target-utilization : How aggressive to be (0.9 = 90th percentile = 10% safety margin) \ud83d\udcca Monitoring VPA Key Metrics to Track # 1. Resource optimization kubectl describe vpa <name> | grep -A5 \"Recommendation\" # 2. Pod evictions (Auto mode) kubectl get events | grep \"vpa.*evict\" # 3. Cost savings # Compare before/after resource requests # 4. Application performance # Monitor app metrics after VPA changes Prometheus Metrics (if exposed) # VPA recommender metrics vpa_recommendation_cpu_cores vpa_recommendation_memory_bytes vpa_checkpoint_created_total # VPA updater metrics vpa_updater_evictions_total vpa_updater_errors_total \ud83c\udfaf When to Use VPA - Decision Guide Use VPA When: \u2705 Memory-intensive applications (Java, .NET, Node.js) \u2705 Applications with growing resource needs \u2705 You're unsure of resource requirements \u2705 Cost optimization is important \u2705 Combined with HPA for complete autoscaling \ud83d\udca1 Perfect Example: A Java microservice that starts with 1GB heap but grows to need 2GB over months. VPA notices and adjusts automatically. Don't Use VPA When: \u274c Stateful applications (databases with persistent storage) \u274c Short-lived batch jobs \u274c When pod recreation causes business impact \u274c You have precise, known resource requirements \u274c Your cluster has strict resource quotas \ud83d\udca1 Bad Example: PostgreSQL database. If VPA kills the pod to resize it, database recovery might take minutes. Recommended VPA Strategy Week 1 : Deploy VPA in Off mode, monitor recommendations Week 2 : Switch to Initial mode, verify new pods get correct resources Week 3+: : If stable, consider Auto mode for continuous optimization Ongoing : Combine with HPA, monitor monthly \ud83d\udca1 The VPA Journey: Off \u2192 Initial \u2192 (maybe) Auto. Like learning to drive: Parking lot (Off) \u2192 Quiet streets (Initial) \u2192 Highway (Auto, if you're brave). \ud83d\udccb Quick Reference Commands VPA Management # Basic commands kubectl get vpa # List VPAs kubectl describe vpa <name> # Detailed view kubectl edit vpa <name> # Edit VPA kubectl delete vpa <name> # Delete VPA # Check components kubectl get pods -n kube-system -l app = vpa kubectl get deployments -n kube-system -l app = vpa kubectl get services -n kube-system -l app = vpa # Debugging kubectl logs -n kube-system deployment/vpa-recommender kubectl logs -n kube-system deployment/vpa-updater kubectl logs -n kube-system deployment/vpa-admission-controller Testing & Validation # Check current resource usage kubectl top pods kubectl describe pod <pod> | grep -A10 \"Resources\" # Generate test load kubectl exec <pod> -- stress --cpu 2 --vm 1 --vm-bytes 200M --timeout 300 # Monitor VPA actions watch -n 5 'kubectl get vpa,pods && echo \"---\" && kubectl describe vpa <name> | grep -A5 \"Recommendation\"' \ud83d\udca1 Pro Tips Always set min/max bounds to prevent extreme recommendations Start with Initial mode in production Monitor for 1-2 weeks before switching to Auto Combine with HPA for complete autoscaling Regularly review recommendations - VPA isn't \"set and forget\" Test pod recreation impact before enabling Auto mode Use with resource quotas to prevent runaway growth \ud83c\udf93 Summary: VPA in One Page Aspect Recommendation Why Installation Use Helm or official manifests Clean management Initial Mode updateMode: \"Initial\" Safe start Resource Bounds Always set min/max Prevent extremes Monitoring Period 1-2 weeks before trusting Learn patterns Production Use Start with Initial , move to Auto cautiously Avoid surprises Best Combo VPA + HPA Complete autoscaling Avoid For Stateful apps, short jobs Wrong tool Remember : VPA is a powerful tool for resource optimization, but requires careful implementation and monitoring. Start small, learn patterns, and expand gradually! Final Thought: VPA is like having a personal tailor for your pods. A good tailor measures carefully, makes small adjustments, and never ruins your favorite suit. A bad tailor cuts without measuring and leaves you with clothes that don't fit. Be a good VPA tailor.","title":"Vertical Pod Autoscaler (VPA)"},{"location":"01-autoscaling/vpa/#vertical-pod-autoscaler-vpa","text":"","title":"Vertical Pod Autoscaler (VPA)"},{"location":"01-autoscaling/vpa/#what-is-vpa","text":"Vertical Pod Autoscaler (VPA) automatically adjusts the CPU and memory requests/limits of your pods based on actual usage patterns. Unlike HPA (which adds/removes pods), VPA right-sizes individual pods by modifying their resource specifications. \ud83d\udca1 Explanation: Think of your pods wearing clothes. HPA buys more shirts when you have more people (adds pods). VPA measures each person and tailors their shirt to fit perfectly (adjusts CPU/memory per pod). If someone grows or shrinks, VPA remeasures and makes them a new shirt.","title":"\ud83c\udfaf What is VPA?"},{"location":"01-autoscaling/vpa/#core-analogy-tailoring-clothes","text":"HPA : Buys more shirts when you have more people VPA : Resizes each shirt to perfectly fit each person","title":"Core Analogy: Tailoring Clothes"},{"location":"01-autoscaling/vpa/#vpa-vs-hpa-comparison","text":"Aspect Horizontal Pod Autoscaler (HPA) Vertical Pod Autoscaler (VPA) Scales Number of pod replicas CPU/Memory per pod Direction Horizontal (more/fewer pods) Vertical (more/fewer resources) Best for Variable traffic Predictable resource patterns Disruption None (adds/removes pods) Pod recreation required Typical Use Web apps, APIs Databases, memory-heavy apps \ud83d\udca1 Key Difference: HPA changes quantity (pod count), VPA changes quality (resources per pod). They can work together: VPA makes each pod the right size, HPA decides how many of those right-sized pods you need.","title":"\ud83d\udcca VPA vs HPA Comparison"},{"location":"01-autoscaling/vpa/#vpa-architecture-components","text":"","title":"\ud83c\udfd7\ufe0f VPA Architecture &amp; Components"},{"location":"01-autoscaling/vpa/#vpa-component-diagram","text":"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 VPA Controller \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 1. RECOMMENDER \u2502 \u2502 \u2502 \u2502 \u2022 Analyzes historical usage \u2502 \u2502 \u2502 \u2502 \u2022 Creates resource profiles \u2502 \u2502 \u2502 \u2502 \u2022 Suggests optimal requests/limits \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 2. UPDATER \u2502 \u2502 \u2502 \u2502 \u2022 Evicts pods needing updates \u2502 \u2502 \u2502 \u2502 \u2022 Only in \"Auto\" mode \u2502 \u2502 \u2502 \u2502 \u2022 Gracefully terminates pods \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 3. ADMISSION CONTROLLER \u2502 \u2502 \u2502 \u2502 \u2022 Intercepts pod creation \u2502 \u2502 \u2502 \u2502 \u2022 Injects recommended resources \u2502 \u2502 \u2502 \u2502 \u2022 Mutating webhook \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 METRICS HISTORY \u2502 \u2502 (Metrics Server / Prometheus) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \ud83d\udca1 How They Work Together: 1. Recommender = The \"brain\" that learns what resources pods actually need 2. Updater = The \"action taker\" that replaces pods when they need new sizes (only in Auto mode) 3. Admission Controller = The \"gatekeeper\" that gives new pods the right resources from the start 4. Metrics = The \"memory\" that stores what happened in the past","title":"VPA Component Diagram"},{"location":"01-autoscaling/vpa/#vpa-installation","text":"","title":"\ud83d\ude80 VPA Installation"},{"location":"01-autoscaling/vpa/#method-1-official-release-recommended","text":"# Clone VPA repository git clone https://github.com/kubernetes/autoscaler.git cd autoscaler/vertical-pod-autoscaler # Install all components ./hack/vpa-up.sh # Verify installation kubectl get pods -n kube-system | grep vpa \ud83d\udca1 What This Installs: - vpa-recommender : Learns and suggests resource sizes - vpa-updater : Takes action to resize pods (in Auto mode) - vpa-admission-controller : Updates new pods as they're created - vpa-crd : Custom Resource Definition for VPA objects","title":"Method 1: Official Release (Recommended)"},{"location":"01-autoscaling/vpa/#method-2-helm-installation","text":"# Add Helm repository helm repo add fairwinds-stable https://charts.fairwinds.com/stable helm repo update # Install VPA helm install vpa fairwinds-stable/vpa \\ --namespace kube-system \\ --set recommender.enabled = true \\ --set updater.enabled = true \\ --set admissionController.enabled = true \ud83d\udca1 Helm Benefits: Easier upgrades, configuration management, and clean uninstalls.","title":"Method 2: Helm Installation"},{"location":"01-autoscaling/vpa/#method-3-manifest-files","text":"# Apply individual components kubectl apply -f https://github.com/kubernetes/autoscaler/releases/latest/download/vertical-pod-autoscaler-recommender.yaml kubectl apply -f https://github.com/kubernetes/autoscaler/releases/latest/download/vertical-pod-autoscaler-updater.yaml kubectl apply -f https://github.com/kubernetes/autoscaler/releases/latest/download/vertical-pod-autoscaler-admission-controller.yaml # Verify kubectl get pods -n kube-system -l app = vpa","title":"Method 3: Manifest Files"},{"location":"01-autoscaling/vpa/#vpa-components-explained","text":"","title":"\u2699\ufe0f VPA Components Explained"},{"location":"01-autoscaling/vpa/#1-vpa-recommender","text":"Purpose : Analyzes historical resource usage Output : Resource recommendations stored in VPA object Data Source : Metrics Server (last 8 days by default) Algorithm : Uses histogram of usage patterns \ud83d\udca1 How It Learns: Imagine you have a pod that uses between 200-400m CPU over time. Recommender watches for 8 days, builds a histogram, and says: \"This pod usually needs 300m CPU, but sometimes hits 400m. Let's recommend 350m to be safe.\"","title":"1. VPA Recommender"},{"location":"01-autoscaling/vpa/#2-vpa-updater","text":"Purpose : Evicts pods that need resource adjustments Action : Deletes pods with incorrect allocations Trigger : When recommendations differ significantly from current Mode : Only active in Auto mode \ud83d\udca1 The \"Pod Killer\": Updater is like a tailor who says \"Your shirt doesn't fit anymore!\" and makes you take it off so they can give you a new one. It doesn't resize the shirt while you're wearing it\u2014it makes you change shirts entirely.","title":"2. VPA Updater"},{"location":"01-autoscaling/vpa/#3-vpa-admission-controller","text":"Purpose : Modifies pod specs during creation How : Mutating admission webhook When : Intercepts all pod creation requests What : Replaces resource requests/limits with recommendations \ud83d\udca1 The \"Birth Certificate\": When a pod is born, Admission Controller checks the Recommender's notes and says: \"This pod should have 350m CPU, not the 100m written in its DNA (YAML file).\" It secretly changes the birth certificate before anyone notices.","title":"3. VPA Admission Controller"},{"location":"01-autoscaling/vpa/#vpa-configuration-modes","text":"","title":"\ud83d\udcdd VPA Configuration Modes"},{"location":"01-autoscaling/vpa/#four-update-modes","text":"","title":"Four Update Modes"},{"location":"01-autoscaling/vpa/#1-initial-mode-most-common","text":"updatePolicy : updateMode : \"Initial\" - Behavior : Only sets resources on pod creation - Existing pods : Never updated - New pods : Get recommended resources - Best for : Safe production use, testing \ud83d\udca1 Analogy: Like a hospital that measures every newborn baby and gives them the right size diaper, but doesn't change diapers on babies who've already left the hospital.","title":"1. Initial Mode (Most Common)"},{"location":"01-autoscaling/vpa/#2-auto-mode-aggressive","text":"updatePolicy : updateMode : \"Auto\" - Behavior : Updates resources and recreates pods - Existing pods : Evicted and recreated with new resources - Risk : Pod disruptions, potential downtime - Best for : Non-critical workloads, dev environments \ud83d\udca1 Analogy: Like a strict parent who makes you change clothes immediately if your shirt doesn't fit perfectly, even if you're in the middle of dinner.","title":"2. Auto Mode (Aggressive)"},{"location":"01-autoscaling/vpa/#3-recreate-mode-rarely-used","text":"updatePolicy : updateMode : \"Recreate\" - Behavior : Only recreates pods (no resource updates) - Use case : When pod recreation needed without resource changes \ud83d\udca1 When to Use: If you want VPA to restart pods periodically but not change their resources. Rare case.","title":"3. Recreate Mode (Rarely Used)"},{"location":"01-autoscaling/vpa/#4-off-mode-monitor-only","text":"updatePolicy : updateMode : \"Off\" - Behavior : Only provides recommendations - Action : No automatic changes - Best for : Learning patterns, manual optimization \ud83d\udca1 Analogy: Like a nutritionist who tells you \"You should eat 2000 calories per day\" but doesn't stop you from eating a whole pizza.","title":"4. Off Mode (Monitor Only)"},{"location":"01-autoscaling/vpa/#basic-vpa-configuration","text":"","title":"\ud83d\udd27 Basic VPA Configuration"},{"location":"01-autoscaling/vpa/#minimal-vpa-example","text":"apiVersion : autoscaling.k8s.io/v1 kind : VerticalPodAutoscaler metadata : name : myapp-vpa spec : targetRef : apiVersion : \"apps/v1\" kind : Deployment name : myapp-deployment updatePolicy : updateMode : \"Initial\" # Safe mode resourcePolicy : containerPolicies : - containerName : \"*\" # Apply to all containers minAllowed : cpu : \"100m\" memory : \"128Mi\" maxAllowed : cpu : \"2\" memory : \"2Gi\" controlledResources : [ \"cpu\" , \"memory\" ] \ud83d\udca1 Breaking It Down: - targetRef : Which deployment/statefulset to watch (like tagging a person for measurement) - updateMode: \"Initial\" : Safe mode\u2014only fix new pods - containerName: \"*\" : Apply to ALL containers in the pod (asterisk = wildcard) - minAllowed/maxAllowed : Safety rails so VPA doesn't recommend crazy sizes - controlledResources : Which resources to adjust (CPU, memory, or both)","title":"Minimal VPA Example"},{"location":"01-autoscaling/vpa/#complete-vpa-with-all-options","text":"apiVersion : autoscaling.k8s.io/v1 kind : VerticalPodAutoscaler metadata : name : complete-vpa spec : targetRef : apiVersion : \"apps/v1\" kind : Deployment # Can be: Deployment, StatefulSet, DaemonSet name : my-application updatePolicy : updateMode : \"Auto\" # Initial, Auto, Recreate, or Off minReplicas : 2 # Optional: Minimum pods to consider resourcePolicy : containerPolicies : - containerName : \"app\" mode : \"Auto\" # Auto, Off, or Initial minAllowed : cpu : \"100m\" memory : \"128Mi\" maxAllowed : cpu : \"4\" memory : \"8Gi\" controlledResources : [ \"cpu\" , \"memory\" ] controlledValues : \"RequestsAndLimits\" # or \"RequestsOnly\" - containerName : \"sidecar\" mode : \"Off\" # Don't autoscale this container \ud83d\udca1 Advanced Options Explained: - minReplicas : Only start recommending if you have at least this many pods (more data = better recommendations) - container-specific mode : Different rules per container in same pod - controlledValues : \"RequestsOnly\" = only adjust requests, not limits (limits stay as you set them)","title":"Complete VPA with All Options"},{"location":"01-autoscaling/vpa/#resource-policy-explained","text":"","title":"\ud83d\udcca Resource Policy Explained"},{"location":"01-autoscaling/vpa/#container-policies","text":"resourcePolicy : containerPolicies : - containerName : \"webapp\" # Specific container minAllowed : # Minimum VPA can recommend cpu : \"100m\" memory : \"256Mi\" maxAllowed : # Maximum VPA can recommend cpu : \"2\" memory : \"4Gi\" controlledResources : # Which resources to adjust - \"cpu\" - \"memory\" controlledValues : \"RequestsAndLimits\" # Options: # - RequestsAndLimits (default) # - RequestsOnly \ud83d\udca1 Why Min/Max Matters: Without these, VPA might recommend 0.001m CPU (too small to run) or 1000 CPU cores (breaks your cluster). These are guardrails.","title":"Container Policies"},{"location":"01-autoscaling/vpa/#wildcard-vs-specific-containers","text":"# Option 1: All containers - containerName : \"*\" # Apply to EVERY container # Option 2: Specific containers - containerName : \"app-server\" # Only this container - containerName : \"cache\" # Different policy for cache - containerName : \"logger\" # Don't set mode: \"Off\" \ud83d\udca1 Real Example: Your pod has 3 containers: app (needs lots of CPU), redis (needs lots of memory), logger (tiny, fixed needs). Use specific names to treat each differently.","title":"Wildcard vs Specific Containers"},{"location":"01-autoscaling/vpa/#practical-vpa-examples","text":"","title":"\ud83d\udee0\ufe0f Practical VPA Examples"},{"location":"01-autoscaling/vpa/#example-1-web-application","text":"apiVersion : autoscaling.k8s.io/v1 kind : VerticalPodAutoscaler metadata : name : webapp-vpa spec : targetRef : apiVersion : \"apps/v1\" kind : Deployment name : webapp updatePolicy : updateMode : \"Initial\" # Safe for production resourcePolicy : containerPolicies : - containerName : \"nginx\" minAllowed : cpu : \"100m\" memory : \"128Mi\" maxAllowed : cpu : \"1\" memory : \"1Gi\" - containerName : \"app\" minAllowed : cpu : \"200m\" memory : \"256Mi\" maxAllowed : cpu : \"2\" memory : \"2Gi\" \ud83d\udca1 Two Containers, Different Needs: Nginx (web server) vs App (application logic) have different resource patterns. VPA handles each separately.","title":"Example 1: Web Application"},{"location":"01-autoscaling/vpa/#example-2-database-with-statefulset","text":"apiVersion : autoscaling.k8s.io/v1 kind : VerticalPodAutoscaler metadata : name : postgres-vpa spec : targetRef : apiVersion : \"apps/v1\" kind : StatefulSet # Works with StatefulSets! name : postgres updatePolicy : updateMode : \"Initial\" # CRITICAL: Databases hate pod restarts! resourcePolicy : containerPolicies : - containerName : \"postgres\" minAllowed : cpu : \"500m\" memory : \"1Gi\" maxAllowed : cpu : \"4\" memory : \"16Gi\" \ud83d\udca1 StatefulSet Warning: Databases store data on disk. If VPA kills the pod in Auto mode, database might get confused. Initial mode is safer\u2014only fixes new pods when they're created during maintenance.","title":"Example 2: Database with StatefulSet"},{"location":"01-autoscaling/vpa/#example-3-multi-container-microservice","text":"apiVersion : autoscaling.k8s.io/v1 kind : VerticalPodAutoscaler metadata : name : microservice-vpa spec : targetRef : apiVersion : \"apps/v1\" kind : Deployment name : order-service updatePolicy : updateMode : \"Auto\" # Will recreate pods resourcePolicy : containerPolicies : - containerName : \"order-processor\" minAllowed : cpu : \"200m\" memory : \"512Mi\" maxAllowed : cpu : \"2\" memory : \"4Gi\" controlledValues : \"RequestsAndLimits\" - containerName : \"redis-sidecar\" mode : \"Off\" # Don't autoscale sidecar # No min/max needed when mode is Off \ud83d\udca1 Sidecar Pattern: Redis runs alongside your app as a cache. You might not want VPA changing it because Redis has known memory patterns. mode: \"Off\" tells VPA to leave it alone.","title":"Example 3: Multi-Container Microservice"},{"location":"01-autoscaling/vpa/#how-vpa-makes-recommendations","text":"","title":"\ud83d\udd0d How VPA Makes Recommendations"},{"location":"01-autoscaling/vpa/#recommendation-algorithm","text":"VPA Recommendation Process: 1. Collect 8 days of usage data (default) 2. Build histogram of CPU/Memory usage 3. Calculate: - Target: 90th percentile of usage + safety margin - Lower Bound: 50th percentile - Upper Bound: 95th percentile + safety margin 4. Store in VPA object status \ud83d\udca1 What This Means: If your pod uses 100m, 200m, 300m, 400m CPU over time: - 50th percentile (Lower Bound) : 250m (half the time it uses less than this) - 90th percentile (Target) : 380m (90% of the time it uses less than this) - 95th percentile (Upper Bound) : 390m (almost never uses more than this) VPA recommends Target (380m) as the \"right size\" with safety margins.","title":"Recommendation Algorithm"},{"location":"01-autoscaling/vpa/#viewing-recommendations","text":"# Check VPA status kubectl get vpa # Detailed view with recommendations kubectl describe vpa myapp-vpa # Raw YAML with recommendations kubectl get vpa myapp-vpa -o yaml","title":"Viewing Recommendations"},{"location":"01-autoscaling/vpa/#sample-vpa-output","text":"$ kubectl describe vpa/webapp-vpa Status: Conditions: Status: True Type: RecommendationProvided Recommendation: Container Recommendations: Container Name: nginx Lower Bound: # Minimum safe Cpu: 100m Memory: 128Mi Target: # \u2b50 RECOMMENDED VALUE \u2b50 Cpu: 350m Memory: 512Mi Uncapped Target: # Without min/max constraints Cpu: 420m Memory: 600Mi Upper Bound: # Maximum safe Cpu: 500m Memory: 1Gi \ud83d\udca1 Reading This Output: - Target (350m CPU) : What VPA wants to set your pod to - Lower Bound (100m) : Below this is dangerously small - Upper Bound (500m) : Above this is wastefully large - Uncapped Target (420m) : What VPA would recommend if you had no min/max limits","title":"Sample VPA Output"},{"location":"01-autoscaling/vpa/#testing-vpa-step-by-step","text":"","title":"\ud83e\uddea Testing VPA Step by Step"},{"location":"01-autoscaling/vpa/#step-1-create-test-deployment","text":"# test-deployment.yaml apiVersion : apps/v1 kind : Deployment metadata : name : vpa-test spec : replicas : 2 selector : matchLabels : app : vpa-test template : metadata : labels : app : vpa-test spec : containers : - name : test-app image : polinux/stress command : [ \"sleep\" , \"3600\" ] resources : requests : cpu : \"50m\" # Intentionally TOO LOW memory : \"64Mi\" # Intentionally TOO LOW \ud83d\udca1 Setting Up the Test: We create pods with obviously wrong resources (50m CPU) so VPA has something to fix.","title":"Step 1: Create Test Deployment"},{"location":"01-autoscaling/vpa/#step-2-create-vpa","text":"# test-vpa.yaml apiVersion : autoscaling.k8s.io/v1 kind : VerticalPodAutoscaler metadata : name : vpa-test-vpa spec : targetRef : apiVersion : \"apps/v1\" kind : Deployment name : vpa-test updatePolicy : updateMode : \"Auto\" # Will show immediate effect resourcePolicy : containerPolicies : - containerName : \"*\" minAllowed : cpu : \"100m\" memory : \"128Mi\" maxAllowed : cpu : \"2\" memory : \"2Gi\" \ud83d\udca1 Using Auto Mode for Testing: In real life, start with Initial . For testing, Auto shows VPA in action immediately.","title":"Step 2: Create VPA"},{"location":"01-autoscaling/vpa/#step-3-apply-and-generate-load","text":"# Apply configurations kubectl apply -f test-deployment.yaml kubectl apply -f test-vpa.yaml # Generate load to help VPA learn kubectl exec deployment/vpa-test -- stress --cpu 2 --vm 1 --vm-bytes 200M --timeout 300 # Wait for VPA to analyze (5-10 minutes) sleep 300 # Check recommendations kubectl describe vpa vpa-test-vpa # Watch pods get recreated (Auto mode) kubectl get pods -l app = vpa-test -w \ud83d\udca1 The Learning Process: VPA needs to see actual usage. stress command simulates load so VPA can say \"Hey, this pod needs more than 50m CPU!\"","title":"Step 3: Apply and Generate Load"},{"location":"01-autoscaling/vpa/#vpa-troubleshooting","text":"","title":"\ud83d\udea8 VPA Troubleshooting"},{"location":"01-autoscaling/vpa/#common-issues-solutions","text":"","title":"Common Issues &amp; Solutions"},{"location":"01-autoscaling/vpa/#issue-1-vpa-shows-no-recommendations","text":"# Check VPA components are running kubectl get pods -n kube-system | grep vpa # Check logs kubectl logs -n kube-system deployment/vpa-recommender # Verify metrics are available kubectl top pods # Check VPA object kubectl describe vpa <name> \ud83d\udca1 Likely Causes: Metrics server not running, VPA pods crashed, or not enough time has passed (VPA needs hours of data).","title":"Issue 1: VPA shows no recommendations"},{"location":"01-autoscaling/vpa/#issue-2-pods-not-being-updated-in-auto-mode","text":"# Check update mode kubectl get vpa <name> -o yaml | grep updateMode # Check if recommendations exist kubectl describe vpa <name> | grep -A5 \"Recommendation\" # Check updater logs kubectl logs -n kube-system deployment/vpa-updater # Check events kubectl get events | grep -i vpa \ud83d\udca1 Common Reason: Recommendations don't differ enough from current resources. VPA won't restart pods for tiny changes.","title":"Issue 2: Pods not being updated in Auto mode"},{"location":"01-autoscaling/vpa/#issue-3-admission-controller-not-working","text":"# Check mutating webhook kubectl get mutatingwebhookconfigurations # Check webhook logs kubectl logs -n kube-system deployment/vpa-admission-controller # Test pod creation kubectl run test --image = nginx --dry-run = client -o yaml | kubectl apply -f - \ud83d\udca1 Webhook Issues: Admission controller is a webhook that intercepts pod creation. If it's down, new pods won't get VPA recommendations.","title":"Issue 3: Admission controller not working"},{"location":"01-autoscaling/vpa/#diagnostic-commands","text":"# Get all VPA resources kubectl get vpa --all-namespaces # Check VPA system status kubectl get pods -n kube-system -l app = vpa kubectl get deployments -n kube-system -l app = vpa kubectl get services -n kube-system -l app = vpa # Check events kubectl get events --field-selector involvedObject.kind = VerticalPodAutoscaler kubectl get events --sort-by = .metadata.creationTimestamp | tail -20 # Check resource usage history kubectl get --raw \"/apis/metrics.k8s.io/v1beta1/pods\" | jq '.items[] | select(.metadata.name | contains(\"your-pod\"))'","title":"Diagnostic Commands"},{"location":"01-autoscaling/vpa/#vpa-best-practices","text":"","title":"\u26a1 VPA Best Practices"},{"location":"01-autoscaling/vpa/#1-start-with-initial-mode","text":"updatePolicy : updateMode : \"Initial\" # Always start here \ud83d\udca1 Why: Zero risk. Existing pods keep running, only new pods get changes. Like dipping your toe in water before jumping in.","title":"1. Start with Initial Mode"},{"location":"01-autoscaling/vpa/#2-set-conservative-bounds","text":"minAllowed : cpu : \"100m\" # Prevent too-small pods memory : \"128Mi\" maxAllowed : cpu : \"4\" # Prevent runaway growth memory : \"8Gi\" \ud83d\udca1 Safety Rails: Without bounds, VPA might recommend 0.1m CPU (pod won't start) or 100 CPU cores (bankrupts your cloud bill).","title":"2. Set Conservative Bounds"},{"location":"01-autoscaling/vpa/#3-monitor-before-switching-to-auto","text":"# Run in Initial/Off mode for 1-2 weeks # Check recommendations are stable kubectl describe vpa <name> | grep -A10 \"Recommendation\" # Only switch to Auto when: # 1. Recommendations are stable for 7+ days # 2. You understand the impact of pod recreation # 3. Your app can handle pod restarts \ud83d\udca1 The 7-Day Rule: VPA needs to see weekly patterns (weekday vs weekend, business hours vs night). Don't trust day 1 recommendations.","title":"3. Monitor Before Switching to Auto"},{"location":"01-autoscaling/vpa/#4-combine-with-hpa","text":"# VPA optimizes resource per pod # HPA adjusts number of pods # Perfect combination for variable workloads \ud83d\udca1 Dream Team: VPA makes each pod the perfect size. HPA decides how many perfect-sized pods you need based on traffic.","title":"4. Combine with HPA"},{"location":"01-autoscaling/vpa/#5-regular-reviews","text":"Weekly: Check VPA recommendations Monthly: Adjust min/max bounds if needed Quarterly: Review if VPA still needed \ud83d\udca1 VPA Isn't Fire-and-Forget: Like a garden, it needs occasional checking. Applications change, usage patterns shift.","title":"5. Regular Reviews"},{"location":"01-autoscaling/vpa/#vpa-with-hpa-combined-strategy","text":"","title":"\ud83d\udcc8 VPA with HPA: Combined Strategy"},{"location":"01-autoscaling/vpa/#why-combine-both","text":"VPA : Gets resource requests right for each pod HPA : Scales pod count based on those correct resources Result : Optimal scaling in both dimensions","title":"Why Combine Both?"},{"location":"01-autoscaling/vpa/#implementation-example","text":"# 1. Deployment with placeholder resources apiVersion : apps/v1 kind : Deployment metadata : name : combined-app spec : replicas : 2 template : spec : containers : - name : app image : nginx resources : requests : cpu : \"100m\" # Placeholder - VPA will fix memory : \"256Mi\" # Placeholder - VPA will fix --- # 2. VPA for resource optimization apiVersion : autoscaling.k8s.io/v1 kind : VerticalPodAutoscaler metadata : name : combined-vpa spec : targetRef : apiVersion : \"apps/v1\" kind : Deployment name : combined-app updatePolicy : updateMode : \"Initial\" # Safe mode --- # 3. HPA for replica scaling apiVersion : autoscaling/v2 kind : HorizontalPodAutoscaler metadata : name : combined-hpa spec : scaleTargetRef : apiVersion : apps/v1 kind : Deployment name : combined-app minReplicas : 2 maxReplicas : 10 metrics : - type : Resource resource : name : cpu target : type : Utilization averageUtilization : 70 # Scale based on VPA-optimized resources","title":"Implementation Example"},{"location":"01-autoscaling/vpa/#important-interaction","text":"HPA uses: (actual usage / requested resources) \u00d7 100% Since VPA optimizes requested resources, HPA makes better scaling decisions! \ud83d\udca1 Example: Your pod actually uses 300m CPU. - Without VPA: You guessed 500m request \u2192 HPA sees 60% usage (300/500) \u2192 No scaling - With VPA: VPA sets 350m request \u2192 HPA sees 85% usage (300/350) \u2192 Scales up! VPA makes HPA's math more accurate.","title":"Important Interaction"},{"location":"01-autoscaling/vpa/#vpa-limitations-gotchas","text":"","title":"\u26a0\ufe0f VPA Limitations &amp; Gotchas"},{"location":"01-autoscaling/vpa/#1-pod-recreation-required","text":"VPA cannot update running pods' resources Pods must be recreated (causes brief downtime) Not suitable for stateful applications that hate restarts \ud83d\udca1 The Shirt Problem: You can't resize a shirt while someone's wearing it. You must give them a new shirt (recreate pod).","title":"1. Pod Recreation Required"},{"location":"01-autoscaling/vpa/#2-learning-period-required","text":"Needs 8+ hours of metrics for good recommendations Longer (24-48 hours) for stable patterns Initial recommendations may be inaccurate \ud83d\udca1 Like a New Doctor: A doctor needs to examine you multiple times before understanding your health patterns. Day 1 diagnosis might be wrong.","title":"2. Learning Period Required"},{"location":"01-autoscaling/vpa/#3-container-name-dependency","text":"# VPA tracks by CONTAINER NAME containers : - name : \"app\" # Profile tied to THIS name - name : \"app-v2\" # Different name = different profile! \ud83d\udca1 Name Change = New Person: If you rename container from \"app\" to \"app-v2\", VPA thinks it's a completely different container and starts learning from scratch.","title":"3. Container Name Dependency"},{"location":"01-autoscaling/vpa/#4-not-for-all-workloads","text":"Avoid VPA for: - \u274c Short-lived Jobs (< 5 minutes) - \u274c StatefulSets with persistent data - \u274c Applications with bursty, unpredictable patterns - \u274c When pod recreation causes issues \ud83d\udca1 Wrong Tool for the Job: VPA is great for steady workloads. For spiky, unpredictable workloads, HPA is better.","title":"4. Not for All Workloads"},{"location":"01-autoscaling/vpa/#5-resource-quota-conflicts","text":"VPA might recommend resources exceeding namespace quotas Can cause pod creation failures Monitor quotas when using VPA \ud83d\udca1 Quota Jail: If your namespace has 2 CPU quota and VPA wants to give one pod 3 CPU, that pod can't be created.","title":"5. Resource Quota Conflicts"},{"location":"01-autoscaling/vpa/#advanced-vpa-configuration","text":"","title":"\ud83d\udd27 Advanced VPA Configuration"},{"location":"01-autoscaling/vpa/#custom-metrics-integration","text":"# VPA can use Prometheus metrics apiVersion : autoscaling.k8s.io/v1 kind : VerticalPodAutoscaler metadata : name : custom-vpa annotations : vpa.custom.metrics.prometheus.io/cpu : | rate(container_cpu_usage_seconds_total{container=\"myapp\"}[5m]) vpa.custom.metrics.prometheus.io/memory : | container_memory_working_set_bytes{container=\"myapp\"} \ud83d\udca1 Beyond CPU/Memory: In theory, VPA could use any metric (requests per second, queue length), but CPU/memory are the standard ones.","title":"Custom Metrics Integration"},{"location":"01-autoscaling/vpa/#resource-specific-controls","text":"controlledValues : \"RequestsOnly\" # Only adjust requests, not limits # or controlledValues : \"RequestsAndLimits\" # Adjust both (default) controlledResources : [ \"cpu\" ] # Only adjust CPU, not memory # or controlledResources : [ \"cpu\" , \"memory\" ] # Adjust both (default) \ud83d\udca1 Requests vs Limits: - Requests : What Kubernetes guarantees you - Limits : Maximum you can use - Most people let VPA adjust both, but RequestsOnly is safer.","title":"Resource-Specific Controls"},{"location":"01-autoscaling/vpa/#target-cpumemory-percentiles","text":"# Adjust safety margins (advanced) # These are VPA recommender flags, not in VPA spec # Set as command-line args to recommender spec : containers : - name : recommender args : - --cpu-histogram-decay-half-life=24h - --memory-histogram-decay-half-life=48h - --target-cpu-utilization=0.9 # 90th percentile - --target-memory-utilization=0.9 \ud83d\udca1 Tuning Knobs: - decay-half-life : How quickly VPA \"forgets\" old data (24h = yesterday's data matters half as much as today's) - target-utilization : How aggressive to be (0.9 = 90th percentile = 10% safety margin)","title":"Target CPU/Memory Percentiles"},{"location":"01-autoscaling/vpa/#monitoring-vpa","text":"","title":"\ud83d\udcca Monitoring VPA"},{"location":"01-autoscaling/vpa/#key-metrics-to-track","text":"# 1. Resource optimization kubectl describe vpa <name> | grep -A5 \"Recommendation\" # 2. Pod evictions (Auto mode) kubectl get events | grep \"vpa.*evict\" # 3. Cost savings # Compare before/after resource requests # 4. Application performance # Monitor app metrics after VPA changes","title":"Key Metrics to Track"},{"location":"01-autoscaling/vpa/#prometheus-metrics-if-exposed","text":"# VPA recommender metrics vpa_recommendation_cpu_cores vpa_recommendation_memory_bytes vpa_checkpoint_created_total # VPA updater metrics vpa_updater_evictions_total vpa_updater_errors_total","title":"Prometheus Metrics (if exposed)"},{"location":"01-autoscaling/vpa/#when-to-use-vpa-decision-guide","text":"","title":"\ud83c\udfaf When to Use VPA - Decision Guide"},{"location":"01-autoscaling/vpa/#use-vpa-when","text":"\u2705 Memory-intensive applications (Java, .NET, Node.js) \u2705 Applications with growing resource needs \u2705 You're unsure of resource requirements \u2705 Cost optimization is important \u2705 Combined with HPA for complete autoscaling \ud83d\udca1 Perfect Example: A Java microservice that starts with 1GB heap but grows to need 2GB over months. VPA notices and adjusts automatically.","title":"Use VPA When:"},{"location":"01-autoscaling/vpa/#dont-use-vpa-when","text":"\u274c Stateful applications (databases with persistent storage) \u274c Short-lived batch jobs \u274c When pod recreation causes business impact \u274c You have precise, known resource requirements \u274c Your cluster has strict resource quotas \ud83d\udca1 Bad Example: PostgreSQL database. If VPA kills the pod to resize it, database recovery might take minutes.","title":"Don't Use VPA When:"},{"location":"01-autoscaling/vpa/#recommended-vpa-strategy","text":"Week 1 : Deploy VPA in Off mode, monitor recommendations Week 2 : Switch to Initial mode, verify new pods get correct resources Week 3+: : If stable, consider Auto mode for continuous optimization Ongoing : Combine with HPA, monitor monthly \ud83d\udca1 The VPA Journey: Off \u2192 Initial \u2192 (maybe) Auto. Like learning to drive: Parking lot (Off) \u2192 Quiet streets (Initial) \u2192 Highway (Auto, if you're brave).","title":"Recommended VPA Strategy"},{"location":"01-autoscaling/vpa/#quick-reference-commands","text":"","title":"\ud83d\udccb Quick Reference Commands"},{"location":"01-autoscaling/vpa/#vpa-management","text":"# Basic commands kubectl get vpa # List VPAs kubectl describe vpa <name> # Detailed view kubectl edit vpa <name> # Edit VPA kubectl delete vpa <name> # Delete VPA # Check components kubectl get pods -n kube-system -l app = vpa kubectl get deployments -n kube-system -l app = vpa kubectl get services -n kube-system -l app = vpa # Debugging kubectl logs -n kube-system deployment/vpa-recommender kubectl logs -n kube-system deployment/vpa-updater kubectl logs -n kube-system deployment/vpa-admission-controller","title":"VPA Management"},{"location":"01-autoscaling/vpa/#testing-validation","text":"# Check current resource usage kubectl top pods kubectl describe pod <pod> | grep -A10 \"Resources\" # Generate test load kubectl exec <pod> -- stress --cpu 2 --vm 1 --vm-bytes 200M --timeout 300 # Monitor VPA actions watch -n 5 'kubectl get vpa,pods && echo \"---\" && kubectl describe vpa <name> | grep -A5 \"Recommendation\"'","title":"Testing &amp; Validation"},{"location":"01-autoscaling/vpa/#pro-tips","text":"Always set min/max bounds to prevent extreme recommendations Start with Initial mode in production Monitor for 1-2 weeks before switching to Auto Combine with HPA for complete autoscaling Regularly review recommendations - VPA isn't \"set and forget\" Test pod recreation impact before enabling Auto mode Use with resource quotas to prevent runaway growth","title":"\ud83d\udca1 Pro Tips"},{"location":"01-autoscaling/vpa/#summary-vpa-in-one-page","text":"Aspect Recommendation Why Installation Use Helm or official manifests Clean management Initial Mode updateMode: \"Initial\" Safe start Resource Bounds Always set min/max Prevent extremes Monitoring Period 1-2 weeks before trusting Learn patterns Production Use Start with Initial , move to Auto cautiously Avoid surprises Best Combo VPA + HPA Complete autoscaling Avoid For Stateful apps, short jobs Wrong tool Remember : VPA is a powerful tool for resource optimization, but requires careful implementation and monitoring. Start small, learn patterns, and expand gradually! Final Thought: VPA is like having a personal tailor for your pods. A good tailor measures carefully, makes small adjustments, and never ruins your favorite suit. A bad tailor cuts without measuring and leaves you with clothes that don't fit. Be a good VPA tailor.","title":"\ud83c\udf93 Summary: VPA in One Page"},{"location":"02%20helm/commands/","text":"Helm Comprehensive Guide TABLE OF CONTENTS Helm Fundamentals Repository Management Release Lifecycle Configuration Management Advanced Operations Debugging & Troubleshooting Best Practices Real-world Examples 1. HELM FUNDAMENTALS What is Helm? Helm is the package manager for Kubernetes , allowing you to: - Define, install, and upgrade Kubernetes applications - Package applications as charts - Manage dependencies between applications - Share applications through repositories Key Concepts Concept Description Example Chart Package containing all resource definitions bitnami/mysql , nginx-ingress Release Instance of a chart running in Kubernetes mysql-release , webapp-prod Repository Collection of charts bitnami , stable , jetstack Values Configurable parameters for a chart Database passwords, replica counts Version Types # Chart outputs show both versions: CHART NAME: tomcat CHART VERSION: 13 .3.0 # \u2190 Helm chart version (templates/config) APP VERSION: 11 .0.15 # \u2190 Application version (actual software) Key Difference: - Chart Version : Changes when Helm templates, dependencies, or configurations change - App Version : Changes when the actual application software version changes 2. REPOSITORY MANAGEMENT Core Commands # Add a repository helm repo add bitnami https://charts.bitnami.com/bitnami # List all repositories helm repo list # Update repository index helm repo update # Search for charts helm search repo apache # Stable releases helm search repo mysql --devel # Include development versions helm search repo --versions nginx # Show all versions # Remove a repository helm repo remove bitnami Bitnami Repository Specifics # Since August 2025, Bitnami requires subscription for latest images # Free tier workaround: helm install mydb bitnami/mysql --set image.repository = bitnamilegacy/mysql # Warnings you'll see: # 1. Limited free tier images available # 2. Rolling tags (:latest) not recommended for production # 3. Resources should be explicitly set 3. RELEASE LIFECYCLE Installation # Basic installation helm install mysql wso2/mysql # With custom name helm install my-database bitnami/mysql # Generate random name helm install bitnami/apache --generate-name # Custom naming template helm install bitnami/apache --generate-name --name-template \"web-{{randAlpha 5 | lower}}\" Upgrade & Rollback # Upgrade with new values helm upgrade mysql wso2/mysql --set testFramework.enabled = false # Upgrade with values file helm upgrade mysql bitnami/mysql -f mysql-values.yaml # Reuse previous values (dangerous - may use defaults for missing values) helm upgrade mydb bitnami/mysql --reuse-values # Force upgrade (deletes and recreates resources - causes downtime) helm upgrade myapp bitnami/tomcat --force # Check upgrade history helm history mysql # Rollback to specific revision helm rollback mysql 1 Uninstallation # Complete removal helm uninstall mysql # Keep history (resources remain, only Helm metadata removed) helm uninstall server --keep-history Status & Inspection # List all releases helm ls helm list helm list --all-namespaces # Get release information helm status mysql helm get notes mysql # Show NOTES.txt helm get values mysql # Show user-supplied values helm get values mysql --all # Show all values (including defaults) helm get values mysql --revision 2 # Show values for specific revision # Get manifest helm get manifest mysql 4. CONFIGURATION MANAGEMENT Setting Values Method 1: Command Line (--set) # Single value helm install mysql bitnami/mysql --set auth.rootPassword = Secret123 # Nested values helm install mysql bitnami/mysql \\ --set auth.rootPassword = Secret123 \\ --set auth.database = appdb \\ --set primary.persistence.size = 10Gi # Arrays/lists helm install myapp bitnami/app --set \"podAnnotations.key=value\" Method 2: Values File (Recommended) # mysql-values.yaml auth : rootPassword : StrongRootPass123 database : appdb username : appuser password : AppUserPass123 primary : persistence : enabled : true size : 10Gi storageClass : standard resources : requests : cpu : 250m memory : 512Mi limits : cpu : 500m memory : 1Gi service : type : ClusterIP # Install with values file helm install mysql bitnami/mysql -f mysql-values.yaml # Override values file with command line helm install mysql bitnami/mysql -f mysql-values.yaml --set replicaCount = 3 Method 3: Multiple Values Files # Base values + environment-specific overrides helm install myapp bitnami/app -f values.yaml -f production.yaml Template Rendering & Validation # Dry run (see what would be deployed) helm install server bitnami/tomcat --dry-run = client # Render templates locally helm template server bitnami/tomcat # Debug template rendering helm template server bitnami/tomcat --debug # Lint chart (validate) helm lint ./mychart 5. ADVANCED OPERATIONS Installation Options # Wait for resources to be ready helm install myapp bitnami/nginx --wait helm install myapp bitnami/nginx --wait --timeout 60s # Custom timeout # Atomic installation (auto-rollback on failure) helm install myapp bitnami/nginx --atomic # Create namespace if it doesn't exist helm install myapp bitnami/nginx --namespace myns --create-namespace # Skip CRDs helm install myapp bitnami/app --skip-crds # Wait for jobs to complete helm install myapp bitnami/app --wait-for-jobs Namespace Management # Set namespace context kubectl config set-context --current --namespace tomcat # Install in specific namespace helm install tomcat bitnami/tomcat --namespace tomcat # List releases across all namespaces helm list --all-namespaces # Get values from specific namespace helm get values mysql --namespace production Upgrade Strategies # Combined install/upgrade helm upgrade --install myapp bitnami/nginx # Three-way merge strategy (Helm 3) helm upgrade myapp bitnami/nginx --three-way-merge # Force resource update helm upgrade myapp bitnami/nginx --force # Reset values to defaults helm upgrade myapp bitnami/nginx --reset-values 6. DEBUGGING & TROUBLESHOOTING Common Issues & Solutions Issue 1: Image Pull Errors (Bitnami Free Tier) # Error: Image not available in free tier # Solution: Use legacy images helm install mysql bitnami/mysql --set image.repository = bitnamilegacy/mysql Issue 2: Readiness Probe Failures # Check pod status kubectl describe pod <pod-name> # Check logs kubectl logs <pod-name> # Common fix: Increase initial delay helm upgrade app bitnami/app --set readinessProbe.initialDelaySeconds = 60 Issue 3: Persistent Volume Claims Pending # Check PVC status kubectl get pvc # Check storage class kubectl get storageclass # Fix: Specify storage class or reduce size helm upgrade mysql bitnami/mysql --set primary.persistence.storageClass = standard Diagnostic Commands # Check Kubernetes resources created by Helm kubectl get all -l app.kubernetes.io/instance = mysql # View Helm release secrets kubectl get secrets | grep helm.release # Decode secret values kubectl get secret mysql -o jsonpath = \"{.data.mysql-root-password}\" | base64 --decode # Test database connection (example) MYSQL_ROOT_PASSWORD = $( kubectl get secret mysql -o jsonpath = \"{.data.mysql-root-password}\" | base64 --decode ) mysql -h 127 .0.0.1 -P 3306 -u root -p $MYSQL_ROOT_PASSWORD Port Forwarding for Testing # Forward service port locally kubectl port-forward svc/mysql 3306 :3306 # Connect to forwarded service mysql -h 127 .0.0.1 -P 3306 -u root -p $( kubectl get secret mysql -o jsonpath = \"{.data.mysql-root-password}\" | base64 --decode ) 7. BEST PRACTICES Configuration Management \u2705 DO: - Use values files for production deployments - Version control your values files - Use separate values files per environment - Set explicit resource limits \u274c DON'T: - Use :latest tags in production - Store secrets in values files (use Kubernetes Secrets) - Use --reuse-values without understanding implications Release Management # Good: Version-controlled values helm upgrade myapp ./chart -f values/production.yaml # Good: Atomic deployments helm upgrade myapp ./chart --atomic --wait # Good: Test before applying helm upgrade myapp ./chart --dry-run # Bad: Unreproducible helm upgrade myapp ./chart --set key1 = val1 --set key2 = val2 Security Practices Use Secrets for sensitive data # Instead of: password : PlainTextPassword # Use: existingSecret : mysql-secret secretKey : password Enable security contexts helm install app bitnami/app --set securityContext.enabled = true Regular updates # Check for updates helm search repo bitnami/mysql --versions # Update dependencies helm dependency update ./mychart Performance Optimization # Set appropriate resource limits helm install app bitnami/app \\ --set resources.requests.cpu = 100m \\ --set resources.requests.memory = 256Mi \\ --set resources.limits.cpu = 500m \\ --set resources.limits.memory = 512Mi # Configure probes helm install app bitnami/app \\ --set livenessProbe.initialDelaySeconds = 30 \\ --set readinessProbe.initialDelaySeconds = 5 8. REAL-WORLD EXAMPLES Complete MySQL Deployment # Step 1: Add repository helm repo add bitnami https://charts.bitnami.com/bitnami helm repo update # Step 2: Create values file cat > mysql-prod.yaml <<EOF auth: rootPassword: \"$(openssl rand -base64 16)\" database: appdb username: appuser password: \"$(openssl rand -base64 16)\" primary: persistence: enabled: true size: 20Gi storageClass: gp2 resources: requests: cpu: 500m memory: 1Gi limits: cpu: 1 memory: 2Gi service: type: ClusterIP metrics: enabled: true EOF # Step 3: Install helm install mysql-prod bitnami/mysql \\ -f mysql-prod.yaml \\ --namespace database \\ --create-namespace \\ --wait \\ --timeout 5m # Step 4: Verify helm status mysql-prod --namespace database kubectl get pods -n database Tomcat with Custom Configuration # Create values file cat > tomcat-custom.yaml <<EOF image: repository: bitnamilegacy/tomcat tag: \"11.0\" service: type: LoadBalancer port: 80 tomcat: username: admin password: \"$(openssl rand -base64 12)\" persistence: enabled: true size: 10Gi resources: requests: cpu: 200m memory: 512Mi limits: cpu: 500m memory: 1Gi readinessProbe: initialDelaySeconds: 60 periodSeconds: 10 livenessProbe: initialDelaySeconds: 120 periodSeconds: 20 EOF # Install helm install tomcat-app bitnami/tomcat \\ -f tomcat-custom.yaml \\ --wait \\ --atomic Application Stack (Multi-chart) # Deploy database helm install postgresql bitnami/postgresql \\ --set auth.database = myapp \\ --set auth.username = myuser \\ --set primary.persistence.size = 10Gi # Deploy Redis cache helm install redis bitnami/redis \\ --set architecture = standalone \\ --set master.persistence.size = 5Gi # Deploy application helm install myapp ./myapp-chart \\ --set database.host = postgresql \\ --set redis.host = redis QUICK REFERENCE CHEAT SHEET Essential Commands # Repository helm repo add <name> <url> helm repo update helm search repo <term> # Installation helm install <name> <chart> [ flags ] helm install <name> <chart> -f values.yaml helm upgrade --install <name> <chart> # Management helm ls helm status <release> helm history <release> helm rollback <release> <revision> helm uninstall <release> # Configuration helm get values <release> helm get manifest <release> helm template <chart> Common Flags --namespace <ns> # Deploy to specific namespace --create-namespace # Create namespace if needed --wait # Wait for resources ready --timeout <duration> # Wait timeout (e.g., 5m) --atomic # Rollback on failure --dry-run # Simulate installation --debug # Enable debug output --set key = value # Set individual values -f values.yaml # Use values file --values values.yaml # Alias for -f --version <version> # Specific chart version Troubleshooting Flow 1. helm status <release> # Check release status 2. kubectl get pods # Check pod status 3. kubectl describe pod <pod> # Get pod details 4. kubectl logs <pod> # Check logs 5. helm get values <release> # Check configuration 6. helm history <release> # Check revision history 7. helm rollback <release> <rev> # Rollback if needed This comprehensive guide covers Helm from basic to advanced usage, with practical examples and best practices for production deployments.","title":"Helm Comprehensive Guide"},{"location":"02%20helm/commands/#helm-comprehensive-guide","text":"","title":"Helm Comprehensive Guide"},{"location":"02%20helm/commands/#table-of-contents","text":"Helm Fundamentals Repository Management Release Lifecycle Configuration Management Advanced Operations Debugging & Troubleshooting Best Practices Real-world Examples","title":"TABLE OF CONTENTS"},{"location":"02%20helm/commands/#1-helm-fundamentals","text":"","title":"1. HELM FUNDAMENTALS"},{"location":"02%20helm/commands/#what-is-helm","text":"Helm is the package manager for Kubernetes , allowing you to: - Define, install, and upgrade Kubernetes applications - Package applications as charts - Manage dependencies between applications - Share applications through repositories","title":"What is Helm?"},{"location":"02%20helm/commands/#key-concepts","text":"Concept Description Example Chart Package containing all resource definitions bitnami/mysql , nginx-ingress Release Instance of a chart running in Kubernetes mysql-release , webapp-prod Repository Collection of charts bitnami , stable , jetstack Values Configurable parameters for a chart Database passwords, replica counts","title":"Key Concepts"},{"location":"02%20helm/commands/#version-types","text":"# Chart outputs show both versions: CHART NAME: tomcat CHART VERSION: 13 .3.0 # \u2190 Helm chart version (templates/config) APP VERSION: 11 .0.15 # \u2190 Application version (actual software) Key Difference: - Chart Version : Changes when Helm templates, dependencies, or configurations change - App Version : Changes when the actual application software version changes","title":"Version Types"},{"location":"02%20helm/commands/#2-repository-management","text":"","title":"2. REPOSITORY MANAGEMENT"},{"location":"02%20helm/commands/#core-commands","text":"# Add a repository helm repo add bitnami https://charts.bitnami.com/bitnami # List all repositories helm repo list # Update repository index helm repo update # Search for charts helm search repo apache # Stable releases helm search repo mysql --devel # Include development versions helm search repo --versions nginx # Show all versions # Remove a repository helm repo remove bitnami","title":"Core Commands"},{"location":"02%20helm/commands/#bitnami-repository-specifics","text":"# Since August 2025, Bitnami requires subscription for latest images # Free tier workaround: helm install mydb bitnami/mysql --set image.repository = bitnamilegacy/mysql # Warnings you'll see: # 1. Limited free tier images available # 2. Rolling tags (:latest) not recommended for production # 3. Resources should be explicitly set","title":"Bitnami Repository Specifics"},{"location":"02%20helm/commands/#3-release-lifecycle","text":"","title":"3. RELEASE LIFECYCLE"},{"location":"02%20helm/commands/#installation","text":"# Basic installation helm install mysql wso2/mysql # With custom name helm install my-database bitnami/mysql # Generate random name helm install bitnami/apache --generate-name # Custom naming template helm install bitnami/apache --generate-name --name-template \"web-{{randAlpha 5 | lower}}\"","title":"Installation"},{"location":"02%20helm/commands/#upgrade-rollback","text":"# Upgrade with new values helm upgrade mysql wso2/mysql --set testFramework.enabled = false # Upgrade with values file helm upgrade mysql bitnami/mysql -f mysql-values.yaml # Reuse previous values (dangerous - may use defaults for missing values) helm upgrade mydb bitnami/mysql --reuse-values # Force upgrade (deletes and recreates resources - causes downtime) helm upgrade myapp bitnami/tomcat --force # Check upgrade history helm history mysql # Rollback to specific revision helm rollback mysql 1","title":"Upgrade &amp; Rollback"},{"location":"02%20helm/commands/#uninstallation","text":"# Complete removal helm uninstall mysql # Keep history (resources remain, only Helm metadata removed) helm uninstall server --keep-history","title":"Uninstallation"},{"location":"02%20helm/commands/#status-inspection","text":"# List all releases helm ls helm list helm list --all-namespaces # Get release information helm status mysql helm get notes mysql # Show NOTES.txt helm get values mysql # Show user-supplied values helm get values mysql --all # Show all values (including defaults) helm get values mysql --revision 2 # Show values for specific revision # Get manifest helm get manifest mysql","title":"Status &amp; Inspection"},{"location":"02%20helm/commands/#4-configuration-management","text":"","title":"4. CONFIGURATION MANAGEMENT"},{"location":"02%20helm/commands/#setting-values","text":"","title":"Setting Values"},{"location":"02%20helm/commands/#method-1-command-line-set","text":"# Single value helm install mysql bitnami/mysql --set auth.rootPassword = Secret123 # Nested values helm install mysql bitnami/mysql \\ --set auth.rootPassword = Secret123 \\ --set auth.database = appdb \\ --set primary.persistence.size = 10Gi # Arrays/lists helm install myapp bitnami/app --set \"podAnnotations.key=value\"","title":"Method 1: Command Line (--set)"},{"location":"02%20helm/commands/#method-2-values-file-recommended","text":"# mysql-values.yaml auth : rootPassword : StrongRootPass123 database : appdb username : appuser password : AppUserPass123 primary : persistence : enabled : true size : 10Gi storageClass : standard resources : requests : cpu : 250m memory : 512Mi limits : cpu : 500m memory : 1Gi service : type : ClusterIP # Install with values file helm install mysql bitnami/mysql -f mysql-values.yaml # Override values file with command line helm install mysql bitnami/mysql -f mysql-values.yaml --set replicaCount = 3","title":"Method 2: Values File (Recommended)"},{"location":"02%20helm/commands/#method-3-multiple-values-files","text":"# Base values + environment-specific overrides helm install myapp bitnami/app -f values.yaml -f production.yaml","title":"Method 3: Multiple Values Files"},{"location":"02%20helm/commands/#template-rendering-validation","text":"# Dry run (see what would be deployed) helm install server bitnami/tomcat --dry-run = client # Render templates locally helm template server bitnami/tomcat # Debug template rendering helm template server bitnami/tomcat --debug # Lint chart (validate) helm lint ./mychart","title":"Template Rendering &amp; Validation"},{"location":"02%20helm/commands/#5-advanced-operations","text":"","title":"5. ADVANCED OPERATIONS"},{"location":"02%20helm/commands/#installation-options","text":"# Wait for resources to be ready helm install myapp bitnami/nginx --wait helm install myapp bitnami/nginx --wait --timeout 60s # Custom timeout # Atomic installation (auto-rollback on failure) helm install myapp bitnami/nginx --atomic # Create namespace if it doesn't exist helm install myapp bitnami/nginx --namespace myns --create-namespace # Skip CRDs helm install myapp bitnami/app --skip-crds # Wait for jobs to complete helm install myapp bitnami/app --wait-for-jobs","title":"Installation Options"},{"location":"02%20helm/commands/#namespace-management","text":"# Set namespace context kubectl config set-context --current --namespace tomcat # Install in specific namespace helm install tomcat bitnami/tomcat --namespace tomcat # List releases across all namespaces helm list --all-namespaces # Get values from specific namespace helm get values mysql --namespace production","title":"Namespace Management"},{"location":"02%20helm/commands/#upgrade-strategies","text":"# Combined install/upgrade helm upgrade --install myapp bitnami/nginx # Three-way merge strategy (Helm 3) helm upgrade myapp bitnami/nginx --three-way-merge # Force resource update helm upgrade myapp bitnami/nginx --force # Reset values to defaults helm upgrade myapp bitnami/nginx --reset-values","title":"Upgrade Strategies"},{"location":"02%20helm/commands/#6-debugging-troubleshooting","text":"","title":"6. DEBUGGING &amp; TROUBLESHOOTING"},{"location":"02%20helm/commands/#common-issues-solutions","text":"","title":"Common Issues &amp; Solutions"},{"location":"02%20helm/commands/#issue-1-image-pull-errors-bitnami-free-tier","text":"# Error: Image not available in free tier # Solution: Use legacy images helm install mysql bitnami/mysql --set image.repository = bitnamilegacy/mysql","title":"Issue 1: Image Pull Errors (Bitnami Free Tier)"},{"location":"02%20helm/commands/#issue-2-readiness-probe-failures","text":"# Check pod status kubectl describe pod <pod-name> # Check logs kubectl logs <pod-name> # Common fix: Increase initial delay helm upgrade app bitnami/app --set readinessProbe.initialDelaySeconds = 60","title":"Issue 2: Readiness Probe Failures"},{"location":"02%20helm/commands/#issue-3-persistent-volume-claims-pending","text":"# Check PVC status kubectl get pvc # Check storage class kubectl get storageclass # Fix: Specify storage class or reduce size helm upgrade mysql bitnami/mysql --set primary.persistence.storageClass = standard","title":"Issue 3: Persistent Volume Claims Pending"},{"location":"02%20helm/commands/#diagnostic-commands","text":"# Check Kubernetes resources created by Helm kubectl get all -l app.kubernetes.io/instance = mysql # View Helm release secrets kubectl get secrets | grep helm.release # Decode secret values kubectl get secret mysql -o jsonpath = \"{.data.mysql-root-password}\" | base64 --decode # Test database connection (example) MYSQL_ROOT_PASSWORD = $( kubectl get secret mysql -o jsonpath = \"{.data.mysql-root-password}\" | base64 --decode ) mysql -h 127 .0.0.1 -P 3306 -u root -p $MYSQL_ROOT_PASSWORD","title":"Diagnostic Commands"},{"location":"02%20helm/commands/#port-forwarding-for-testing","text":"# Forward service port locally kubectl port-forward svc/mysql 3306 :3306 # Connect to forwarded service mysql -h 127 .0.0.1 -P 3306 -u root -p $( kubectl get secret mysql -o jsonpath = \"{.data.mysql-root-password}\" | base64 --decode )","title":"Port Forwarding for Testing"},{"location":"02%20helm/commands/#7-best-practices","text":"","title":"7. BEST PRACTICES"},{"location":"02%20helm/commands/#configuration-management","text":"\u2705 DO: - Use values files for production deployments - Version control your values files - Use separate values files per environment - Set explicit resource limits \u274c DON'T: - Use :latest tags in production - Store secrets in values files (use Kubernetes Secrets) - Use --reuse-values without understanding implications","title":"Configuration Management"},{"location":"02%20helm/commands/#release-management","text":"# Good: Version-controlled values helm upgrade myapp ./chart -f values/production.yaml # Good: Atomic deployments helm upgrade myapp ./chart --atomic --wait # Good: Test before applying helm upgrade myapp ./chart --dry-run # Bad: Unreproducible helm upgrade myapp ./chart --set key1 = val1 --set key2 = val2","title":"Release Management"},{"location":"02%20helm/commands/#security-practices","text":"Use Secrets for sensitive data # Instead of: password : PlainTextPassword # Use: existingSecret : mysql-secret secretKey : password Enable security contexts helm install app bitnami/app --set securityContext.enabled = true Regular updates # Check for updates helm search repo bitnami/mysql --versions # Update dependencies helm dependency update ./mychart","title":"Security Practices"},{"location":"02%20helm/commands/#performance-optimization","text":"# Set appropriate resource limits helm install app bitnami/app \\ --set resources.requests.cpu = 100m \\ --set resources.requests.memory = 256Mi \\ --set resources.limits.cpu = 500m \\ --set resources.limits.memory = 512Mi # Configure probes helm install app bitnami/app \\ --set livenessProbe.initialDelaySeconds = 30 \\ --set readinessProbe.initialDelaySeconds = 5","title":"Performance Optimization"},{"location":"02%20helm/commands/#8-real-world-examples","text":"","title":"8. REAL-WORLD EXAMPLES"},{"location":"02%20helm/commands/#complete-mysql-deployment","text":"# Step 1: Add repository helm repo add bitnami https://charts.bitnami.com/bitnami helm repo update # Step 2: Create values file cat > mysql-prod.yaml <<EOF auth: rootPassword: \"$(openssl rand -base64 16)\" database: appdb username: appuser password: \"$(openssl rand -base64 16)\" primary: persistence: enabled: true size: 20Gi storageClass: gp2 resources: requests: cpu: 500m memory: 1Gi limits: cpu: 1 memory: 2Gi service: type: ClusterIP metrics: enabled: true EOF # Step 3: Install helm install mysql-prod bitnami/mysql \\ -f mysql-prod.yaml \\ --namespace database \\ --create-namespace \\ --wait \\ --timeout 5m # Step 4: Verify helm status mysql-prod --namespace database kubectl get pods -n database","title":"Complete MySQL Deployment"},{"location":"02%20helm/commands/#tomcat-with-custom-configuration","text":"# Create values file cat > tomcat-custom.yaml <<EOF image: repository: bitnamilegacy/tomcat tag: \"11.0\" service: type: LoadBalancer port: 80 tomcat: username: admin password: \"$(openssl rand -base64 12)\" persistence: enabled: true size: 10Gi resources: requests: cpu: 200m memory: 512Mi limits: cpu: 500m memory: 1Gi readinessProbe: initialDelaySeconds: 60 periodSeconds: 10 livenessProbe: initialDelaySeconds: 120 periodSeconds: 20 EOF # Install helm install tomcat-app bitnami/tomcat \\ -f tomcat-custom.yaml \\ --wait \\ --atomic","title":"Tomcat with Custom Configuration"},{"location":"02%20helm/commands/#application-stack-multi-chart","text":"# Deploy database helm install postgresql bitnami/postgresql \\ --set auth.database = myapp \\ --set auth.username = myuser \\ --set primary.persistence.size = 10Gi # Deploy Redis cache helm install redis bitnami/redis \\ --set architecture = standalone \\ --set master.persistence.size = 5Gi # Deploy application helm install myapp ./myapp-chart \\ --set database.host = postgresql \\ --set redis.host = redis","title":"Application Stack (Multi-chart)"},{"location":"02%20helm/commands/#quick-reference-cheat-sheet","text":"","title":"QUICK REFERENCE CHEAT SHEET"},{"location":"02%20helm/commands/#essential-commands","text":"# Repository helm repo add <name> <url> helm repo update helm search repo <term> # Installation helm install <name> <chart> [ flags ] helm install <name> <chart> -f values.yaml helm upgrade --install <name> <chart> # Management helm ls helm status <release> helm history <release> helm rollback <release> <revision> helm uninstall <release> # Configuration helm get values <release> helm get manifest <release> helm template <chart>","title":"Essential Commands"},{"location":"02%20helm/commands/#common-flags","text":"--namespace <ns> # Deploy to specific namespace --create-namespace # Create namespace if needed --wait # Wait for resources ready --timeout <duration> # Wait timeout (e.g., 5m) --atomic # Rollback on failure --dry-run # Simulate installation --debug # Enable debug output --set key = value # Set individual values -f values.yaml # Use values file --values values.yaml # Alias for -f --version <version> # Specific chart version","title":"Common Flags"},{"location":"02%20helm/commands/#troubleshooting-flow","text":"1. helm status <release> # Check release status 2. kubectl get pods # Check pod status 3. kubectl describe pod <pod> # Get pod details 4. kubectl logs <pod> # Check logs 5. helm get values <release> # Check configuration 6. helm history <release> # Check revision history 7. helm rollback <release> <rev> # Rollback if needed This comprehensive guide covers Helm from basic to advanced usage, with practical examples and best practices for production deployments.","title":"Troubleshooting Flow"},{"location":"02%20helm/create/","text":"Creating Helm Charts Basic Chart Structure helm create firstchart tree firstchart/ firstchart/ \u251c\u2500\u2500 Chart.yaml # Chart metadata \u251c\u2500\u2500 values.yaml # Default values \u251c\u2500\u2500 templates/ # Kubernetes manifests \u2502 \u251c\u2500\u2500 deployment.yaml \u2502 \u251c\u2500\u2500 service.yaml \u2502 \u251c\u2500\u2500 ingress.yaml \u2502 \u251c\u2500\u2500 _helpers.tpl # Helper templates \u2502 \u2514\u2500\u2500 tests/ # Test files \u2502 \u2514\u2500\u2500 test-connection.yaml \u2514\u2500\u2500 charts/ # Subcharts/dependencies Chart.yaml - Essential Fields apiVersion : v2 name : firstchart description : A Helm chart for Kubernetes type : application # application or library version : 0.1.0 # Chart version appVersion : \"1.0.0\" # Application version Packaging Charts # Create .tgz package helm package firstchart/ # Output: firstchart-0.1.0.tgz # Package with custom directory helm package firstchart/ -d ./packaged # Update dependencies in chart helm package firstchart/ -u # Update dependencies first Testing & Linting # Test a release (requires release name, not version) helm test chart # Not v1.0.0 or v2 # Lint your chart helm lint firstchart/ # Lint with strict mode helm lint firstchart/ --strict # Dry run install helm install myrelease firstchart/ --dry-run # Template rendering helm template myrelease firstchart/ Chart Development Workflow # 1. Create chart helm create myapp # 2. Edit Chart.yaml, values.yaml, templates # 3. Lint helm lint myapp/ # 4. Dry run helm install myapp myapp/ --dry-run # 5. Package helm package myapp/ # 6. Install helm install myapp myapp-0.1.0.tgz # 7. Test (after install) helm test myapp Quick Tips Test command needs release name (not version number) Package creates .tgz file in current directory Lint catches YAML syntax errors Dry-run shows what will be deployed port-forward from NOTES.txt to test locally","title":"Creating Helm Charts"},{"location":"02%20helm/create/#creating-helm-charts","text":"","title":"Creating Helm Charts"},{"location":"02%20helm/create/#basic-chart-structure","text":"helm create firstchart tree firstchart/ firstchart/ \u251c\u2500\u2500 Chart.yaml # Chart metadata \u251c\u2500\u2500 values.yaml # Default values \u251c\u2500\u2500 templates/ # Kubernetes manifests \u2502 \u251c\u2500\u2500 deployment.yaml \u2502 \u251c\u2500\u2500 service.yaml \u2502 \u251c\u2500\u2500 ingress.yaml \u2502 \u251c\u2500\u2500 _helpers.tpl # Helper templates \u2502 \u2514\u2500\u2500 tests/ # Test files \u2502 \u2514\u2500\u2500 test-connection.yaml \u2514\u2500\u2500 charts/ # Subcharts/dependencies","title":"Basic Chart Structure"},{"location":"02%20helm/create/#chartyaml-essential-fields","text":"apiVersion : v2 name : firstchart description : A Helm chart for Kubernetes type : application # application or library version : 0.1.0 # Chart version appVersion : \"1.0.0\" # Application version","title":"Chart.yaml - Essential Fields"},{"location":"02%20helm/create/#packaging-charts","text":"# Create .tgz package helm package firstchart/ # Output: firstchart-0.1.0.tgz # Package with custom directory helm package firstchart/ -d ./packaged # Update dependencies in chart helm package firstchart/ -u # Update dependencies first","title":"Packaging Charts"},{"location":"02%20helm/create/#testing-linting","text":"# Test a release (requires release name, not version) helm test chart # Not v1.0.0 or v2 # Lint your chart helm lint firstchart/ # Lint with strict mode helm lint firstchart/ --strict # Dry run install helm install myrelease firstchart/ --dry-run # Template rendering helm template myrelease firstchart/","title":"Testing &amp; Linting"},{"location":"02%20helm/create/#chart-development-workflow","text":"# 1. Create chart helm create myapp # 2. Edit Chart.yaml, values.yaml, templates # 3. Lint helm lint myapp/ # 4. Dry run helm install myapp myapp/ --dry-run # 5. Package helm package myapp/ # 6. Install helm install myapp myapp-0.1.0.tgz # 7. Test (after install) helm test myapp","title":"Chart Development Workflow"},{"location":"02%20helm/create/#quick-tips","text":"Test command needs release name (not version number) Package creates .tgz file in current directory Lint catches YAML syntax errors Dry-run shows what will be deployed port-forward from NOTES.txt to test locally","title":"Quick Tips"},{"location":"02%20helm/notes/","text":"sed (surface newlines, no duplicates) helm template . | sed -n 'l' Shows line endings explicitly ( $ = \\n , \\r$ = CRLF) without printing each line twice; works consistently on Linux, WSL, and macOS. yamllint Validates rendered output for basic YAML syntax and indentation errors before Kubernetes processing. kubeconform Validates rendered manifests against Kubernetes API schemas to catch invalid kinds, fields, or apiVersions. Output only the final content. No preface, no acknowledgements, no closing remarks. {{- }} {{ .Values.my.custome.data }} {{ .Chart.Name }} {{ .Chart.Version }} {{ .Chart.AppVersion }} {{ .Chart.Annotation }} {{.Release.Name}} {{.Release.Namespace}} {{.Release.IsInstall}} {{.Release.IsUpgrade}} {{.Release.Service}} {{Template.Name}} {{Template.BasePath}} --- Pipeline --- output of one command -> passed as input to right side {{ .Values.my.custom.data | default \"testdefault\" | upper | quote }}","title":"Notes"},{"location":"02%20helm/notes/#sed-surface-newlines-no-duplicates","text":"helm template . | sed -n 'l' Shows line endings explicitly ( $ = \\n , \\r$ = CRLF) without printing each line twice; works consistently on Linux, WSL, and macOS.","title":"sed (surface newlines, no duplicates)"},{"location":"02%20helm/notes/#yamllint","text":"Validates rendered output for basic YAML syntax and indentation errors before Kubernetes processing.","title":"yamllint"},{"location":"02%20helm/notes/#kubeconform","text":"Validates rendered manifests against Kubernetes API schemas to catch invalid kinds, fields, or apiVersions. Output only the final content. No preface, no acknowledgements, no closing remarks. {{- }} {{ .Values.my.custome.data }} {{ .Chart.Name }} {{ .Chart.Version }} {{ .Chart.AppVersion }} {{ .Chart.Annotation }} {{.Release.Name}} {{.Release.Namespace}} {{.Release.IsInstall}} {{.Release.IsUpgrade}} {{.Release.Service}} {{Template.Name}} {{Template.BasePath}} --- Pipeline --- output of one command -> passed as input to right side {{ .Values.my.custom.data | default \"testdefault\" | upper | quote }}","title":"kubeconform"},{"location":"02%20helm/practice-questions/","text":"Helm Practice Questions - Progressive Learning Path Level 1: Fundamentals (Basic Templates & Values) Question 1.1: Simple Deployment Chart Objective : Create a basic Helm chart that deploys a single Nginx application. Requirements : - Chart should accept the following configurable parameters through values.yaml : - Application name - Container image and tag - Number of replicas - Container port - Service type (ClusterIP, NodePort, or LoadBalancer) - Generate Deployment and Service manifests - Service should expose the application on the configured port - Use proper naming conventions with release name and chart name Constraints : - Do not hardcode any values in templates - All configuration must come from values.yaml Question 1.2: ConfigMap and Environment Variables Objective : Deploy an application that uses ConfigMap for configuration management. Requirements : - Create a chart that deploys an application with environment variables - ConfigMap should store: - Application log level - API endpoint URL - Database host - Deployment should inject these values as environment variables - Allow overriding all ConfigMap values through values.yaml Constraints : - ConfigMap data must be stored separately from deployment spec - Environment variables should be clearly documented Question 1.3: Multi-Container Pod Objective : Create a Deployment with multiple containers in a single pod. Requirements : - Main application container (app container) - Sidecar container (logging aggregator or monitoring agent) - Both containers should have: - Configurable image and tag - Configurable resource requests/limits - Configurable container port - Pod should expose both container ports through the Service Constraints : - Each container must have its own configuration section in values.yaml - Resource limits must be mandatory (not optional) Level 2: Template Logic & Control Flow Question 2.1: Conditional Template Rendering Objective : Create a chart that conditionally includes/excludes Kubernetes resources based on configuration. Requirements : - Deployment should only include: - Ingress resource when ingress.enabled is true - HorizontalPodAutoscaler when autoscaling.enabled is true - ResourceQuota when resourceQuota.enabled is true - Each optional resource must have its complete configuration in values.yaml - Template conditions should be clear and documented Constraints : - Use only Helm template conditionals (if/else) - No resource duplication Question 2.2: Loops and Template Iteration Objective : Deploy an application with multiple configuration volumes and volume mounts. Requirements : - Support mounting multiple ConfigMaps as volumes - Support mounting multiple Secrets as volumes - Each volume should be independently configurable with: - Name - Mount path - Permissions (optional) - Deployment spec should iterate through all configured volumes Constraints : - Volumes list must be defined in values.yaml as an array - Template must use range/loop constructs Question 2.3: Template Functions and Filters Objective : Create templates that transform and format values using Helm functions. Requirements : - Application labels should be generated dynamically with: - Proper casing transformations - Version tagging - Environment tagging - Resource names should be truncated to valid Kubernetes length (using trunc function) - Annotations should concatenate multiple values - Template should include: - String manipulation (upper, lower, title case) - Quote functions for proper YAML formatting - Default value handling Constraints : - Must demonstrate at least 5 different Helm template functions - All transformations must be properly quoted for YAML validity Level 3: Advanced Templating & Helm Features Question 3.1: Helm Hooks and Lifecycle Objective : Create a chart that executes pre and post deployment tasks. Requirements : - Pre-install hook: Job to validate prerequisites - Pre-upgrade hook: Job to backup current state - Post-install hook: Job to initialize application - Each hook should: - Have appropriate deletion-policy annotation - Be weight-ordered for execution sequence - Include proper RBAC if needed - Hooks should be optional (can be disabled via values.yaml ) Constraints : - Hooks must not cause deployment failure if they fail (unless critical) - Proper cleanup policies must be defined Question 3.2: Named Templates and Template Reuse Objective : Create reusable template components for common patterns. Requirements : - Create named templates for: - Pod label generation - Common annotations generation - Resource requests/limits - Security context - Probe configuration - Main templates should use these named templates extensively - Support both individual container probes and pod-wide security settings Constraints : - Each named template must be in separate _*.tpl file - Demonstrate template scope and parameter passing - Minimize code duplication Question 3.3: Helm Values Validation & Defaults Objective : Create a chart with complex nested values and validation. Requirements : - Define a complex values.yaml with: - Nested objects for database configuration - Nested objects for cache configuration - Nested arrays for multiple environments - Proper defaults for all optional values - Template must: - Validate that required values are provided - Apply defaults for missing optional values - Handle type mismatches gracefully - Include template that documents all available values Constraints : - Use required function for mandatory values - Use nested object structure (3+ levels deep) Question 3.4: Multi-Chart Dependencies Objective : Create a parent chart that depends on child charts. Requirements : - Create main application chart - Create dependency charts for: - Database (PostgreSQL/MySQL) - Cache (Redis) - Message Queue (RabbitMQ) - Parent chart should: - Declare dependencies in Chart.yaml - Override dependency values from parent values.yaml - Control which dependencies are installed - Pass values to child charts with proper scoping Constraints : - Dependencies must be properly versioned - Parent chart must demonstrate selective dependency enablement Level 4: Production Patterns & Advanced Features Question 4.1: Multi-Environment & Namespace Strategy Objective : Create a single chart that deploys across development, staging, and production environments. Requirements : - Chart should support values files for each environment: - values-dev.yaml - for development - values-stg.yaml - for staging - values-prod.yaml - for production - Each environment should have different: - Replica counts - Resource limits - Security policies - Ingress configurations - Storage configurations - Template should conditionally include environment-specific ConfigMaps and Secrets Constraints : - Single chart.yaml, multiple values files - No environment hardcoding in templates Question 4.2: Secrets Management Integration Objective : Create templates that integrate with external secret management systems. Requirements : - Support multiple secret backends: - Kubernetes native Secrets - External Secrets Operator references - HashiCorp Vault integration - Templates should: - Reference secrets by name/path - Handle secret rotation annotations - Support sealed secrets compatibility - Deployment should mount secrets: - As environment variables - As volume mounts - Both methods should be configurable Constraints : - No secret values in values.yaml - Template must support switching between secret backends via configuration Question 4.3: RBAC and Security Context Templates Objective : Create comprehensive RBAC and security-focused templates. Requirements : - Generate: - ServiceAccount - Role/ClusterRole (with appropriate permissions) - RoleBinding/ClusterRoleBinding - Pod Security Policy or Security Context - Network Policy (optional) - Configuration should include: - Custom RBAC rules (configurable) - Pod security context (read-only filesystem, non-root user) - Container security context (capabilities dropping) - Service Account Token automounting control Constraints : - RBAC rules must be minimal and specific - Security context must follow Kubernetes best practices Question 4.4: Complex Deployment Strategy Objective : Create templates supporting multiple deployment strategies. Requirements : - Support multiple deployment patterns: - Rolling update (standard) - Blue-green deployment (two separate deployments) - Canary deployment (with weighted traffic) - Feature flag driven releases - For each strategy: - Define required configuration in values.yaml - Template conditionally generates appropriate resources - Support gradual traffic shifting - Include status tracking resources (optional) Constraints : - Single chart must support all strategies - Switching strategies should only require values.yaml changes Question 4.5: Helm Testing & Validation Objective : Create test charts and validation templates. Requirements : - Create Helm test pods that validate: - Application connectivity - Configuration correctness - Environment variables - Volume mounts - Security context - Test pod should: - Run after deployment - Have proper cleanup policies - Exit with appropriate status - Generate meaningful output - Include template that validates values.yaml schema Constraints : - Tests must not require external dependencies - Test cleanup must be automatic Level 5: Expert Scenarios & Real-World Patterns Question 5.1: Stateful Application with Storage Objective : Create a chart for a stateful application with persistent storage. Requirements : - Statefulset for ordered, stable Pod identities - Storage configuration: - PersistentVolumeClaim generation - Storage class selection - Mount path configuration - Support: - StatefulSet specific features (headless service, pod DNS) - Rolling updates with PVC retention - Backup/restore hooks - Data migration between storage classes Constraints : - Must use StatefulSet (not Deployment) - Storage configuration must be flexible Question 5.2: Helm Plugins and Custom Functions Objective : Create templates that use custom plugin functions or advanced templating. Requirements : - Implement custom template functions for: - Environment-specific variable substitution - Kubernetes resource validation - Image digest resolution - Custom label generation - Template must: - Use these custom functions effectively - Handle function failures gracefully - Document custom function usage Constraints : - Must create actual working functions (not pseudocode) - Functions must be reusable across templates Question 5.3: GitOps and Helm Operator Integration Objective : Create a chart designed for GitOps workflow. Requirements : - Chart structure optimized for: - Flux CD or ArgoCD integration - HelmRelease resource generation - Automated sync and monitoring - Include: - Health check configuration - Sync behavior controls - Rollback capabilities - Notification annotations - Support value-driven deployments from Git Constraints : - Chart must be compatible with popular GitOps tools - Proper CRD references if needed Question 5.4: Umbrella Chart with Microservices Objective : Create a parent chart managing multiple microservices. Requirements : - Parent chart (umbrella) that orchestrates: - API service - Frontend service - Backend service - Worker service - Database - Cache - Features: - Selective component enablement - Shared configuration management - Service discovery and DNS configuration - Cross-service communication security - Centralized logging configuration Constraints : - Must handle 6+ child charts - Shared values must be properly scoped Question 5.5: Production Readiness Checklist Objective : Create a comprehensive production-ready Helm chart. Requirements : - Chart must include and configure: - Health checks (liveness and readiness probes) - Resource requests and limits - Horizontal Pod Autoscaling - Pod Disruption Budgets - Affinity rules (node/pod affinity) - Tolerations for node taints - Proper logging and monitoring hooks - RBAC and security contexts - Network policies - Service mesh integration (if applicable) - Documentation: - Values.yaml documentation - Deployment guide - Troubleshooting guide - Upgrade procedure Constraints : - All components must be optional (configurable) - Security requirements must be non-negotiable - Performance/reliability cannot be compromised Learning Path Recommendation Start with Level 1 (1.1 \u2192 1.2 \u2192 1.3) to understand basic Helm mechanics Progress to Level 2 (2.1 \u2192 2.2 \u2192 2.3) to learn template logic and features Move to Level 3 (3.1 \u2192 3.2 \u2192 3.3 \u2192 3.4) for advanced templating patterns Explore Level 4 (4.1 \u2192 4.2 \u2192 4.3 \u2192 4.4 \u2192 4.5) for production scenarios Master Level 5 (5.1 \u2192 5.2 \u2192 5.3 \u2192 5.4 \u2192 5.5) for expert implementations Each level builds on previous knowledge. Complete at least 2-3 questions per level before advancing.","title":"Helm Practice Questions - Progressive Learning Path"},{"location":"02%20helm/practice-questions/#helm-practice-questions-progressive-learning-path","text":"","title":"Helm Practice Questions - Progressive Learning Path"},{"location":"02%20helm/practice-questions/#level-1-fundamentals-basic-templates-values","text":"","title":"Level 1: Fundamentals (Basic Templates &amp; Values)"},{"location":"02%20helm/practice-questions/#question-11-simple-deployment-chart","text":"Objective : Create a basic Helm chart that deploys a single Nginx application. Requirements : - Chart should accept the following configurable parameters through values.yaml : - Application name - Container image and tag - Number of replicas - Container port - Service type (ClusterIP, NodePort, or LoadBalancer) - Generate Deployment and Service manifests - Service should expose the application on the configured port - Use proper naming conventions with release name and chart name Constraints : - Do not hardcode any values in templates - All configuration must come from values.yaml","title":"Question 1.1: Simple Deployment Chart"},{"location":"02%20helm/practice-questions/#question-12-configmap-and-environment-variables","text":"Objective : Deploy an application that uses ConfigMap for configuration management. Requirements : - Create a chart that deploys an application with environment variables - ConfigMap should store: - Application log level - API endpoint URL - Database host - Deployment should inject these values as environment variables - Allow overriding all ConfigMap values through values.yaml Constraints : - ConfigMap data must be stored separately from deployment spec - Environment variables should be clearly documented","title":"Question 1.2: ConfigMap and Environment Variables"},{"location":"02%20helm/practice-questions/#question-13-multi-container-pod","text":"Objective : Create a Deployment with multiple containers in a single pod. Requirements : - Main application container (app container) - Sidecar container (logging aggregator or monitoring agent) - Both containers should have: - Configurable image and tag - Configurable resource requests/limits - Configurable container port - Pod should expose both container ports through the Service Constraints : - Each container must have its own configuration section in values.yaml - Resource limits must be mandatory (not optional)","title":"Question 1.3: Multi-Container Pod"},{"location":"02%20helm/practice-questions/#level-2-template-logic-control-flow","text":"","title":"Level 2: Template Logic &amp; Control Flow"},{"location":"02%20helm/practice-questions/#question-21-conditional-template-rendering","text":"Objective : Create a chart that conditionally includes/excludes Kubernetes resources based on configuration. Requirements : - Deployment should only include: - Ingress resource when ingress.enabled is true - HorizontalPodAutoscaler when autoscaling.enabled is true - ResourceQuota when resourceQuota.enabled is true - Each optional resource must have its complete configuration in values.yaml - Template conditions should be clear and documented Constraints : - Use only Helm template conditionals (if/else) - No resource duplication","title":"Question 2.1: Conditional Template Rendering"},{"location":"02%20helm/practice-questions/#question-22-loops-and-template-iteration","text":"Objective : Deploy an application with multiple configuration volumes and volume mounts. Requirements : - Support mounting multiple ConfigMaps as volumes - Support mounting multiple Secrets as volumes - Each volume should be independently configurable with: - Name - Mount path - Permissions (optional) - Deployment spec should iterate through all configured volumes Constraints : - Volumes list must be defined in values.yaml as an array - Template must use range/loop constructs","title":"Question 2.2: Loops and Template Iteration"},{"location":"02%20helm/practice-questions/#question-23-template-functions-and-filters","text":"Objective : Create templates that transform and format values using Helm functions. Requirements : - Application labels should be generated dynamically with: - Proper casing transformations - Version tagging - Environment tagging - Resource names should be truncated to valid Kubernetes length (using trunc function) - Annotations should concatenate multiple values - Template should include: - String manipulation (upper, lower, title case) - Quote functions for proper YAML formatting - Default value handling Constraints : - Must demonstrate at least 5 different Helm template functions - All transformations must be properly quoted for YAML validity","title":"Question 2.3: Template Functions and Filters"},{"location":"02%20helm/practice-questions/#level-3-advanced-templating-helm-features","text":"","title":"Level 3: Advanced Templating &amp; Helm Features"},{"location":"02%20helm/practice-questions/#question-31-helm-hooks-and-lifecycle","text":"Objective : Create a chart that executes pre and post deployment tasks. Requirements : - Pre-install hook: Job to validate prerequisites - Pre-upgrade hook: Job to backup current state - Post-install hook: Job to initialize application - Each hook should: - Have appropriate deletion-policy annotation - Be weight-ordered for execution sequence - Include proper RBAC if needed - Hooks should be optional (can be disabled via values.yaml ) Constraints : - Hooks must not cause deployment failure if they fail (unless critical) - Proper cleanup policies must be defined","title":"Question 3.1: Helm Hooks and Lifecycle"},{"location":"02%20helm/practice-questions/#question-32-named-templates-and-template-reuse","text":"Objective : Create reusable template components for common patterns. Requirements : - Create named templates for: - Pod label generation - Common annotations generation - Resource requests/limits - Security context - Probe configuration - Main templates should use these named templates extensively - Support both individual container probes and pod-wide security settings Constraints : - Each named template must be in separate _*.tpl file - Demonstrate template scope and parameter passing - Minimize code duplication","title":"Question 3.2: Named Templates and Template Reuse"},{"location":"02%20helm/practice-questions/#question-33-helm-values-validation-defaults","text":"Objective : Create a chart with complex nested values and validation. Requirements : - Define a complex values.yaml with: - Nested objects for database configuration - Nested objects for cache configuration - Nested arrays for multiple environments - Proper defaults for all optional values - Template must: - Validate that required values are provided - Apply defaults for missing optional values - Handle type mismatches gracefully - Include template that documents all available values Constraints : - Use required function for mandatory values - Use nested object structure (3+ levels deep)","title":"Question 3.3: Helm Values Validation &amp; Defaults"},{"location":"02%20helm/practice-questions/#question-34-multi-chart-dependencies","text":"Objective : Create a parent chart that depends on child charts. Requirements : - Create main application chart - Create dependency charts for: - Database (PostgreSQL/MySQL) - Cache (Redis) - Message Queue (RabbitMQ) - Parent chart should: - Declare dependencies in Chart.yaml - Override dependency values from parent values.yaml - Control which dependencies are installed - Pass values to child charts with proper scoping Constraints : - Dependencies must be properly versioned - Parent chart must demonstrate selective dependency enablement","title":"Question 3.4: Multi-Chart Dependencies"},{"location":"02%20helm/practice-questions/#level-4-production-patterns-advanced-features","text":"","title":"Level 4: Production Patterns &amp; Advanced Features"},{"location":"02%20helm/practice-questions/#question-41-multi-environment-namespace-strategy","text":"Objective : Create a single chart that deploys across development, staging, and production environments. Requirements : - Chart should support values files for each environment: - values-dev.yaml - for development - values-stg.yaml - for staging - values-prod.yaml - for production - Each environment should have different: - Replica counts - Resource limits - Security policies - Ingress configurations - Storage configurations - Template should conditionally include environment-specific ConfigMaps and Secrets Constraints : - Single chart.yaml, multiple values files - No environment hardcoding in templates","title":"Question 4.1: Multi-Environment &amp; Namespace Strategy"},{"location":"02%20helm/practice-questions/#question-42-secrets-management-integration","text":"Objective : Create templates that integrate with external secret management systems. Requirements : - Support multiple secret backends: - Kubernetes native Secrets - External Secrets Operator references - HashiCorp Vault integration - Templates should: - Reference secrets by name/path - Handle secret rotation annotations - Support sealed secrets compatibility - Deployment should mount secrets: - As environment variables - As volume mounts - Both methods should be configurable Constraints : - No secret values in values.yaml - Template must support switching between secret backends via configuration","title":"Question 4.2: Secrets Management Integration"},{"location":"02%20helm/practice-questions/#question-43-rbac-and-security-context-templates","text":"Objective : Create comprehensive RBAC and security-focused templates. Requirements : - Generate: - ServiceAccount - Role/ClusterRole (with appropriate permissions) - RoleBinding/ClusterRoleBinding - Pod Security Policy or Security Context - Network Policy (optional) - Configuration should include: - Custom RBAC rules (configurable) - Pod security context (read-only filesystem, non-root user) - Container security context (capabilities dropping) - Service Account Token automounting control Constraints : - RBAC rules must be minimal and specific - Security context must follow Kubernetes best practices","title":"Question 4.3: RBAC and Security Context Templates"},{"location":"02%20helm/practice-questions/#question-44-complex-deployment-strategy","text":"Objective : Create templates supporting multiple deployment strategies. Requirements : - Support multiple deployment patterns: - Rolling update (standard) - Blue-green deployment (two separate deployments) - Canary deployment (with weighted traffic) - Feature flag driven releases - For each strategy: - Define required configuration in values.yaml - Template conditionally generates appropriate resources - Support gradual traffic shifting - Include status tracking resources (optional) Constraints : - Single chart must support all strategies - Switching strategies should only require values.yaml changes","title":"Question 4.4: Complex Deployment Strategy"},{"location":"02%20helm/practice-questions/#question-45-helm-testing-validation","text":"Objective : Create test charts and validation templates. Requirements : - Create Helm test pods that validate: - Application connectivity - Configuration correctness - Environment variables - Volume mounts - Security context - Test pod should: - Run after deployment - Have proper cleanup policies - Exit with appropriate status - Generate meaningful output - Include template that validates values.yaml schema Constraints : - Tests must not require external dependencies - Test cleanup must be automatic","title":"Question 4.5: Helm Testing &amp; Validation"},{"location":"02%20helm/practice-questions/#level-5-expert-scenarios-real-world-patterns","text":"","title":"Level 5: Expert Scenarios &amp; Real-World Patterns"},{"location":"02%20helm/practice-questions/#question-51-stateful-application-with-storage","text":"Objective : Create a chart for a stateful application with persistent storage. Requirements : - Statefulset for ordered, stable Pod identities - Storage configuration: - PersistentVolumeClaim generation - Storage class selection - Mount path configuration - Support: - StatefulSet specific features (headless service, pod DNS) - Rolling updates with PVC retention - Backup/restore hooks - Data migration between storage classes Constraints : - Must use StatefulSet (not Deployment) - Storage configuration must be flexible","title":"Question 5.1: Stateful Application with Storage"},{"location":"02%20helm/practice-questions/#question-52-helm-plugins-and-custom-functions","text":"Objective : Create templates that use custom plugin functions or advanced templating. Requirements : - Implement custom template functions for: - Environment-specific variable substitution - Kubernetes resource validation - Image digest resolution - Custom label generation - Template must: - Use these custom functions effectively - Handle function failures gracefully - Document custom function usage Constraints : - Must create actual working functions (not pseudocode) - Functions must be reusable across templates","title":"Question 5.2: Helm Plugins and Custom Functions"},{"location":"02%20helm/practice-questions/#question-53-gitops-and-helm-operator-integration","text":"Objective : Create a chart designed for GitOps workflow. Requirements : - Chart structure optimized for: - Flux CD or ArgoCD integration - HelmRelease resource generation - Automated sync and monitoring - Include: - Health check configuration - Sync behavior controls - Rollback capabilities - Notification annotations - Support value-driven deployments from Git Constraints : - Chart must be compatible with popular GitOps tools - Proper CRD references if needed","title":"Question 5.3: GitOps and Helm Operator Integration"},{"location":"02%20helm/practice-questions/#question-54-umbrella-chart-with-microservices","text":"Objective : Create a parent chart managing multiple microservices. Requirements : - Parent chart (umbrella) that orchestrates: - API service - Frontend service - Backend service - Worker service - Database - Cache - Features: - Selective component enablement - Shared configuration management - Service discovery and DNS configuration - Cross-service communication security - Centralized logging configuration Constraints : - Must handle 6+ child charts - Shared values must be properly scoped","title":"Question 5.4: Umbrella Chart with Microservices"},{"location":"02%20helm/practice-questions/#question-55-production-readiness-checklist","text":"Objective : Create a comprehensive production-ready Helm chart. Requirements : - Chart must include and configure: - Health checks (liveness and readiness probes) - Resource requests and limits - Horizontal Pod Autoscaling - Pod Disruption Budgets - Affinity rules (node/pod affinity) - Tolerations for node taints - Proper logging and monitoring hooks - RBAC and security contexts - Network policies - Service mesh integration (if applicable) - Documentation: - Values.yaml documentation - Deployment guide - Troubleshooting guide - Upgrade procedure Constraints : - All components must be optional (configurable) - Security requirements must be non-negotiable - Performance/reliability cannot be compromised","title":"Question 5.5: Production Readiness Checklist"},{"location":"02%20helm/practice-questions/#learning-path-recommendation","text":"Start with Level 1 (1.1 \u2192 1.2 \u2192 1.3) to understand basic Helm mechanics Progress to Level 2 (2.1 \u2192 2.2 \u2192 2.3) to learn template logic and features Move to Level 3 (3.1 \u2192 3.2 \u2192 3.3 \u2192 3.4) for advanced templating patterns Explore Level 4 (4.1 \u2192 4.2 \u2192 4.3 \u2192 4.4 \u2192 4.5) for production scenarios Master Level 5 (5.1 \u2192 5.2 \u2192 5.3 \u2192 5.4 \u2192 5.5) for expert implementations Each level builds on previous knowledge. Complete at least 2-3 questions per level before advancing.","title":"Learning Path Recommendation"},{"location":"02%20helm/question1/","text":"Helm Practice Exercise - 1 These exercises are designed to help you master Helm mechanics only \u2014chart structure, templating, values, conditionals, hooks, dependencies, upgrades, and reuse\u2014using simple dummy workloads ( busybox , sleep , echo ). There is no application logic complexity involved. Level 1: Basic Chart Structure 1.1 Simple Dummy Application Objective: Create a Helm chart named simple-app that deploys a container which only sleeps. Requirements: Chart metadata: name : simple-app version : 0.1.0 appVersion : 1.0 One Deployment replicas: 1 Image: busybox:latest Command: [ \"sh\" , \"-c\" , \"echo 'App started' && sleep 3600\" ] * One Service Exposes port 8080 (even if unused) Constraint: All configuration must be hardcoded in the manifests. No values.yaml usage yet. 1.2 Add Configuration Values Objective: Parameterize the chart using values.yaml . Requirements: Create values.yaml with: replicaCount : 2 image : busybox:latest command : - sh - -c - echo 'Default command' && sleep 3600 service : port : 8080 Template Expectations: All values must be referenced using {{ .Values.* }} Test: helm install test simple-app --set replicaCount = 3 Verify that 3 Pods are created. Level 2: Template Logic 2.1 Conditional Sidecar Container Objective: Add a sidecar container conditionally. Requirements (values.yaml): sidecar : enabled : false image : nginx:alpine port : 80 Template Logic: Use: {{- if .Values.sidecar.enabled }} * Add a second container: Name: sidecar Image: from values Exposes containerPort Test: Install normally \u2192 no sidecar Install with: --set sidecar.enabled = true 2.2 Dynamic Names and Labels Objective: Generate labels and names dynamically. Requirements (values.yaml): environment : dev team : platform Deployment Labels: environment: <value> team: <value> fullname: <release-name>-<chart-name> Additional Resource: Create a ConfigMap : Name: {{ .Release.Name }}-config * Data: environment : <value> Key Challenge: Proper quoting and sanitization of label values. Level 3: Advanced Templating 3.1 Looping Environment Variables Objective: Dynamically inject environment variables. values.yaml: env : - name : LOG_LEVEL value : \"INFO\" - name : TIMEOUT value : \"30\" envSecrets : [] Requirements: Use: {{- range .Values.env }} to render environment variables. * Create a demo Secret with a similar structure. * Conditionally mount secrets only if envSecrets is non-empty. 3.2 ConfigMap from File with tpl Objective: Render templated config content stored in values. values.yaml: configData : | app.name: {{ .Chart.Name }} app.version: {{ .Chart.Version }} templates/config.yaml: Use: {{ tpl .Values.configData . }} Mounting: Mount ConfigMap at: /app/config/app.conf Test: Verify rendered ConfigMap shows actual chart name and version , not template text. Level 4: Named Templates & Helper Functions 4.1 Create _helpers.tpl for Labels Objective: Extract common labels into reusable named templates. Create templates/_helpers.tpl : Define these named templates: simple-app.labels - standard labels simple-app.selectorLabels - selector labels only simple-app.name - chart name simple-app.fullname - full resource name Example Structure: {{- define \"simple-app.labels\" -}} app.kubernetes.io/name: {{ include \"simple-app.name\" . }} app.kubernetes.io/instance: {{ .Release.Name }} app.kubernetes.io/version: {{ .Chart.AppVersion | quote }} app.kubernetes.io/managed-by: {{ .Release.Service }} {{- end }} Requirements: Update Deployment and Service to use: labels: {{- include \"simple-app.labels\" . | nindent 4 }} * Use proper nindent for alignment 4.2 String Manipulation Functions Objective: Practice Helm string functions. values.yaml additions: appName : \"My Demo App\" namespace : \"production\" imageTag : \"v1.2.3-alpha\" Template Requirements: Create a ConfigMap that demonstrates: upper - convert appName to uppercase lower - convert namespace to lowercase title - title case the appName trim - remove whitespace trimSuffix - remove -alpha from tag replace - replace spaces with dashes quote - properly quote values Example Data: data : APP_NAME_UPPER : {{ .Values.appName | upper | quote }} NAMESPACE_LOWER : {{ .Values.namespace | lower | quote }} TAG_CLEAN : {{ .Values.imageTag | trimSuffix \"-alpha\" | quote }} APP_SLUG : {{ .Values.appName | replace \" \" \"-\" | lower | quote }} 4.3 Default Values with default Function Objective: Handle missing or optional values gracefully. values.yaml: service : type : ClusterIP # port is intentionally missing resources : {} # limits and requests are optional Template Logic: In Service template: port: {{ default 8080 .Values.service.port }} In Deployment template: resources: {{- if .Values.resources.limits }} limits: cpu: {{ default \"100m\" .Values.resources.limits.cpu }} memory: {{ default \"128Mi\" .Values.resources.limits.memory }} {{- end }} requests: cpu: {{ default \"50m\" .Values.resources.requests.cpu }} memory: {{ default \"64Mi\" .Values.resources.requests.memory }} Test: Install without setting values \u2192 uses defaults Override specific values \u2192 uses overrides Level 5: Data Structures & Advanced Logic 5.1 Working with Lists and range Objective: Master iteration over lists and maps. values.yaml: additionalLabels : cost-center : \"engineering\" project : \"demo\" owner : \"platform-team\" volumes : - name : cache mountPath : /cache emptyDir : {} - name : data mountPath : /data emptyDir : sizeLimit : 1Gi Deployment Template: Add labels section: labels: {{- include \"simple-app.labels\" . | nindent 4 }} {{- range $key, $value := .Values.additionalLabels }} {{ $key }}: {{ $value | quote }} {{- end }} Add volumes and volumeMounts: volumes: {{- range .Values.volumes }} - name: {{ .name }} emptyDir: {{- toYaml .emptyDir | nindent 4 }} {{- end }} volumeMounts: {{- range .Values.volumes }} - name: {{ .name }} mountPath: {{ .mountPath }} {{- end }} 5.2 Conditionals with and , or , not Objective: Use logical operators for complex conditions. values.yaml: monitoring : enabled : true prometheus : true grafana : false security : enabled : true readOnlyRootFilesystem : true runAsNonRoot : true Template Logic: {{- if and .Values.monitoring.enabled .Values.monitoring.prometheus }} annotations: prometheus.io/scrape: \"true\" prometheus.io/port: \"8080\" {{- end }} {{- if or .Values.security.enabled .Values.security.runAsNonRoot }} securityContext: {{- if .Values.security.runAsNonRoot }} runAsNonRoot: true runAsUser: 1000 {{- end }} {{- if .Values.security.readOnlyRootFilesystem }} readOnlyRootFilesystem: true {{- end }} {{- end }} 5.3 Required Values Validation Objective: Ensure critical values are provided. values.yaml: # image is required - no default image : \"\" database : # host is required host : \"\" port : 5432 Template with Validation: image: {{ required \"A valid .Values.image is required!\" .Values.image }} env: - name: DB_HOST value: {{ required \"database.host is required!\" .Values.database.host | quote }} - name: DB_PORT value: {{ .Values.database.port | quote }} Test: Install without image \u2192 should fail with error message Install with image \u2192 succeeds Level 6: YAML Formatting & Complex Types 6.1 Working with toYaml and nindent Objective: Properly render complex YAML structures. values.yaml: podAnnotations : backup.velero.io/backup-volumes : data prometheus.io/scrape : \"true\" tolerations : - key : \"node.kubernetes.io/disk-pressure\" operator : \"Exists\" effect : \"NoSchedule\" - key : \"environment\" operator : \"Equal\" value : \"production\" effect : \"NoSchedule\" nodeSelector : disktype : ssd zone : us-west-1a Deployment Template: {{- with .Values.podAnnotations }} annotations: {{- toYaml . | nindent 2 }} {{- end }} spec: {{- with .Values.nodeSelector }} nodeSelector: {{- toYaml . | nindent 4 }} {{- end }} {{- with .Values.tolerations }} tolerations: {{- toYaml . | nindent 2 }} {{- end }} Key Concepts: with for scoping toYaml for complex structures Correct nindent values for alignment 6.2 Merging Values with merge and mustMerge Objective: Combine multiple value sources. values.yaml: commonLabels : team : platform managed-by : helm deploymentLabels : component : backend podLabels : version : v1 Template: {{- $labels := merge .Values.podLabels .Values.deploymentLabels .Values.commonLabels }} labels: {{- range $key, $value := $labels }} {{ $key }}: {{ $value | quote }} {{- end }} Expected Result: All three label sets merged together (podLabels takes precedence). 6.3 Type Conversion Functions Objective: Convert between types safely. values.yaml: replicas : \"3\" # string instead of int port : 8080 # int features : caching : \"true\" # string bool debug : false # real bool Template: replicas: {{ .Values.replicas | int }} env: - name: CACHE_ENABLED value: {{ .Values.features.caching | toString | quote }} - name: DEBUG value: {{ .Values.features.debug | toString | quote }} Functions to Practice: int - convert to integer toString - convert to string float64 - convert to float Level 7: Production-Ready Patterns 7.1 Health Checks (Probes) Objective: Add comprehensive health checking. values.yaml: livenessProbe : httpGet : path : /healthz port : http initialDelaySeconds : 30 periodSeconds : 10 timeoutSeconds : 5 failureThreshold : 3 readinessProbe : httpGet : path : /ready port : http initialDelaySeconds : 10 periodSeconds : 5 timeoutSeconds : 3 failureThreshold : 3 startupProbe : httpGet : path : /startup port : http initialDelaySeconds : 0 periodSeconds : 5 failureThreshold : 30 Deployment Template: {{- if .Values.livenessProbe }} livenessProbe: {{- toYaml .Values.livenessProbe | nindent 2 }} {{- end }} {{- if .Values.readinessProbe }} readinessProbe: {{- toYaml .Values.readinessProbe | nindent 2 }} {{- end }} {{- if .Values.startupProbe }} startupProbe: {{- toYaml .Values.startupProbe | nindent 2 }} {{- end }} 7.2 Resource Management Objective: Properly define resource requests and limits. values.yaml: resources : limits : cpu : 500m memory : 512Mi requests : cpu : 100m memory : 128Mi autoscaling : enabled : false minReplicas : 2 maxReplicas : 10 targetCPUUtilizationPercentage : 80 targetMemoryUtilizationPercentage : 80 Deployment: resources: {{- toYaml .Values.resources | nindent 2 }} Create hpa.yaml : {{- if .Values.autoscaling.enabled }} apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: {{ include \"simple-app.fullname\" . }} spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: {{ include \"simple-app.fullname\" . }} minReplicas: {{ .Values.autoscaling.minReplicas }} maxReplicas: {{ .Values.autoscaling.maxReplicas }} metrics: {{- if .Values.autoscaling.targetCPUUtilizationPercentage }} - type: Resource resource: name: cpu target: type: Utilization averageUtilization: {{ .Values.autoscaling.targetCPUUtilizationPercentage }} {{- end }} {{- if .Values.autoscaling.targetMemoryUtilizationPercentage }} - type: Resource resource: name: memory target: type: Utilization averageUtilization: {{ .Values.autoscaling.targetMemoryUtilizationPercentage }} {{- end }} {{- end }} 7.3 NOTES.txt Template Objective: Create helpful installation output. Create templates/NOTES.txt : Thank you for installing {{ .Chart.Name }}! Your release is named {{ .Release.Name }}. To learn more about the release, try: $ helm status {{ .Release.Name }} $ helm get all {{ .Release.Name }} {{- if .Values.service.type }} Service Type: {{ .Values.service.type }} {{- if eq .Values.service.type \"NodePort\" }} export NODE_PORT=$(kubectl get --namespace {{ .Release.Namespace }} -o jsonpath=\"{.spec.ports[0].nodePort}\" services {{ include \"simple-app.fullname\" . }}) export NODE_IP=$(kubectl get nodes --namespace {{ .Release.Namespace }} -o jsonpath=\"{.items[0].status.addresses[0].address}\") echo http://$NODE_IP:$NODE_PORT {{- else if eq .Values.service.type \"LoadBalancer\" }} NOTE: It may take a few minutes for the LoadBalancer IP to be available. Watch the status with: kubectl get --namespace {{ .Release.Namespace }} svc -w {{ include \"simple-app.fullname\" . }} {{- else if eq .Values.service.type \"ClusterIP\" }} export POD_NAME=$(kubectl get pods --namespace {{ .Release.Namespace }} -l \"app.kubernetes.io/name={{ include \"simple-app.name\" . }},app.kubernetes.io/instance={{ .Release.Name }}\" -o jsonpath=\"{.items[0].metadata.name}\") echo \"Visit http://127.0.0.1:8080\" kubectl --namespace {{ .Release.Namespace }} port-forward $POD_NAME 8080:{{ .Values.service.port }} {{- end }} {{- end }} {{- if .Values.autoscaling.enabled }} Autoscaling is ENABLED: Min Replicas: {{ .Values.autoscaling.minReplicas }} Max Replicas: {{ .Values.autoscaling.maxReplicas }} {{- else }} Autoscaling is DISABLED. Using {{ .Values.replicaCount }} replica(s). {{- end }} Final Challenge: Comprehensive Chart Package Objective: Combine everything into a production-ready chart. Requirements Checklist: [ ] Chart.yaml with proper metadata (name, version, description, keywords, maintainers) [ ] values.yaml with comprehensive defaults and comments [ ] _helpers.tpl with all named templates [ ] Deployment with: [ ] Named template labels [ ] ConfigMap/Secret mounts [ ] Resource limits [ ] All three probe types [ ] Security context [ ] Node selectors and tolerations [ ] Service with configurable type [ ] ConfigMap with templated content [ ] HorizontalPodAutoscaler (conditional) [ ] ServiceAccount (conditional) [ ] Ingress (conditional) [ ] NOTES.txt with helpful instructions [ ] Proper toYaml , nindent , quote usage throughout [ ] Required value validation [ ] No hardcoded values Validation Commands: # Lint the chart helm lint . # Dry run with debug helm install --dry-run --debug test-release . # Template with different values helm template test-release . -f values-prod.yaml # Install and verify helm install my-app . kubectl get all helm get values my-app # Upgrade helm upgrade my-app . --set replicaCount = 3 # Check history helm history my-app Recommended Practice Flow Level 1-2 : Get basic structure working Level 3 : Add templating and loops Level 4 : Create helpers and use functions Level 5-6 : Master data structures and YAML handling Level 7 : Production-ready features Final Challenge : Package everything together Pro Tips: Always use helm lint before installing Use --dry-run --debug to see rendered templates Test with helm template to avoid cluster pollution Validate YAML syntax with yamllint Keep templates readable with proper indentation Document values.yaml thoroughly","title":"Question1"},{"location":"02%20helm/question1/#helm-practice-exercise-1","text":"These exercises are designed to help you master Helm mechanics only \u2014chart structure, templating, values, conditionals, hooks, dependencies, upgrades, and reuse\u2014using simple dummy workloads ( busybox , sleep , echo ). There is no application logic complexity involved.","title":"Helm Practice Exercise - 1"},{"location":"02%20helm/question1/#level-1-basic-chart-structure","text":"","title":"Level 1: Basic Chart Structure"},{"location":"02%20helm/question1/#11-simple-dummy-application","text":"Objective: Create a Helm chart named simple-app that deploys a container which only sleeps. Requirements: Chart metadata: name : simple-app version : 0.1.0 appVersion : 1.0 One Deployment replicas: 1 Image: busybox:latest Command: [ \"sh\" , \"-c\" , \"echo 'App started' && sleep 3600\" ] * One Service Exposes port 8080 (even if unused) Constraint: All configuration must be hardcoded in the manifests. No values.yaml usage yet.","title":"1.1 Simple Dummy Application"},{"location":"02%20helm/question1/#12-add-configuration-values","text":"Objective: Parameterize the chart using values.yaml . Requirements: Create values.yaml with: replicaCount : 2 image : busybox:latest command : - sh - -c - echo 'Default command' && sleep 3600 service : port : 8080 Template Expectations: All values must be referenced using {{ .Values.* }} Test: helm install test simple-app --set replicaCount = 3 Verify that 3 Pods are created.","title":"1.2 Add Configuration Values"},{"location":"02%20helm/question1/#level-2-template-logic","text":"","title":"Level 2: Template Logic"},{"location":"02%20helm/question1/#21-conditional-sidecar-container","text":"Objective: Add a sidecar container conditionally. Requirements (values.yaml): sidecar : enabled : false image : nginx:alpine port : 80 Template Logic: Use: {{- if .Values.sidecar.enabled }} * Add a second container: Name: sidecar Image: from values Exposes containerPort Test: Install normally \u2192 no sidecar Install with: --set sidecar.enabled = true","title":"2.1 Conditional Sidecar Container"},{"location":"02%20helm/question1/#22-dynamic-names-and-labels","text":"Objective: Generate labels and names dynamically. Requirements (values.yaml): environment : dev team : platform Deployment Labels: environment: <value> team: <value> fullname: <release-name>-<chart-name> Additional Resource: Create a ConfigMap : Name: {{ .Release.Name }}-config * Data: environment : <value> Key Challenge: Proper quoting and sanitization of label values.","title":"2.2 Dynamic Names and Labels"},{"location":"02%20helm/question1/#level-3-advanced-templating","text":"","title":"Level 3: Advanced Templating"},{"location":"02%20helm/question1/#31-looping-environment-variables","text":"Objective: Dynamically inject environment variables. values.yaml: env : - name : LOG_LEVEL value : \"INFO\" - name : TIMEOUT value : \"30\" envSecrets : [] Requirements: Use: {{- range .Values.env }} to render environment variables. * Create a demo Secret with a similar structure. * Conditionally mount secrets only if envSecrets is non-empty.","title":"3.1 Looping Environment Variables"},{"location":"02%20helm/question1/#32-configmap-from-file-with-tpl","text":"Objective: Render templated config content stored in values. values.yaml: configData : | app.name: {{ .Chart.Name }} app.version: {{ .Chart.Version }} templates/config.yaml: Use: {{ tpl .Values.configData . }} Mounting: Mount ConfigMap at: /app/config/app.conf Test: Verify rendered ConfigMap shows actual chart name and version , not template text.","title":"3.2 ConfigMap from File with tpl"},{"location":"02%20helm/question1/#level-4-named-templates-helper-functions","text":"","title":"Level 4: Named Templates &amp; Helper Functions"},{"location":"02%20helm/question1/#41-create-_helperstpl-for-labels","text":"Objective: Extract common labels into reusable named templates. Create templates/_helpers.tpl : Define these named templates: simple-app.labels - standard labels simple-app.selectorLabels - selector labels only simple-app.name - chart name simple-app.fullname - full resource name Example Structure: {{- define \"simple-app.labels\" -}} app.kubernetes.io/name: {{ include \"simple-app.name\" . }} app.kubernetes.io/instance: {{ .Release.Name }} app.kubernetes.io/version: {{ .Chart.AppVersion | quote }} app.kubernetes.io/managed-by: {{ .Release.Service }} {{- end }} Requirements: Update Deployment and Service to use: labels: {{- include \"simple-app.labels\" . | nindent 4 }} * Use proper nindent for alignment","title":"4.1 Create _helpers.tpl for Labels"},{"location":"02%20helm/question1/#42-string-manipulation-functions","text":"Objective: Practice Helm string functions. values.yaml additions: appName : \"My Demo App\" namespace : \"production\" imageTag : \"v1.2.3-alpha\" Template Requirements: Create a ConfigMap that demonstrates: upper - convert appName to uppercase lower - convert namespace to lowercase title - title case the appName trim - remove whitespace trimSuffix - remove -alpha from tag replace - replace spaces with dashes quote - properly quote values Example Data: data : APP_NAME_UPPER : {{ .Values.appName | upper | quote }} NAMESPACE_LOWER : {{ .Values.namespace | lower | quote }} TAG_CLEAN : {{ .Values.imageTag | trimSuffix \"-alpha\" | quote }} APP_SLUG : {{ .Values.appName | replace \" \" \"-\" | lower | quote }}","title":"4.2 String Manipulation Functions"},{"location":"02%20helm/question1/#43-default-values-with-default-function","text":"Objective: Handle missing or optional values gracefully. values.yaml: service : type : ClusterIP # port is intentionally missing resources : {} # limits and requests are optional Template Logic: In Service template: port: {{ default 8080 .Values.service.port }} In Deployment template: resources: {{- if .Values.resources.limits }} limits: cpu: {{ default \"100m\" .Values.resources.limits.cpu }} memory: {{ default \"128Mi\" .Values.resources.limits.memory }} {{- end }} requests: cpu: {{ default \"50m\" .Values.resources.requests.cpu }} memory: {{ default \"64Mi\" .Values.resources.requests.memory }} Test: Install without setting values \u2192 uses defaults Override specific values \u2192 uses overrides","title":"4.3 Default Values with default Function"},{"location":"02%20helm/question1/#level-5-data-structures-advanced-logic","text":"","title":"Level 5: Data Structures &amp; Advanced Logic"},{"location":"02%20helm/question1/#51-working-with-lists-and-range","text":"Objective: Master iteration over lists and maps. values.yaml: additionalLabels : cost-center : \"engineering\" project : \"demo\" owner : \"platform-team\" volumes : - name : cache mountPath : /cache emptyDir : {} - name : data mountPath : /data emptyDir : sizeLimit : 1Gi Deployment Template: Add labels section: labels: {{- include \"simple-app.labels\" . | nindent 4 }} {{- range $key, $value := .Values.additionalLabels }} {{ $key }}: {{ $value | quote }} {{- end }} Add volumes and volumeMounts: volumes: {{- range .Values.volumes }} - name: {{ .name }} emptyDir: {{- toYaml .emptyDir | nindent 4 }} {{- end }} volumeMounts: {{- range .Values.volumes }} - name: {{ .name }} mountPath: {{ .mountPath }} {{- end }}","title":"5.1 Working with Lists and range"},{"location":"02%20helm/question1/#52-conditionals-with-and-or-not","text":"Objective: Use logical operators for complex conditions. values.yaml: monitoring : enabled : true prometheus : true grafana : false security : enabled : true readOnlyRootFilesystem : true runAsNonRoot : true Template Logic: {{- if and .Values.monitoring.enabled .Values.monitoring.prometheus }} annotations: prometheus.io/scrape: \"true\" prometheus.io/port: \"8080\" {{- end }} {{- if or .Values.security.enabled .Values.security.runAsNonRoot }} securityContext: {{- if .Values.security.runAsNonRoot }} runAsNonRoot: true runAsUser: 1000 {{- end }} {{- if .Values.security.readOnlyRootFilesystem }} readOnlyRootFilesystem: true {{- end }} {{- end }}","title":"5.2 Conditionals with and, or, not"},{"location":"02%20helm/question1/#53-required-values-validation","text":"Objective: Ensure critical values are provided. values.yaml: # image is required - no default image : \"\" database : # host is required host : \"\" port : 5432 Template with Validation: image: {{ required \"A valid .Values.image is required!\" .Values.image }} env: - name: DB_HOST value: {{ required \"database.host is required!\" .Values.database.host | quote }} - name: DB_PORT value: {{ .Values.database.port | quote }} Test: Install without image \u2192 should fail with error message Install with image \u2192 succeeds","title":"5.3 Required Values Validation"},{"location":"02%20helm/question1/#level-6-yaml-formatting-complex-types","text":"","title":"Level 6: YAML Formatting &amp; Complex Types"},{"location":"02%20helm/question1/#61-working-with-toyaml-and-nindent","text":"Objective: Properly render complex YAML structures. values.yaml: podAnnotations : backup.velero.io/backup-volumes : data prometheus.io/scrape : \"true\" tolerations : - key : \"node.kubernetes.io/disk-pressure\" operator : \"Exists\" effect : \"NoSchedule\" - key : \"environment\" operator : \"Equal\" value : \"production\" effect : \"NoSchedule\" nodeSelector : disktype : ssd zone : us-west-1a Deployment Template: {{- with .Values.podAnnotations }} annotations: {{- toYaml . | nindent 2 }} {{- end }} spec: {{- with .Values.nodeSelector }} nodeSelector: {{- toYaml . | nindent 4 }} {{- end }} {{- with .Values.tolerations }} tolerations: {{- toYaml . | nindent 2 }} {{- end }} Key Concepts: with for scoping toYaml for complex structures Correct nindent values for alignment","title":"6.1 Working with toYaml and nindent"},{"location":"02%20helm/question1/#62-merging-values-with-merge-and-mustmerge","text":"Objective: Combine multiple value sources. values.yaml: commonLabels : team : platform managed-by : helm deploymentLabels : component : backend podLabels : version : v1 Template: {{- $labels := merge .Values.podLabels .Values.deploymentLabels .Values.commonLabels }} labels: {{- range $key, $value := $labels }} {{ $key }}: {{ $value | quote }} {{- end }} Expected Result: All three label sets merged together (podLabels takes precedence).","title":"6.2 Merging Values with merge and mustMerge"},{"location":"02%20helm/question1/#63-type-conversion-functions","text":"Objective: Convert between types safely. values.yaml: replicas : \"3\" # string instead of int port : 8080 # int features : caching : \"true\" # string bool debug : false # real bool Template: replicas: {{ .Values.replicas | int }} env: - name: CACHE_ENABLED value: {{ .Values.features.caching | toString | quote }} - name: DEBUG value: {{ .Values.features.debug | toString | quote }} Functions to Practice: int - convert to integer toString - convert to string float64 - convert to float","title":"6.3 Type Conversion Functions"},{"location":"02%20helm/question1/#level-7-production-ready-patterns","text":"","title":"Level 7: Production-Ready Patterns"},{"location":"02%20helm/question1/#71-health-checks-probes","text":"Objective: Add comprehensive health checking. values.yaml: livenessProbe : httpGet : path : /healthz port : http initialDelaySeconds : 30 periodSeconds : 10 timeoutSeconds : 5 failureThreshold : 3 readinessProbe : httpGet : path : /ready port : http initialDelaySeconds : 10 periodSeconds : 5 timeoutSeconds : 3 failureThreshold : 3 startupProbe : httpGet : path : /startup port : http initialDelaySeconds : 0 periodSeconds : 5 failureThreshold : 30 Deployment Template: {{- if .Values.livenessProbe }} livenessProbe: {{- toYaml .Values.livenessProbe | nindent 2 }} {{- end }} {{- if .Values.readinessProbe }} readinessProbe: {{- toYaml .Values.readinessProbe | nindent 2 }} {{- end }} {{- if .Values.startupProbe }} startupProbe: {{- toYaml .Values.startupProbe | nindent 2 }} {{- end }}","title":"7.1 Health Checks (Probes)"},{"location":"02%20helm/question1/#72-resource-management","text":"Objective: Properly define resource requests and limits. values.yaml: resources : limits : cpu : 500m memory : 512Mi requests : cpu : 100m memory : 128Mi autoscaling : enabled : false minReplicas : 2 maxReplicas : 10 targetCPUUtilizationPercentage : 80 targetMemoryUtilizationPercentage : 80 Deployment: resources: {{- toYaml .Values.resources | nindent 2 }} Create hpa.yaml : {{- if .Values.autoscaling.enabled }} apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: {{ include \"simple-app.fullname\" . }} spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: {{ include \"simple-app.fullname\" . }} minReplicas: {{ .Values.autoscaling.minReplicas }} maxReplicas: {{ .Values.autoscaling.maxReplicas }} metrics: {{- if .Values.autoscaling.targetCPUUtilizationPercentage }} - type: Resource resource: name: cpu target: type: Utilization averageUtilization: {{ .Values.autoscaling.targetCPUUtilizationPercentage }} {{- end }} {{- if .Values.autoscaling.targetMemoryUtilizationPercentage }} - type: Resource resource: name: memory target: type: Utilization averageUtilization: {{ .Values.autoscaling.targetMemoryUtilizationPercentage }} {{- end }} {{- end }}","title":"7.2 Resource Management"},{"location":"02%20helm/question1/#73-notestxt-template","text":"Objective: Create helpful installation output. Create templates/NOTES.txt : Thank you for installing {{ .Chart.Name }}! Your release is named {{ .Release.Name }}. To learn more about the release, try: $ helm status {{ .Release.Name }} $ helm get all {{ .Release.Name }} {{- if .Values.service.type }} Service Type: {{ .Values.service.type }} {{- if eq .Values.service.type \"NodePort\" }} export NODE_PORT=$(kubectl get --namespace {{ .Release.Namespace }} -o jsonpath=\"{.spec.ports[0].nodePort}\" services {{ include \"simple-app.fullname\" . }}) export NODE_IP=$(kubectl get nodes --namespace {{ .Release.Namespace }} -o jsonpath=\"{.items[0].status.addresses[0].address}\") echo http://$NODE_IP:$NODE_PORT {{- else if eq .Values.service.type \"LoadBalancer\" }} NOTE: It may take a few minutes for the LoadBalancer IP to be available. Watch the status with: kubectl get --namespace {{ .Release.Namespace }} svc -w {{ include \"simple-app.fullname\" . }} {{- else if eq .Values.service.type \"ClusterIP\" }} export POD_NAME=$(kubectl get pods --namespace {{ .Release.Namespace }} -l \"app.kubernetes.io/name={{ include \"simple-app.name\" . }},app.kubernetes.io/instance={{ .Release.Name }}\" -o jsonpath=\"{.items[0].metadata.name}\") echo \"Visit http://127.0.0.1:8080\" kubectl --namespace {{ .Release.Namespace }} port-forward $POD_NAME 8080:{{ .Values.service.port }} {{- end }} {{- end }} {{- if .Values.autoscaling.enabled }} Autoscaling is ENABLED: Min Replicas: {{ .Values.autoscaling.minReplicas }} Max Replicas: {{ .Values.autoscaling.maxReplicas }} {{- else }} Autoscaling is DISABLED. Using {{ .Values.replicaCount }} replica(s). {{- end }}","title":"7.3 NOTES.txt Template"},{"location":"02%20helm/question1/#final-challenge-comprehensive-chart-package","text":"Objective: Combine everything into a production-ready chart.","title":"Final Challenge: Comprehensive Chart Package"},{"location":"02%20helm/question1/#requirements-checklist","text":"[ ] Chart.yaml with proper metadata (name, version, description, keywords, maintainers) [ ] values.yaml with comprehensive defaults and comments [ ] _helpers.tpl with all named templates [ ] Deployment with: [ ] Named template labels [ ] ConfigMap/Secret mounts [ ] Resource limits [ ] All three probe types [ ] Security context [ ] Node selectors and tolerations [ ] Service with configurable type [ ] ConfigMap with templated content [ ] HorizontalPodAutoscaler (conditional) [ ] ServiceAccount (conditional) [ ] Ingress (conditional) [ ] NOTES.txt with helpful instructions [ ] Proper toYaml , nindent , quote usage throughout [ ] Required value validation [ ] No hardcoded values","title":"Requirements Checklist:"},{"location":"02%20helm/question1/#validation-commands","text":"# Lint the chart helm lint . # Dry run with debug helm install --dry-run --debug test-release . # Template with different values helm template test-release . -f values-prod.yaml # Install and verify helm install my-app . kubectl get all helm get values my-app # Upgrade helm upgrade my-app . --set replicaCount = 3 # Check history helm history my-app","title":"Validation Commands:"},{"location":"02%20helm/question1/#recommended-practice-flow","text":"Level 1-2 : Get basic structure working Level 3 : Add templating and loops Level 4 : Create helpers and use functions Level 5-6 : Master data structures and YAML handling Level 7 : Production-ready features Final Challenge : Package everything together Pro Tips: Always use helm lint before installing Use --dry-run --debug to see rendered templates Test with helm template to avoid cluster pollution Validate YAML syntax with yamllint Keep templates readable with proper indentation Document values.yaml thoroughly","title":"Recommended Practice Flow"},{"location":"02%20helm/question2/","text":"Comprehensive Helm Chart Practice: From Zero to Production Expert Chart Name: webapp (use this name throughout all phases) Principle: You build ONE chart progressively. Each phase adds features while maintaining backward compatibility. Phase 1: Foundation - Hello Helm Duration: 15 minutes | Difficulty: Beginner Objective Create the absolute minimum valid Helm chart and verify it renders. What You'll Learn Chart structure and metadata Basic templating syntax Helm template rendering Requirements Step 1.1 - Create Chart Structure webapp/ \u251c\u2500\u2500 Chart.yaml \u251c\u2500\u2500 values.yaml \u2514\u2500\u2500 templates/ \u2514\u2500\u2500 configmap.yaml Step 1.2 - Chart.yaml apiVersion : v2 name : webapp description : Progressive Helm Learning Chart type : application version : 0.1.0 appVersion : \"1.0\" keywords : - helm - practice - kubernetes maintainers : - name : Your Name email : your@email.com Step 1.3 - values.yaml appName : webapp enabled : true Step 1.4 - templates/configmap.yaml Create a simple ConfigMap: {{ - if .Values.enabled }} apiVersion : v1 kind : ConfigMap metadata : name : {{ .Release.Name }} -config namespace : {{ .Release.Namespace }} labels : app : {{ .Values.appName }} data : app-name : {{ .Values.appName }} chart-name : {{ .Chart.Name }} chart-version : {{ .Chart.Version }} {{ - end }} Validation # Render the template helm template my-release . # Expected output: ConfigMap resource with metadata and data # Verify disabling works helm template my-release . --set enabled = false # Expected: No output (resource not rendered) Concepts Introduced .Chart.* (chart metadata) .Release.* (release information) .Values.* (user-provided values) Basic conditional if/end String interpolation with {{ }} Phase 2: Workloads - Deployment & Service Duration: 30 minutes | Difficulty: Beginner Objective Deploy a functional application with Service exposure. What You'll Learn Deployment templating Service configuration Pod specification basics Value quoting and safety Requirements Step 2.1 - Update values.yaml appName : webapp enabled : true replicaCount : 2 image : busybox:latest imagePullPolicy : IfNotPresent containerPort : 8080 servicePort : 8080 serviceType : ClusterIP command : - /bin/sh - -c - echo \"App started on port 8080\" && sleep 3600 nameOverride : \"\" fullnameOverride : \"\" Step 2.2 - Create templates/_helpers.tpl {{/* Generate common labels */}} {{- define \"webapp.labels\" -}} app.kubernetes.io/name: {{ .Chart.Name }} app.kubernetes.io/instance: {{ .Release.Name }} app.kubernetes.io/version: {{ .Chart.AppVersion | quote }} app.kubernetes.io/managed-by: {{ .Release.Service }} {{- end }} {{/* Generate selector labels */}} {{- define \"webapp.selectorLabels\" -}} app.kubernetes.io/name: {{ .Chart.Name }} app.kubernetes.io/instance: {{ .Release.Name }} {{- end }} {{/* Expand the name of the chart */}} {{- define \"webapp.name\" -}} {{- default .Chart.Name .Values.nameOverride | trunc 63 | trimSuffix \"-\" }} {{- end }} {{/* Expand the full name */}} {{- define \"webapp.fullname\" -}} {{- if .Values.fullnameOverride }} {{- .Values.fullnameOverride | trunc 63 | trimSuffix \"-\" }} {{- else }} {{- $name := default .Chart.Name .Values.nameOverride }} {{- if contains $name .Release.Name }} {{- .Release.Name | trunc 63 | trimSuffix \"-\" }} {{- else }} {{- printf \"%s-%s\" .Release.Name $name | trunc 63 | trimSuffix \"-\" }} {{- end }} {{- end }} {{- end }} Step 2.3 - Create templates/deployment.yaml {{ - if .Values.enabled }} apiVersion : apps/v1 kind : Deployment metadata : name : {{ include \"webapp.fullname\" . }} namespace : {{ .Release.Namespace }} labels : {{ - include \"webapp.labels\" . | nindent 4 }} spec : replicas : {{ .Values.replicaCount }} selector : matchLabels : {{ - include \"webapp.selectorLabels\" . | nindent 6 }} template : metadata : labels : {{ - include \"webapp.selectorLabels\" . | nindent 8 }} spec : containers : - name : {{ .Chart.Name }} image : {{ .Values.image | quote }} imagePullPolicy : {{ .Values.imagePullPolicy }} ports : - name : http containerPort : {{ .Values.containerPort }} protocol : TCP command : {{ - toYaml .Values.command | nindent 10 }} livenessProbe : exec : command : - /bin/sh - -c - ps aux | grep sleep || exit 1 initialDelaySeconds : 10 periodSeconds : 10 readinessProbe : exec : command : - /bin/sh - -c - ps aux | grep sleep || exit 1 initialDelaySeconds : 5 periodSeconds : 5 {{ - end }} Step 2.4 - Create templates/service.yaml {{ - if .Values.enabled }} apiVersion : v1 kind : Service metadata : name : {{ include \"webapp.fullname\" . }} namespace : {{ .Release.Namespace }} labels : {{ - include \"webapp.labels\" . | nindent 4 }} spec : type : {{ .Values.serviceType }} ports : - port : {{ .Values.servicePort }} targetPort : http protocol : TCP name : http selector : {{ - include \"webapp.selectorLabels\" . | nindent 4 }} {{ - end }} Validation # Verify template rendering helm template my-app . --debug # Dry-run install helm install --dry-run my-app . # Real install helm install my-app . # Verify resources kubectl get deployment,service kubectl logs deployment/my-app-webapp # Scale up helm upgrade my-app . --set replicaCount = 3 kubectl get pods Concepts Introduced Named templates (helpers) include function nindent for proper indentation toYaml for array rendering trunc , trimSuffix string functions Deployment and Service specs Label management Phase 3: Configuration Management Duration: 45 minutes | Difficulty: Beginner-Intermediate Objective Handle configuration through ConfigMaps and environment variables dynamically. What You'll Learn ConfigMap templating Environment variable injection Looping with range Nested values Requirements Step 3.1 - Update values.yaml # ... previous values ... # Configuration Maps configMaps : app-config : data : LOG_LEVEL : \"INFO\" APP_VERSION : \"1.0\" ENVIRONMENT : \"dev\" app-settings : data : MAX_CONNECTIONS : \"100\" TIMEOUT : \"30\" # Environment variables from ConfigMap envFromConfigMaps : - name : app-config - name : app-settings # Direct environment variables env : - name : POD_NAME valueFrom : fieldRef : fieldPath : metadata.name - name : POD_NAMESPACE valueFrom : fieldRef : fieldPath : metadata.namespace Step 3.2 - Create templates/configmap.yaml {{ - if .Values.enabled }} {{ - range $cmName , $cmData : = .Values.configMaps }} --- apiVersion : v1 kind : ConfigMap metadata : name : {{ include \"webapp.fullname\" $ }} -{{ $cmName }} namespace : {{ $.Release.Namespace }} labels : {{ - include \"webapp.labels\" $ | nindent 4 }} config-type : {{ $cmName }} data : {{ - range $key , $value : = $cmData.data }} {{ $key }}: {{ $value | quote }} {{ - end }} {{ - end }} {{ - end }} Step 3.3 - Update templates/deployment.yaml Add envFrom section to containers: envFrom : {{ - range .Values.envFromConfigMaps }} - configMapRef : name : {{ include \"webapp.fullname\" . }} -{{ .name }} {{ - end }} env : {{ - range .Values.env }} - name : {{ .name }} valueFrom : fieldRef : fieldPath : {{ .valueFrom.fieldRef.fieldPath }} {{ - end }} Validation # Check ConfigMaps created helm install my-app . --debug kubectl get configmaps kubectl get configmap my-app-webapp-app-config -o yaml # Verify environment variables in pod kubectl exec deployment/my-app-webapp -- env | grep APP # Update config and restart helm upgrade my-app . --set configMaps.app-config.data.LOG_LEVEL = DEBUG kubectl rollout restart deployment/my-app-webapp kubectl logs deployment/my-app-webapp Concepts Introduced range with maps and key-value pairs Multiple ConfigMaps from values envFrom and env injection Fieldref (metadata access) Variable scoping with $ Phase 4: Conditional Features & Advanced Logic Duration: 60 minutes | Difficulty: Intermediate Objective Control feature availability and implement complex logic conditions. What You'll Learn Feature flags Multiple conditionals with and , or , not ServiceAccount and RBAC Ingress configuration Requirements Step 4.1 - Update values.yaml # ... previous values ... serviceAccount : create : true name : \"\" annotations : {} rbac : create : true ingress : enabled : false className : \"nginx\" annotations : cert-manager.io/cluster-issuer : \"letsencrypt-prod\" hosts : - host : webapp.example.com paths : - path : / pathType : Prefix tls : - secretName : webapp-tls hosts : - webapp.example.com persistence : enabled : false storageClassName : \"\" size : 1Gi mountPath : /data monitoring : enabled : false scrapeInterval : 30s Step 4.2 - Create templates/serviceaccount.yaml {{ - if and .Values.enabled .Values.serviceAccount.create }} apiVersion : v1 kind : ServiceAccount metadata : name : {{ include \"webapp.fullname\" . }} namespace : {{ .Release.Namespace }} labels : {{ - include \"webapp.labels\" . | nindent 4 }} {{ - with .Values.serviceAccount.annotations }} annotations : {{ - toYaml . | nindent 4 }} {{ - end }} {{ - end }} Step 4.3 - Create templates/role.yaml {{ - if and .Values.enabled .Values.rbac.create .Values.serviceAccount.create }} apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : {{ include \"webapp.fullname\" . }} namespace : {{ .Release.Namespace }} labels : {{ - include \"webapp.labels\" . | nindent 4 }} rules : - apiGroups : [ \"\" ] resources : [ \"pods\" ] verbs : [ \"get\" , \"list\" , \"watch\" ] - apiGroups : [ \"\" ] resources : [ \"configmaps\" ] verbs : [ \"get\" , \"list\" ] {{ - end }} Step 4.4 - Create templates/rolebinding.yaml {{ - if and .Values.enabled .Values.rbac.create .Values.serviceAccount.create }} apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : {{ include \"webapp.fullname\" . }} namespace : {{ .Release.Namespace }} labels : {{ - include \"webapp.labels\" . | nindent 4 }} roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : {{ include \"webapp.fullname\" . }} subjects : - kind : ServiceAccount name : {{ include \"webapp.fullname\" . }} namespace : {{ .Release.Namespace }} {{ - end }} Step 4.5 - Create templates/ingress.yaml {{ - if and .Values.enabled .Values.ingress.enabled }} apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : {{ include \"webapp.fullname\" . }} namespace : {{ .Release.Namespace }} labels : {{ - include \"webapp.labels\" . | nindent 4 }} {{ - with .Values.ingress.annotations }} annotations : {{ - toYaml . | nindent 4 }} {{ - end }} spec : {{ - if .Values.ingress.className }} ingressClassName : {{ .Values.ingress.className }} {{ - end }} {{ - if .Values.ingress.tls }} tls : {{ - range .Values.ingress.tls }} - hosts : {{ - range .hosts }} - {{ . | quote }} {{ - end }} secretName : {{ .secretName }} {{ - end }} {{ - end }} rules : {{ - range .Values.ingress.hosts }} - host : {{ .host | quote }} http : paths : {{ - range .paths }} - path : {{ .path }} pathType : {{ .pathType }} backend : service : name : {{ include \"webapp.fullname\" $ }} port : number : {{ $.Values.servicePort }} {{ - end }} {{ - end }} {{ - end }} Step 4.6 - Update templates/deployment.yaml Add ServiceAccount and monitoring annotations: spec : {{ - if .Values.serviceAccount.create }} serviceAccountName : {{ include \"webapp.fullname\" . }} automountServiceAccountToken : true {{ - end }} {{ - if .Values.monitoring.enabled }} podAnnotations : prometheus.io/scrape : \"true\" prometheus.io/port : \"8080\" prometheus.io/path : \"/metrics\" {{ - end }} Validation # Enable features incrementally helm install my-app . --set rbac.create = true # Verify RBAC kubectl get role,rolebinding # Enable Ingress helm upgrade my-app . --set ingress.enabled = true --set ingress.hosts [ 0 ] .host = app.local kubectl get ingress # Verify conditional logic helm template . --set rbac.create = false | grep -c \"kind: Role\" # Should be 0 helm template . --set rbac.create = true | grep -c \"kind: Role\" # Should be 1 Concepts Introduced and , or , not operators with statement for scoping Multiple conditional levels RBAC templating Ingress with TLS Optional features pattern Phase 5: Secrets & Sensitive Data Duration: 45 minutes | Difficulty: Intermediate Objective Safely handle sensitive information and credential management. What You'll Learn Secret templating Best practices for sensitive data Base64 encoding Multiple secret sources Requirements Step 5.1 - Update values.yaml # ... previous values ... secrets : create : false database : username : admin password : \"changeme\" api : key : \"your-api-key-here\" secret : \"your-api-secret\" externalSecrets : enabled : false backend : vault secretStore : vault-backend secretPath : secret/data/webapp Step 5.2 - Create templates/secret.yaml {{ - if and .Values.enabled .Values.secrets.create }} apiVersion : v1 kind : Secret metadata : name : {{ include \"webapp.fullname\" . }} namespace : {{ .Release.Namespace }} labels : {{ - include \"webapp.labels\" . | nindent 4 }} type : Opaque data : {{ - range $key , $secret : = .Values.secrets }} {{ - if and (not (eq $key \"create\")) (ne $secret nil) }} {{ - range $subkey , $value : = $secret }} {{ $key }} -{{ $subkey }} : {{ $value | b64enc | quote }} {{ - end }} {{ - end }} {{ - end }} {{ - end }} Step 5.3 - Create templates/externalsecrets.yaml {{ - if and .Values.enabled .Values.externalSecrets.enabled }} apiVersion : external-secrets.io/v1beta1 kind : SecretStore metadata : name : {{ include \"webapp.fullname\" . }} namespace : {{ .Release.Namespace }} spec : provider : vault : server : \"https://vault.example.com\" path : \"{{ .Values.externalSecrets.secretPath }}\" auth : kubernetes : mountPath : \"kubernetes\" role : \"{{ .Release.Name }}\" --- apiVersion : external-secrets.io/v1beta1 kind : ExternalSecret metadata : name : {{ include \"webapp.fullname\" . }} namespace : {{ .Release.Namespace }} spec : refreshInterval : 1h secretStoreRef : name : {{ include \"webapp.fullname\" . }} kind : SecretStore target : name : {{ include \"webapp.fullname\" . }} -external creationPolicy : Owner data : - secretKey : database-password remoteRef : key : database property : password - secretKey : api-key remoteRef : key : api property : key {{ - end }} Step 5.4 - Update templates/deployment.yaml Add secret injection: {{ - if or .Values.secrets.create .Values.externalSecrets.enabled }} envFrom : - secretRef : {{ - if .Values.externalSecrets.enabled }} name : {{ include \"webapp.fullname\" . }} -external {{ - else }} name : {{ include \"webapp.fullname\" . }} {{ - end }} {{ - end }} volumeMounts : - name : secret-volume mountPath : /etc/secrets readOnly : true {{ - end }} volumes : {{ - if or .Values.secrets.create .Values.externalSecrets.enabled }} - name : secret-volume secret : {{ - if .Values.externalSecrets.enabled }} secretName : {{ include \"webapp.fullname\" . }} -external {{ - else }} secretName : {{ include \"webapp.fullname\" . }} {{ - end }} {{ - end }} Validation # Create secrets in values helm install my-app . --set secrets.create = true --set-string secrets.database.password = 'mysecretpass' # Verify secret created kubectl get secret my-app-webapp -o yaml | grep database-username # Decode and verify kubectl get secret my-app-webapp -o jsonpath = '{.data.database-password}' | base64 -d # Test external secrets helm install my-app . --set externalSecrets.enabled = true kubectl get externalsecrets Concepts Introduced b64enc function for base64 encoding Secret creation patterns External Secrets operator Conditional secret sources Volume mounting secrets Phase 6: Storage & Persistence Duration: 45 minutes | Difficulty: Intermediate-Advanced Objective Implement persistent storage and volume management. What You'll Learn PersistentVolumeClaim templating Storage class configuration StatefulSet basics Volume management patterns Requirements Step 6.1 - Update values.yaml # ... previous values ... persistence : enabled : false type : pvc # pvc or emptyDir storageClassName : standard accessMode : ReadWriteOnce size : 5Gi mountPath : /data subPath : \"\" # For StatefulSet-style storage volumeClaimTemplates : - name : data size : 5Gi mountPath : /data emptyDir : enabled : false sizeLimit : 1Gi Step 6.2 - Create templates/pvc.yaml {{ - if and .Values.enabled .Values.persistence.enabled (eq .Values.persistence.type \"pvc\") }} apiVersion : v1 kind : PersistentVolumeClaim metadata : name : {{ include \"webapp.fullname\" . }} -data namespace : {{ .Release.Namespace }} labels : {{ - include \"webapp.labels\" . | nindent 4 }} spec : accessModes : - {{ .Values.persistence.accessMode }} storageClassName : {{ .Values.persistence.storageClassName }} resources : requests : storage : {{ .Values.persistence.size }} {{ - end }} Step 6.3 - Update templates/deployment.yaml Add volume mounts: volumeMounts : {{ - if .Values.persistence.enabled }} {{ - if eq .Values.persistence.type \"pvc\" }} - name : data mountPath : {{ .Values.persistence.mountPath }} {{ - if .Values.persistence.subPath }} subPath : {{ .Values.persistence.subPath }} {{ - end }} {{ - end }} {{ - end }} {{ - if .Values.emptyDir.enabled }} - name : cache mountPath : /cache {{ - end }} volumes : {{ - if and .Values.persistence.enabled (eq .Values.persistence.type \"pvc\") }} - name : data persistentVolumeClaim : claimName : {{ include \"webapp.fullname\" . }} -data {{ - end }} {{ - if .Values.emptyDir.enabled }} - name : cache emptyDir : sizeLimit : {{ .Values.emptyDir.sizeLimit }} {{ - end }} Validation # Enable persistence helm install my-app . --set persistence.enabled = true # Verify PVC kubectl get pvc kubectl describe pvc my-app-webapp-data # Check volume mounted in pod kubectl exec deployment/my-app-webapp -- ls -la /data # Write data and verify persistence kubectl exec deployment/my-app-webapp -- sh -c 'echo \"test\" > /data/test.txt' kubectl delete pod -l app.kubernetes.io/instance = my-app kubectl exec deployment/my-app-webapp -- cat /data/test.txt # Should still exist Concepts Introduced PersistentVolumeClaim templates Storage class configuration EmptyDir volumes Volume mounting patterns Data persistence verification Phase 7: Scaling & Performance Duration: 60 minutes | Difficulty: Intermediate-Advanced Objective Implement autoscaling, resource management, and performance tuning. What You'll Learn HorizontalPodAutoscaler Resource requests and limits PodDisruptionBudget Pod affinity Requirements Step 7.1 - Update values.yaml # ... previous values ... resources : limits : cpu : 500m memory : 512Mi requests : cpu : 250m memory : 256Mi autoscaling : enabled : false minReplicas : 2 maxReplicas : 10 targetCPUUtilizationPercentage : 80 targetMemoryUtilizationPercentage : 80 podDisruptionBudget : enabled : false minAvailable : 1 # maxUnavailable: 1 affinity : podAntiAffinity : soft # soft or hard nodeAffinity : {} # Example nodeAffinity: # requiredDuringSchedulingIgnoredDuringExecution: # nodeSelectorTerms: # - matchExpressions: # - key: node-role.kubernetes.io/master # operator: DoesNotExist topologySpreadConstraints : enabled : false maxSkew : 1 topologyKey : kubernetes.io/hostname Step 7.2 - Create templates/hpa.yaml {{ - if and .Values.enabled .Values.autoscaling.enabled }} apiVersion : autoscaling/v2 kind : HorizontalPodAutoscaler metadata : name : {{ include \"webapp.fullname\" . }} namespace : {{ .Release.Namespace }} labels : {{ - include \"webapp.labels\" . | nindent 4 }} spec : scaleTargetRef : apiVersion : apps/v1 kind : Deployment name : {{ include \"webapp.fullname\" . }} minReplicas : {{ .Values.autoscaling.minReplicas }} maxReplicas : {{ .Values.autoscaling.maxReplicas }} metrics : {{ - if .Values.autoscaling.targetCPUUtilizationPercentage }} - type : Resource resource : name : cpu target : type : Utilization averageUtilization : {{ .Values.autoscaling.targetCPUUtilizationPercentage }} {{ - end }} {{ - if .Values.autoscaling.targetMemoryUtilizationPercentage }} - type : Resource resource : name : memory target : type : Utilization averageUtilization : {{ .Values.autoscaling.targetMemoryUtilizationPercentage }} {{ - end }} behavior : scaleDown : stabilizationWindowSeconds : 300 policies : - type : Percent value : 50 periodSeconds : 60 scaleUp : stabilizationWindowSeconds : 0 policies : - type : Percent value : 100 periodSeconds : 30 {{ - end }} Step 7.3 - Create templates/pdb.yaml {{ - if and .Values.enabled .Values.podDisruptionBudget.enabled }} apiVersion : policy/v1 kind : PodDisruptionBudget metadata : name : {{ include \"webapp.fullname\" . }} namespace : {{ .Release.Namespace }} labels : {{ - include \"webapp.labels\" . | nindent 4 }} spec : {{ - if .Values.podDisruptionBudget.minAvailable }} minAvailable : {{ .Values.podDisruptionBudget.minAvailable }} {{ - end }} {{ - if .Values.podDisruptionBudget.maxUnavailable }} maxUnavailable : {{ .Values.podDisruptionBudget.maxUnavailable }} {{ - end }} selector : matchLabels : {{ - include \"webapp.selectorLabels\" . | nindent 6 }} {{ - end }} Step 7.4 - Update templates/deployment.yaml Add resource limits, affinity, and topology spread: spec : replicas : {{ .Values.replicaCount }} selector : matchLabels : {{ - include \"webapp.selectorLabels\" . | nindent 6 }} template : metadata : labels : {{ - include \"webapp.selectorLabels\" . | nindent 8 }} spec : {{ - if .Values.affinity.podAntiAffinity }} affinity : podAntiAffinity : {{ - if eq .Values.affinity.podAntiAffinity \"hard\" }} requiredDuringSchedulingIgnoredDuringExecution : {{ - else }} preferredDuringSchedulingIgnoredDuringExecution : {{ - end }} - podAffinityTerm : labelSelector : matchExpressions : - key : app.kubernetes.io/name operator : In values : - {{ .Chart.Name }} topologyKey : kubernetes.io/hostname {{ - if eq .Values.affinity.podAntiAffinity \"soft\" }} weight : 100 {{ - end }} {{ - with .Values.affinity.nodeAffinity }} nodeAffinity : {{ - toYaml . | nindent 10 }} {{ - end }} {{ - end }} {{ - if .Values.topologySpreadConstraints.enabled }} topologySpreadConstraints : - maxSkew : {{ .Values.topologySpreadConstraints.maxSkew }} topologyKey : {{ .Values.topologySpreadConstraints.topologyKey }} whenUnsatisfiable : DoNotSchedule labelSelector : matchLabels : {{ - include \"webapp.selectorLabels\" . | nindent 12 }} {{ - end }} containers : - name : {{ .Chart.Name }} resources : {{ - toYaml .Values.resources | nindent 10 }} Validation # Enable autoscaling helm install my-app . --set autoscaling.enabled = true --set autoscaling.minReplicas = 2 # Verify HPA kubectl get hpa kubectl describe hpa my-app-webapp # Enable PDB helm upgrade my-app . --set podDisruptionBudget.enabled = true # Check resource requests kubectl get pods -o json | jq '.items[0].spec.containers[0].resources' # Simulate load and watch HPA scale kubectl run -it --rm load-generator --image = busybox /bin/sh # Inside: while true; do wget -q -O- http://my-app-webapp:8080; done # Watch HPA respond kubectl get hpa -w Concepts Introduced HorizontalPodAutoscaler (v2) Resource requests and limits PodDisruptionBudget Pod affinity and anti-affinity Topology spread constraints Scaling policies and behavior Phase 8: Monitoring & Observability Duration: 60 minutes | Difficulty: Intermediate-Advanced Objective Integrate monitoring, logging, and observability features. What You'll Learn Prometheus integration Service Monitor templating Logging configuration Tracing support Requirements Step 8.1 - Update values.yaml # ... previous values ... monitoring : enabled : false serviceMonitor : enabled : false interval : 30s scrapeTimeout : 10s prometheus : rules : enabled : false logging : enabled : false level : INFO format : json tracing : enabled : false jaeger : enabled : false agent : jaeger-agent port : 6831 Step 8.2 - Create templates/servicemonitor.yaml {{ - if and .Values.enabled .Values.monitoring.serviceMonitor.enabled }} apiVersion : monitoring.coreos.com/v1 kind : ServiceMonitor metadata : name : {{ include \"webapp.fullname\" . }} namespace : {{ .Release.Namespace }} labels : {{ - include \"webapp.labels\" . | nindent 4 }} spec : selector : matchLabels : {{ - include \"webapp.selectorLabels\" . | nindent 6 }} endpoints : - port : http interval : {{ .Values.monitoring.serviceMonitor.interval }} scrapeTimeout : {{ .Values.monitoring.serviceMonitor.scrapeTimeout }} path : /metrics {{ - end }} Step 8.3 - Create templates/prometheusrule.yaml {{ - if and .Values.enabled .Values.monitoring.prometheus.rules.enabled }} apiVersion : monitoring.coreos.com/v1 kind : PrometheusRule metadata : name : {{ include \"webapp.fullname\" . }} namespace : {{ .Release.Namespace }} labels : {{ - include \"webapp.labels\" . | nindent 4 }} prometheus : kube-prometheus spec : groups : - name : {{ include \"webapp.fullname\" . }} interval : {{ .Values.monitoring.serviceMonitor.interval }} rules : - alert : {{ include \"webapp.name\" . | upper }} HighErrorRate expr : | (sum(rate(http_requests_total{job=\"{{ include \"webapp.fullname\" . }}\", status=~\"5..\"}[5m])) / sum(rate(http_requests_total{job=\"{{ include \"webapp.fullname\" . }}\"}[5m]))) > 0.05 for : 5m labels : severity : warning annotations : summary : \"High error rate detected\" description : \"{{ include \" webapp.name\" . }} error rate is above 5%\" {{ - end }} Step 8.4 - Update templates/deployment.yaml Add monitoring annotations and environment: metadata : {{ - if .Values.monitoring.enabled }} annotations : prometheus.io/scrape : \"true\" prometheus.io/port : \"{{ .Values.containerPort }}\" prometheus.io/path : \"/metrics\" {{ - end }} labels : {{ - include \"webapp.selectorLabels\" . | nindent 8 }} spec : containers : - name : {{ .Chart.Name }} {{ - if or .Values.logging.enabled .Values.tracing.enabled }} env : {{ - if .Values.logging.enabled }} - name : LOG_LEVEL value : {{ .Values.logging.level | quote }} - name : LOG_FORMAT value : {{ .Values.logging.format | quote }} {{ - end }} {{ - if and .Values.tracing.enabled .Values.tracing.jaeger.enabled }} - name : JAEGER_AGENT_HOST value : {{ .Values.tracing.jaeger.agent | quote }} - name : JAEGER_AGENT_PORT value : {{ .Values.tracing.jaeger.port | quote }} {{ - end }} {{ - end }} Validation # Enable monitoring helm install my-app . --set monitoring.enabled = true --set monitoring.serviceMonitor.enabled = true # Verify ServiceMonitor kubectl get servicemonitor kubectl describe servicemonitor my-app-webapp # Enable logging helm upgrade my-app . --set logging.enabled = true --set logging.level = DEBUG # Verify pod annotations kubectl get pods -o jsonpath = '{.items[0].metadata.annotations}' | jq . # Check prometheus targets (if Prometheus is running) kubectl port-forward -n prometheus svc/prometheus 9090 :9090 # Visit http://localhost:9090/targets Concepts Introduced ServiceMonitor for Prometheus PrometheusRule for alerting Monitoring annotations Logging configuration Tracing integration Custom Resource integration Phase 9: Lifecycle & Hooks Duration: 45 minutes | Difficulty: Advanced Objective Control application lifecycle with pre/post hooks and tests. What You'll Learn Helm hooks (pre-install, post-install, etc.) Hook weights and deletion policies Helm tests Job templates Requirements Step 9.1 - Update values.yaml # ... previous values ... hooks : preInstall : enabled : false image : busybox command : [ \"sh\" , \"-c\" , \"echo 'Pre-install checks'\" ] postInstall : enabled : false image : busybox command : [ \"sh\" , \"-c\" , \"echo 'Post-install setup'\" ] preUpgrade : enabled : false image : busybox command : [ \"sh\" , \"-c\" , \"echo 'Pre-upgrade validation'\" ] tests : enabled : false image : busybox Step 9.2 - Create templates/hooks/pre-install.yaml {{ - if and .Values.enabled .Values.hooks.preInstall.enabled }} apiVersion : batch/v1 kind : Job metadata : name : {{ include \"webapp.fullname\" . }} -pre-install namespace : {{ .Release.Namespace }} labels : {{ - include \"webapp.labels\" . | nindent 4 }} annotations : \"helm.sh/hook\" : pre-install \"helm.sh/hook-weight\" : \"-5\" \"helm.sh/hook-delete-policy\" : before-hook-creation,hook-succeeded spec : template : metadata : labels : {{ - include \"webapp.selectorLabels\" . | nindent 8 }} spec : serviceAccountName : {{ include \"webapp.fullname\" . }} containers : - name : pre-install image : {{ .Values.hooks.preInstall.image }} command : {{ .Values.hooks.preInstall.command | toJson }} restartPolicy : Never backoffLimit : 3 {{ - end }} Step 9.3 - Create templates/tests/test-connection.yaml {{ - if and .Values.enabled .Values.tests.enabled }} apiVersion : v1 kind : Pod metadata : name : {{ include \"webapp.fullname\" . }} -test-connection namespace : {{ .Release.Namespace }} labels : {{ - include \"webapp.labels\" . | nindent 4 }} annotations : \"helm.sh/hook\" : test \"helm.sh/hook-delete-policy\" : before-hook-creation,hook-succeeded spec : containers : - name : wget image : {{ .Values.tests.image }} command : [ 'wget' ] args : [ '{{ include \"webapp.fullname\" . }}:{{ .Values.servicePort }}' ] restartPolicy : Never {{ - end }} Validation # Enable hooks helm install my-app . --set hooks.preInstall.enabled = true # Watch hook execution kubectl get jobs kubectl logs job/my-app-webapp-pre-install # Enable tests helm install my-app . --set tests.enabled = true # Run tests helm test my-app # Check test pod status kubectl get pods -l app.kubernetes.io/instance = my-app Concepts Introduced Helm hooks (pre-install, post-install, pre-upgrade, etc.) Hook weights and execution order Hook deletion policies Helm test pods Job templates for hooks Phase 10: Production Readiness & Best Practices Duration: 90 minutes | Difficulty: Advanced Objective Implement complete production-grade features and best practices. What You'll Learn Chart validation and linting NOTES.txt documentation Chart values schema Security best practices Upgrade strategies Requirements Step 10.1 - Create Chart.yaml enhancements apiVersion : v2 name : webapp description : Production-Grade Web Application Helm Chart type : application version : 1.0.0 appVersion : \"1.0\" keywords : - helm - practice - kubernetes - production home : https://github.com/example/webapp sources : - https://github.com/example/webapp maintainers : - name : Your Name email : your@email.com url : https://github.com/yourname dependencies : [] annotations : category : Application licenses : MIT Step 10.2 - Create values.schema.json { \"$schema\" : \"https://json-schema.org/draft-07/schema\" , \"type\" : \"object\" , \"required\" : [ \"enabled\" , \"replicaCount\" , \"image\" ], \"properties\" : { \"enabled\" : { \"type\" : \"boolean\" , \"description\" : \"Enable or disable the chart\" }, \"replicaCount\" : { \"type\" : \"integer\" , \"minimum\" : 1 , \"maximum\" : 100 , \"description\" : \"Number of replicas\" }, \"image\" : { \"type\" : \"string\" , \"pattern\" : \"^[a-z0-9-]+:[a-zA-Z0-9.-]+$\" , \"description\" : \"Docker image in format: name:tag\" }, \"servicePort\" : { \"type\" : \"integer\" , \"minimum\" : 1 , \"maximum\" : 65535 , \"description\" : \"Service port number\" }, \"resources\" : { \"type\" : \"object\" , \"properties\" : { \"limits\" : { \"type\" : \"object\" , \"properties\" : { \"cpu\" : { \"type\" : \"string\" , \"pattern\" : \"^[0-9]+m?$\" }, \"memory\" : { \"type\" : \"string\" , \"pattern\" : \"^[0-9]+Mi$|^[0-9]+Gi$\" } } } } }, \"autoscaling\" : { \"type\" : \"object\" , \"properties\" : { \"enabled\" : { \"type\" : \"boolean\" }, \"minReplicas\" : { \"type\" : \"integer\" , \"minimum\" : 1 }, \"maxReplicas\" : { \"type\" : \"integer\" , \"minimum\" : 1 } } } } } Step 10.3 - Create templates/NOTES.txt 1. Get the application URL by running these commands: {{- if .Values.ingress.enabled }} {{- range .Values.ingress.hosts }} {{- range .paths }} http{{ if $.Values.ingress.tls }}s{{ end }}://{{ .host }}{{ .path }} {{- end }} {{- end }} {{- else if contains \"NodePort\" .Values.serviceType }} export NODE_PORT=$(kubectl get --namespace {{ .Release.Namespace }} -o jsonpath=\"{.spec.ports[0].nodePort}\" services {{ include \"webapp.fullname\" . }}) export NODE_IP=$(kubectl get nodes --namespace {{ .Release.Namespace }} -o jsonpath=\"{.items[0].status.addresses[0].address}\") echo http://$NODE_IP:$NODE_PORT {{- else if contains \"LoadBalancer\" .Values.serviceType }} NOTE: It may take a few minutes for the LoadBalancer IP to be available. You can watch the status of by running 'kubectl get svc -w {{ include \"webapp.fullname\" . }}' export SERVICE_IP=$(kubectl get svc --namespace {{ .Release.Namespace }} {{ include \"webapp.fullname\" . }} --template \"{{\"{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}\"}}\") echo http://$SERVICE_IP:{{ .Values.servicePort }} {{- else if eq .Values.serviceType \"ClusterIP\" }} export POD_NAME=$(kubectl get pods --namespace {{ .Release.Namespace }} -l \"app.kubernetes.io/name={{ include \"webapp.name\" . }},app.kubernetes.io/instance={{ .Release.Name }}\" -o jsonpath=\"{.items[0].metadata.name}\") export CONTAINER_PORT=$(kubectl get pod --namespace {{ .Release.Namespace }} $POD_NAME -o jsonpath=\"{.spec.containers[0].ports[0].containerPort}\") echo \"Visit http://127.0.0.1:8080 to use your application\" kubectl --namespace {{ .Release.Namespace }} port-forward $POD_NAME 8080:$CONTAINER_PORT {{- end }} 2. Watch the deployment rollout status: kubectl rollout status deployment/{{ include \"webapp.fullname\" . }} --namespace {{ .Release.Namespace }} 3. Check the pod logs: kubectl logs -f deployment/{{ include \"webapp.fullname\" . }} --namespace {{ .Release.Namespace }} 4. Get more information: kubectl get deployment --namespace {{ .Release.Namespace }} kubectl get service --namespace {{ .Release.Namespace }} helm status {{ .Release.Name }} {{- if .Values.autoscaling.enabled }} 5. Autoscaling is enabled: kubectl get hpa {{ include \"webapp.fullname\" . }} --namespace {{ .Release.Namespace }} -w {{- end }} {{- if .Values.ingress.enabled }} 6. Ingress is enabled: kubectl get ingress {{ include \"webapp.fullname\" . }} --namespace {{ .Release.Namespace }} {{- end }} {{- if .Values.persistence.enabled }} 7. Persistence is enabled: kubectl get pvc --namespace {{ .Release.Namespace }} -l app.kubernetes.io/instance={{ .Release.Name }} {{- end }} For help and documentation, visit: https://github.com/example/webapp Step 10.4 - Update values.yaml with comprehensive defaults Ensure complete, documented values.yaml with: # Application Configuration appName : webapp enabled : true nameOverride : \"\" fullnameOverride : \"\" # Deployment Configuration replicaCount : 2 image : busybox:latest imagePullPolicy : IfNotPresent containerPort : 8080 servicePort : 8080 serviceType : ClusterIP command : - /bin/sh - -c - echo \"App started\" && sleep 3600 # ConfigMaps and Secrets configMaps : app-config : data : LOG_LEVEL : \"INFO\" secrets : create : false database : username : admin password : \"changeme\" # Service Account & RBAC serviceAccount : create : true name : \"\" annotations : {} rbac : create : true # Ingress ingress : enabled : false className : \"nginx\" annotations : {} hosts : - host : webapp.example.com paths : - path : / pathType : Prefix tls : [] # Storage persistence : enabled : false type : pvc storageClassName : standard accessMode : ReadWriteOnce size : 5Gi mountPath : /data subPath : \"\" emptyDir : enabled : false sizeLimit : 1Gi # Resource Management resources : limits : cpu : 500m memory : 512Mi requests : cpu : 250m memory : 256Mi # Scaling autoscaling : enabled : false minReplicas : 2 maxReplicas : 10 targetCPUUtilizationPercentage : 80 targetMemoryUtilizationPercentage : 80 podDisruptionBudget : enabled : false minAvailable : 1 # Affinity affinity : podAntiAffinity : soft nodeAffinity : {} topologySpreadConstraints : enabled : false maxSkew : 1 topologyKey : kubernetes.io/hostname # Monitoring monitoring : enabled : false serviceMonitor : enabled : false interval : 30s scrapeTimeout : 10s prometheus : rules : enabled : false # Logging logging : enabled : false level : INFO format : json # Tracing tracing : enabled : false jaeger : enabled : false agent : jaeger-agent port : 6831 # Hooks hooks : preInstall : enabled : false image : busybox command : [ \"echo\" , \"Pre-install\" ] postInstall : enabled : false image : busybox command : [ \"echo\" , \"Post-install\" ] preUpgrade : enabled : false image : busybox command : [ \"echo\" , \"Pre-upgrade\" ] # Tests tests : enabled : false image : busybox Step 10.5 - Create README.md # Webapp Helm Chart Production-ready Helm chart for deploying web applications on Kubernetes. ## Prerequisites - Kubernetes 1.21+ - Helm 3.0+ ## Installation ```bash helm repo add myrepo https://example.com/charts helm repo update helm install my-app myrepo/webapp Configuration See values.yaml for all available options. Quick start: # Basic installation helm install my-app . # With autoscaling helm install my-app . --set autoscaling.enabled = true # With ingress helm install my-app . --set ingress.enabled = true --set ingress.hosts [ 0 ] .host = app.example.com # With persistence helm install my-app . --set persistence.enabled = true Features \u2705 ConfigMap and Secret management \u2705 ServiceAccount and RBAC \u2705 Horizontal Pod Autoscaler \u2705 Pod Disruption Budget \u2705 Ingress support \u2705 Persistent storage \u2705 Prometheus monitoring \u2705 Health checks (liveness, readiness, startup) \u2705 Helm tests \u2705 Lifecycle hooks Upgrading helm upgrade my-app . --values new-values.yaml Uninstalling helm uninstall my-app Validation # Lint the chart helm lint . # Dry-run installation helm install my-app . --dry-run --debug # Validate values helm template my-app . --validate # Run tests helm test my-app License MIT ### Validation ```bash # Lint the chart helm lint . # Validate against schema helm template . --validate # Dry-run comprehensive install with all features helm install my-app . \\ --set autoscaling.enabled=true \\ --set persistence.enabled=true \\ --set monitoring.enabled=true \\ --set ingress.enabled=true \\ --set rbac.create=true \\ --set secrets.create=true \\ --dry-run --debug # Real installation helm install my-app . # Verify all resources kubectl get all -l app.kubernetes.io/instance=my-app # Run tests helm test my-app # Check release info helm history my-app helm get values my-app helm get manifest my-app Concepts Introduced Chart linting and validation JSON schema validation NOTES.txt template README documentation Chart versioning strategy Production-grade defaults Comprehensive testing Summary & Mastery Checklist What You've Built A single, production-ready Helm chart that: \u2705 Starts from absolute basics (Phase 1) \u2705 Grows to include complex features (Phase 2-9) \u2705 Implements best practices (Phase 10) \u2705 Maintains backward compatibility throughout \u2705 Validates with helm lint and helm template \u2705 Includes comprehensive documentation \u2705 Supports real-world deployment patterns Helm Concepts Mastered Templating: - {{ }} interpolation - | piping and filters - if/else/end conditionals - range iteration - Named templates ( define ) - Template helpers Functions: - quote , nindent , toYaml , tojson - upper , lower , title , trimSuffix - default , required - include for template reuse - b64enc for encoding - trunc for string truncation Advanced Patterns: - Conditional resources (feature flags) - Dynamic resource generation - Multi-tier templating with helpers - Scoping with $ and with - Proper YAML indentation Kubernetes Objects: - Deployment, Service, ConfigMap, Secret - ServiceAccount, Role, RoleBinding - Ingress, PersistentVolumeClaim - HorizontalPodAutoscaler, PodDisruptionBudget - ServiceMonitor, PrometheusRule (CRDs) - Jobs (for hooks) Helm-Specific: - Chart metadata and versioning - Release management - Hooks (pre-install, post-install, etc.) - Tests - Values schema validation - Chart linting Practice Goals By completing all phases, you can: \u2705 Create charts from scratch \u2705 Use proper templating patterns \u2705 Implement feature flags and conditionals \u2705 Manage configuration and secrets safely \u2705 Scale applications with HPA \u2705 Implement health checks \u2705 Add persistence to applications \u2705 Integrate monitoring and observability \u2705 Control application lifecycle with hooks \u2705 Deploy production-grade applications Testing & Validation Throughout At every phase , use: # Template rendering helm template my-release . # Dry-run installation (no actual deployment) helm install --dry-run --debug my-release . # Linting helm lint . # Actual deployment helm install my-release . # Verification kubectl get all -l app.kubernetes.io/instance = my-release kubectl describe deployment my-release-webapp kubectl logs deployment/my-release-webapp # Upgrades helm upgrade my-release . --set replicaCount = 5 # Check history helm history my-release helm get values my-release helm get manifest my-release Pro Tips for Success Go slowly - Complete one phase before moving to the next Test constantly - Use helm template and --dry-run liberally Read error messages - They guide you to the problem Keep helpers organized - Use _helpers.tpl for all named templates Document your values - Add comments explaining each value Use meaningful names - Make resource names clear and descriptive Validate YAML - Use yamllint to check output Version your chart - Bump version when behavior changes Test upgrades - Ensure each phase works with upgrades Commit to git - Track all changes and understand evolution Learning Path Summary Phase 1: Foundation (15 min) \u2193 Phase 2: Workloads (30 min) \u2193 Phase 3: Configuration (45 min) \u2193 Phase 4: Advanced Logic (60 min) \u2193 Phase 5: Secrets (45 min) \u2193 Phase 6: Storage (45 min) \u2193 Phase 7: Scaling (60 min) \u2193 Phase 8: Observability (60 min) \u2193 Phase 9: Lifecycle (45 min) \u2193 Phase 10: Production Ready (90 min) Total Time: ~6 hours of hands-on learning Outcome: Complete mastery of Helm chart development for production applications. Good luck! \ud83d\ude80","title":"Question2"},{"location":"02%20helm/question2/#comprehensive-helm-chart-practice-from-zero-to-production-expert","text":"Chart Name: webapp (use this name throughout all phases) Principle: You build ONE chart progressively. Each phase adds features while maintaining backward compatibility.","title":"Comprehensive Helm Chart Practice: From Zero to Production Expert"},{"location":"02%20helm/question2/#phase-1-foundation-hello-helm","text":"Duration: 15 minutes | Difficulty: Beginner","title":"Phase 1: Foundation - Hello Helm"},{"location":"02%20helm/question2/#objective","text":"Create the absolute minimum valid Helm chart and verify it renders.","title":"Objective"},{"location":"02%20helm/question2/#what-youll-learn","text":"Chart structure and metadata Basic templating syntax Helm template rendering","title":"What You'll Learn"},{"location":"02%20helm/question2/#requirements","text":"Step 1.1 - Create Chart Structure webapp/ \u251c\u2500\u2500 Chart.yaml \u251c\u2500\u2500 values.yaml \u2514\u2500\u2500 templates/ \u2514\u2500\u2500 configmap.yaml Step 1.2 - Chart.yaml apiVersion : v2 name : webapp description : Progressive Helm Learning Chart type : application version : 0.1.0 appVersion : \"1.0\" keywords : - helm - practice - kubernetes maintainers : - name : Your Name email : your@email.com Step 1.3 - values.yaml appName : webapp enabled : true Step 1.4 - templates/configmap.yaml Create a simple ConfigMap: {{ - if .Values.enabled }} apiVersion : v1 kind : ConfigMap metadata : name : {{ .Release.Name }} -config namespace : {{ .Release.Namespace }} labels : app : {{ .Values.appName }} data : app-name : {{ .Values.appName }} chart-name : {{ .Chart.Name }} chart-version : {{ .Chart.Version }} {{ - end }}","title":"Requirements"},{"location":"02%20helm/question2/#validation","text":"# Render the template helm template my-release . # Expected output: ConfigMap resource with metadata and data # Verify disabling works helm template my-release . --set enabled = false # Expected: No output (resource not rendered)","title":"Validation"},{"location":"02%20helm/question2/#concepts-introduced","text":".Chart.* (chart metadata) .Release.* (release information) .Values.* (user-provided values) Basic conditional if/end String interpolation with {{ }}","title":"Concepts Introduced"},{"location":"02%20helm/question2/#phase-2-workloads-deployment-service","text":"Duration: 30 minutes | Difficulty: Beginner","title":"Phase 2: Workloads - Deployment &amp; Service"},{"location":"02%20helm/question2/#objective_1","text":"Deploy a functional application with Service exposure.","title":"Objective"},{"location":"02%20helm/question2/#what-youll-learn_1","text":"Deployment templating Service configuration Pod specification basics Value quoting and safety","title":"What You'll Learn"},{"location":"02%20helm/question2/#requirements_1","text":"Step 2.1 - Update values.yaml appName : webapp enabled : true replicaCount : 2 image : busybox:latest imagePullPolicy : IfNotPresent containerPort : 8080 servicePort : 8080 serviceType : ClusterIP command : - /bin/sh - -c - echo \"App started on port 8080\" && sleep 3600 nameOverride : \"\" fullnameOverride : \"\" Step 2.2 - Create templates/_helpers.tpl {{/* Generate common labels */}} {{- define \"webapp.labels\" -}} app.kubernetes.io/name: {{ .Chart.Name }} app.kubernetes.io/instance: {{ .Release.Name }} app.kubernetes.io/version: {{ .Chart.AppVersion | quote }} app.kubernetes.io/managed-by: {{ .Release.Service }} {{- end }} {{/* Generate selector labels */}} {{- define \"webapp.selectorLabels\" -}} app.kubernetes.io/name: {{ .Chart.Name }} app.kubernetes.io/instance: {{ .Release.Name }} {{- end }} {{/* Expand the name of the chart */}} {{- define \"webapp.name\" -}} {{- default .Chart.Name .Values.nameOverride | trunc 63 | trimSuffix \"-\" }} {{- end }} {{/* Expand the full name */}} {{- define \"webapp.fullname\" -}} {{- if .Values.fullnameOverride }} {{- .Values.fullnameOverride | trunc 63 | trimSuffix \"-\" }} {{- else }} {{- $name := default .Chart.Name .Values.nameOverride }} {{- if contains $name .Release.Name }} {{- .Release.Name | trunc 63 | trimSuffix \"-\" }} {{- else }} {{- printf \"%s-%s\" .Release.Name $name | trunc 63 | trimSuffix \"-\" }} {{- end }} {{- end }} {{- end }} Step 2.3 - Create templates/deployment.yaml {{ - if .Values.enabled }} apiVersion : apps/v1 kind : Deployment metadata : name : {{ include \"webapp.fullname\" . }} namespace : {{ .Release.Namespace }} labels : {{ - include \"webapp.labels\" . | nindent 4 }} spec : replicas : {{ .Values.replicaCount }} selector : matchLabels : {{ - include \"webapp.selectorLabels\" . | nindent 6 }} template : metadata : labels : {{ - include \"webapp.selectorLabels\" . | nindent 8 }} spec : containers : - name : {{ .Chart.Name }} image : {{ .Values.image | quote }} imagePullPolicy : {{ .Values.imagePullPolicy }} ports : - name : http containerPort : {{ .Values.containerPort }} protocol : TCP command : {{ - toYaml .Values.command | nindent 10 }} livenessProbe : exec : command : - /bin/sh - -c - ps aux | grep sleep || exit 1 initialDelaySeconds : 10 periodSeconds : 10 readinessProbe : exec : command : - /bin/sh - -c - ps aux | grep sleep || exit 1 initialDelaySeconds : 5 periodSeconds : 5 {{ - end }} Step 2.4 - Create templates/service.yaml {{ - if .Values.enabled }} apiVersion : v1 kind : Service metadata : name : {{ include \"webapp.fullname\" . }} namespace : {{ .Release.Namespace }} labels : {{ - include \"webapp.labels\" . | nindent 4 }} spec : type : {{ .Values.serviceType }} ports : - port : {{ .Values.servicePort }} targetPort : http protocol : TCP name : http selector : {{ - include \"webapp.selectorLabels\" . | nindent 4 }} {{ - end }}","title":"Requirements"},{"location":"02%20helm/question2/#validation_1","text":"# Verify template rendering helm template my-app . --debug # Dry-run install helm install --dry-run my-app . # Real install helm install my-app . # Verify resources kubectl get deployment,service kubectl logs deployment/my-app-webapp # Scale up helm upgrade my-app . --set replicaCount = 3 kubectl get pods","title":"Validation"},{"location":"02%20helm/question2/#concepts-introduced_1","text":"Named templates (helpers) include function nindent for proper indentation toYaml for array rendering trunc , trimSuffix string functions Deployment and Service specs Label management","title":"Concepts Introduced"},{"location":"02%20helm/question2/#phase-3-configuration-management","text":"Duration: 45 minutes | Difficulty: Beginner-Intermediate","title":"Phase 3: Configuration Management"},{"location":"02%20helm/question2/#objective_2","text":"Handle configuration through ConfigMaps and environment variables dynamically.","title":"Objective"},{"location":"02%20helm/question2/#what-youll-learn_2","text":"ConfigMap templating Environment variable injection Looping with range Nested values","title":"What You'll Learn"},{"location":"02%20helm/question2/#requirements_2","text":"Step 3.1 - Update values.yaml # ... previous values ... # Configuration Maps configMaps : app-config : data : LOG_LEVEL : \"INFO\" APP_VERSION : \"1.0\" ENVIRONMENT : \"dev\" app-settings : data : MAX_CONNECTIONS : \"100\" TIMEOUT : \"30\" # Environment variables from ConfigMap envFromConfigMaps : - name : app-config - name : app-settings # Direct environment variables env : - name : POD_NAME valueFrom : fieldRef : fieldPath : metadata.name - name : POD_NAMESPACE valueFrom : fieldRef : fieldPath : metadata.namespace Step 3.2 - Create templates/configmap.yaml {{ - if .Values.enabled }} {{ - range $cmName , $cmData : = .Values.configMaps }} --- apiVersion : v1 kind : ConfigMap metadata : name : {{ include \"webapp.fullname\" $ }} -{{ $cmName }} namespace : {{ $.Release.Namespace }} labels : {{ - include \"webapp.labels\" $ | nindent 4 }} config-type : {{ $cmName }} data : {{ - range $key , $value : = $cmData.data }} {{ $key }}: {{ $value | quote }} {{ - end }} {{ - end }} {{ - end }} Step 3.3 - Update templates/deployment.yaml Add envFrom section to containers: envFrom : {{ - range .Values.envFromConfigMaps }} - configMapRef : name : {{ include \"webapp.fullname\" . }} -{{ .name }} {{ - end }} env : {{ - range .Values.env }} - name : {{ .name }} valueFrom : fieldRef : fieldPath : {{ .valueFrom.fieldRef.fieldPath }} {{ - end }}","title":"Requirements"},{"location":"02%20helm/question2/#validation_2","text":"# Check ConfigMaps created helm install my-app . --debug kubectl get configmaps kubectl get configmap my-app-webapp-app-config -o yaml # Verify environment variables in pod kubectl exec deployment/my-app-webapp -- env | grep APP # Update config and restart helm upgrade my-app . --set configMaps.app-config.data.LOG_LEVEL = DEBUG kubectl rollout restart deployment/my-app-webapp kubectl logs deployment/my-app-webapp","title":"Validation"},{"location":"02%20helm/question2/#concepts-introduced_2","text":"range with maps and key-value pairs Multiple ConfigMaps from values envFrom and env injection Fieldref (metadata access) Variable scoping with $","title":"Concepts Introduced"},{"location":"02%20helm/question2/#phase-4-conditional-features-advanced-logic","text":"Duration: 60 minutes | Difficulty: Intermediate","title":"Phase 4: Conditional Features &amp; Advanced Logic"},{"location":"02%20helm/question2/#objective_3","text":"Control feature availability and implement complex logic conditions.","title":"Objective"},{"location":"02%20helm/question2/#what-youll-learn_3","text":"Feature flags Multiple conditionals with and , or , not ServiceAccount and RBAC Ingress configuration","title":"What You'll Learn"},{"location":"02%20helm/question2/#requirements_3","text":"Step 4.1 - Update values.yaml # ... previous values ... serviceAccount : create : true name : \"\" annotations : {} rbac : create : true ingress : enabled : false className : \"nginx\" annotations : cert-manager.io/cluster-issuer : \"letsencrypt-prod\" hosts : - host : webapp.example.com paths : - path : / pathType : Prefix tls : - secretName : webapp-tls hosts : - webapp.example.com persistence : enabled : false storageClassName : \"\" size : 1Gi mountPath : /data monitoring : enabled : false scrapeInterval : 30s Step 4.2 - Create templates/serviceaccount.yaml {{ - if and .Values.enabled .Values.serviceAccount.create }} apiVersion : v1 kind : ServiceAccount metadata : name : {{ include \"webapp.fullname\" . }} namespace : {{ .Release.Namespace }} labels : {{ - include \"webapp.labels\" . | nindent 4 }} {{ - with .Values.serviceAccount.annotations }} annotations : {{ - toYaml . | nindent 4 }} {{ - end }} {{ - end }} Step 4.3 - Create templates/role.yaml {{ - if and .Values.enabled .Values.rbac.create .Values.serviceAccount.create }} apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : {{ include \"webapp.fullname\" . }} namespace : {{ .Release.Namespace }} labels : {{ - include \"webapp.labels\" . | nindent 4 }} rules : - apiGroups : [ \"\" ] resources : [ \"pods\" ] verbs : [ \"get\" , \"list\" , \"watch\" ] - apiGroups : [ \"\" ] resources : [ \"configmaps\" ] verbs : [ \"get\" , \"list\" ] {{ - end }} Step 4.4 - Create templates/rolebinding.yaml {{ - if and .Values.enabled .Values.rbac.create .Values.serviceAccount.create }} apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : {{ include \"webapp.fullname\" . }} namespace : {{ .Release.Namespace }} labels : {{ - include \"webapp.labels\" . | nindent 4 }} roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : {{ include \"webapp.fullname\" . }} subjects : - kind : ServiceAccount name : {{ include \"webapp.fullname\" . }} namespace : {{ .Release.Namespace }} {{ - end }} Step 4.5 - Create templates/ingress.yaml {{ - if and .Values.enabled .Values.ingress.enabled }} apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : {{ include \"webapp.fullname\" . }} namespace : {{ .Release.Namespace }} labels : {{ - include \"webapp.labels\" . | nindent 4 }} {{ - with .Values.ingress.annotations }} annotations : {{ - toYaml . | nindent 4 }} {{ - end }} spec : {{ - if .Values.ingress.className }} ingressClassName : {{ .Values.ingress.className }} {{ - end }} {{ - if .Values.ingress.tls }} tls : {{ - range .Values.ingress.tls }} - hosts : {{ - range .hosts }} - {{ . | quote }} {{ - end }} secretName : {{ .secretName }} {{ - end }} {{ - end }} rules : {{ - range .Values.ingress.hosts }} - host : {{ .host | quote }} http : paths : {{ - range .paths }} - path : {{ .path }} pathType : {{ .pathType }} backend : service : name : {{ include \"webapp.fullname\" $ }} port : number : {{ $.Values.servicePort }} {{ - end }} {{ - end }} {{ - end }} Step 4.6 - Update templates/deployment.yaml Add ServiceAccount and monitoring annotations: spec : {{ - if .Values.serviceAccount.create }} serviceAccountName : {{ include \"webapp.fullname\" . }} automountServiceAccountToken : true {{ - end }} {{ - if .Values.monitoring.enabled }} podAnnotations : prometheus.io/scrape : \"true\" prometheus.io/port : \"8080\" prometheus.io/path : \"/metrics\" {{ - end }}","title":"Requirements"},{"location":"02%20helm/question2/#validation_3","text":"# Enable features incrementally helm install my-app . --set rbac.create = true # Verify RBAC kubectl get role,rolebinding # Enable Ingress helm upgrade my-app . --set ingress.enabled = true --set ingress.hosts [ 0 ] .host = app.local kubectl get ingress # Verify conditional logic helm template . --set rbac.create = false | grep -c \"kind: Role\" # Should be 0 helm template . --set rbac.create = true | grep -c \"kind: Role\" # Should be 1","title":"Validation"},{"location":"02%20helm/question2/#concepts-introduced_3","text":"and , or , not operators with statement for scoping Multiple conditional levels RBAC templating Ingress with TLS Optional features pattern","title":"Concepts Introduced"},{"location":"02%20helm/question2/#phase-5-secrets-sensitive-data","text":"Duration: 45 minutes | Difficulty: Intermediate","title":"Phase 5: Secrets &amp; Sensitive Data"},{"location":"02%20helm/question2/#objective_4","text":"Safely handle sensitive information and credential management.","title":"Objective"},{"location":"02%20helm/question2/#what-youll-learn_4","text":"Secret templating Best practices for sensitive data Base64 encoding Multiple secret sources","title":"What You'll Learn"},{"location":"02%20helm/question2/#requirements_4","text":"Step 5.1 - Update values.yaml # ... previous values ... secrets : create : false database : username : admin password : \"changeme\" api : key : \"your-api-key-here\" secret : \"your-api-secret\" externalSecrets : enabled : false backend : vault secretStore : vault-backend secretPath : secret/data/webapp Step 5.2 - Create templates/secret.yaml {{ - if and .Values.enabled .Values.secrets.create }} apiVersion : v1 kind : Secret metadata : name : {{ include \"webapp.fullname\" . }} namespace : {{ .Release.Namespace }} labels : {{ - include \"webapp.labels\" . | nindent 4 }} type : Opaque data : {{ - range $key , $secret : = .Values.secrets }} {{ - if and (not (eq $key \"create\")) (ne $secret nil) }} {{ - range $subkey , $value : = $secret }} {{ $key }} -{{ $subkey }} : {{ $value | b64enc | quote }} {{ - end }} {{ - end }} {{ - end }} {{ - end }} Step 5.3 - Create templates/externalsecrets.yaml {{ - if and .Values.enabled .Values.externalSecrets.enabled }} apiVersion : external-secrets.io/v1beta1 kind : SecretStore metadata : name : {{ include \"webapp.fullname\" . }} namespace : {{ .Release.Namespace }} spec : provider : vault : server : \"https://vault.example.com\" path : \"{{ .Values.externalSecrets.secretPath }}\" auth : kubernetes : mountPath : \"kubernetes\" role : \"{{ .Release.Name }}\" --- apiVersion : external-secrets.io/v1beta1 kind : ExternalSecret metadata : name : {{ include \"webapp.fullname\" . }} namespace : {{ .Release.Namespace }} spec : refreshInterval : 1h secretStoreRef : name : {{ include \"webapp.fullname\" . }} kind : SecretStore target : name : {{ include \"webapp.fullname\" . }} -external creationPolicy : Owner data : - secretKey : database-password remoteRef : key : database property : password - secretKey : api-key remoteRef : key : api property : key {{ - end }} Step 5.4 - Update templates/deployment.yaml Add secret injection: {{ - if or .Values.secrets.create .Values.externalSecrets.enabled }} envFrom : - secretRef : {{ - if .Values.externalSecrets.enabled }} name : {{ include \"webapp.fullname\" . }} -external {{ - else }} name : {{ include \"webapp.fullname\" . }} {{ - end }} {{ - end }} volumeMounts : - name : secret-volume mountPath : /etc/secrets readOnly : true {{ - end }} volumes : {{ - if or .Values.secrets.create .Values.externalSecrets.enabled }} - name : secret-volume secret : {{ - if .Values.externalSecrets.enabled }} secretName : {{ include \"webapp.fullname\" . }} -external {{ - else }} secretName : {{ include \"webapp.fullname\" . }} {{ - end }} {{ - end }}","title":"Requirements"},{"location":"02%20helm/question2/#validation_4","text":"# Create secrets in values helm install my-app . --set secrets.create = true --set-string secrets.database.password = 'mysecretpass' # Verify secret created kubectl get secret my-app-webapp -o yaml | grep database-username # Decode and verify kubectl get secret my-app-webapp -o jsonpath = '{.data.database-password}' | base64 -d # Test external secrets helm install my-app . --set externalSecrets.enabled = true kubectl get externalsecrets","title":"Validation"},{"location":"02%20helm/question2/#concepts-introduced_4","text":"b64enc function for base64 encoding Secret creation patterns External Secrets operator Conditional secret sources Volume mounting secrets","title":"Concepts Introduced"},{"location":"02%20helm/question2/#phase-6-storage-persistence","text":"Duration: 45 minutes | Difficulty: Intermediate-Advanced","title":"Phase 6: Storage &amp; Persistence"},{"location":"02%20helm/question2/#objective_5","text":"Implement persistent storage and volume management.","title":"Objective"},{"location":"02%20helm/question2/#what-youll-learn_5","text":"PersistentVolumeClaim templating Storage class configuration StatefulSet basics Volume management patterns","title":"What You'll Learn"},{"location":"02%20helm/question2/#requirements_5","text":"Step 6.1 - Update values.yaml # ... previous values ... persistence : enabled : false type : pvc # pvc or emptyDir storageClassName : standard accessMode : ReadWriteOnce size : 5Gi mountPath : /data subPath : \"\" # For StatefulSet-style storage volumeClaimTemplates : - name : data size : 5Gi mountPath : /data emptyDir : enabled : false sizeLimit : 1Gi Step 6.2 - Create templates/pvc.yaml {{ - if and .Values.enabled .Values.persistence.enabled (eq .Values.persistence.type \"pvc\") }} apiVersion : v1 kind : PersistentVolumeClaim metadata : name : {{ include \"webapp.fullname\" . }} -data namespace : {{ .Release.Namespace }} labels : {{ - include \"webapp.labels\" . | nindent 4 }} spec : accessModes : - {{ .Values.persistence.accessMode }} storageClassName : {{ .Values.persistence.storageClassName }} resources : requests : storage : {{ .Values.persistence.size }} {{ - end }} Step 6.3 - Update templates/deployment.yaml Add volume mounts: volumeMounts : {{ - if .Values.persistence.enabled }} {{ - if eq .Values.persistence.type \"pvc\" }} - name : data mountPath : {{ .Values.persistence.mountPath }} {{ - if .Values.persistence.subPath }} subPath : {{ .Values.persistence.subPath }} {{ - end }} {{ - end }} {{ - end }} {{ - if .Values.emptyDir.enabled }} - name : cache mountPath : /cache {{ - end }} volumes : {{ - if and .Values.persistence.enabled (eq .Values.persistence.type \"pvc\") }} - name : data persistentVolumeClaim : claimName : {{ include \"webapp.fullname\" . }} -data {{ - end }} {{ - if .Values.emptyDir.enabled }} - name : cache emptyDir : sizeLimit : {{ .Values.emptyDir.sizeLimit }} {{ - end }}","title":"Requirements"},{"location":"02%20helm/question2/#validation_5","text":"# Enable persistence helm install my-app . --set persistence.enabled = true # Verify PVC kubectl get pvc kubectl describe pvc my-app-webapp-data # Check volume mounted in pod kubectl exec deployment/my-app-webapp -- ls -la /data # Write data and verify persistence kubectl exec deployment/my-app-webapp -- sh -c 'echo \"test\" > /data/test.txt' kubectl delete pod -l app.kubernetes.io/instance = my-app kubectl exec deployment/my-app-webapp -- cat /data/test.txt # Should still exist","title":"Validation"},{"location":"02%20helm/question2/#concepts-introduced_5","text":"PersistentVolumeClaim templates Storage class configuration EmptyDir volumes Volume mounting patterns Data persistence verification","title":"Concepts Introduced"},{"location":"02%20helm/question2/#phase-7-scaling-performance","text":"Duration: 60 minutes | Difficulty: Intermediate-Advanced","title":"Phase 7: Scaling &amp; Performance"},{"location":"02%20helm/question2/#objective_6","text":"Implement autoscaling, resource management, and performance tuning.","title":"Objective"},{"location":"02%20helm/question2/#what-youll-learn_6","text":"HorizontalPodAutoscaler Resource requests and limits PodDisruptionBudget Pod affinity","title":"What You'll Learn"},{"location":"02%20helm/question2/#requirements_6","text":"Step 7.1 - Update values.yaml # ... previous values ... resources : limits : cpu : 500m memory : 512Mi requests : cpu : 250m memory : 256Mi autoscaling : enabled : false minReplicas : 2 maxReplicas : 10 targetCPUUtilizationPercentage : 80 targetMemoryUtilizationPercentage : 80 podDisruptionBudget : enabled : false minAvailable : 1 # maxUnavailable: 1 affinity : podAntiAffinity : soft # soft or hard nodeAffinity : {} # Example nodeAffinity: # requiredDuringSchedulingIgnoredDuringExecution: # nodeSelectorTerms: # - matchExpressions: # - key: node-role.kubernetes.io/master # operator: DoesNotExist topologySpreadConstraints : enabled : false maxSkew : 1 topologyKey : kubernetes.io/hostname Step 7.2 - Create templates/hpa.yaml {{ - if and .Values.enabled .Values.autoscaling.enabled }} apiVersion : autoscaling/v2 kind : HorizontalPodAutoscaler metadata : name : {{ include \"webapp.fullname\" . }} namespace : {{ .Release.Namespace }} labels : {{ - include \"webapp.labels\" . | nindent 4 }} spec : scaleTargetRef : apiVersion : apps/v1 kind : Deployment name : {{ include \"webapp.fullname\" . }} minReplicas : {{ .Values.autoscaling.minReplicas }} maxReplicas : {{ .Values.autoscaling.maxReplicas }} metrics : {{ - if .Values.autoscaling.targetCPUUtilizationPercentage }} - type : Resource resource : name : cpu target : type : Utilization averageUtilization : {{ .Values.autoscaling.targetCPUUtilizationPercentage }} {{ - end }} {{ - if .Values.autoscaling.targetMemoryUtilizationPercentage }} - type : Resource resource : name : memory target : type : Utilization averageUtilization : {{ .Values.autoscaling.targetMemoryUtilizationPercentage }} {{ - end }} behavior : scaleDown : stabilizationWindowSeconds : 300 policies : - type : Percent value : 50 periodSeconds : 60 scaleUp : stabilizationWindowSeconds : 0 policies : - type : Percent value : 100 periodSeconds : 30 {{ - end }} Step 7.3 - Create templates/pdb.yaml {{ - if and .Values.enabled .Values.podDisruptionBudget.enabled }} apiVersion : policy/v1 kind : PodDisruptionBudget metadata : name : {{ include \"webapp.fullname\" . }} namespace : {{ .Release.Namespace }} labels : {{ - include \"webapp.labels\" . | nindent 4 }} spec : {{ - if .Values.podDisruptionBudget.minAvailable }} minAvailable : {{ .Values.podDisruptionBudget.minAvailable }} {{ - end }} {{ - if .Values.podDisruptionBudget.maxUnavailable }} maxUnavailable : {{ .Values.podDisruptionBudget.maxUnavailable }} {{ - end }} selector : matchLabels : {{ - include \"webapp.selectorLabels\" . | nindent 6 }} {{ - end }} Step 7.4 - Update templates/deployment.yaml Add resource limits, affinity, and topology spread: spec : replicas : {{ .Values.replicaCount }} selector : matchLabels : {{ - include \"webapp.selectorLabels\" . | nindent 6 }} template : metadata : labels : {{ - include \"webapp.selectorLabels\" . | nindent 8 }} spec : {{ - if .Values.affinity.podAntiAffinity }} affinity : podAntiAffinity : {{ - if eq .Values.affinity.podAntiAffinity \"hard\" }} requiredDuringSchedulingIgnoredDuringExecution : {{ - else }} preferredDuringSchedulingIgnoredDuringExecution : {{ - end }} - podAffinityTerm : labelSelector : matchExpressions : - key : app.kubernetes.io/name operator : In values : - {{ .Chart.Name }} topologyKey : kubernetes.io/hostname {{ - if eq .Values.affinity.podAntiAffinity \"soft\" }} weight : 100 {{ - end }} {{ - with .Values.affinity.nodeAffinity }} nodeAffinity : {{ - toYaml . | nindent 10 }} {{ - end }} {{ - end }} {{ - if .Values.topologySpreadConstraints.enabled }} topologySpreadConstraints : - maxSkew : {{ .Values.topologySpreadConstraints.maxSkew }} topologyKey : {{ .Values.topologySpreadConstraints.topologyKey }} whenUnsatisfiable : DoNotSchedule labelSelector : matchLabels : {{ - include \"webapp.selectorLabels\" . | nindent 12 }} {{ - end }} containers : - name : {{ .Chart.Name }} resources : {{ - toYaml .Values.resources | nindent 10 }}","title":"Requirements"},{"location":"02%20helm/question2/#validation_6","text":"# Enable autoscaling helm install my-app . --set autoscaling.enabled = true --set autoscaling.minReplicas = 2 # Verify HPA kubectl get hpa kubectl describe hpa my-app-webapp # Enable PDB helm upgrade my-app . --set podDisruptionBudget.enabled = true # Check resource requests kubectl get pods -o json | jq '.items[0].spec.containers[0].resources' # Simulate load and watch HPA scale kubectl run -it --rm load-generator --image = busybox /bin/sh # Inside: while true; do wget -q -O- http://my-app-webapp:8080; done # Watch HPA respond kubectl get hpa -w","title":"Validation"},{"location":"02%20helm/question2/#concepts-introduced_6","text":"HorizontalPodAutoscaler (v2) Resource requests and limits PodDisruptionBudget Pod affinity and anti-affinity Topology spread constraints Scaling policies and behavior","title":"Concepts Introduced"},{"location":"02%20helm/question2/#phase-8-monitoring-observability","text":"Duration: 60 minutes | Difficulty: Intermediate-Advanced","title":"Phase 8: Monitoring &amp; Observability"},{"location":"02%20helm/question2/#objective_7","text":"Integrate monitoring, logging, and observability features.","title":"Objective"},{"location":"02%20helm/question2/#what-youll-learn_7","text":"Prometheus integration Service Monitor templating Logging configuration Tracing support","title":"What You'll Learn"},{"location":"02%20helm/question2/#requirements_7","text":"Step 8.1 - Update values.yaml # ... previous values ... monitoring : enabled : false serviceMonitor : enabled : false interval : 30s scrapeTimeout : 10s prometheus : rules : enabled : false logging : enabled : false level : INFO format : json tracing : enabled : false jaeger : enabled : false agent : jaeger-agent port : 6831 Step 8.2 - Create templates/servicemonitor.yaml {{ - if and .Values.enabled .Values.monitoring.serviceMonitor.enabled }} apiVersion : monitoring.coreos.com/v1 kind : ServiceMonitor metadata : name : {{ include \"webapp.fullname\" . }} namespace : {{ .Release.Namespace }} labels : {{ - include \"webapp.labels\" . | nindent 4 }} spec : selector : matchLabels : {{ - include \"webapp.selectorLabels\" . | nindent 6 }} endpoints : - port : http interval : {{ .Values.monitoring.serviceMonitor.interval }} scrapeTimeout : {{ .Values.monitoring.serviceMonitor.scrapeTimeout }} path : /metrics {{ - end }} Step 8.3 - Create templates/prometheusrule.yaml {{ - if and .Values.enabled .Values.monitoring.prometheus.rules.enabled }} apiVersion : monitoring.coreos.com/v1 kind : PrometheusRule metadata : name : {{ include \"webapp.fullname\" . }} namespace : {{ .Release.Namespace }} labels : {{ - include \"webapp.labels\" . | nindent 4 }} prometheus : kube-prometheus spec : groups : - name : {{ include \"webapp.fullname\" . }} interval : {{ .Values.monitoring.serviceMonitor.interval }} rules : - alert : {{ include \"webapp.name\" . | upper }} HighErrorRate expr : | (sum(rate(http_requests_total{job=\"{{ include \"webapp.fullname\" . }}\", status=~\"5..\"}[5m])) / sum(rate(http_requests_total{job=\"{{ include \"webapp.fullname\" . }}\"}[5m]))) > 0.05 for : 5m labels : severity : warning annotations : summary : \"High error rate detected\" description : \"{{ include \" webapp.name\" . }} error rate is above 5%\" {{ - end }} Step 8.4 - Update templates/deployment.yaml Add monitoring annotations and environment: metadata : {{ - if .Values.monitoring.enabled }} annotations : prometheus.io/scrape : \"true\" prometheus.io/port : \"{{ .Values.containerPort }}\" prometheus.io/path : \"/metrics\" {{ - end }} labels : {{ - include \"webapp.selectorLabels\" . | nindent 8 }} spec : containers : - name : {{ .Chart.Name }} {{ - if or .Values.logging.enabled .Values.tracing.enabled }} env : {{ - if .Values.logging.enabled }} - name : LOG_LEVEL value : {{ .Values.logging.level | quote }} - name : LOG_FORMAT value : {{ .Values.logging.format | quote }} {{ - end }} {{ - if and .Values.tracing.enabled .Values.tracing.jaeger.enabled }} - name : JAEGER_AGENT_HOST value : {{ .Values.tracing.jaeger.agent | quote }} - name : JAEGER_AGENT_PORT value : {{ .Values.tracing.jaeger.port | quote }} {{ - end }} {{ - end }}","title":"Requirements"},{"location":"02%20helm/question2/#validation_7","text":"# Enable monitoring helm install my-app . --set monitoring.enabled = true --set monitoring.serviceMonitor.enabled = true # Verify ServiceMonitor kubectl get servicemonitor kubectl describe servicemonitor my-app-webapp # Enable logging helm upgrade my-app . --set logging.enabled = true --set logging.level = DEBUG # Verify pod annotations kubectl get pods -o jsonpath = '{.items[0].metadata.annotations}' | jq . # Check prometheus targets (if Prometheus is running) kubectl port-forward -n prometheus svc/prometheus 9090 :9090 # Visit http://localhost:9090/targets","title":"Validation"},{"location":"02%20helm/question2/#concepts-introduced_7","text":"ServiceMonitor for Prometheus PrometheusRule for alerting Monitoring annotations Logging configuration Tracing integration Custom Resource integration","title":"Concepts Introduced"},{"location":"02%20helm/question2/#phase-9-lifecycle-hooks","text":"Duration: 45 minutes | Difficulty: Advanced","title":"Phase 9: Lifecycle &amp; Hooks"},{"location":"02%20helm/question2/#objective_8","text":"Control application lifecycle with pre/post hooks and tests.","title":"Objective"},{"location":"02%20helm/question2/#what-youll-learn_8","text":"Helm hooks (pre-install, post-install, etc.) Hook weights and deletion policies Helm tests Job templates","title":"What You'll Learn"},{"location":"02%20helm/question2/#requirements_8","text":"Step 9.1 - Update values.yaml # ... previous values ... hooks : preInstall : enabled : false image : busybox command : [ \"sh\" , \"-c\" , \"echo 'Pre-install checks'\" ] postInstall : enabled : false image : busybox command : [ \"sh\" , \"-c\" , \"echo 'Post-install setup'\" ] preUpgrade : enabled : false image : busybox command : [ \"sh\" , \"-c\" , \"echo 'Pre-upgrade validation'\" ] tests : enabled : false image : busybox Step 9.2 - Create templates/hooks/pre-install.yaml {{ - if and .Values.enabled .Values.hooks.preInstall.enabled }} apiVersion : batch/v1 kind : Job metadata : name : {{ include \"webapp.fullname\" . }} -pre-install namespace : {{ .Release.Namespace }} labels : {{ - include \"webapp.labels\" . | nindent 4 }} annotations : \"helm.sh/hook\" : pre-install \"helm.sh/hook-weight\" : \"-5\" \"helm.sh/hook-delete-policy\" : before-hook-creation,hook-succeeded spec : template : metadata : labels : {{ - include \"webapp.selectorLabels\" . | nindent 8 }} spec : serviceAccountName : {{ include \"webapp.fullname\" . }} containers : - name : pre-install image : {{ .Values.hooks.preInstall.image }} command : {{ .Values.hooks.preInstall.command | toJson }} restartPolicy : Never backoffLimit : 3 {{ - end }} Step 9.3 - Create templates/tests/test-connection.yaml {{ - if and .Values.enabled .Values.tests.enabled }} apiVersion : v1 kind : Pod metadata : name : {{ include \"webapp.fullname\" . }} -test-connection namespace : {{ .Release.Namespace }} labels : {{ - include \"webapp.labels\" . | nindent 4 }} annotations : \"helm.sh/hook\" : test \"helm.sh/hook-delete-policy\" : before-hook-creation,hook-succeeded spec : containers : - name : wget image : {{ .Values.tests.image }} command : [ 'wget' ] args : [ '{{ include \"webapp.fullname\" . }}:{{ .Values.servicePort }}' ] restartPolicy : Never {{ - end }}","title":"Requirements"},{"location":"02%20helm/question2/#validation_8","text":"# Enable hooks helm install my-app . --set hooks.preInstall.enabled = true # Watch hook execution kubectl get jobs kubectl logs job/my-app-webapp-pre-install # Enable tests helm install my-app . --set tests.enabled = true # Run tests helm test my-app # Check test pod status kubectl get pods -l app.kubernetes.io/instance = my-app","title":"Validation"},{"location":"02%20helm/question2/#concepts-introduced_8","text":"Helm hooks (pre-install, post-install, pre-upgrade, etc.) Hook weights and execution order Hook deletion policies Helm test pods Job templates for hooks","title":"Concepts Introduced"},{"location":"02%20helm/question2/#phase-10-production-readiness-best-practices","text":"Duration: 90 minutes | Difficulty: Advanced","title":"Phase 10: Production Readiness &amp; Best Practices"},{"location":"02%20helm/question2/#objective_9","text":"Implement complete production-grade features and best practices.","title":"Objective"},{"location":"02%20helm/question2/#what-youll-learn_9","text":"Chart validation and linting NOTES.txt documentation Chart values schema Security best practices Upgrade strategies","title":"What You'll Learn"},{"location":"02%20helm/question2/#requirements_9","text":"Step 10.1 - Create Chart.yaml enhancements apiVersion : v2 name : webapp description : Production-Grade Web Application Helm Chart type : application version : 1.0.0 appVersion : \"1.0\" keywords : - helm - practice - kubernetes - production home : https://github.com/example/webapp sources : - https://github.com/example/webapp maintainers : - name : Your Name email : your@email.com url : https://github.com/yourname dependencies : [] annotations : category : Application licenses : MIT Step 10.2 - Create values.schema.json { \"$schema\" : \"https://json-schema.org/draft-07/schema\" , \"type\" : \"object\" , \"required\" : [ \"enabled\" , \"replicaCount\" , \"image\" ], \"properties\" : { \"enabled\" : { \"type\" : \"boolean\" , \"description\" : \"Enable or disable the chart\" }, \"replicaCount\" : { \"type\" : \"integer\" , \"minimum\" : 1 , \"maximum\" : 100 , \"description\" : \"Number of replicas\" }, \"image\" : { \"type\" : \"string\" , \"pattern\" : \"^[a-z0-9-]+:[a-zA-Z0-9.-]+$\" , \"description\" : \"Docker image in format: name:tag\" }, \"servicePort\" : { \"type\" : \"integer\" , \"minimum\" : 1 , \"maximum\" : 65535 , \"description\" : \"Service port number\" }, \"resources\" : { \"type\" : \"object\" , \"properties\" : { \"limits\" : { \"type\" : \"object\" , \"properties\" : { \"cpu\" : { \"type\" : \"string\" , \"pattern\" : \"^[0-9]+m?$\" }, \"memory\" : { \"type\" : \"string\" , \"pattern\" : \"^[0-9]+Mi$|^[0-9]+Gi$\" } } } } }, \"autoscaling\" : { \"type\" : \"object\" , \"properties\" : { \"enabled\" : { \"type\" : \"boolean\" }, \"minReplicas\" : { \"type\" : \"integer\" , \"minimum\" : 1 }, \"maxReplicas\" : { \"type\" : \"integer\" , \"minimum\" : 1 } } } } } Step 10.3 - Create templates/NOTES.txt 1. Get the application URL by running these commands: {{- if .Values.ingress.enabled }} {{- range .Values.ingress.hosts }} {{- range .paths }} http{{ if $.Values.ingress.tls }}s{{ end }}://{{ .host }}{{ .path }} {{- end }} {{- end }} {{- else if contains \"NodePort\" .Values.serviceType }} export NODE_PORT=$(kubectl get --namespace {{ .Release.Namespace }} -o jsonpath=\"{.spec.ports[0].nodePort}\" services {{ include \"webapp.fullname\" . }}) export NODE_IP=$(kubectl get nodes --namespace {{ .Release.Namespace }} -o jsonpath=\"{.items[0].status.addresses[0].address}\") echo http://$NODE_IP:$NODE_PORT {{- else if contains \"LoadBalancer\" .Values.serviceType }} NOTE: It may take a few minutes for the LoadBalancer IP to be available. You can watch the status of by running 'kubectl get svc -w {{ include \"webapp.fullname\" . }}' export SERVICE_IP=$(kubectl get svc --namespace {{ .Release.Namespace }} {{ include \"webapp.fullname\" . }} --template \"{{\"{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}\"}}\") echo http://$SERVICE_IP:{{ .Values.servicePort }} {{- else if eq .Values.serviceType \"ClusterIP\" }} export POD_NAME=$(kubectl get pods --namespace {{ .Release.Namespace }} -l \"app.kubernetes.io/name={{ include \"webapp.name\" . }},app.kubernetes.io/instance={{ .Release.Name }}\" -o jsonpath=\"{.items[0].metadata.name}\") export CONTAINER_PORT=$(kubectl get pod --namespace {{ .Release.Namespace }} $POD_NAME -o jsonpath=\"{.spec.containers[0].ports[0].containerPort}\") echo \"Visit http://127.0.0.1:8080 to use your application\" kubectl --namespace {{ .Release.Namespace }} port-forward $POD_NAME 8080:$CONTAINER_PORT {{- end }} 2. Watch the deployment rollout status: kubectl rollout status deployment/{{ include \"webapp.fullname\" . }} --namespace {{ .Release.Namespace }} 3. Check the pod logs: kubectl logs -f deployment/{{ include \"webapp.fullname\" . }} --namespace {{ .Release.Namespace }} 4. Get more information: kubectl get deployment --namespace {{ .Release.Namespace }} kubectl get service --namespace {{ .Release.Namespace }} helm status {{ .Release.Name }} {{- if .Values.autoscaling.enabled }} 5. Autoscaling is enabled: kubectl get hpa {{ include \"webapp.fullname\" . }} --namespace {{ .Release.Namespace }} -w {{- end }} {{- if .Values.ingress.enabled }} 6. Ingress is enabled: kubectl get ingress {{ include \"webapp.fullname\" . }} --namespace {{ .Release.Namespace }} {{- end }} {{- if .Values.persistence.enabled }} 7. Persistence is enabled: kubectl get pvc --namespace {{ .Release.Namespace }} -l app.kubernetes.io/instance={{ .Release.Name }} {{- end }} For help and documentation, visit: https://github.com/example/webapp Step 10.4 - Update values.yaml with comprehensive defaults Ensure complete, documented values.yaml with: # Application Configuration appName : webapp enabled : true nameOverride : \"\" fullnameOverride : \"\" # Deployment Configuration replicaCount : 2 image : busybox:latest imagePullPolicy : IfNotPresent containerPort : 8080 servicePort : 8080 serviceType : ClusterIP command : - /bin/sh - -c - echo \"App started\" && sleep 3600 # ConfigMaps and Secrets configMaps : app-config : data : LOG_LEVEL : \"INFO\" secrets : create : false database : username : admin password : \"changeme\" # Service Account & RBAC serviceAccount : create : true name : \"\" annotations : {} rbac : create : true # Ingress ingress : enabled : false className : \"nginx\" annotations : {} hosts : - host : webapp.example.com paths : - path : / pathType : Prefix tls : [] # Storage persistence : enabled : false type : pvc storageClassName : standard accessMode : ReadWriteOnce size : 5Gi mountPath : /data subPath : \"\" emptyDir : enabled : false sizeLimit : 1Gi # Resource Management resources : limits : cpu : 500m memory : 512Mi requests : cpu : 250m memory : 256Mi # Scaling autoscaling : enabled : false minReplicas : 2 maxReplicas : 10 targetCPUUtilizationPercentage : 80 targetMemoryUtilizationPercentage : 80 podDisruptionBudget : enabled : false minAvailable : 1 # Affinity affinity : podAntiAffinity : soft nodeAffinity : {} topologySpreadConstraints : enabled : false maxSkew : 1 topologyKey : kubernetes.io/hostname # Monitoring monitoring : enabled : false serviceMonitor : enabled : false interval : 30s scrapeTimeout : 10s prometheus : rules : enabled : false # Logging logging : enabled : false level : INFO format : json # Tracing tracing : enabled : false jaeger : enabled : false agent : jaeger-agent port : 6831 # Hooks hooks : preInstall : enabled : false image : busybox command : [ \"echo\" , \"Pre-install\" ] postInstall : enabled : false image : busybox command : [ \"echo\" , \"Post-install\" ] preUpgrade : enabled : false image : busybox command : [ \"echo\" , \"Pre-upgrade\" ] # Tests tests : enabled : false image : busybox Step 10.5 - Create README.md # Webapp Helm Chart Production-ready Helm chart for deploying web applications on Kubernetes. ## Prerequisites - Kubernetes 1.21+ - Helm 3.0+ ## Installation ```bash helm repo add myrepo https://example.com/charts helm repo update helm install my-app myrepo/webapp","title":"Requirements"},{"location":"02%20helm/question2/#configuration","text":"See values.yaml for all available options. Quick start: # Basic installation helm install my-app . # With autoscaling helm install my-app . --set autoscaling.enabled = true # With ingress helm install my-app . --set ingress.enabled = true --set ingress.hosts [ 0 ] .host = app.example.com # With persistence helm install my-app . --set persistence.enabled = true","title":"Configuration"},{"location":"02%20helm/question2/#features","text":"\u2705 ConfigMap and Secret management \u2705 ServiceAccount and RBAC \u2705 Horizontal Pod Autoscaler \u2705 Pod Disruption Budget \u2705 Ingress support \u2705 Persistent storage \u2705 Prometheus monitoring \u2705 Health checks (liveness, readiness, startup) \u2705 Helm tests \u2705 Lifecycle hooks","title":"Features"},{"location":"02%20helm/question2/#upgrading","text":"helm upgrade my-app . --values new-values.yaml","title":"Upgrading"},{"location":"02%20helm/question2/#uninstalling","text":"helm uninstall my-app","title":"Uninstalling"},{"location":"02%20helm/question2/#validation_9","text":"# Lint the chart helm lint . # Dry-run installation helm install my-app . --dry-run --debug # Validate values helm template my-app . --validate # Run tests helm test my-app","title":"Validation"},{"location":"02%20helm/question2/#license","text":"MIT ### Validation ```bash # Lint the chart helm lint . # Validate against schema helm template . --validate # Dry-run comprehensive install with all features helm install my-app . \\ --set autoscaling.enabled=true \\ --set persistence.enabled=true \\ --set monitoring.enabled=true \\ --set ingress.enabled=true \\ --set rbac.create=true \\ --set secrets.create=true \\ --dry-run --debug # Real installation helm install my-app . # Verify all resources kubectl get all -l app.kubernetes.io/instance=my-app # Run tests helm test my-app # Check release info helm history my-app helm get values my-app helm get manifest my-app","title":"License"},{"location":"02%20helm/question2/#concepts-introduced_9","text":"Chart linting and validation JSON schema validation NOTES.txt template README documentation Chart versioning strategy Production-grade defaults Comprehensive testing","title":"Concepts Introduced"},{"location":"02%20helm/question2/#summary-mastery-checklist","text":"","title":"Summary &amp; Mastery Checklist"},{"location":"02%20helm/question2/#what-youve-built","text":"A single, production-ready Helm chart that: \u2705 Starts from absolute basics (Phase 1) \u2705 Grows to include complex features (Phase 2-9) \u2705 Implements best practices (Phase 10) \u2705 Maintains backward compatibility throughout \u2705 Validates with helm lint and helm template \u2705 Includes comprehensive documentation \u2705 Supports real-world deployment patterns","title":"What You've Built"},{"location":"02%20helm/question2/#helm-concepts-mastered","text":"Templating: - {{ }} interpolation - | piping and filters - if/else/end conditionals - range iteration - Named templates ( define ) - Template helpers Functions: - quote , nindent , toYaml , tojson - upper , lower , title , trimSuffix - default , required - include for template reuse - b64enc for encoding - trunc for string truncation Advanced Patterns: - Conditional resources (feature flags) - Dynamic resource generation - Multi-tier templating with helpers - Scoping with $ and with - Proper YAML indentation Kubernetes Objects: - Deployment, Service, ConfigMap, Secret - ServiceAccount, Role, RoleBinding - Ingress, PersistentVolumeClaim - HorizontalPodAutoscaler, PodDisruptionBudget - ServiceMonitor, PrometheusRule (CRDs) - Jobs (for hooks) Helm-Specific: - Chart metadata and versioning - Release management - Hooks (pre-install, post-install, etc.) - Tests - Values schema validation - Chart linting","title":"Helm Concepts Mastered"},{"location":"02%20helm/question2/#practice-goals","text":"By completing all phases, you can: \u2705 Create charts from scratch \u2705 Use proper templating patterns \u2705 Implement feature flags and conditionals \u2705 Manage configuration and secrets safely \u2705 Scale applications with HPA \u2705 Implement health checks \u2705 Add persistence to applications \u2705 Integrate monitoring and observability \u2705 Control application lifecycle with hooks \u2705 Deploy production-grade applications","title":"Practice Goals"},{"location":"02%20helm/question2/#testing-validation-throughout","text":"At every phase , use: # Template rendering helm template my-release . # Dry-run installation (no actual deployment) helm install --dry-run --debug my-release . # Linting helm lint . # Actual deployment helm install my-release . # Verification kubectl get all -l app.kubernetes.io/instance = my-release kubectl describe deployment my-release-webapp kubectl logs deployment/my-release-webapp # Upgrades helm upgrade my-release . --set replicaCount = 5 # Check history helm history my-release helm get values my-release helm get manifest my-release","title":"Testing &amp; Validation Throughout"},{"location":"02%20helm/question2/#pro-tips-for-success","text":"Go slowly - Complete one phase before moving to the next Test constantly - Use helm template and --dry-run liberally Read error messages - They guide you to the problem Keep helpers organized - Use _helpers.tpl for all named templates Document your values - Add comments explaining each value Use meaningful names - Make resource names clear and descriptive Validate YAML - Use yamllint to check output Version your chart - Bump version when behavior changes Test upgrades - Ensure each phase works with upgrades Commit to git - Track all changes and understand evolution","title":"Pro Tips for Success"},{"location":"02%20helm/question2/#learning-path-summary","text":"Phase 1: Foundation (15 min) \u2193 Phase 2: Workloads (30 min) \u2193 Phase 3: Configuration (45 min) \u2193 Phase 4: Advanced Logic (60 min) \u2193 Phase 5: Secrets (45 min) \u2193 Phase 6: Storage (45 min) \u2193 Phase 7: Scaling (60 min) \u2193 Phase 8: Observability (60 min) \u2193 Phase 9: Lifecycle (45 min) \u2193 Phase 10: Production Ready (90 min) Total Time: ~6 hours of hands-on learning Outcome: Complete mastery of Helm chart development for production applications. Good luck! \ud83d\ude80","title":"Learning Path Summary"},{"location":"02%20helm/question3/","text":"Practice Question 3: Build a Scalable Web Application Chart - Progressive Complexity Chart Name: scalable-web-app Philosophy: One chart. One application. Progressively add features, making it production-ready step by step. Phase 1: Basic Deployment Objective: Create a simple Deployment with basic pod specifications. What You Have Application image: scalable-web-app:latest Need to deploy to Kubernetes What You Build Create chart structure with: - templates/deployment.yaml - simple Deployment with 1 replica - values.yaml - image, tag, port configuration - Chart.yaml with metadata - templates/service.yaml - ClusterIP service for internal access Requirements Deployment creates 1 Pod running the application Service exposes the app internally helm template produces valid YAML helm install deploys successfully Validation helm template scalable-web-app . | kubectl apply -f - kubectl get deployment kubectl get pods kubectl port-forward svc/scalable-web-app 8080 :8080 # Test: curl http://localhost:8080 Phase 2: Configuration Management Objective: Add ConfigMap for application configuration (not hardcoded). Current State Phase 1 is working (basic deployment) What You Add templates/configmap.yaml - application configuration file Update values.yaml with config section containing app settings Update templates/deployment.yaml to mount the ConfigMap as a volume Create _helpers.tpl for reusable template snippets Requirements ConfigMap contains application configuration (key-value pairs) Deployment mounts ConfigMap as a volume at /etc/config Application reads configuration from mounted file Configuration changes update ConfigMap without redeploying pod Helm values override default configuration Validation # Check ConfigMap is created kubectl get configmap # Verify mount kubectl exec <pod-name> -- cat /etc/config # Update config and reinstall helm upgrade scalable-web-app . kubectl get configmap -o yaml Phase 3: Multi-Environment Support Objective: Support different configurations for dev, staging, prod. Current State Phase 2 is working (configuration management) What You Add Create values-dev.yaml , values-staging.yaml , values-prod.yaml Each file overrides resource limits, replicas, etc. Update values.yaml with environment-specific sections Add conditional logic in templates to validate correct environment selected Requirements helm install -f values-dev.yaml deploys dev version (1 replica, small resources) helm install -f values-prod.yaml deploys prod version (3+ replicas, large resources) Cannot accidentally mix configurations Different storage classes per environment Different ingress rules per environment Chart validates that only one environment is selected Validation # Deploy to dev helm install scalable-web-app . -f values-dev.yaml kubectl get deployment,configmap # Verify: 1 replica, small resources kubectl get deployment -o yaml | grep replicas # Deploy to prod (different release) helm install scalable-web-app-prod . -f values-prod.yaml # Verify: 3+ replicas, large resources kubectl get deployment scalable-web-app-prod -o yaml | grep replicas Phase 4: Scaling & Performance (HPA & PDB) Objective: Auto-scale based on metrics and protect disruptions. Current State Phase 3 is working (multi-environment support) What You Add templates/hpa.yaml - HorizontalPodAutoscaler (scale based on CPU/memory) templates/pdb.yaml - PodDisruptionBudget (maintain availability during node maintenance) Update values.yaml with autoscaling config (min/max replicas, metrics) Metrics Server must be installed in cluster for HPA to work Requirements HPA scales between min (2) and max (10) replicas based on CPU usage PDB maintains at least 1 Pod available always Pods have resource requests/limits defined (required for HPA) HPA triggers when CPU exceeds threshold Scaling down respects graceful termination period Validation # Check HPA status kubectl get hpa kubectl describe hpa scalable-web-app # Check PDB kubectl get pdb # Simulate load (in separate terminal) kubectl run -it --rm load-generator --image = busybox -- /bin/sh # Inside pod: while sleep 0.01; do wget -q -O- http://scalable-web-app:8080; done # Watch scaling in progress kubectl get hpa -w kubectl get pods -w Phase 5: Security Hardening Objective: Apply security best practices (RBAC, SecurityContext, NetworkPolicy). Current State Phase 4 is working (HPA & PDB) What You Add templates/serviceaccount.yaml - ServiceAccount for the app templates/role.yaml - RBAC Role with minimal permissions templates/rolebinding.yaml - Bind role to ServiceAccount Update templates/deployment.yaml with SecurityContext: Non-root user (runAsUser: 1000) Read-only root filesystem No privileged container Drop unnecessary capabilities templates/networkpolicy.yaml - Deny all ingress by default, allow from ingress controller only Requirements Pod runs as non-root user Pod cannot write to root filesystem Pod has minimal RBAC permissions (only needed APIs) NetworkPolicy blocks unauthorized traffic No secrets in environment variables (will add later) Container cannot escalate privileges Validation # Verify SecurityContext kubectl get deployment -o yaml | grep -A 10 securityContext # Verify ServiceAccount kubectl get sa # Verify RBAC kubectl auth can-i list pods --as = system:serviceaccount:default:scalable-web-app # Verify NetworkPolicy kubectl get networkpolicy kubectl describe networkpolicy scalable-web-app # Test: Pod cannot execute as root kubectl exec <pod-name> -- id # Should show uid=1000 Phase 6: Storage & Persistence Objective: Add persistent data storage with PVC. Current State Phase 5 is working (security) What You Add templates/pvc.yaml - PersistentVolumeClaim for application data Update templates/deployment.yaml to mount PVC as volume Update values.yaml with storage size and storage class name Add support for different storage classes per environment (dev: hostPath, prod: EBS/Azure Disk) Requirements PVC is created from appropriate storage class Deployment mounts PVC at /data Application can write to /data persistently Pod restart doesn't lose data Storage size is configurable per environment StatefulSet (not Deployment) used if multiple replicas need separate PVCs Validation # Check PVC kubectl get pvc kubectl describe pvc scalable-web-app-data # Verify mount kubectl exec <pod-name> -- ls -la /data # Write test file kubectl exec <pod-name> -- sh -c \"echo 'test' > /data/test.txt\" # Delete pod and verify data persists kubectl delete pod <pod-name> # Wait for new pod kubectl exec <new-pod-name> -- cat /data/test.txt # Should show \"test\" Phase 7: Networking & Exposure (Service, Ingress) Objective: Expose app to external traffic securely. Current State Phase 6 is working (storage) What You Add Update templates/service.yaml to type: LoadBalancer (or NodePort for dev) templates/ingress.yaml - expose app via Ingress with: TLS/HTTPS configuration (self-signed cert for dev, real cert for prod) Path-based routing Host-based routing (environment-specific domains) Authentication/authorization (basic auth for dev, OAuth for prod) Update values.yaml with ingress domain, TLS cert config Requirements Service exposes pod on port 8080 Ingress exposes app on domain (ingress-class: nginx) HTTPS/TLS works with proper certificate Traffic routing works to correct pod Different ingress configs for dev vs prod (TLS required only in prod) Validation # Check Service kubectl get svc kubectl describe svc scalable-web-app # Check Ingress kubectl get ingress kubectl describe ingress scalable-web-app # Test routing (if minikube/local cluster) curl http://scalable-web-app.example.com # Should route to pod curl -k https://scalable-web-app.example.com # Should work with TLS # Verify certificate kubectl get ingress -o yaml | grep cert Phase 8: Observability (Prometheus & Logging) Objective: Collect metrics and logs for monitoring. Current State Phase 7 is working (networking) What You Add templates/servicemonitor.yaml - Prometheus ServiceMonitor for metrics scraping Update templates/deployment.yaml to expose /metrics endpoint (port 9090) Structured logging configuration (JSON format logs) Update values.yaml with observability config (enable metrics, log level) templates/prometheusrule.yaml - Alert rules for SLA violations Dashboard definition (as ConfigMap) for Grafana Requirements Pod exposes Prometheus metrics on /metrics port 9090 ServiceMonitor tells Prometheus where to scrape metrics Application logs are in JSON format (structured) Logs include request ID for tracing Alert rules fire when: Pod restarts frequently (>3 restarts/hour) Error rate exceeds 5% Response time exceeds SLA (e.g., 500ms p99) Metrics show: request count, latency, errors, resource usage Validation # Check ServiceMonitor kubectl get servicemonitor # Verify metrics endpoint kubectl port-forward <pod-name> 9090 :9090 curl http://localhost:9090/metrics | grep requests_total # Check logs kubectl logs <pod-name> # Should be JSON formatted # If Prometheus running, check scrape config kubectl exec -it prometheus-pod -c prometheus -- cat /etc/prometheus/prometheus.yml # Check alerts are defined kubectl get prometheusrule kubectl describe prometheusrule scalable-web-app Phase 9: Deployment Strategies Objective: Safely roll out updates with zero downtime. Current State Phase 8 is working (observability) What You Add Multiple deployment strategies as values option: Rolling Update (default): gradual pod replacement with maxSurge/maxUnavailable Blue-Green : two deployments, instant traffic switch Canary : gradual traffic shift to new version (requires ServiceMesh or Ingress rules) Add pre-upgrade job to backup data Add post-upgrade job to verify deployment health Update values.yaml to select strategy Requirements Rolling update: maxUnavailable=1, maxSurge=1 (always 1-2 pods running) Blue-green: create both v1 and v2 deployments, switch service selector Canary: gradually route traffic (10% \u2192 50% \u2192 100%) to new version Old pods gracefully shutdown (preStop hook, 30s termination grace) Automatic rollback if health checks fail Zero-downtime upgrades (no traffic loss) Validation # Simulate deployment update helm upgrade scalable-web-app . --set image.tag = v2 # Watch rolling update kubectl rollout status deployment/scalable-web-app kubectl get pods -w # Verify zero downtime (in separate terminal, hit endpoint) while true ; do curl http://scalable-web-app:8080 ; sleep 1 ; done # Should see continuous responses, no connection errors # Rollback if needed helm rollback scalable-web-app # Blue-green test helm install scalable-web-app . --set deploymentStrategy = blue-green --set image.tag = v1 helm upgrade scalable-web-app . --set image.tag = v2 # Verify both deployments exist, traffic switches to v2 Phase 10: Production Readiness Objective: Validate chart quality, documentation, and operational readiness. Current State Phase 9 is working (deployment strategies) What You Add values.schema.json - JSON Schema for values validation NOTES.txt - deployment instructions and next steps README.md - comprehensive documentation Architecture overview Installation instructions Configuration reference Troubleshooting guide Performance tuning recommendations Chart tests in templates/tests/ directory Verify pod is running Verify service is accessible Verify ingress works Verify configmap was created Verify metrics endpoint responds Update Chart.yaml with proper metadata (description, keywords, maintainers) Requirements helm lint passes without errors/warnings helm template produces valid YAML for all environments helm install --dry-run --debug succeeds Chart values validate against schema (invalid values rejected) All 10 phases are working together cohesively Documentation answers: What? How? Why? When to use? Tests verify critical functionality automatically Chart follows Helm best practices Validation # Lint chart helm lint . # Validate against schema helm template . | kubectl apply -f - --dry-run = client # Run tests helm test scalable-web-app # Verify all resources created kubectl get all kubectl get configmap,pvc,networkpolicy,ingress,servicemonitor # Check NOTES output helm install scalable-web-app . # Should print helpful next steps # Verify documentation # README.md should be comprehensive and clear # values.schema.json should validate all settings # Final check: can delete and redeploy cleanly helm uninstall scalable-web-app helm install scalable-web-app . Progression Summary Phase Focus Key Concepts 1 Basics Deployment, Service, templates 2 Config ConfigMap, volumes, helper functions 3 Environments values files, conditionals, validation 4 Scaling HPA, PDB, resource requests/limits 5 Security RBAC, SecurityContext, NetworkPolicy 6 Storage PVC, persistent data, StatefulSet 7 Networking Ingress, TLS, routing 8 Observability Prometheus, ServiceMonitor, alerts 9 Deployments Rolling, Blue-Green, Canary strategies 10 Production Schema validation, tests, documentation What You're Building By end of Phase 10, you have: \u2705 Chart that deploys a complete application \u2705 Multi-environment support (dev/staging/prod) \u2705 Auto-scaling based on metrics \u2705 High availability (PDB maintains availability) \u2705 Persistent storage for application data \u2705 HTTPS/TLS secured external access \u2705 Security hardening (RBAC, SecurityContext, NetworkPolicy) \u2705 Full observability (metrics, alerts, logs) \u2705 Safe deployment strategies (zero downtime) \u2705 Production-quality documentation and validation Total Implementation Time: 25-35 hours Key Helm Concepts You'll Learn Chart Structure - metadata, templates, values hierarchy Templating - conditionals, loops, range, variables, functions Helper Functions - DRY principle, _helpers.tpl, named templates Values Management - defaults, overrides, precedence, schema validation Reusability - composing complex configs from simple building blocks Kubernetes Patterns - RBAC, NetworkPolicy, Deployments, StatefulSets Observability - metrics, logging, alerting at chart level Deployment Strategies - safe rollouts, health checks, rollbacks Best Practices - documentation, testing, linting, security Advanced Templating - computed values, dynamic resource generation Notes Each phase depends on previous phases You'll iterate and refine as you discover edge cases Real-world constraints matter (actual cluster limitations, quotas) Documentation is as important as code Chart should be idempotent (safe to run multiple times) Good luck! Build incrementally, test thoroughly, and focus on one feature at a time.","title":"Question3"},{"location":"02%20helm/question3/#practice-question-3-build-a-scalable-web-application-chart-progressive-complexity","text":"Chart Name: scalable-web-app Philosophy: One chart. One application. Progressively add features, making it production-ready step by step.","title":"Practice Question 3: Build a Scalable Web Application Chart - Progressive Complexity"},{"location":"02%20helm/question3/#phase-1-basic-deployment","text":"Objective: Create a simple Deployment with basic pod specifications.","title":"Phase 1: Basic Deployment"},{"location":"02%20helm/question3/#what-you-have","text":"Application image: scalable-web-app:latest Need to deploy to Kubernetes","title":"What You Have"},{"location":"02%20helm/question3/#what-you-build","text":"Create chart structure with: - templates/deployment.yaml - simple Deployment with 1 replica - values.yaml - image, tag, port configuration - Chart.yaml with metadata - templates/service.yaml - ClusterIP service for internal access","title":"What You Build"},{"location":"02%20helm/question3/#requirements","text":"Deployment creates 1 Pod running the application Service exposes the app internally helm template produces valid YAML helm install deploys successfully","title":"Requirements"},{"location":"02%20helm/question3/#validation","text":"helm template scalable-web-app . | kubectl apply -f - kubectl get deployment kubectl get pods kubectl port-forward svc/scalable-web-app 8080 :8080 # Test: curl http://localhost:8080","title":"Validation"},{"location":"02%20helm/question3/#phase-2-configuration-management","text":"Objective: Add ConfigMap for application configuration (not hardcoded).","title":"Phase 2: Configuration Management"},{"location":"02%20helm/question3/#current-state","text":"Phase 1 is working (basic deployment)","title":"Current State"},{"location":"02%20helm/question3/#what-you-add","text":"templates/configmap.yaml - application configuration file Update values.yaml with config section containing app settings Update templates/deployment.yaml to mount the ConfigMap as a volume Create _helpers.tpl for reusable template snippets","title":"What You Add"},{"location":"02%20helm/question3/#requirements_1","text":"ConfigMap contains application configuration (key-value pairs) Deployment mounts ConfigMap as a volume at /etc/config Application reads configuration from mounted file Configuration changes update ConfigMap without redeploying pod Helm values override default configuration","title":"Requirements"},{"location":"02%20helm/question3/#validation_1","text":"# Check ConfigMap is created kubectl get configmap # Verify mount kubectl exec <pod-name> -- cat /etc/config # Update config and reinstall helm upgrade scalable-web-app . kubectl get configmap -o yaml","title":"Validation"},{"location":"02%20helm/question3/#phase-3-multi-environment-support","text":"Objective: Support different configurations for dev, staging, prod.","title":"Phase 3: Multi-Environment Support"},{"location":"02%20helm/question3/#current-state_1","text":"Phase 2 is working (configuration management)","title":"Current State"},{"location":"02%20helm/question3/#what-you-add_1","text":"Create values-dev.yaml , values-staging.yaml , values-prod.yaml Each file overrides resource limits, replicas, etc. Update values.yaml with environment-specific sections Add conditional logic in templates to validate correct environment selected","title":"What You Add"},{"location":"02%20helm/question3/#requirements_2","text":"helm install -f values-dev.yaml deploys dev version (1 replica, small resources) helm install -f values-prod.yaml deploys prod version (3+ replicas, large resources) Cannot accidentally mix configurations Different storage classes per environment Different ingress rules per environment Chart validates that only one environment is selected","title":"Requirements"},{"location":"02%20helm/question3/#validation_2","text":"# Deploy to dev helm install scalable-web-app . -f values-dev.yaml kubectl get deployment,configmap # Verify: 1 replica, small resources kubectl get deployment -o yaml | grep replicas # Deploy to prod (different release) helm install scalable-web-app-prod . -f values-prod.yaml # Verify: 3+ replicas, large resources kubectl get deployment scalable-web-app-prod -o yaml | grep replicas","title":"Validation"},{"location":"02%20helm/question3/#phase-4-scaling-performance-hpa-pdb","text":"Objective: Auto-scale based on metrics and protect disruptions.","title":"Phase 4: Scaling &amp; Performance (HPA &amp; PDB)"},{"location":"02%20helm/question3/#current-state_2","text":"Phase 3 is working (multi-environment support)","title":"Current State"},{"location":"02%20helm/question3/#what-you-add_2","text":"templates/hpa.yaml - HorizontalPodAutoscaler (scale based on CPU/memory) templates/pdb.yaml - PodDisruptionBudget (maintain availability during node maintenance) Update values.yaml with autoscaling config (min/max replicas, metrics) Metrics Server must be installed in cluster for HPA to work","title":"What You Add"},{"location":"02%20helm/question3/#requirements_3","text":"HPA scales between min (2) and max (10) replicas based on CPU usage PDB maintains at least 1 Pod available always Pods have resource requests/limits defined (required for HPA) HPA triggers when CPU exceeds threshold Scaling down respects graceful termination period","title":"Requirements"},{"location":"02%20helm/question3/#validation_3","text":"# Check HPA status kubectl get hpa kubectl describe hpa scalable-web-app # Check PDB kubectl get pdb # Simulate load (in separate terminal) kubectl run -it --rm load-generator --image = busybox -- /bin/sh # Inside pod: while sleep 0.01; do wget -q -O- http://scalable-web-app:8080; done # Watch scaling in progress kubectl get hpa -w kubectl get pods -w","title":"Validation"},{"location":"02%20helm/question3/#phase-5-security-hardening","text":"Objective: Apply security best practices (RBAC, SecurityContext, NetworkPolicy).","title":"Phase 5: Security Hardening"},{"location":"02%20helm/question3/#current-state_3","text":"Phase 4 is working (HPA & PDB)","title":"Current State"},{"location":"02%20helm/question3/#what-you-add_3","text":"templates/serviceaccount.yaml - ServiceAccount for the app templates/role.yaml - RBAC Role with minimal permissions templates/rolebinding.yaml - Bind role to ServiceAccount Update templates/deployment.yaml with SecurityContext: Non-root user (runAsUser: 1000) Read-only root filesystem No privileged container Drop unnecessary capabilities templates/networkpolicy.yaml - Deny all ingress by default, allow from ingress controller only","title":"What You Add"},{"location":"02%20helm/question3/#requirements_4","text":"Pod runs as non-root user Pod cannot write to root filesystem Pod has minimal RBAC permissions (only needed APIs) NetworkPolicy blocks unauthorized traffic No secrets in environment variables (will add later) Container cannot escalate privileges","title":"Requirements"},{"location":"02%20helm/question3/#validation_4","text":"# Verify SecurityContext kubectl get deployment -o yaml | grep -A 10 securityContext # Verify ServiceAccount kubectl get sa # Verify RBAC kubectl auth can-i list pods --as = system:serviceaccount:default:scalable-web-app # Verify NetworkPolicy kubectl get networkpolicy kubectl describe networkpolicy scalable-web-app # Test: Pod cannot execute as root kubectl exec <pod-name> -- id # Should show uid=1000","title":"Validation"},{"location":"02%20helm/question3/#phase-6-storage-persistence","text":"Objective: Add persistent data storage with PVC.","title":"Phase 6: Storage &amp; Persistence"},{"location":"02%20helm/question3/#current-state_4","text":"Phase 5 is working (security)","title":"Current State"},{"location":"02%20helm/question3/#what-you-add_4","text":"templates/pvc.yaml - PersistentVolumeClaim for application data Update templates/deployment.yaml to mount PVC as volume Update values.yaml with storage size and storage class name Add support for different storage classes per environment (dev: hostPath, prod: EBS/Azure Disk)","title":"What You Add"},{"location":"02%20helm/question3/#requirements_5","text":"PVC is created from appropriate storage class Deployment mounts PVC at /data Application can write to /data persistently Pod restart doesn't lose data Storage size is configurable per environment StatefulSet (not Deployment) used if multiple replicas need separate PVCs","title":"Requirements"},{"location":"02%20helm/question3/#validation_5","text":"# Check PVC kubectl get pvc kubectl describe pvc scalable-web-app-data # Verify mount kubectl exec <pod-name> -- ls -la /data # Write test file kubectl exec <pod-name> -- sh -c \"echo 'test' > /data/test.txt\" # Delete pod and verify data persists kubectl delete pod <pod-name> # Wait for new pod kubectl exec <new-pod-name> -- cat /data/test.txt # Should show \"test\"","title":"Validation"},{"location":"02%20helm/question3/#phase-7-networking-exposure-service-ingress","text":"Objective: Expose app to external traffic securely.","title":"Phase 7: Networking &amp; Exposure (Service, Ingress)"},{"location":"02%20helm/question3/#current-state_5","text":"Phase 6 is working (storage)","title":"Current State"},{"location":"02%20helm/question3/#what-you-add_5","text":"Update templates/service.yaml to type: LoadBalancer (or NodePort for dev) templates/ingress.yaml - expose app via Ingress with: TLS/HTTPS configuration (self-signed cert for dev, real cert for prod) Path-based routing Host-based routing (environment-specific domains) Authentication/authorization (basic auth for dev, OAuth for prod) Update values.yaml with ingress domain, TLS cert config","title":"What You Add"},{"location":"02%20helm/question3/#requirements_6","text":"Service exposes pod on port 8080 Ingress exposes app on domain (ingress-class: nginx) HTTPS/TLS works with proper certificate Traffic routing works to correct pod Different ingress configs for dev vs prod (TLS required only in prod)","title":"Requirements"},{"location":"02%20helm/question3/#validation_6","text":"# Check Service kubectl get svc kubectl describe svc scalable-web-app # Check Ingress kubectl get ingress kubectl describe ingress scalable-web-app # Test routing (if minikube/local cluster) curl http://scalable-web-app.example.com # Should route to pod curl -k https://scalable-web-app.example.com # Should work with TLS # Verify certificate kubectl get ingress -o yaml | grep cert","title":"Validation"},{"location":"02%20helm/question3/#phase-8-observability-prometheus-logging","text":"Objective: Collect metrics and logs for monitoring.","title":"Phase 8: Observability (Prometheus &amp; Logging)"},{"location":"02%20helm/question3/#current-state_6","text":"Phase 7 is working (networking)","title":"Current State"},{"location":"02%20helm/question3/#what-you-add_6","text":"templates/servicemonitor.yaml - Prometheus ServiceMonitor for metrics scraping Update templates/deployment.yaml to expose /metrics endpoint (port 9090) Structured logging configuration (JSON format logs) Update values.yaml with observability config (enable metrics, log level) templates/prometheusrule.yaml - Alert rules for SLA violations Dashboard definition (as ConfigMap) for Grafana","title":"What You Add"},{"location":"02%20helm/question3/#requirements_7","text":"Pod exposes Prometheus metrics on /metrics port 9090 ServiceMonitor tells Prometheus where to scrape metrics Application logs are in JSON format (structured) Logs include request ID for tracing Alert rules fire when: Pod restarts frequently (>3 restarts/hour) Error rate exceeds 5% Response time exceeds SLA (e.g., 500ms p99) Metrics show: request count, latency, errors, resource usage","title":"Requirements"},{"location":"02%20helm/question3/#validation_7","text":"# Check ServiceMonitor kubectl get servicemonitor # Verify metrics endpoint kubectl port-forward <pod-name> 9090 :9090 curl http://localhost:9090/metrics | grep requests_total # Check logs kubectl logs <pod-name> # Should be JSON formatted # If Prometheus running, check scrape config kubectl exec -it prometheus-pod -c prometheus -- cat /etc/prometheus/prometheus.yml # Check alerts are defined kubectl get prometheusrule kubectl describe prometheusrule scalable-web-app","title":"Validation"},{"location":"02%20helm/question3/#phase-9-deployment-strategies","text":"Objective: Safely roll out updates with zero downtime.","title":"Phase 9: Deployment Strategies"},{"location":"02%20helm/question3/#current-state_7","text":"Phase 8 is working (observability)","title":"Current State"},{"location":"02%20helm/question3/#what-you-add_7","text":"Multiple deployment strategies as values option: Rolling Update (default): gradual pod replacement with maxSurge/maxUnavailable Blue-Green : two deployments, instant traffic switch Canary : gradual traffic shift to new version (requires ServiceMesh or Ingress rules) Add pre-upgrade job to backup data Add post-upgrade job to verify deployment health Update values.yaml to select strategy","title":"What You Add"},{"location":"02%20helm/question3/#requirements_8","text":"Rolling update: maxUnavailable=1, maxSurge=1 (always 1-2 pods running) Blue-green: create both v1 and v2 deployments, switch service selector Canary: gradually route traffic (10% \u2192 50% \u2192 100%) to new version Old pods gracefully shutdown (preStop hook, 30s termination grace) Automatic rollback if health checks fail Zero-downtime upgrades (no traffic loss)","title":"Requirements"},{"location":"02%20helm/question3/#validation_8","text":"# Simulate deployment update helm upgrade scalable-web-app . --set image.tag = v2 # Watch rolling update kubectl rollout status deployment/scalable-web-app kubectl get pods -w # Verify zero downtime (in separate terminal, hit endpoint) while true ; do curl http://scalable-web-app:8080 ; sleep 1 ; done # Should see continuous responses, no connection errors # Rollback if needed helm rollback scalable-web-app # Blue-green test helm install scalable-web-app . --set deploymentStrategy = blue-green --set image.tag = v1 helm upgrade scalable-web-app . --set image.tag = v2 # Verify both deployments exist, traffic switches to v2","title":"Validation"},{"location":"02%20helm/question3/#phase-10-production-readiness","text":"Objective: Validate chart quality, documentation, and operational readiness.","title":"Phase 10: Production Readiness"},{"location":"02%20helm/question3/#current-state_8","text":"Phase 9 is working (deployment strategies)","title":"Current State"},{"location":"02%20helm/question3/#what-you-add_8","text":"values.schema.json - JSON Schema for values validation NOTES.txt - deployment instructions and next steps README.md - comprehensive documentation Architecture overview Installation instructions Configuration reference Troubleshooting guide Performance tuning recommendations Chart tests in templates/tests/ directory Verify pod is running Verify service is accessible Verify ingress works Verify configmap was created Verify metrics endpoint responds Update Chart.yaml with proper metadata (description, keywords, maintainers)","title":"What You Add"},{"location":"02%20helm/question3/#requirements_9","text":"helm lint passes without errors/warnings helm template produces valid YAML for all environments helm install --dry-run --debug succeeds Chart values validate against schema (invalid values rejected) All 10 phases are working together cohesively Documentation answers: What? How? Why? When to use? Tests verify critical functionality automatically Chart follows Helm best practices","title":"Requirements"},{"location":"02%20helm/question3/#validation_9","text":"# Lint chart helm lint . # Validate against schema helm template . | kubectl apply -f - --dry-run = client # Run tests helm test scalable-web-app # Verify all resources created kubectl get all kubectl get configmap,pvc,networkpolicy,ingress,servicemonitor # Check NOTES output helm install scalable-web-app . # Should print helpful next steps # Verify documentation # README.md should be comprehensive and clear # values.schema.json should validate all settings # Final check: can delete and redeploy cleanly helm uninstall scalable-web-app helm install scalable-web-app .","title":"Validation"},{"location":"02%20helm/question3/#progression-summary","text":"Phase Focus Key Concepts 1 Basics Deployment, Service, templates 2 Config ConfigMap, volumes, helper functions 3 Environments values files, conditionals, validation 4 Scaling HPA, PDB, resource requests/limits 5 Security RBAC, SecurityContext, NetworkPolicy 6 Storage PVC, persistent data, StatefulSet 7 Networking Ingress, TLS, routing 8 Observability Prometheus, ServiceMonitor, alerts 9 Deployments Rolling, Blue-Green, Canary strategies 10 Production Schema validation, tests, documentation","title":"Progression Summary"},{"location":"02%20helm/question3/#what-youre-building","text":"By end of Phase 10, you have: \u2705 Chart that deploys a complete application \u2705 Multi-environment support (dev/staging/prod) \u2705 Auto-scaling based on metrics \u2705 High availability (PDB maintains availability) \u2705 Persistent storage for application data \u2705 HTTPS/TLS secured external access \u2705 Security hardening (RBAC, SecurityContext, NetworkPolicy) \u2705 Full observability (metrics, alerts, logs) \u2705 Safe deployment strategies (zero downtime) \u2705 Production-quality documentation and validation Total Implementation Time: 25-35 hours","title":"What You're Building"},{"location":"02%20helm/question3/#key-helm-concepts-youll-learn","text":"Chart Structure - metadata, templates, values hierarchy Templating - conditionals, loops, range, variables, functions Helper Functions - DRY principle, _helpers.tpl, named templates Values Management - defaults, overrides, precedence, schema validation Reusability - composing complex configs from simple building blocks Kubernetes Patterns - RBAC, NetworkPolicy, Deployments, StatefulSets Observability - metrics, logging, alerting at chart level Deployment Strategies - safe rollouts, health checks, rollbacks Best Practices - documentation, testing, linting, security Advanced Templating - computed values, dynamic resource generation","title":"Key Helm Concepts You'll Learn"},{"location":"02%20helm/question3/#notes","text":"Each phase depends on previous phases You'll iterate and refine as you discover edge cases Real-world constraints matter (actual cluster limitations, quotas) Documentation is as important as code Chart should be idempotent (safe to run multiple times) Good luck! Build incrementally, test thoroughly, and focus on one feature at a time.","title":"Notes"},{"location":"02%20helm/question4/","text":"Meaningful Question 4: Build an Enterprise Helm Chart Library - Real Helm Complexity What You'll Build: A reusable Helm chart library that powers your entire organization. 50+ teams use it to deploy their applications without touching Kubernetes manifests. Chart Name: app-platform (base chart library) Philosophy: This is about Helm, not Kubernetes. You're solving real templating, composition, and reusability challenges. The Business Context Your organization has 50+ teams deploying applications to Kubernetes: Current State (The Problem): - Each team writes their own manifests (Deployment, Service, Ingress, etc.) - Inconsistent patterns across teams (no standards) - Security varies wildly (some pods run as root, some don't) - Upgrades are manual and error-prone - Different teams solve the same problem 50 different ways - Onboarding new teams takes weeks Your Goal: - Build ONE Helm chart that 50+ teams can use - Teams only write values.yaml , no template changes - Standardized patterns (security, monitoring, networking) - Self-service deployments - Automatic compliance checking The Challenge: Extreme Helm Flexibility Your chart must support: Use Case Example Helm Challenge Stateless Web App Node.js API Basic Deployment Stateful Database PostgreSQL StatefulSet with PVC Batch Job Data processor CronJob, Job cleanup Worker Queue RabbitMQ consumer Deployment + custom config External Service Third-party API Service without pods Microservices 10 services in 1 chart Dependencies, shared values API Gateway Kong, Traefik Multiple replicas, plugins Lambda-like Function runner Pod per invocation Cache Layer Redis Optional, conditional Message Queue Kafka, RabbitMQ Optional, conditional Problem: How do you make ONE chart flexible enough for all these, but simple enough that teams only specify values? Part 1: Chart Architecture & Flexibility The Design Challenge You need to support different workload types with minimal configuration: # Team 1: Simple web app workloadType : deployment replicas : 3 image : myapp:1.0 # Team 2: Stateful database workloadType : statefulset replicas : 3 persistence : enabled : true size : 100Gi # Team 3: Scheduled job workloadType : cronjob schedule : \"0 2 * * *\" # Team 4: Multiple services in one chart services : api : workloadType : deployment replicas : 3 worker : workloadType : deployment replicas : 2 cache : workloadType : deployment replicas : 1 Requirements Template Structure Use conditional logic to include only needed manifests DRY principle: reuse pod specs across workload types Support 5+ workload types (Deployment, StatefulSet, DaemonSet, Job, CronJob) Shared template helpers for common patterns Workload Types deployment : Stateless app (replicas, rolling update) statefulset : Stateful app (stable identity, persistent storage) daemonset : Node-local app (one per node) job : Run once, complete cronjob : Run on schedule external : No pods (external service proxy) Flexible Configuration One values.yaml controls everything Support single service or multiple services Optional components (cache, database, monitoring) Different security levels per component Validation Invalid workloadType combinations fail validation Missing required values fail early StatefulSet must have persistence enabled CronJob must have valid cron schedule Validation Criteria # Deploy web app (deployment) helm install web-app . -f values-web.yaml kubectl get deployment # Deploy database (statefulset) helm install database . -f values-statefulset.yaml kubectl get statefulset # Deploy cronjob helm install scheduler . -f values-cronjob.yaml kubectl get cronjob # Deploy multi-service helm install platform . -f values-multi.yaml kubectl get deployment,statefulset # Should show api deployment, worker deployment, cache deployment # Validation test: invalid workloadType helm template . -f values-invalid.yaml 2 > & 1 | grep -i error # Should fail with clear error message Part 2: Values Schema & Auto-Documentation The Problem Teams ask: - \"What values can I set?\" - \"What are the defaults?\" - \"Which values are required?\" - \"Can I use image tags or only repo URLs?\" You need to answer without manually documenting 200+ values. Requirements JSON Schema (values.schema.json) Complete schema for all valid values Type checking (string, number, boolean, array, object) Required vs optional fields Enum validation (e.g., workloadType must be one of: deployment, statefulset, ...) Min/max validation (replicas: min 1, max 1000) Pattern validation (image: must match regex) Descriptions for each value Auto-Generated Documentation README generated from schema Example values.yaml files for common use cases Inline comments in schema explaining each field Validation helm template fails if invalid values provided Clear error messages: \"replicas must be >= 1, got -5\" Suggest fixes: \"workloadType must be one of: [deployment, statefulset, job, cronjob, daemonset]\" Validation Criteria # Valid values should work helm template . -f valid-values.yaml > /dev/null echo $? # Should be 0 # Invalid values should fail with clear error helm template . -f invalid-values.yaml 2 > & 1 # Should show: \"Error: replicas must be >= 1, got 0\" # Should show: \"Error: workloadType must be one of: [deployment, statefulset, ...]\" # Schema should be complete cat values.schema.json | jq '.properties | keys | length' # Should be 30+ properties # Documentation should be auto-generated cat README.md | grep -c \"##\" # Should have multiple sections auto-generated Part 3: Reusable Template Blocks & DRY Principle The Problem You have 20+ manifest templates. Many are similar: # deployment.yaml apiVersion : apps/v1 kind : Deployment metadata : name : {{ .Values.name }} labels : app : {{ .Values.name }} version : {{ .Values.version }} spec : replicas : {{ .Values.replicas }} selector : matchLabels : app : {{ .Values.name }} template : metadata : labels : app : {{ .Values.name }} spec : containers : - name : app image : {{ .Values.image }} ports : - containerPort : {{ .Values.port }} # statefulset.yaml apiVersion : apps/v1 kind : StatefulSet metadata : name : {{ .Values.name }} labels : app : {{ .Values.name }} version : {{ .Values.version }} spec : replicas : {{ .Values.replicas }} selector : matchLabels : app : {{ .Values.name }} template : metadata : labels : app : {{ .Values.name }} spec : containers : - name : app image : {{ .Values.image }} ports : - containerPort : {{ .Values.port }} Problem: Labels, selectors, container specs are duplicated. Change one place, forgot 5 others. Requirements Template Helpers (_helpers.tpl) Reusable blocks for common patterns Labels helper: generate consistent labels Selectors helper: generate consistent selectors Container spec helper: generate container template Health checks helper: generate probes Security context helper: generate security settings Environment variables helper: handle secrets vs configmaps DRY Principle Pod spec defined once, reused in Deployment, StatefulSet, Job Labels defined once, used everywhere Probes defined once, applied to all workloads Security context defined once, applied everywhere Consistency All pods get same security standards All pods have same labels All pods have same monitoring sidecars Changing one helper updates everywhere Validation Criteria # Generate manifests helm template . -f values.yaml > manifests.yaml # Check: All labels are consistent grep -c \"app: myapp\" manifests.yaml # Should be 10+ (pod spec, deployment, service, etc.) # Check: Selector matches labels grep -A 2 \"matchLabels:\" manifests.yaml # Should show \"app: myapp\" matching pod labels # Check: Security context applied grep -c \"runAsNonRoot\" manifests.yaml # Should be 4+ (one per pod) # Verify DRY: Change label in helper # Rerun template, all manifests should update # Count lines of code wc -l templates/*.yaml templates/_helpers.tpl # _helpers.tpl should be significant (200+), reused across 5+ templates Part 4: Conditional Rendering & Feature Toggles The Problem Teams ask: - \"Can I disable monitoring for dev environment?\" - \"Can I skip database backups for testing?\" - \"Can I disable security policies for local development?\" You need sophisticated conditional logic. Requirements Feature Toggles monitoring.enabled : Include ServiceMonitor, PrometheusRule ingress.enabled : Include Ingress, adjust Service type persistence.enabled : Include PVC, use emptyDir if not rbac.enabled : Include ServiceAccount, Role, RoleBinding networkPolicy.enabled : Include NetworkPolicy backup.enabled : Include backup CronJob Conditional Dependencies If persistence.enabled , must have persistence.storageClass If monitoring.enabled , must have monitoring.scrapeInterval If ingress.enabled , must have ingress.host If rbac.enabled , can't use runAsUser: 0 Environment-Specific Configs Development: monitoring disabled, no ingress, local storage Staging: monitoring enabled, ingress + TLS, persistent storage Production: monitoring required, security hardened, HA Validation Criteria # Deploy with minimal features (dev) helm template . -f values-dev.yaml | kubectl apply -f - --dry-run = client # Should work, only pod+service # Deploy with all features (prod) helm template . -f values-prod.yaml | kubectl apply -f - --dry-run = client # Should include monitoring, ingress, security, persistence # Verify conditional logic helm template . -f values-monitoring-disabled.yaml | grep -i servicemonitor # Should return nothing helm template . -f values-monitoring-enabled.yaml | grep -i servicemonitor # Should show ServiceMonitor resource # Verify dependency validation helm template . -f values-persistence-enabled-but-no-storageclass.yaml 2 > & 1 # Should fail with clear error Part 5: Multi-Environment & Values Inheritance The Problem Teams deploy same app to dev, staging, production with different configs: # All similar structure, different values replicas : dev : 1 staging : 2 prod : 3 resources : dev : { cpu : 100m , memory : 128Mi } staging : { cpu : 500m , memory : 512Mi } prod : { cpu : 1000m , memory : 2Gi } ingress : dev : { tls : false , domain : dev.example.com } staging : { tls : true , domain : staging.example.com } prod : { tls : true , domain : app.example.com } Problem: How do you manage all this without duplicating values 3 times? Requirements Values Hierarchy values.yaml : defaults (works for most cases) values-dev.yaml : override for dev (small replicas, low resources) values-staging.yaml : override for staging (medium replicas, medium resources) values-prod.yaml : override for prod (high replicas, high resources, security hardened) Smart Defaults Default values.yaml is functional for dev Staging only needs to override replicas and resources Prod only needs to override security and monitoring settings No value specified in 3 places unnecessarily Values Composition Parent chart passes values to child charts correctly Global values accessible to all children Component-specific values only affect that component Validation Criteria # Dev deployment (minimal) helm template . -f values-dev.yaml | grep replicas: # Should show replicas: 1 # Staging deployment helm template . -f values-staging.yaml | grep replicas: # Should show replicas: 2 # Prod deployment helm template . -f values-prod.yaml | grep replicas: # Should show replicas: 3 # Values files should be small (no duplication) wc -l values.yaml values-*.yaml # values.yaml should be 100+ lines, each override should be 20-30 lines only Part 6: Parent Charts & Multi-Service Deployments The Problem One team wants to deploy: - API server - Worker - Cache - Database All as one unit, with shared configuration. Requirements Parent Chart (Umbrella) Includes child charts as dependencies Each child is a service (api, worker, cache, database) Shared values passed to relevant children Version constraints on child charts Child Charts Each service is a reusable chart Can be deployed standalone or as part of parent Accepts shared values (labels, monitoring config, security) Can override specific settings Dependency Management api depends on database (waits for startup) worker depends on cache Ordering in templates ensures dependencies start first Health checks verify dependencies are ready Validation Criteria # Deploy parent chart helm install platform . -f values-multi.yaml # Verify all services deployed kubectl get deployment # Should show: api, worker kubectl get statefulset # Should show: cache, database # Verify dependency ordering kubectl get events | grep Created # Database created first, then api # Verify shared values applied kubectl get deployment -o yaml | grep -i monitoring # All deployments should have monitoring enabled Part 7: Helm Hooks & Lifecycle Events The Problem Teams need to run tasks at specific lifecycle points: Pre-install: Create namespace, apply CRDs Post-install: Run migrations, seed data Pre-upgrade: Backup database Post-upgrade: Verify app is healthy Pre-delete: Export data, cleanup Post-delete: Remove PVCs, cleanup resources Requirements Pre-Install Hooks Install CRDs (must run before any resources reference them) Create namespaces with labels Apply RBAC cluster-wide (if needed) Validate cluster prerequisites Post-Install Hooks Run database migrations Seed initial data Run smoke tests Generate initial config Pre-Upgrade Hooks Backup database to S3 Run pre-upgrade validation Check for breaking changes Warn if downtime required Post-Upgrade Hooks Run schema migrations Validate data integrity Run smoke tests Notify team of upgrade completion Pre-Delete Hooks Export data to S3 Run cleanup tasks Collect logs for archival Validation Criteria # Install and watch hooks run helm install app . --debug # Should see: pre-install hook running, post-install hook running # Upgrade and watch hooks helm upgrade app . --set version = 2 --debug # Should see: pre-upgrade hook (backup), post-upgrade hook (migrate) # Verify hook ordering kubectl get events | grep hook # Should show: pre-install, post-install, pre-upgrade, post-upgrade in order # Verify hooks only run once helm upgrade app . --debug # Should not re-run post-install hook # Should run pre-upgrade and post-upgrade hooks Part 8: Values Validation & Error Handling The Problem Teams sometimes provide invalid values: replicas : -5 # negative! image : \"\" # empty! port : 99999 # invalid! workloadType : lambda # not supported! Result: Chart installs but deployment fails mysteriously. Requirements Schema Validation replicas : integer, >= 1, <= 100 image : string, required, matches regex ^[a-z0-9:/.]+$ port : integer, >= 1, <= 65535 workloadType : enum [deployment, statefulset, job, cronjob, daemonset] resources.requests.cpu : string, matches Kubernetes CPU regex Smart Error Messages Show what went wrong Show what should be fixed Suggest valid options Optional Pre-Deployment Validation Custom template validation logic Check interdependencies Fail early with clear errors Validation Criteria # Invalid values fail immediately helm template . -f values-invalid.yaml 2 > & 1 # Should show: \"Error: replicas must be >= 1 and <= 100, got -5\" # Should show: \"Did you mean: replicas: 1?\" # Missing required values fail helm template . -f values-incomplete.yaml 2 > & 1 # Should show: \"Error: image is required, got empty string\" # Invalid workloadType fails helm template . --set workloadType = lambda 2 > & 1 # Should show: \"Error: workloadType must be one of: [deployment, statefulset, job, ...]\" # \"Got: lambda\" # Invalid port fails helm template . --set port = 99999 2 > & 1 # Should show: \"Error: port must be between 1 and 65535, got 99999\" # Schema validation helm template . -f values.yaml | kubectl apply -f - --dry-run = client # Should succeed Part 9: Testing & Validation The Problem How do you verify 50+ teams' deployments are correct? Teams might deploy: - Without required probes - Without resource limits - With security vulnerabilities - With incorrect monitoring Requirements Helm Chart Tests Test: Pod is running Test: Service is accessible Test: Health checks respond Test: Metrics are exported Test: Security policies enforced Test: Ingress routing works Policy as Code No root containers No privileged mode Resources limits required Health checks required Labels present on all resources ServiceAccount bound to restricted role Pre-Install Validation Check cluster prerequisites Check available storage classes Check RBAC permissions Check API server version compatibility Validation Criteria # Run helm tests helm test app # Tests should verify: \u2713 Pod is running \u2713 Service is accessible \u2713 Health checks respond \u2713 Metrics endpoint works \u2713 Security context applied \u2713 Resource limits set # Policy validation helm template . | kubesec scan - # Should pass security checks helm template . | kubeval - # Should produce valid Kubernetes manifests helm lint . # Should pass without errors Part 10: Documentation & User Experience The Problem 50+ teams using your chart need to understand: - How to use it - What values are available - Examples for common scenarios - Troubleshooting help - Best practices Requirements README.md Quick start (3-step deployment) Values reference (all 100+ values documented) Examples (5+ common use cases) Troubleshooting (10+ common issues) Architecture diagram Performance tuning guide Example Values Files values-web-app.yaml : Simple web app values-stateful-app.yaml : App with database values-microservices.yaml : Multiple services values-dev.yaml : Development environment values-prod.yaml : Production environment Inline Documentation Comments in values.yaml explaining each field Comments in templates explaining complex logic Comments in _helpers.tpl explaining each function Generated Documentation Values schema auto-generates reference docs Examples auto-generated from schema Architecture rendered from code Validation Criteria # README quality checks wc -l README.md # Should be 500+ lines grep -c \"##\" README.md # Should have 10+ sections grep -c \"example\" README.md # Should have examples # Values are documented grep \"^# \" values.yaml | wc -l # Should have 50+ comment lines # Examples are valid for f in values-*.yaml ; do helm template . -f $f > /dev/null || echo \"Invalid: $f \" done # All should succeed # Schema is complete cat values.schema.json | jq '.properties | keys | length' # Should be 30+ documented fields What You're Building Summary By completing all 10 parts, you have: \u2705 Flexible chart supporting 5+ workload types \u2705 Complete schema validation with auto-generated docs \u2705 DRY templates with maximum reusability \u2705 Feature toggles for optional components \u2705 Multi-environment support with value inheritance \u2705 Parent/child charts for multi-service deployments \u2705 Lifecycle hooks for pre/post install/upgrade/delete \u2705 Comprehensive validation and error handling \u2705 Testing and policy enforcement \u2705 Complete user documentation This is a library. 50+ teams depend on it. Teams deploy new services in 5 minutes using just values.yaml. Total Implementation Time: 35-50 hours Key Real-World Helm Concepts Extreme Flexibility - Supporting diverse use cases with one template Helm Best Practices - Schema, hooks, testing, documentation Template Reusability - Helpers, DRY principle, avoiding duplication Composition Patterns - Parent charts, child charts, dependency management Conditional Rendering - Feature toggles, environment-specific configs Validation & Safety - Schema, pre-install checks, policy enforcement User Experience - Documentation, error messages, examples Testing & Verification - Helm tests, policy as code, integration tests Lifecycle Management - Hooks for install, upgrade, delete Enterprise Scale - Supporting 50+ teams with standardized patterns Progression Approach Start with: Part 1 (basic flexibility with workload types) - Then add: Part 2 (schema and documentation) - Then add: Part 3 (DRY templates with helpers) - Then add: Part 4 (feature toggles and conditionals) - Then add: Part 5 (multi-environment support) - Then add: Part 6 (parent/child charts) - Then add: Part 7 (lifecycle hooks) - Then add: Part 8 (validation) - Then add: Part 9 (testing) - Finally: Part 10 (documentation) Each part builds on previous ones. Don't skip steps. Notes for Success This is Helm , not Kubernetes architecture Focus on templating, composition, and reusability Real organizations have this exact problem Real constraints: 50+ teams, diverse use cases, zero documentation errors Real testing: comprehensive validation, no surprises Real documentation: teams should understand without asking This chart will be used by hundreds of deployments. Quality matters. Good luck. Build something that empowers your organization. \ud83d\ude80 What You'll Manage Cluster Provisioning Define cluster spec (size, region, version, node types) Automatically provision infrastructure (IaC) Install CNI, CSI, ingress controller Bootstrap cluster with base applications Multi-Cloud Support AWS (EKS) templates Azure (AKS) templates GCP (GKE) templates Same chart, different cloud backend Add-Ons Management Networking (Cilium, Calico, Flannel) Storage (Local, EBS, Azure Disk, GCP Disk) Ingress (NGINX, Traefik, AWS ALB) Monitoring (Prometheus, Datadog, New Relic) Logging (Loki, ELK, Splunk) Service Mesh (Istio, Linkerd, optional) Cluster Lifecycle Provision new cluster Upgrade cluster version Scale cluster up/down Decommission cluster Backup cluster state Restore from backup Multi-Tenancy Multiple teams on one cluster Resource quotas and isolation Network policies between teams RBAC scoped by team Compliance & Governance Enforce security policies Audit all changes Policy validation (no unencrypted storage, no privileged pods, etc.) Cost tracking per team GitOps Workflow All changes described in Git Automatic sync via Flux/ArgoCD PR-based approval workflow Audit trail of who changed what Self-Healing Automatic recovery from node failures Replace failed nodes Rebalance pods when needed Alert on degradation Architecture You'll Design Helm Charts Repo (Git) \u251c\u2500\u2500 clusters/ \u2502 \u251c\u2500\u2500 values-aws-prod-us-east-1.yaml \u2502 \u251c\u2500\u2500 values-aws-staging-us-west-2.yaml \u2502 \u251c\u2500\u2500 values-azure-prod-eastus.yaml \u2502 \u2514\u2500\u2500 values-gcp-dev-us-central1.yaml \u251c\u2500\u2500 base-chart/ \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u251c\u2500\u2500 values.yaml \u2502 \u2514\u2500\u2500 templates/ \u2502 \u251c\u2500\u2500 infrastructure/ \u2502 \u251c\u2500\u2500 cluster-addons/ \u2502 \u251c\u2500\u2500 networking/ \u2502 \u251c\u2500\u2500 storage/ \u2502 \u251c\u2500\u2500 monitoring/ \u2502 \u2514\u2500\u2500 governance/ \u2514\u2500\u2500 README.md Your Challenges Multi-Cloud Abstraction Write once, deploy to AWS/Azure/GCP Handle cloud-specific resources Manage secret injection per cloud Complex Dependencies CNI must be installed before workloads Storage CSI before PVCs Ingress after API server healthy Monitoring after clusters ready How do you order this? Cluster Size Variations Dev: 1 control plane, 2 workers Staging: 2 control planes, 5 workers Prod: 3 control planes, 20+ workers Different resource classes Regional Considerations Multi-region failover Data locality compliance Network latency optimization Different pricing per region Upgrade Strategy Upgrade Kubernetes version without downtime Upgrade add-ons without disruption Handle breaking changes in dependencies Rollback capability Cost Management Right-size clusters by workload Use spot instances where possible Shutdown dev clusters after hours Track spending per team Observability Across Clusters Centralized monitoring (see all clusters) Logs aggregated from all clusters Alerts fired from central monitoring But don't expose one cluster's data to another team Security at Scale Enforce Pod Security Standards Network policies prevent cluster escape RBAC prevents privilege escalation Audit logging immutable Secret management across regions Success Criteria New cluster provisioned from values.yaml without manual steps Cluster supports 3+ sizes (dev, staging, prod) Works across AWS, Azure, GCP Multi-tenancy with strong isolation Upgrades work without downtime Cost dashboard shows spending by team All changes tracked in Git Can provision/destroy cluster in <1 hour Monitoring shows health across all clusters Security policies enforced automatically Estimated Complexity 20+ resource types to manage 3 cloud providers Multiple add-on combinations Complex dependency ordering Multi-tenancy isolation Observability across clusters Time: 25-35 hours Project 3: CI/CD Pipeline as Helm Chart What You'll Build: A complete CI/CD platform (equivalent to Jenkins + GitLab CI + Argo Workflows) as a Helm chart. The Scenario Your organization's CI/CD setup is complex: - Multiple teams use different tools (Jenkins, GitLab CI, GitHub Actions) - Pipeline configs are scattered across repos - No standard way to define pipelines - Security is ad-hoc (secrets leaked in logs) - Integration between tools is manual You're building a unified CI/CD platform where: - Pipelines are defined in Git (PipelineResource CRDs) - Execution is automatic (Tekton Pipelines) - Security is built-in (secrets, RBAC, audit) - Observability is complete (logs, metrics, traces) - Teams can self-service without DevOps What You'll Build Pipeline Execution Engine Tekton Pipelines (or similar) Define pipelines as code (YAML) Run multiple pipeline versions simultaneously Support for parallel stages Conditional stage execution Code Repository Integration GitHub webhooks trigger pipelines GitLab push \u2192 pipeline runs GitOps: changes in Git automatically apply to cluster Pull request checks (CI gates) Build Orchestration Docker image builds Artifact storage (Nexus, Artifactory) Container scanning (Trivy, Snyk) Image signing and verification Multi-platform builds (ARM, x86) Testing Automation Unit tests in CI Integration tests Smoke tests on staged version Performance tests Security scanning (SAST, DAST) Deployment Pipeline Deploy to dev after merge Deploy to staging after tag Canary to prod (5% traffic) Full rollout after validation Automated rollback on failure Artifact Management Build artifacts versioned Container images with tags Helm charts pushed to repo SBOM (software bill of materials) Provenance tracking Security & Compliance No secrets in logs RBAC on pipeline execution Audit trail of all deployments Signed artifacts Compliance checks Notifications & Feedback Slack on pipeline success/failure Email to team leads Dashboard showing pipeline status Metrics on success rate, speed Alerts on broken builds Pipeline Templates Standard Node.js pipeline Standard Java/Maven pipeline Standard Python pipeline Standard Go pipeline Custom pipeline for special cases Resource Management Pipeline pods don't consume resources Auto-scaling for heavy pipelines Spot instances for build nodes Cache builds to speed up Your Challenges Pipeline as Code How do you define pipelines declaratively? How do you support 10+ languages? How do you allow customization? How do you validate pipeline configs? Secure Secrets Secrets for GitHub, Docker, databases Secrets rotated automatically Secrets never logged Different secrets per environment How do you inject them safely? Parallelization Multiple tests run in parallel Multiple image builds in parallel Coordinate between stages Manage resource contention Artifact Traceability Track what code produced which artifact Track which artifact deployed where Rollback by artifact version Reproducible builds Performance Optimization Cache dependencies (maven, npm, pip) Cache Docker layers Parallel test execution Reuse build output Target only affected services Multi-Version Testing Test against multiple Node versions Test against multiple Python versions Test against multiple Java versions Test on multiple OS (Linux, Windows) Deployment Strategies Blue-green for zero-downtime Canary with automatic rollback Rolling update with health checks Shadow traffic testing Dark launch capability Monitoring Deployments Is new version healthy? Are error rates increasing? Are latencies acceptable? Is resource usage normal? Automatic rollback if degradation Architecture You'll Design Git Repo Structure: \u251c\u2500\u2500 .pipelines/ \u2502 \u251c\u2500\u2500 nodejs.yaml # Reusable pipeline template \u2502 \u251c\u2500\u2500 python.yaml \u2502 \u251c\u2500\u2500 java.yaml \u2502 \u2514\u2500\u2500 deploy.yaml # Deployment pipeline \u251c\u2500\u2500 src/ \u2514\u2500\u2500 helm-chart/ # Application Helm chart Pipeline Execution: Code Push \u2192 GitHub Webhook \u2192 Tekton EventListener \u2193 Create PipelineRun \u2192 Execute Tasks in Parallel \u251c\u2500\u2500 Build Docker Image \u251c\u2500\u2500 Run Unit Tests \u251c\u2500\u2500 Run Integration Tests \u2514\u2500\u2500 Security Scan \u2193 Push Image to Registry (if tests pass) \u2193 Trigger Deployment Pipeline \u251c\u2500\u2500 Deploy to Dev \u251c\u2500\u2500 Deploy to Staging \u2514\u2500\u2500 Deploy to Prod (canary) \u2193 Monitor for Issues \u2192 Rollback if Needed Success Criteria Define pipeline in Git, runs automatically on push Multiple stages execute in parallel Secure secret handling (never logged) Build artifacts traced to code Deploy with zero downtime Automatic rollback on failure All actions audited and logged Performance metrics tracked Scaling works for heavy builds Team can define custom pipelines Estimated Complexity Tekton + EventListeners Multiple task types Conditional execution Artifact management Secret handling Deployment orchestration Monitoring and observability Time: 20-25 hours Project 4: Machine Learning Operations (MLOps) Platform What You'll Build: A Helm-based platform for training, deploying, and monitoring ML models. The Scenario Your company has data scientists building ML models but: - Model training is manual (scripts on laptops) - Deployment to production is ad-hoc - No A/B testing framework - No retraining pipeline - No model monitoring - Different versions conflict You're building MLOps: a platform where scientists describe experiments in code, and Helm manages training jobs, model deployment, and monitoring. What You'll Build Experiment Management Define training job parameters Track hyperparameters used Compare results across runs Reproducible experiments Training Infrastructure GPU/TPU provisioning Distributed training (multiple GPUs) Fault tolerance (resume from checkpoint) Notebook environments for exploration Job scheduling and queuing Model Registry Store trained models Version all models Track model lineage (which data, code) Quality metrics (accuracy, precision, recall) Approval workflow for production Model Serving Deploy models as HTTP endpoints Multiple model versions simultaneously A/B testing between versions Traffic shifting (canary deployments) Model scaling based on load Model monitoring and alerts Feature Engineering Shared feature store Consistent features in training and serving Feature versioning Feature pipeline (compute features) Cache for performance Data Pipeline Fetch data for training Data validation (detect drift) Data labeling workflow Data versioning Privacy-preserving (redaction, anonymization) Monitoring & Governance Track model performance over time Detect data drift (input distribution changed) Detect model drift (predictions changing) Alert when retraining needed Explainability and model interpretability Audit predictions for bias AutoML & Tuning Hyperparameter optimization Neural Architecture Search (optional) Model selection automation Ensemble methods Your Challenges Resource Management GPUs are expensive and shared How do you schedule training jobs fairly? How do you prevent one team from monopolizing resources? How do you autoscale GPU nodes? Experiment Reproducibility Same seed, same code \u2192 same results Track all dependencies (library versions) Containerize everything Tag images immutably Model Versioning Multiple models in production simultaneously Different model versions for different requests Gradual traffic shift from old to new Quick rollback if new model fails Data Handling Large datasets (TB scale) Efficient storage and access Privacy compliance (GDPR, HIPAA) Data retention policies Model Serving Performance Sub-100ms inference latency Handle traffic spikes GPU sharing between models CPU inference when GPU not needed Monitoring Model Quality Accuracy stays above threshold Latency stays below SLO Data drift detection Fairness/bias monitoring Automatic retraining Collaboration Scientists define experiments in notebooks Experiments become production pipelines Track who changed what Reproducible results Architecture You'll Design Training Phase: Git Push (new model code) \u2193 Trigger Training Job \u251c\u2500\u2500 Pull training data \u251c\u2500\u2500 Run hyperparameter tuning (parallel) \u251c\u2500\u2500 Select best model \u251c\u2500\u2500 Validate on test set \u2514\u2500\u2500 Push to model registry Serving Phase: Model Registry Push \u2193 Approval Gate (data scientist reviews) \u2193 Deploy to Staging \u251c\u2500\u2500 Serve model \u251c\u2500\u2500 Run smoke tests \u2514\u2500\u2500 Collect metrics \u2193 Deploy to Production (canary, 5% traffic) \u251c\u2500\u2500 Monitor error rates \u251c\u2500\u2500 Monitor latency \u251c\u2500\u2500 Monitor fairness metrics \u2514\u2500\u2500 After 24h, shift to 100% (or rollback) Monitoring Phase: Continuous Monitoring \u251c\u2500\u2500 Data drift detection \u251c\u2500\u2500 Model drift detection \u251c\u2500\u2500 Performance degradation \u2514\u2500\u2500 Alert if retraining needed \u2193 Trigger Retraining Pipeline Success Criteria Scientist defines experiment, training runs automatically Multiple models in production simultaneously Canary deployment for new models Automatic rollback if model performance drops Data drift detection triggers retraining All experiments reproducible Model lineage tracked (code, data, hyperparameters) Serving latency < 100ms GPU resources scheduled fairly Full audit trail Estimated Complexity Model training orchestration Model serving (KServe or Seldon) Feature store integration Data pipeline management Monitoring and governance Resource management (GPUs) Time: 25-30 hours Project 5: Complete Observability & AIOps Platform What You'll Build: An end-to-end observability platform with intelligent alerting and automatic remediation. The Scenario Your organization has: - Metrics from Prometheus (500+ servers) - Logs from ELK (petabytes/day) - Traces from Jaeger (millions/minute) - Hundreds of alert rules But: - Alert noise (70% false positives) - Slow incident response (manual investigation) - No correlation between metrics/logs/traces - Difficult root cause analysis - Manual remediation steps You're building AIOps: AI-powered observability with: - Intelligent alerting (correlation, deduplication) - Automatic root cause analysis - Self-healing (auto-remediation) - Predictive alerts (issue before it happens) What You'll Build Metrics Collection Prometheus for infrastructure Application metrics (custom) Business metrics (revenue, usage) Metric aggregation and long-term storage Cardinality management Logging Centralized log collection (Filebeat, Fluentd) Structured JSON logging Log parsing and enrichment Log-based alerting Long-term log archival Distributed Tracing Request tracing across services Latency analysis Dependency mapping Error tracing Trace sampling (intelligent) Alerting Rule engine (Prometheus rules, custom rules) Multi-condition alerts (AND, OR) Alert grouping (reduce noise) Alert deduplication Alert escalation Incident Management Create incidents from alerts Incident timeline (what happened when) Call incident commander Coordinate team response Post-mortem automation AIOps Features Anomaly detection (ML models) Correlation engine (alert A + alert B = root cause C) Root cause analysis (automatic) Predictive alerts (issue before it happens) Auto-remediation (trigger actions) Dashboarding KPI dashboards (SLI/SLO) Service health dashboard Dependency map visualization Trend analysis Cost impact of incidents Automation & Remediation Auto-scale on high load Restart failing services Drain misbehaving nodes Trigger disaster recovery Rollback bad deployments Your Challenges Scaling to Massive Data Handle petabytes of logs Query billions of metrics Trace millions of requests Keep response time < 1 second Alert Quality 70% of alerts are noise How do you deduplicate? How do you correlate? How do you reduce false positives? Root Cause Analysis Request took 10s (slow) Which service is slow? Which database query is slow? Which code change caused it? Automatically detect and report Multi-Tenancy Team A shouldn't see Team B's metrics/logs/traces But correlation needs cross-team data How do you handle this tension? Data Retention Metrics for 2 years (expensive) Logs for 1 year Traces for 30 days Smart data tiering Compression strategies Predictive Alerting Predict failures before they happen How do you train models? How do you avoid false positives? How do you explain predictions? Automated Remediation Auto-restart services on failure Auto-scale on high load Auto-drain failing nodes Know which actions are safe Prevent cascading failures Architecture You'll Design Data Collection: \u251c\u2500\u2500 Metrics (Prometheus) \u2192 TSDB \u251c\u2500\u2500 Logs (Fluentd) \u2192 Search (Elasticsearch) \u2514\u2500\u2500 Traces (Jaeger) \u2192 Trace Storage Processing Pipeline: \u251c\u2500\u2500 Alert Rules (Prometheus) \u2192 Create Alerts \u251c\u2500\u2500 Correlation Engine \u2192 Group Related Alerts \u251c\u2500\u2500 Anomaly Detection \u2192 Predict Issues \u251c\u2500\u2500 Root Cause Analysis \u2192 Identify Culprit \u2514\u2500\u2500 Recommendation Engine \u2192 Suggest Fix Output: \u251c\u2500\u2500 Alert to PagerDuty \u251c\u2500\u2500 Incident Created \u251c\u2500\u2500 Auto-Remediation Triggered \u251c\u2500\u2500 Dashboards Updated \u2514\u2500\u2500 Post-Mortem Data Collected Success Criteria Ingest petabytes of data Alert latency < 10 seconds Alert accuracy > 95% (few false positives) Root cause identified in < 5 minutes Auto-remediation success > 90% Reduce MTTR (mean time to recovery) by 70% Predictive alerts work (catch issues early) Full audit trail of all changes Multi-tenant with strong isolation Cost tracking per team Estimated Complexity Multiple data sources (metrics, logs, traces) Scaling and performance optimization ML models for anomaly detection Correlation and root cause logic Automated remediation Multi-tenancy Time: 30-40 hours Project 6: Data Platform (Lakehouse) What You'll Build: A complete data platform (data lake + warehouse) for analytics and reporting. The Scenario Your company collects data from: - Web application (event data) - Mobile app (user behavior) - IoT devices (sensor data) - Third-party APIs (external data) Currently: - Data is siloed in different systems - No unified schema - Difficult to correlate data - Slow to answer questions - No data governance You're building a data lakehouse: unified data platform for analytics. What You'll Build Data Ingestion Streaming (events in real-time) Batch (nightly imports) CDC (change data capture) API polling File uploads Data Processing ETL (extract, transform, load) Data validation Schema enforcement Deduplication Aggregations Data Storage Object storage (Parquet, Iceberg) Columnar format for analytics Time-series optimized Partitioning for performance Data tiering (hot/warm/cold) Data Warehouse SQL interface (ClickHouse, Snowflake) Dimensional modeling (facts, dimensions) Slowly changing dimensions Materialized views Incremental updates Metadata & Governance Data catalog (what data exists) Data lineage (where did this come from) Data quality checks Privacy controls (PII masking) Retention policies Analytics & BI Ad-hoc SQL queries Pre-built dashboards Self-service analytics Drill-down capability Scheduled reports Machine Learning Export data for training Features for models Model predictions back to warehouse A/B testing framework Your Challenges Volume at Scale Ingest 1TB/day Store 100TB total Query across years Keep costs reasonable Schema Evolution New fields added over time Old fields deprecated Type changes (string \u2192 integer) How do you handle this? Data Quality Validate as data arrives Detect anomalies Alert on schema violations Quarantine bad data Privacy Compliance GDPR: right to be forgotten HIPAA: audit access PII: redact or encrypt Retention: delete after period Performance Query 1TB of data in < 5 seconds Aggregations very fast Joins across large tables Incremental updates efficient Cost Optimization S3 storage cheaper than database Compression reduces cost Tiering saves money Prune old data Success Criteria Ingest 1TB/day without issues Queries run in < 5 seconds Data quality > 99% Privacy controls enforced Full audit trail Self-service analytics works Metadata is complete and accurate Cost efficient Retention policies followed Estimated Complexity Multiple data sources ETL pipeline orchestration Schema management Query optimization Governance and compliance Time: 20-25 hours Which Project Should You Start With? If you want end-to-end experience: Start with Project 1 (SaaS Platform) - Teaches multi-component charts - Covers scaling, security, upgrades - Most useful for real-world apps - Most satisfying to deploy If you want infrastructure focus: Start with Project 2 (IaC Platform) - Teaches multi-cloud abstraction - Covers infrastructure orchestration - Complex dependency management - Valuable for DevOps/platform teams If you want automation focus: Start with Project 3 (CI/CD Platform) - Teaches workflow orchestration - Covers secret management - Automated deployment pipelines - Valuable for DevOps engineers If you want ML focus: Start with Project 4 (MLOps Platform) - Teaches specialized workload management - Covers GPU scheduling, model serving - Valuable for ML engineers - Unique challenges around experiments If you want observability focus: Start with Project 5 (Observability Platform) - Teaches time-series data handling - Covers monitoring and alerting - Valuable for SREs/platform teams - Complex aggregation logic If you want analytics focus: Start with Project 6 (Data Platform) - Teaches data pipeline orchestration - Covers large-scale data handling - Valuable for data engineers - Cost optimization important Project Complexity Comparison Project Components Complexity Time Best For 1 (SaaS) 8+ services High 20-30h Full-stack Helm mastery 2 (IaC) Multi-cloud, add-ons Very High 25-35h Infrastructure teams 3 (CI/CD) Pipeline orchestration Very High 20-25h DevOps automation 4 (MLOps) ML workloads, serving High 25-30h ML infrastructure 5 (Observability) Multi-datasource Very High 30-40h SRE/Monitoring 6 (Data) Data pipelines High 20-25h Data engineers Learning Path Start with Project 1 or 3 (most applicable) Then pick a second project based on your domain Then tackle hardest project (usually Project 2 or 5) Combine learnings from all projects into your own system What You'll Learn Across All Projects \u2705 Multi-component chart architecture \u2705 Dependency management and ordering \u2705 Multi-environment support \u2705 Scaling patterns (horizontal, vertical) \u2705 Stateful application management \u2705 Backup and recovery \u2705 Security hardening and RBAC \u2705 Observability integration \u2705 Upgrade strategies \u2705 Cost optimization \u2705 Production-grade operations \u2705 Git-based workflows \u2705 Automated testing and validation \u2705 Real-world complexity handling Success Criteria for Any Project When you finish, you should be able to: \u2705 Deploy the complete system with one Helm command \u2705 Scale up/down smoothly without downtime \u2705 Upgrade to new versions safely \u2705 Handle failures gracefully \u2705 Monitor and observe everything \u2705 Maintain all state (backups, recovery) \u2705 Enforce security policies \u2705 Track costs \u2705 Audit all actions \u2705 Explain your design to others If you can do all 10, you've mastered Helm at a professional level. \ud83c\udfaf Good luck! Pick one project and build something meaningful! \ud83d\ude80","title":"Question4"},{"location":"02%20helm/question4/#meaningful-question-4-build-an-enterprise-helm-chart-library-real-helm-complexity","text":"What You'll Build: A reusable Helm chart library that powers your entire organization. 50+ teams use it to deploy their applications without touching Kubernetes manifests. Chart Name: app-platform (base chart library) Philosophy: This is about Helm, not Kubernetes. You're solving real templating, composition, and reusability challenges.","title":"Meaningful Question 4: Build an Enterprise Helm Chart Library - Real Helm Complexity"},{"location":"02%20helm/question4/#the-business-context","text":"Your organization has 50+ teams deploying applications to Kubernetes: Current State (The Problem): - Each team writes their own manifests (Deployment, Service, Ingress, etc.) - Inconsistent patterns across teams (no standards) - Security varies wildly (some pods run as root, some don't) - Upgrades are manual and error-prone - Different teams solve the same problem 50 different ways - Onboarding new teams takes weeks Your Goal: - Build ONE Helm chart that 50+ teams can use - Teams only write values.yaml , no template changes - Standardized patterns (security, monitoring, networking) - Self-service deployments - Automatic compliance checking","title":"The Business Context"},{"location":"02%20helm/question4/#the-challenge-extreme-helm-flexibility","text":"Your chart must support: Use Case Example Helm Challenge Stateless Web App Node.js API Basic Deployment Stateful Database PostgreSQL StatefulSet with PVC Batch Job Data processor CronJob, Job cleanup Worker Queue RabbitMQ consumer Deployment + custom config External Service Third-party API Service without pods Microservices 10 services in 1 chart Dependencies, shared values API Gateway Kong, Traefik Multiple replicas, plugins Lambda-like Function runner Pod per invocation Cache Layer Redis Optional, conditional Message Queue Kafka, RabbitMQ Optional, conditional Problem: How do you make ONE chart flexible enough for all these, but simple enough that teams only specify values?","title":"The Challenge: Extreme Helm Flexibility"},{"location":"02%20helm/question4/#part-1-chart-architecture-flexibility","text":"","title":"Part 1: Chart Architecture &amp; Flexibility"},{"location":"02%20helm/question4/#the-design-challenge","text":"You need to support different workload types with minimal configuration: # Team 1: Simple web app workloadType : deployment replicas : 3 image : myapp:1.0 # Team 2: Stateful database workloadType : statefulset replicas : 3 persistence : enabled : true size : 100Gi # Team 3: Scheduled job workloadType : cronjob schedule : \"0 2 * * *\" # Team 4: Multiple services in one chart services : api : workloadType : deployment replicas : 3 worker : workloadType : deployment replicas : 2 cache : workloadType : deployment replicas : 1","title":"The Design Challenge"},{"location":"02%20helm/question4/#requirements","text":"Template Structure Use conditional logic to include only needed manifests DRY principle: reuse pod specs across workload types Support 5+ workload types (Deployment, StatefulSet, DaemonSet, Job, CronJob) Shared template helpers for common patterns Workload Types deployment : Stateless app (replicas, rolling update) statefulset : Stateful app (stable identity, persistent storage) daemonset : Node-local app (one per node) job : Run once, complete cronjob : Run on schedule external : No pods (external service proxy) Flexible Configuration One values.yaml controls everything Support single service or multiple services Optional components (cache, database, monitoring) Different security levels per component Validation Invalid workloadType combinations fail validation Missing required values fail early StatefulSet must have persistence enabled CronJob must have valid cron schedule","title":"Requirements"},{"location":"02%20helm/question4/#validation-criteria","text":"# Deploy web app (deployment) helm install web-app . -f values-web.yaml kubectl get deployment # Deploy database (statefulset) helm install database . -f values-statefulset.yaml kubectl get statefulset # Deploy cronjob helm install scheduler . -f values-cronjob.yaml kubectl get cronjob # Deploy multi-service helm install platform . -f values-multi.yaml kubectl get deployment,statefulset # Should show api deployment, worker deployment, cache deployment # Validation test: invalid workloadType helm template . -f values-invalid.yaml 2 > & 1 | grep -i error # Should fail with clear error message","title":"Validation Criteria"},{"location":"02%20helm/question4/#part-2-values-schema-auto-documentation","text":"","title":"Part 2: Values Schema &amp; Auto-Documentation"},{"location":"02%20helm/question4/#the-problem","text":"Teams ask: - \"What values can I set?\" - \"What are the defaults?\" - \"Which values are required?\" - \"Can I use image tags or only repo URLs?\" You need to answer without manually documenting 200+ values.","title":"The Problem"},{"location":"02%20helm/question4/#requirements_1","text":"JSON Schema (values.schema.json) Complete schema for all valid values Type checking (string, number, boolean, array, object) Required vs optional fields Enum validation (e.g., workloadType must be one of: deployment, statefulset, ...) Min/max validation (replicas: min 1, max 1000) Pattern validation (image: must match regex) Descriptions for each value Auto-Generated Documentation README generated from schema Example values.yaml files for common use cases Inline comments in schema explaining each field Validation helm template fails if invalid values provided Clear error messages: \"replicas must be >= 1, got -5\" Suggest fixes: \"workloadType must be one of: [deployment, statefulset, job, cronjob, daemonset]\"","title":"Requirements"},{"location":"02%20helm/question4/#validation-criteria_1","text":"# Valid values should work helm template . -f valid-values.yaml > /dev/null echo $? # Should be 0 # Invalid values should fail with clear error helm template . -f invalid-values.yaml 2 > & 1 # Should show: \"Error: replicas must be >= 1, got 0\" # Should show: \"Error: workloadType must be one of: [deployment, statefulset, ...]\" # Schema should be complete cat values.schema.json | jq '.properties | keys | length' # Should be 30+ properties # Documentation should be auto-generated cat README.md | grep -c \"##\" # Should have multiple sections auto-generated","title":"Validation Criteria"},{"location":"02%20helm/question4/#part-3-reusable-template-blocks-dry-principle","text":"","title":"Part 3: Reusable Template Blocks &amp; DRY Principle"},{"location":"02%20helm/question4/#the-problem_1","text":"You have 20+ manifest templates. Many are similar: # deployment.yaml apiVersion : apps/v1 kind : Deployment metadata : name : {{ .Values.name }} labels : app : {{ .Values.name }} version : {{ .Values.version }} spec : replicas : {{ .Values.replicas }} selector : matchLabels : app : {{ .Values.name }} template : metadata : labels : app : {{ .Values.name }} spec : containers : - name : app image : {{ .Values.image }} ports : - containerPort : {{ .Values.port }} # statefulset.yaml apiVersion : apps/v1 kind : StatefulSet metadata : name : {{ .Values.name }} labels : app : {{ .Values.name }} version : {{ .Values.version }} spec : replicas : {{ .Values.replicas }} selector : matchLabels : app : {{ .Values.name }} template : metadata : labels : app : {{ .Values.name }} spec : containers : - name : app image : {{ .Values.image }} ports : - containerPort : {{ .Values.port }} Problem: Labels, selectors, container specs are duplicated. Change one place, forgot 5 others.","title":"The Problem"},{"location":"02%20helm/question4/#requirements_2","text":"Template Helpers (_helpers.tpl) Reusable blocks for common patterns Labels helper: generate consistent labels Selectors helper: generate consistent selectors Container spec helper: generate container template Health checks helper: generate probes Security context helper: generate security settings Environment variables helper: handle secrets vs configmaps DRY Principle Pod spec defined once, reused in Deployment, StatefulSet, Job Labels defined once, used everywhere Probes defined once, applied to all workloads Security context defined once, applied everywhere Consistency All pods get same security standards All pods have same labels All pods have same monitoring sidecars Changing one helper updates everywhere","title":"Requirements"},{"location":"02%20helm/question4/#validation-criteria_2","text":"# Generate manifests helm template . -f values.yaml > manifests.yaml # Check: All labels are consistent grep -c \"app: myapp\" manifests.yaml # Should be 10+ (pod spec, deployment, service, etc.) # Check: Selector matches labels grep -A 2 \"matchLabels:\" manifests.yaml # Should show \"app: myapp\" matching pod labels # Check: Security context applied grep -c \"runAsNonRoot\" manifests.yaml # Should be 4+ (one per pod) # Verify DRY: Change label in helper # Rerun template, all manifests should update # Count lines of code wc -l templates/*.yaml templates/_helpers.tpl # _helpers.tpl should be significant (200+), reused across 5+ templates","title":"Validation Criteria"},{"location":"02%20helm/question4/#part-4-conditional-rendering-feature-toggles","text":"","title":"Part 4: Conditional Rendering &amp; Feature Toggles"},{"location":"02%20helm/question4/#the-problem_2","text":"Teams ask: - \"Can I disable monitoring for dev environment?\" - \"Can I skip database backups for testing?\" - \"Can I disable security policies for local development?\" You need sophisticated conditional logic.","title":"The Problem"},{"location":"02%20helm/question4/#requirements_3","text":"Feature Toggles monitoring.enabled : Include ServiceMonitor, PrometheusRule ingress.enabled : Include Ingress, adjust Service type persistence.enabled : Include PVC, use emptyDir if not rbac.enabled : Include ServiceAccount, Role, RoleBinding networkPolicy.enabled : Include NetworkPolicy backup.enabled : Include backup CronJob Conditional Dependencies If persistence.enabled , must have persistence.storageClass If monitoring.enabled , must have monitoring.scrapeInterval If ingress.enabled , must have ingress.host If rbac.enabled , can't use runAsUser: 0 Environment-Specific Configs Development: monitoring disabled, no ingress, local storage Staging: monitoring enabled, ingress + TLS, persistent storage Production: monitoring required, security hardened, HA","title":"Requirements"},{"location":"02%20helm/question4/#validation-criteria_3","text":"# Deploy with minimal features (dev) helm template . -f values-dev.yaml | kubectl apply -f - --dry-run = client # Should work, only pod+service # Deploy with all features (prod) helm template . -f values-prod.yaml | kubectl apply -f - --dry-run = client # Should include monitoring, ingress, security, persistence # Verify conditional logic helm template . -f values-monitoring-disabled.yaml | grep -i servicemonitor # Should return nothing helm template . -f values-monitoring-enabled.yaml | grep -i servicemonitor # Should show ServiceMonitor resource # Verify dependency validation helm template . -f values-persistence-enabled-but-no-storageclass.yaml 2 > & 1 # Should fail with clear error","title":"Validation Criteria"},{"location":"02%20helm/question4/#part-5-multi-environment-values-inheritance","text":"","title":"Part 5: Multi-Environment &amp; Values Inheritance"},{"location":"02%20helm/question4/#the-problem_3","text":"Teams deploy same app to dev, staging, production with different configs: # All similar structure, different values replicas : dev : 1 staging : 2 prod : 3 resources : dev : { cpu : 100m , memory : 128Mi } staging : { cpu : 500m , memory : 512Mi } prod : { cpu : 1000m , memory : 2Gi } ingress : dev : { tls : false , domain : dev.example.com } staging : { tls : true , domain : staging.example.com } prod : { tls : true , domain : app.example.com } Problem: How do you manage all this without duplicating values 3 times?","title":"The Problem"},{"location":"02%20helm/question4/#requirements_4","text":"Values Hierarchy values.yaml : defaults (works for most cases) values-dev.yaml : override for dev (small replicas, low resources) values-staging.yaml : override for staging (medium replicas, medium resources) values-prod.yaml : override for prod (high replicas, high resources, security hardened) Smart Defaults Default values.yaml is functional for dev Staging only needs to override replicas and resources Prod only needs to override security and monitoring settings No value specified in 3 places unnecessarily Values Composition Parent chart passes values to child charts correctly Global values accessible to all children Component-specific values only affect that component","title":"Requirements"},{"location":"02%20helm/question4/#validation-criteria_4","text":"# Dev deployment (minimal) helm template . -f values-dev.yaml | grep replicas: # Should show replicas: 1 # Staging deployment helm template . -f values-staging.yaml | grep replicas: # Should show replicas: 2 # Prod deployment helm template . -f values-prod.yaml | grep replicas: # Should show replicas: 3 # Values files should be small (no duplication) wc -l values.yaml values-*.yaml # values.yaml should be 100+ lines, each override should be 20-30 lines only","title":"Validation Criteria"},{"location":"02%20helm/question4/#part-6-parent-charts-multi-service-deployments","text":"","title":"Part 6: Parent Charts &amp; Multi-Service Deployments"},{"location":"02%20helm/question4/#the-problem_4","text":"One team wants to deploy: - API server - Worker - Cache - Database All as one unit, with shared configuration.","title":"The Problem"},{"location":"02%20helm/question4/#requirements_5","text":"Parent Chart (Umbrella) Includes child charts as dependencies Each child is a service (api, worker, cache, database) Shared values passed to relevant children Version constraints on child charts Child Charts Each service is a reusable chart Can be deployed standalone or as part of parent Accepts shared values (labels, monitoring config, security) Can override specific settings Dependency Management api depends on database (waits for startup) worker depends on cache Ordering in templates ensures dependencies start first Health checks verify dependencies are ready","title":"Requirements"},{"location":"02%20helm/question4/#validation-criteria_5","text":"# Deploy parent chart helm install platform . -f values-multi.yaml # Verify all services deployed kubectl get deployment # Should show: api, worker kubectl get statefulset # Should show: cache, database # Verify dependency ordering kubectl get events | grep Created # Database created first, then api # Verify shared values applied kubectl get deployment -o yaml | grep -i monitoring # All deployments should have monitoring enabled","title":"Validation Criteria"},{"location":"02%20helm/question4/#part-7-helm-hooks-lifecycle-events","text":"","title":"Part 7: Helm Hooks &amp; Lifecycle Events"},{"location":"02%20helm/question4/#the-problem_5","text":"Teams need to run tasks at specific lifecycle points: Pre-install: Create namespace, apply CRDs Post-install: Run migrations, seed data Pre-upgrade: Backup database Post-upgrade: Verify app is healthy Pre-delete: Export data, cleanup Post-delete: Remove PVCs, cleanup resources","title":"The Problem"},{"location":"02%20helm/question4/#requirements_6","text":"Pre-Install Hooks Install CRDs (must run before any resources reference them) Create namespaces with labels Apply RBAC cluster-wide (if needed) Validate cluster prerequisites Post-Install Hooks Run database migrations Seed initial data Run smoke tests Generate initial config Pre-Upgrade Hooks Backup database to S3 Run pre-upgrade validation Check for breaking changes Warn if downtime required Post-Upgrade Hooks Run schema migrations Validate data integrity Run smoke tests Notify team of upgrade completion Pre-Delete Hooks Export data to S3 Run cleanup tasks Collect logs for archival","title":"Requirements"},{"location":"02%20helm/question4/#validation-criteria_6","text":"# Install and watch hooks run helm install app . --debug # Should see: pre-install hook running, post-install hook running # Upgrade and watch hooks helm upgrade app . --set version = 2 --debug # Should see: pre-upgrade hook (backup), post-upgrade hook (migrate) # Verify hook ordering kubectl get events | grep hook # Should show: pre-install, post-install, pre-upgrade, post-upgrade in order # Verify hooks only run once helm upgrade app . --debug # Should not re-run post-install hook # Should run pre-upgrade and post-upgrade hooks","title":"Validation Criteria"},{"location":"02%20helm/question4/#part-8-values-validation-error-handling","text":"","title":"Part 8: Values Validation &amp; Error Handling"},{"location":"02%20helm/question4/#the-problem_6","text":"Teams sometimes provide invalid values: replicas : -5 # negative! image : \"\" # empty! port : 99999 # invalid! workloadType : lambda # not supported! Result: Chart installs but deployment fails mysteriously.","title":"The Problem"},{"location":"02%20helm/question4/#requirements_7","text":"Schema Validation replicas : integer, >= 1, <= 100 image : string, required, matches regex ^[a-z0-9:/.]+$ port : integer, >= 1, <= 65535 workloadType : enum [deployment, statefulset, job, cronjob, daemonset] resources.requests.cpu : string, matches Kubernetes CPU regex Smart Error Messages Show what went wrong Show what should be fixed Suggest valid options Optional Pre-Deployment Validation Custom template validation logic Check interdependencies Fail early with clear errors","title":"Requirements"},{"location":"02%20helm/question4/#validation-criteria_7","text":"# Invalid values fail immediately helm template . -f values-invalid.yaml 2 > & 1 # Should show: \"Error: replicas must be >= 1 and <= 100, got -5\" # Should show: \"Did you mean: replicas: 1?\" # Missing required values fail helm template . -f values-incomplete.yaml 2 > & 1 # Should show: \"Error: image is required, got empty string\" # Invalid workloadType fails helm template . --set workloadType = lambda 2 > & 1 # Should show: \"Error: workloadType must be one of: [deployment, statefulset, job, ...]\" # \"Got: lambda\" # Invalid port fails helm template . --set port = 99999 2 > & 1 # Should show: \"Error: port must be between 1 and 65535, got 99999\" # Schema validation helm template . -f values.yaml | kubectl apply -f - --dry-run = client # Should succeed","title":"Validation Criteria"},{"location":"02%20helm/question4/#part-9-testing-validation","text":"","title":"Part 9: Testing &amp; Validation"},{"location":"02%20helm/question4/#the-problem_7","text":"How do you verify 50+ teams' deployments are correct? Teams might deploy: - Without required probes - Without resource limits - With security vulnerabilities - With incorrect monitoring","title":"The Problem"},{"location":"02%20helm/question4/#requirements_8","text":"Helm Chart Tests Test: Pod is running Test: Service is accessible Test: Health checks respond Test: Metrics are exported Test: Security policies enforced Test: Ingress routing works Policy as Code No root containers No privileged mode Resources limits required Health checks required Labels present on all resources ServiceAccount bound to restricted role Pre-Install Validation Check cluster prerequisites Check available storage classes Check RBAC permissions Check API server version compatibility","title":"Requirements"},{"location":"02%20helm/question4/#validation-criteria_8","text":"# Run helm tests helm test app # Tests should verify: \u2713 Pod is running \u2713 Service is accessible \u2713 Health checks respond \u2713 Metrics endpoint works \u2713 Security context applied \u2713 Resource limits set # Policy validation helm template . | kubesec scan - # Should pass security checks helm template . | kubeval - # Should produce valid Kubernetes manifests helm lint . # Should pass without errors","title":"Validation Criteria"},{"location":"02%20helm/question4/#part-10-documentation-user-experience","text":"","title":"Part 10: Documentation &amp; User Experience"},{"location":"02%20helm/question4/#the-problem_8","text":"50+ teams using your chart need to understand: - How to use it - What values are available - Examples for common scenarios - Troubleshooting help - Best practices","title":"The Problem"},{"location":"02%20helm/question4/#requirements_9","text":"README.md Quick start (3-step deployment) Values reference (all 100+ values documented) Examples (5+ common use cases) Troubleshooting (10+ common issues) Architecture diagram Performance tuning guide Example Values Files values-web-app.yaml : Simple web app values-stateful-app.yaml : App with database values-microservices.yaml : Multiple services values-dev.yaml : Development environment values-prod.yaml : Production environment Inline Documentation Comments in values.yaml explaining each field Comments in templates explaining complex logic Comments in _helpers.tpl explaining each function Generated Documentation Values schema auto-generates reference docs Examples auto-generated from schema Architecture rendered from code","title":"Requirements"},{"location":"02%20helm/question4/#validation-criteria_9","text":"# README quality checks wc -l README.md # Should be 500+ lines grep -c \"##\" README.md # Should have 10+ sections grep -c \"example\" README.md # Should have examples # Values are documented grep \"^# \" values.yaml | wc -l # Should have 50+ comment lines # Examples are valid for f in values-*.yaml ; do helm template . -f $f > /dev/null || echo \"Invalid: $f \" done # All should succeed # Schema is complete cat values.schema.json | jq '.properties | keys | length' # Should be 30+ documented fields","title":"Validation Criteria"},{"location":"02%20helm/question4/#what-youre-building-summary","text":"By completing all 10 parts, you have: \u2705 Flexible chart supporting 5+ workload types \u2705 Complete schema validation with auto-generated docs \u2705 DRY templates with maximum reusability \u2705 Feature toggles for optional components \u2705 Multi-environment support with value inheritance \u2705 Parent/child charts for multi-service deployments \u2705 Lifecycle hooks for pre/post install/upgrade/delete \u2705 Comprehensive validation and error handling \u2705 Testing and policy enforcement \u2705 Complete user documentation This is a library. 50+ teams depend on it. Teams deploy new services in 5 minutes using just values.yaml. Total Implementation Time: 35-50 hours","title":"What You're Building Summary"},{"location":"02%20helm/question4/#key-real-world-helm-concepts","text":"Extreme Flexibility - Supporting diverse use cases with one template Helm Best Practices - Schema, hooks, testing, documentation Template Reusability - Helpers, DRY principle, avoiding duplication Composition Patterns - Parent charts, child charts, dependency management Conditional Rendering - Feature toggles, environment-specific configs Validation & Safety - Schema, pre-install checks, policy enforcement User Experience - Documentation, error messages, examples Testing & Verification - Helm tests, policy as code, integration tests Lifecycle Management - Hooks for install, upgrade, delete Enterprise Scale - Supporting 50+ teams with standardized patterns","title":"Key Real-World Helm Concepts"},{"location":"02%20helm/question4/#progression-approach","text":"Start with: Part 1 (basic flexibility with workload types) - Then add: Part 2 (schema and documentation) - Then add: Part 3 (DRY templates with helpers) - Then add: Part 4 (feature toggles and conditionals) - Then add: Part 5 (multi-environment support) - Then add: Part 6 (parent/child charts) - Then add: Part 7 (lifecycle hooks) - Then add: Part 8 (validation) - Then add: Part 9 (testing) - Finally: Part 10 (documentation) Each part builds on previous ones. Don't skip steps.","title":"Progression Approach"},{"location":"02%20helm/question4/#notes-for-success","text":"This is Helm , not Kubernetes architecture Focus on templating, composition, and reusability Real organizations have this exact problem Real constraints: 50+ teams, diverse use cases, zero documentation errors Real testing: comprehensive validation, no surprises Real documentation: teams should understand without asking This chart will be used by hundreds of deployments. Quality matters. Good luck. Build something that empowers your organization. \ud83d\ude80","title":"Notes for Success"},{"location":"02%20helm/question4/#what-youll-manage","text":"Cluster Provisioning Define cluster spec (size, region, version, node types) Automatically provision infrastructure (IaC) Install CNI, CSI, ingress controller Bootstrap cluster with base applications Multi-Cloud Support AWS (EKS) templates Azure (AKS) templates GCP (GKE) templates Same chart, different cloud backend Add-Ons Management Networking (Cilium, Calico, Flannel) Storage (Local, EBS, Azure Disk, GCP Disk) Ingress (NGINX, Traefik, AWS ALB) Monitoring (Prometheus, Datadog, New Relic) Logging (Loki, ELK, Splunk) Service Mesh (Istio, Linkerd, optional) Cluster Lifecycle Provision new cluster Upgrade cluster version Scale cluster up/down Decommission cluster Backup cluster state Restore from backup Multi-Tenancy Multiple teams on one cluster Resource quotas and isolation Network policies between teams RBAC scoped by team Compliance & Governance Enforce security policies Audit all changes Policy validation (no unencrypted storage, no privileged pods, etc.) Cost tracking per team GitOps Workflow All changes described in Git Automatic sync via Flux/ArgoCD PR-based approval workflow Audit trail of who changed what Self-Healing Automatic recovery from node failures Replace failed nodes Rebalance pods when needed Alert on degradation","title":"What You'll Manage"},{"location":"02%20helm/question4/#architecture-youll-design","text":"Helm Charts Repo (Git) \u251c\u2500\u2500 clusters/ \u2502 \u251c\u2500\u2500 values-aws-prod-us-east-1.yaml \u2502 \u251c\u2500\u2500 values-aws-staging-us-west-2.yaml \u2502 \u251c\u2500\u2500 values-azure-prod-eastus.yaml \u2502 \u2514\u2500\u2500 values-gcp-dev-us-central1.yaml \u251c\u2500\u2500 base-chart/ \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u251c\u2500\u2500 values.yaml \u2502 \u2514\u2500\u2500 templates/ \u2502 \u251c\u2500\u2500 infrastructure/ \u2502 \u251c\u2500\u2500 cluster-addons/ \u2502 \u251c\u2500\u2500 networking/ \u2502 \u251c\u2500\u2500 storage/ \u2502 \u251c\u2500\u2500 monitoring/ \u2502 \u2514\u2500\u2500 governance/ \u2514\u2500\u2500 README.md","title":"Architecture You'll Design"},{"location":"02%20helm/question4/#your-challenges","text":"Multi-Cloud Abstraction Write once, deploy to AWS/Azure/GCP Handle cloud-specific resources Manage secret injection per cloud Complex Dependencies CNI must be installed before workloads Storage CSI before PVCs Ingress after API server healthy Monitoring after clusters ready How do you order this? Cluster Size Variations Dev: 1 control plane, 2 workers Staging: 2 control planes, 5 workers Prod: 3 control planes, 20+ workers Different resource classes Regional Considerations Multi-region failover Data locality compliance Network latency optimization Different pricing per region Upgrade Strategy Upgrade Kubernetes version without downtime Upgrade add-ons without disruption Handle breaking changes in dependencies Rollback capability Cost Management Right-size clusters by workload Use spot instances where possible Shutdown dev clusters after hours Track spending per team Observability Across Clusters Centralized monitoring (see all clusters) Logs aggregated from all clusters Alerts fired from central monitoring But don't expose one cluster's data to another team Security at Scale Enforce Pod Security Standards Network policies prevent cluster escape RBAC prevents privilege escalation Audit logging immutable Secret management across regions","title":"Your Challenges"},{"location":"02%20helm/question4/#success-criteria","text":"New cluster provisioned from values.yaml without manual steps Cluster supports 3+ sizes (dev, staging, prod) Works across AWS, Azure, GCP Multi-tenancy with strong isolation Upgrades work without downtime Cost dashboard shows spending by team All changes tracked in Git Can provision/destroy cluster in <1 hour Monitoring shows health across all clusters Security policies enforced automatically","title":"Success Criteria"},{"location":"02%20helm/question4/#estimated-complexity","text":"20+ resource types to manage 3 cloud providers Multiple add-on combinations Complex dependency ordering Multi-tenancy isolation Observability across clusters Time: 25-35 hours","title":"Estimated Complexity"},{"location":"02%20helm/question4/#project-3-cicd-pipeline-as-helm-chart","text":"What You'll Build: A complete CI/CD platform (equivalent to Jenkins + GitLab CI + Argo Workflows) as a Helm chart.","title":"Project 3: CI/CD Pipeline as Helm Chart"},{"location":"02%20helm/question4/#the-scenario","text":"Your organization's CI/CD setup is complex: - Multiple teams use different tools (Jenkins, GitLab CI, GitHub Actions) - Pipeline configs are scattered across repos - No standard way to define pipelines - Security is ad-hoc (secrets leaked in logs) - Integration between tools is manual You're building a unified CI/CD platform where: - Pipelines are defined in Git (PipelineResource CRDs) - Execution is automatic (Tekton Pipelines) - Security is built-in (secrets, RBAC, audit) - Observability is complete (logs, metrics, traces) - Teams can self-service without DevOps","title":"The Scenario"},{"location":"02%20helm/question4/#what-youll-build","text":"Pipeline Execution Engine Tekton Pipelines (or similar) Define pipelines as code (YAML) Run multiple pipeline versions simultaneously Support for parallel stages Conditional stage execution Code Repository Integration GitHub webhooks trigger pipelines GitLab push \u2192 pipeline runs GitOps: changes in Git automatically apply to cluster Pull request checks (CI gates) Build Orchestration Docker image builds Artifact storage (Nexus, Artifactory) Container scanning (Trivy, Snyk) Image signing and verification Multi-platform builds (ARM, x86) Testing Automation Unit tests in CI Integration tests Smoke tests on staged version Performance tests Security scanning (SAST, DAST) Deployment Pipeline Deploy to dev after merge Deploy to staging after tag Canary to prod (5% traffic) Full rollout after validation Automated rollback on failure Artifact Management Build artifacts versioned Container images with tags Helm charts pushed to repo SBOM (software bill of materials) Provenance tracking Security & Compliance No secrets in logs RBAC on pipeline execution Audit trail of all deployments Signed artifacts Compliance checks Notifications & Feedback Slack on pipeline success/failure Email to team leads Dashboard showing pipeline status Metrics on success rate, speed Alerts on broken builds Pipeline Templates Standard Node.js pipeline Standard Java/Maven pipeline Standard Python pipeline Standard Go pipeline Custom pipeline for special cases Resource Management Pipeline pods don't consume resources Auto-scaling for heavy pipelines Spot instances for build nodes Cache builds to speed up","title":"What You'll Build"},{"location":"02%20helm/question4/#your-challenges_1","text":"Pipeline as Code How do you define pipelines declaratively? How do you support 10+ languages? How do you allow customization? How do you validate pipeline configs? Secure Secrets Secrets for GitHub, Docker, databases Secrets rotated automatically Secrets never logged Different secrets per environment How do you inject them safely? Parallelization Multiple tests run in parallel Multiple image builds in parallel Coordinate between stages Manage resource contention Artifact Traceability Track what code produced which artifact Track which artifact deployed where Rollback by artifact version Reproducible builds Performance Optimization Cache dependencies (maven, npm, pip) Cache Docker layers Parallel test execution Reuse build output Target only affected services Multi-Version Testing Test against multiple Node versions Test against multiple Python versions Test against multiple Java versions Test on multiple OS (Linux, Windows) Deployment Strategies Blue-green for zero-downtime Canary with automatic rollback Rolling update with health checks Shadow traffic testing Dark launch capability Monitoring Deployments Is new version healthy? Are error rates increasing? Are latencies acceptable? Is resource usage normal? Automatic rollback if degradation","title":"Your Challenges"},{"location":"02%20helm/question4/#architecture-youll-design_1","text":"Git Repo Structure: \u251c\u2500\u2500 .pipelines/ \u2502 \u251c\u2500\u2500 nodejs.yaml # Reusable pipeline template \u2502 \u251c\u2500\u2500 python.yaml \u2502 \u251c\u2500\u2500 java.yaml \u2502 \u2514\u2500\u2500 deploy.yaml # Deployment pipeline \u251c\u2500\u2500 src/ \u2514\u2500\u2500 helm-chart/ # Application Helm chart Pipeline Execution: Code Push \u2192 GitHub Webhook \u2192 Tekton EventListener \u2193 Create PipelineRun \u2192 Execute Tasks in Parallel \u251c\u2500\u2500 Build Docker Image \u251c\u2500\u2500 Run Unit Tests \u251c\u2500\u2500 Run Integration Tests \u2514\u2500\u2500 Security Scan \u2193 Push Image to Registry (if tests pass) \u2193 Trigger Deployment Pipeline \u251c\u2500\u2500 Deploy to Dev \u251c\u2500\u2500 Deploy to Staging \u2514\u2500\u2500 Deploy to Prod (canary) \u2193 Monitor for Issues \u2192 Rollback if Needed","title":"Architecture You'll Design"},{"location":"02%20helm/question4/#success-criteria_1","text":"Define pipeline in Git, runs automatically on push Multiple stages execute in parallel Secure secret handling (never logged) Build artifacts traced to code Deploy with zero downtime Automatic rollback on failure All actions audited and logged Performance metrics tracked Scaling works for heavy builds Team can define custom pipelines","title":"Success Criteria"},{"location":"02%20helm/question4/#estimated-complexity_1","text":"Tekton + EventListeners Multiple task types Conditional execution Artifact management Secret handling Deployment orchestration Monitoring and observability Time: 20-25 hours","title":"Estimated Complexity"},{"location":"02%20helm/question4/#project-4-machine-learning-operations-mlops-platform","text":"What You'll Build: A Helm-based platform for training, deploying, and monitoring ML models.","title":"Project 4: Machine Learning Operations (MLOps) Platform"},{"location":"02%20helm/question4/#the-scenario_1","text":"Your company has data scientists building ML models but: - Model training is manual (scripts on laptops) - Deployment to production is ad-hoc - No A/B testing framework - No retraining pipeline - No model monitoring - Different versions conflict You're building MLOps: a platform where scientists describe experiments in code, and Helm manages training jobs, model deployment, and monitoring.","title":"The Scenario"},{"location":"02%20helm/question4/#what-youll-build_1","text":"Experiment Management Define training job parameters Track hyperparameters used Compare results across runs Reproducible experiments Training Infrastructure GPU/TPU provisioning Distributed training (multiple GPUs) Fault tolerance (resume from checkpoint) Notebook environments for exploration Job scheduling and queuing Model Registry Store trained models Version all models Track model lineage (which data, code) Quality metrics (accuracy, precision, recall) Approval workflow for production Model Serving Deploy models as HTTP endpoints Multiple model versions simultaneously A/B testing between versions Traffic shifting (canary deployments) Model scaling based on load Model monitoring and alerts Feature Engineering Shared feature store Consistent features in training and serving Feature versioning Feature pipeline (compute features) Cache for performance Data Pipeline Fetch data for training Data validation (detect drift) Data labeling workflow Data versioning Privacy-preserving (redaction, anonymization) Monitoring & Governance Track model performance over time Detect data drift (input distribution changed) Detect model drift (predictions changing) Alert when retraining needed Explainability and model interpretability Audit predictions for bias AutoML & Tuning Hyperparameter optimization Neural Architecture Search (optional) Model selection automation Ensemble methods","title":"What You'll Build"},{"location":"02%20helm/question4/#your-challenges_2","text":"Resource Management GPUs are expensive and shared How do you schedule training jobs fairly? How do you prevent one team from monopolizing resources? How do you autoscale GPU nodes? Experiment Reproducibility Same seed, same code \u2192 same results Track all dependencies (library versions) Containerize everything Tag images immutably Model Versioning Multiple models in production simultaneously Different model versions for different requests Gradual traffic shift from old to new Quick rollback if new model fails Data Handling Large datasets (TB scale) Efficient storage and access Privacy compliance (GDPR, HIPAA) Data retention policies Model Serving Performance Sub-100ms inference latency Handle traffic spikes GPU sharing between models CPU inference when GPU not needed Monitoring Model Quality Accuracy stays above threshold Latency stays below SLO Data drift detection Fairness/bias monitoring Automatic retraining Collaboration Scientists define experiments in notebooks Experiments become production pipelines Track who changed what Reproducible results","title":"Your Challenges"},{"location":"02%20helm/question4/#architecture-youll-design_2","text":"Training Phase: Git Push (new model code) \u2193 Trigger Training Job \u251c\u2500\u2500 Pull training data \u251c\u2500\u2500 Run hyperparameter tuning (parallel) \u251c\u2500\u2500 Select best model \u251c\u2500\u2500 Validate on test set \u2514\u2500\u2500 Push to model registry Serving Phase: Model Registry Push \u2193 Approval Gate (data scientist reviews) \u2193 Deploy to Staging \u251c\u2500\u2500 Serve model \u251c\u2500\u2500 Run smoke tests \u2514\u2500\u2500 Collect metrics \u2193 Deploy to Production (canary, 5% traffic) \u251c\u2500\u2500 Monitor error rates \u251c\u2500\u2500 Monitor latency \u251c\u2500\u2500 Monitor fairness metrics \u2514\u2500\u2500 After 24h, shift to 100% (or rollback) Monitoring Phase: Continuous Monitoring \u251c\u2500\u2500 Data drift detection \u251c\u2500\u2500 Model drift detection \u251c\u2500\u2500 Performance degradation \u2514\u2500\u2500 Alert if retraining needed \u2193 Trigger Retraining Pipeline","title":"Architecture You'll Design"},{"location":"02%20helm/question4/#success-criteria_2","text":"Scientist defines experiment, training runs automatically Multiple models in production simultaneously Canary deployment for new models Automatic rollback if model performance drops Data drift detection triggers retraining All experiments reproducible Model lineage tracked (code, data, hyperparameters) Serving latency < 100ms GPU resources scheduled fairly Full audit trail","title":"Success Criteria"},{"location":"02%20helm/question4/#estimated-complexity_2","text":"Model training orchestration Model serving (KServe or Seldon) Feature store integration Data pipeline management Monitoring and governance Resource management (GPUs) Time: 25-30 hours","title":"Estimated Complexity"},{"location":"02%20helm/question4/#project-5-complete-observability-aiops-platform","text":"What You'll Build: An end-to-end observability platform with intelligent alerting and automatic remediation.","title":"Project 5: Complete Observability &amp; AIOps Platform"},{"location":"02%20helm/question4/#the-scenario_2","text":"Your organization has: - Metrics from Prometheus (500+ servers) - Logs from ELK (petabytes/day) - Traces from Jaeger (millions/minute) - Hundreds of alert rules But: - Alert noise (70% false positives) - Slow incident response (manual investigation) - No correlation between metrics/logs/traces - Difficult root cause analysis - Manual remediation steps You're building AIOps: AI-powered observability with: - Intelligent alerting (correlation, deduplication) - Automatic root cause analysis - Self-healing (auto-remediation) - Predictive alerts (issue before it happens)","title":"The Scenario"},{"location":"02%20helm/question4/#what-youll-build_2","text":"Metrics Collection Prometheus for infrastructure Application metrics (custom) Business metrics (revenue, usage) Metric aggregation and long-term storage Cardinality management Logging Centralized log collection (Filebeat, Fluentd) Structured JSON logging Log parsing and enrichment Log-based alerting Long-term log archival Distributed Tracing Request tracing across services Latency analysis Dependency mapping Error tracing Trace sampling (intelligent) Alerting Rule engine (Prometheus rules, custom rules) Multi-condition alerts (AND, OR) Alert grouping (reduce noise) Alert deduplication Alert escalation Incident Management Create incidents from alerts Incident timeline (what happened when) Call incident commander Coordinate team response Post-mortem automation AIOps Features Anomaly detection (ML models) Correlation engine (alert A + alert B = root cause C) Root cause analysis (automatic) Predictive alerts (issue before it happens) Auto-remediation (trigger actions) Dashboarding KPI dashboards (SLI/SLO) Service health dashboard Dependency map visualization Trend analysis Cost impact of incidents Automation & Remediation Auto-scale on high load Restart failing services Drain misbehaving nodes Trigger disaster recovery Rollback bad deployments","title":"What You'll Build"},{"location":"02%20helm/question4/#your-challenges_3","text":"Scaling to Massive Data Handle petabytes of logs Query billions of metrics Trace millions of requests Keep response time < 1 second Alert Quality 70% of alerts are noise How do you deduplicate? How do you correlate? How do you reduce false positives? Root Cause Analysis Request took 10s (slow) Which service is slow? Which database query is slow? Which code change caused it? Automatically detect and report Multi-Tenancy Team A shouldn't see Team B's metrics/logs/traces But correlation needs cross-team data How do you handle this tension? Data Retention Metrics for 2 years (expensive) Logs for 1 year Traces for 30 days Smart data tiering Compression strategies Predictive Alerting Predict failures before they happen How do you train models? How do you avoid false positives? How do you explain predictions? Automated Remediation Auto-restart services on failure Auto-scale on high load Auto-drain failing nodes Know which actions are safe Prevent cascading failures","title":"Your Challenges"},{"location":"02%20helm/question4/#architecture-youll-design_3","text":"Data Collection: \u251c\u2500\u2500 Metrics (Prometheus) \u2192 TSDB \u251c\u2500\u2500 Logs (Fluentd) \u2192 Search (Elasticsearch) \u2514\u2500\u2500 Traces (Jaeger) \u2192 Trace Storage Processing Pipeline: \u251c\u2500\u2500 Alert Rules (Prometheus) \u2192 Create Alerts \u251c\u2500\u2500 Correlation Engine \u2192 Group Related Alerts \u251c\u2500\u2500 Anomaly Detection \u2192 Predict Issues \u251c\u2500\u2500 Root Cause Analysis \u2192 Identify Culprit \u2514\u2500\u2500 Recommendation Engine \u2192 Suggest Fix Output: \u251c\u2500\u2500 Alert to PagerDuty \u251c\u2500\u2500 Incident Created \u251c\u2500\u2500 Auto-Remediation Triggered \u251c\u2500\u2500 Dashboards Updated \u2514\u2500\u2500 Post-Mortem Data Collected","title":"Architecture You'll Design"},{"location":"02%20helm/question4/#success-criteria_3","text":"Ingest petabytes of data Alert latency < 10 seconds Alert accuracy > 95% (few false positives) Root cause identified in < 5 minutes Auto-remediation success > 90% Reduce MTTR (mean time to recovery) by 70% Predictive alerts work (catch issues early) Full audit trail of all changes Multi-tenant with strong isolation Cost tracking per team","title":"Success Criteria"},{"location":"02%20helm/question4/#estimated-complexity_3","text":"Multiple data sources (metrics, logs, traces) Scaling and performance optimization ML models for anomaly detection Correlation and root cause logic Automated remediation Multi-tenancy Time: 30-40 hours","title":"Estimated Complexity"},{"location":"02%20helm/question4/#project-6-data-platform-lakehouse","text":"What You'll Build: A complete data platform (data lake + warehouse) for analytics and reporting.","title":"Project 6: Data Platform (Lakehouse)"},{"location":"02%20helm/question4/#the-scenario_3","text":"Your company collects data from: - Web application (event data) - Mobile app (user behavior) - IoT devices (sensor data) - Third-party APIs (external data) Currently: - Data is siloed in different systems - No unified schema - Difficult to correlate data - Slow to answer questions - No data governance You're building a data lakehouse: unified data platform for analytics.","title":"The Scenario"},{"location":"02%20helm/question4/#what-youll-build_3","text":"Data Ingestion Streaming (events in real-time) Batch (nightly imports) CDC (change data capture) API polling File uploads Data Processing ETL (extract, transform, load) Data validation Schema enforcement Deduplication Aggregations Data Storage Object storage (Parquet, Iceberg) Columnar format for analytics Time-series optimized Partitioning for performance Data tiering (hot/warm/cold) Data Warehouse SQL interface (ClickHouse, Snowflake) Dimensional modeling (facts, dimensions) Slowly changing dimensions Materialized views Incremental updates Metadata & Governance Data catalog (what data exists) Data lineage (where did this come from) Data quality checks Privacy controls (PII masking) Retention policies Analytics & BI Ad-hoc SQL queries Pre-built dashboards Self-service analytics Drill-down capability Scheduled reports Machine Learning Export data for training Features for models Model predictions back to warehouse A/B testing framework","title":"What You'll Build"},{"location":"02%20helm/question4/#your-challenges_4","text":"Volume at Scale Ingest 1TB/day Store 100TB total Query across years Keep costs reasonable Schema Evolution New fields added over time Old fields deprecated Type changes (string \u2192 integer) How do you handle this? Data Quality Validate as data arrives Detect anomalies Alert on schema violations Quarantine bad data Privacy Compliance GDPR: right to be forgotten HIPAA: audit access PII: redact or encrypt Retention: delete after period Performance Query 1TB of data in < 5 seconds Aggregations very fast Joins across large tables Incremental updates efficient Cost Optimization S3 storage cheaper than database Compression reduces cost Tiering saves money Prune old data","title":"Your Challenges"},{"location":"02%20helm/question4/#success-criteria_4","text":"Ingest 1TB/day without issues Queries run in < 5 seconds Data quality > 99% Privacy controls enforced Full audit trail Self-service analytics works Metadata is complete and accurate Cost efficient Retention policies followed","title":"Success Criteria"},{"location":"02%20helm/question4/#estimated-complexity_4","text":"Multiple data sources ETL pipeline orchestration Schema management Query optimization Governance and compliance Time: 20-25 hours","title":"Estimated Complexity"},{"location":"02%20helm/question4/#which-project-should-you-start-with","text":"If you want end-to-end experience: Start with Project 1 (SaaS Platform) - Teaches multi-component charts - Covers scaling, security, upgrades - Most useful for real-world apps - Most satisfying to deploy If you want infrastructure focus: Start with Project 2 (IaC Platform) - Teaches multi-cloud abstraction - Covers infrastructure orchestration - Complex dependency management - Valuable for DevOps/platform teams If you want automation focus: Start with Project 3 (CI/CD Platform) - Teaches workflow orchestration - Covers secret management - Automated deployment pipelines - Valuable for DevOps engineers If you want ML focus: Start with Project 4 (MLOps Platform) - Teaches specialized workload management - Covers GPU scheduling, model serving - Valuable for ML engineers - Unique challenges around experiments If you want observability focus: Start with Project 5 (Observability Platform) - Teaches time-series data handling - Covers monitoring and alerting - Valuable for SREs/platform teams - Complex aggregation logic If you want analytics focus: Start with Project 6 (Data Platform) - Teaches data pipeline orchestration - Covers large-scale data handling - Valuable for data engineers - Cost optimization important","title":"Which Project Should You Start With?"},{"location":"02%20helm/question4/#project-complexity-comparison","text":"Project Components Complexity Time Best For 1 (SaaS) 8+ services High 20-30h Full-stack Helm mastery 2 (IaC) Multi-cloud, add-ons Very High 25-35h Infrastructure teams 3 (CI/CD) Pipeline orchestration Very High 20-25h DevOps automation 4 (MLOps) ML workloads, serving High 25-30h ML infrastructure 5 (Observability) Multi-datasource Very High 30-40h SRE/Monitoring 6 (Data) Data pipelines High 20-25h Data engineers","title":"Project Complexity Comparison"},{"location":"02%20helm/question4/#learning-path","text":"Start with Project 1 or 3 (most applicable) Then pick a second project based on your domain Then tackle hardest project (usually Project 2 or 5) Combine learnings from all projects into your own system","title":"Learning Path"},{"location":"02%20helm/question4/#what-youll-learn-across-all-projects","text":"\u2705 Multi-component chart architecture \u2705 Dependency management and ordering \u2705 Multi-environment support \u2705 Scaling patterns (horizontal, vertical) \u2705 Stateful application management \u2705 Backup and recovery \u2705 Security hardening and RBAC \u2705 Observability integration \u2705 Upgrade strategies \u2705 Cost optimization \u2705 Production-grade operations \u2705 Git-based workflows \u2705 Automated testing and validation \u2705 Real-world complexity handling","title":"What You'll Learn Across All Projects"},{"location":"02%20helm/question4/#success-criteria-for-any-project","text":"When you finish, you should be able to: \u2705 Deploy the complete system with one Helm command \u2705 Scale up/down smoothly without downtime \u2705 Upgrade to new versions safely \u2705 Handle failures gracefully \u2705 Monitor and observe everything \u2705 Maintain all state (backups, recovery) \u2705 Enforce security policies \u2705 Track costs \u2705 Audit all actions \u2705 Explain your design to others If you can do all 10, you've mastered Helm at a professional level. \ud83c\udfaf Good luck! Pick one project and build something meaningful! \ud83d\ude80","title":"Success Criteria for Any Project"},{"location":"03-kustomize/notes/","text":"Kustomize Master Guide: Advanced Patterns & Modern Practices \ud83d\ude80 Overview & Philosophy Kustomize is a declarative configuration overlay system for Kubernetes, not a templating engine. It follows the \"Everything as YAML\" philosophy with patch-based inheritance . # Modern installation (standalone) brew install kustomize # macOS # or curl -s \"https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh\" | bash # Built-in (kubectl v1.14+) kubectl kustomize <dir> \ud83d\udcc1 Advanced Project Structure k8s/ \u251c\u2500\u2500 base/ \u2502 \u251c\u2500\u2500 kustomization.yaml # Base resources \u2502 \u251c\u2500\u2500 namespace.yaml \u2502 \u251c\u2500\u2500 rbac/ \u2502 \u2502 \u251c\u2500\u2500 kustomization.yaml # Component composition \u2502 \u2502 \u251c\u2500\u2500 serviceaccount.yaml \u2502 \u2502 \u2514\u2500\u2500 rolebinding.yaml \u2502 \u251c\u2500\u2500 networking/ \u2502 \u251c\u2500\u2500 deployments/ \u2502 \u2514\u2500\u2500 monitoring/ \u251c\u2500\u2500 components/ # Reusable transformations \u2502 \u251c\u2500\u2500 ingress-ssl/ \u2502 \u251c\u2500\u2500 pod-security/ \u2502 \u2514\u2500\u2500 resource-limits/ \u251c\u2500\u2500 overlays/ \u2502 \u251c\u2500\u2500 region/ # Multi-dimensional overlays \u2502 \u2502 \u251c\u2500\u2500 us-west/ \u2502 \u2502 \u2514\u2500\u2500 eu-central/ \u2502 \u251c\u2500\u2500 environment/ \u2502 \u2502 \u251c\u2500\u2500 dev/ \u2502 \u2502 \u2502 \u251c\u2500\u2500 kustomization.yaml # Environment config \u2502 \u2502 \u2502 \u251c\u2500\u2500 namespace.yaml \u2502 \u2502 \u2502 \u2514\u2500\u2500 patch-deployment.yaml \u2502 \u2502 \u251c\u2500\u2500 staging/ \u2502 \u2502 \u2514\u2500\u2500 production/ \u2502 \u2514\u2500\u2500 tenant/ # Multi-tenancy \u2502 \u251c\u2500\u2500 team-a/ \u2502 \u2514\u2500\u2500 team-b/ \u251c\u2500\u2500 clusters/ # Cluster-specific configs \u2502 \u251c\u2500\u2500 cluster-01/ \u2502 \u2514\u2500\u2500 cluster-02/ \u2514\u2500\u2500 generators/ # Dynamic resource generation \u251c\u2500\u2500 helm-charts/ \u2514\u2500\u2500 jsonnet/ \u26a1 Modern kustomization.yaml Features Components (Kustomize v4+) # components/pod-security/kustomization.yaml apiVersion : kustomize.config.k8s.io/v1alpha1 kind : Component patches : - target : kind : Pod patch : | - op: add path: /spec/securityContext value: runAsNonRoot: true seccompProfile: type: RuntimeDefault - target : kind : Deployment patch : | - op: add path: /spec/template/spec/containers/0/securityContext value: allowPrivilegeEscalation: false capabilities: drop: [\"ALL\"] Replacements (Kustomize v4.5+) - Advanced Variable Substitution # Base configmap apiVersion : v1 kind : ConfigMap metadata : name : app-config data : appVersion : \"2.1.0\" logLevel : \"INFO\" replicas : \"3\" # Overlay using replacements apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization resources : - deployment.yaml - configmap.yaml replacements : # Simple value copy - source : kind : ConfigMap name : app-config fieldPath : data.appVersion targets : - select : kind : Deployment name : app-deployment fieldPaths : - spec.template.metadata.labels.version # Multiple field mapping - source : kind : ConfigMap name : app-config fieldPath : data.replicas targets : - select : kind : Deployment fieldPaths : - spec.replicas # Complex transformations - source : kind : ConfigMap name : app-config fieldPath : data.logLevel targets : - select : kind : Deployment fieldPaths : - spec.template.spec.containers.[name=app].env.[name=LOG_LEVEL].value options : create : true # Create if doesn't exist Generators - Dynamic Resource Creation # generators/secrets/kustomization.yaml apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization secretGenerator : - name : tls-certs type : \"kubernetes.io/tls\" files : - tls.crt=./certs/server.crt - tls.key=./certs/server.key options : annotations : sealedsecrets.bitnami.com/managed : \"true\" labels : cert-manager.io/certificate-name : \"app-tls\" - name : dynamic-secret type : Opaque literals : - DB_PASSWORD=$(cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 32 | head -n 1) - API_KEY=$(openssl rand -base64 32) behavior : create # create, merge, or replace configMapGenerator : - name : app-config envs : - .env.production - .env.secrets files : - config.yaml - \"*.conf\" # Wildcard support options : immutable : true # Kubernetes 1.19+ feature generators : - | # Inline generators apiVersion: batch/v1 kind: Job metadata: name: db-migration spec: template: spec: containers: - name: migrator image: alpine:latest restartPolicy: Never \ud83c\udfaf Advanced Patching Techniques JSON Patch (RFC 6902) - Most Powerful patches : - target : kind : Deployment name : \".*\" # Regex support labelSelector : \"app=frontend\" patch : | [ { \"op\": \"add\", \"path\": \"/spec/template/spec/tolerations\", \"value\": [ { \"key\": \"gpu\", \"operator\": \"Equal\", \"value\": \"nvidia\", \"effect\": \"NoSchedule\" } ] }, { \"op\": \"replace\", \"path\": \"/spec/template/spec/containers/0/resources/limits/memory\", \"value\": \"2Gi\" }, { \"op\": \"copy\", \"from\": \"/spec/replicas\", \"path\": \"/spec/template/metadata/annotations/initialReplicas\" }, { \"op\": \"test\", # Conditional - only apply if condition matches \"path\": \"/metadata/namespace\", \"value\": \"production\" } ] # Inline YAML patch - patch : | apiVersion: apps/v1 kind: Deployment metadata: name: app spec: template: spec: containers: - name: main envFrom: - configMapRef: name: dynamic-config-$(CONFIG_HASH) target : name : app Strategic Merge Patch - Kubernetes-Specific patchesStrategicMerge : # Add sidecar container (K8s knows how to merge arrays) - | apiVersion: apps/v1 kind: Deployment metadata: name: app spec: template: spec: containers: - name: istio-proxy image: istio/proxyv2:1.16 resources: requests: cpu: 100m memory: 128Mi # Add volume (merged by name) - | apiVersion: apps/v1 kind: Deployment metadata: name: app spec: template: spec: volumes: - name: nginx-config configMap: name: nginx-config \ud83d\udd17 Multi-Base Composition & Cross-Cutting Cross-Cutting Concerns as Components # components/istio-sidecar-injection/kustomization.yaml (Component) apiVersion : kustomize.config.k8s.io/v1alpha1 kind : Component namespace : istio-system patches : - target : kind : Namespace patch : | - op: add path: /metadata/labels/istio-injection value: enabled # components/linkerd-proxy/kustomization.yaml apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization resources : - https://raw.githubusercontent.com/linkerd/linkerd2/main/manifests/linkerd-crds.yaml - https://raw.githubusercontent.com/linkerd/linkerd2/main/manifests/linkerd-control-plane.yaml transformers : - | # Inline transformer apiVersion: builtin kind: LabelTransformer metadata: name: linkerd-injection labels: linkerd.io/inject: enabled fieldSpecs: - path: metadata/labels create: true Dynamic Resource Inclusion apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization # Include remote bases resources : - github.com/kubernetes-sigs/cluster-api//config/default?ref=v1.4.0 - https://raw.githubusercontent.com/prometheus-operator/kube-prometheus/main/manifests/setup/prometheus-operator-0servicemonitorCustomResourceDefinition.yaml # Include local generated manifests resources : - ../manifests-generated/ # Helm output - ../jsonnet-output/ - ./dynamic/ # Generated by scripts # Conditional inclusion using build args vars : - name : FEATURE_FLAG objref : kind : ConfigMap name : feature-flags fieldref : fieldpath : data.enableMonitoring resources : - name : monitoring resource : ../monitoring/ if : $(FEATURE_FLAG) == \"true\" \ud83d\udd04 CI/CD Integration Patterns GitHub Actions Workflow name : Kustomize CI/CD on : [ push , pull_request ] jobs : kustomize-validation : runs-on : ubuntu-latest steps : - uses : actions/checkout@v3 - name : Setup Kustomize uses : imranismail/setup-kustomize@v2 with : kustomize-version : 'v5.0.0' - name : Validate All Overlays run : | for overlay in overlays/*/; do echo \"Validating $overlay\" kustomize build $overlay --load-restrictor=LoadRestrictionsNone | \\ kubectl apply --dry-run=server --validate=true -f - done kustomize-diff : runs-on : ubuntu-latest steps : - uses : actions/checkout@v3 with : fetch-depth : 0 - name : Generate Kustomize Diff run : | git diff --name-only HEAD^ HEAD | grep -E '(kustomization\\.yaml|\\.yaml$|\\.yml$)' | \\ while read file; do dir=$(dirname \"$file\") if [ -f \"$dir/kustomization.yaml\" ]; then echo \"Changes detected in $dir\" kustomize build \"$dir\" --load-restrictor=LoadRestrictionsNone > /tmp/new.yaml git checkout HEAD^ -- \"$dir\" kustomize build \"$dir\" --load-restrictor=LoadRestrictionsNone > /tmp/old.yaml diff -u /tmp/old.yaml /tmp/new.yaml || true fi done ArgoCD Integration # argocd-application.yaml apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : production-app spec : project : default source : repoURL : https://github.com/org/repo.git targetRevision : main path : k8s/overlays/production kustomize : # ArgoCD-specific kustomize options namePrefix : prod- nameSuffix : -v1 images : - nginx:1.21.0 commonAnnotations : deployed-by : argocd commonLabels : environment : production # Force common labels/annotations on all resources forceCommonLabels : true forceCommonAnnotations : true # Replicas override replicas : - name : app-deployment count : 5 destination : server : https://kubernetes.default.svc namespace : production syncPolicy : automated : prune : true selfHeal : true syncOptions : - CreateNamespace=true - PruneLast=true \ud83d\udd10 Security & Compliance Patterns Pod Security Standards (K8s 1.23+) # components/psa-baseline/kustomization.yaml apiVersion : kustomize.config.k8s.io/v1alpha1 kind : Component patches : - target : kind : Namespace patch : | - op: add path: /metadata/labels value: pod-security.kubernetes.io/enforce: baseline pod-security.kubernetes.io/enforce-version: latest pod-security.kubernetes.io/audit: restricted - target : kind : Pod patch : | - op: add path: /spec/securityContext value: seccompProfile: type: RuntimeDefault runAsNonRoot: true Secrets Management with External Secrets # overlays/production/secrets/kustomization.yaml apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization generators : - | # ExternalSecret (external-secrets.io) apiVersion: external-secrets.io/v1beta1 kind: ExternalSecret metadata: name: database-credentials spec: refreshInterval: 1h secretStoreRef: name: vault-backend kind: SecretStore target: name: database-secret creationPolicy: Owner data: - secretKey: password remoteRef: key: /secrets/production/db property: password - secretKey: username remoteRef: key: /secrets/production/db property: username configMapGenerator : - name : sealed-secret-params literals : - namespace=production - scope=strict \ud83d\udea8 Production-Grade Patterns Multi-Cluster Management with Kustomize # clusters/aws-us-west-2/production/kustomization.yaml apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization # Import environment overlay resources : - ../../../overlays/production # Cluster-specific patches patches : - target : kind : Ingress patch : | - op: replace path: /metadata/annotations/nginx.ingress.kubernetes.io~1load-balancer-id value: alb-1234567890 - target : kind : PersistentVolumeClaim patch : | - op: replace path: /spec/storageClassName value: gp3 # Cluster-specific generators configMapGenerator : - name : cluster-info literals : - CLUSTER_NAME=aws-us-west-2 - REGION=us-west-2 - PROVIDER=aws - VPC_ID=vpc-123456 Feature Flag Management # kustomization.yaml with feature flags apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization # Feature flag ConfigMap configMapGenerator : - name : feature-flags literals : - enableNewUI=true - enableBetaFeatures=false - maintenanceMode=false # Conditional resources based on feature flags transformers : - target : kind : Deployment name : app patch : | - op: add path: /spec/template/spec/containers/0/env value: - name: FEATURE_NEW_UI valueFrom: configMapKeyRef: name: feature-flags key: enableNewUI - name: BETA_FEATURES valueFrom: configMapKeyRef: name: feature-flags key: enableBetaFeatures # Remove resources if feature is disabled patches : - target : kind : Deployment name : beta-service patch : | - op: remove path: /spec options : allowMissingTarget : true \ud83e\uddea Testing & Validation Kustomize Test Framework # kustomization.yaml apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization resources : - deployment.yaml - service.yaml # tests/test.yaml apiVersion : kustomize.config.k8s.io/v1beta1 kind : KustomizationTest tests : - name : deployment-has-correct-replicas resources : - ../deployment.yaml asserts : - equals : fieldPath : spec.replicas value : 3 - name : service-selector-matches-deployment resources : - ../deployment.yaml - ../service.yaml asserts : - and : - equals : fieldPath : spec.selector.matchLabels.app value : myapp - exists : fieldPath : metadata.labels.app Conftest Integration (Open Policy Agent) # conftest-policy.rego package main deny [ msg ] { input.kind == \"Deployment\" not input.spec.template.spec.securityContext.runAsNonRoot msg : = \"Deployment must set runAsNonRoot\" } # Test with kustomize build overlays/production | conftest test - \u2699\ufe0f Performance Optimizations # .kustomizerc (global config) apiVersion : kustomize.config.k8s.io/v1beta1 kind : KustomizationConfig buildMetadata : [ originAnnotations , transformerAnnotations ] loadRestrictor : LoadRestrictionsNone enableExec : true enableStar : true # Use caching for remote resources reorder : legacy # In production builds kustomize build \\ --enable-alpha-plugins \\ --load-restrictor=LoadRestrictionsNone \\ --reorder=legacy \\ --enable-exec \\ overlays/production \ud83d\udd27 Custom Transformers & Generators (Go Plugins) // main.go package main import ( \"sigs.k8s.io/kustomize/api/types\" \"sigs.k8s.io/kustomize/kyaml/fn/framework\" \"sigs.k8s.io/kustomize/kyaml/yaml\" ) type AnnotationTransformer struct { Annotations map [ string ] string `yaml:\"annotations,omitempty\"` } func ( at * AnnotationTransformer ) Filter ( objects [] * yaml . RNode ) ([] * yaml . RNode , error ) { for _ , obj := range objects { meta , err := obj . GetMeta () if err != nil { return nil , err } if meta . Annotations == nil { meta . Annotations = make ( map [ string ] string ) } for k , v := range at . Annotations { meta . Annotations [ k ] = v } err = obj . SetAnnotations ( meta . Annotations ) if err != nil { return nil , err } } return objects , nil } func main () { resource := & framework . ResourceList { FunctionConfig : & AnnotationTransformer {}, } framework . Command ( resource , func () error { return resource . Filter ( & AnnotationTransformer {}) }). Execute () } # Use custom transformer apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization transformers : - kind : AnnotationTransformer annotations : managed-by : kustomize git-sha : $(git rev-parse HEAD) \ud83d\udcca Comparison with Modern Alternatives Feature Kustomize Helm Carvel ytt CUE Paradigm Overlay/Patch Templating Templating Configuration Language Learning Curve Low Medium High Very High GitOps Ready \u2705 Excellent \u26a0\ufe0f Needs Tillerless \u2705 Good \u2705 Good Multi-Environment \u2705 Native \u26a0\ufe0f Values files \u2705 Good \u2705 Excellent Type Safety \u274c \u274c \u26a0\ufe0f Limited \u2705 Excellent Secret Management \u26a0\ufe0f Basic \u26a0\ufe0f Basic \u2705 Good \u2705 Good Community Adoption \u2705 High \u2705 Very High \u26a0\ufe0f Moderate \u26a0\ufe0f Moderate \ud83c\udfaf Best Practices Summary Use components for cross-cutting concerns (v4+) Leverage replacements over vars (deprecated) Structure overlays by concern: environment \u00d7 region \u00d7 tenant Always validate with kustomize build --load-restrictor=LoadRestrictionsNone Use generators for dynamic content Implement testing with kustomize test framework Cache remote resources in CI/CD Seal secrets before committing Use JSON patches for complex transformations Monitor kustomize releases - rapid evolution This master guide covers advanced modern patterns for enterprise-grade Kubernetes management with Kustomize. The key is embracing its declarative, patch-based philosophy while leveraging new features like components, replacements, and generators for maximum power and flexibility. When you use: yaml secretGenerator: - name: app-secret behavior: merge envs: - prod-secret.txt The behavior field controls how Kustomize handles the existing secret in your manifests, NOT the source file. Here's what happens: If the secret DOES NOT exist in base manifests: With behavior: merge \u2192 Will create the secret With behavior: create \u2192 Will create the secret With behavior: replace \u2192 Will fail (no secret to replace) If the secret DOES exist in base manifests: With behavior: merge \u2192 Merge values from generator with base secret With behavior: create \u2192 Error (can't create, already exists) With behavior: replace \u2192 Replace base secret completely","title":"Kustomize Master Guide: Advanced Patterns &amp; Modern Practices"},{"location":"03-kustomize/notes/#kustomize-master-guide-advanced-patterns-modern-practices","text":"","title":"Kustomize Master Guide: Advanced Patterns &amp; Modern Practices"},{"location":"03-kustomize/notes/#overview-philosophy","text":"Kustomize is a declarative configuration overlay system for Kubernetes, not a templating engine. It follows the \"Everything as YAML\" philosophy with patch-based inheritance . # Modern installation (standalone) brew install kustomize # macOS # or curl -s \"https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh\" | bash # Built-in (kubectl v1.14+) kubectl kustomize <dir>","title":"\ud83d\ude80 Overview &amp; Philosophy"},{"location":"03-kustomize/notes/#advanced-project-structure","text":"k8s/ \u251c\u2500\u2500 base/ \u2502 \u251c\u2500\u2500 kustomization.yaml # Base resources \u2502 \u251c\u2500\u2500 namespace.yaml \u2502 \u251c\u2500\u2500 rbac/ \u2502 \u2502 \u251c\u2500\u2500 kustomization.yaml # Component composition \u2502 \u2502 \u251c\u2500\u2500 serviceaccount.yaml \u2502 \u2502 \u2514\u2500\u2500 rolebinding.yaml \u2502 \u251c\u2500\u2500 networking/ \u2502 \u251c\u2500\u2500 deployments/ \u2502 \u2514\u2500\u2500 monitoring/ \u251c\u2500\u2500 components/ # Reusable transformations \u2502 \u251c\u2500\u2500 ingress-ssl/ \u2502 \u251c\u2500\u2500 pod-security/ \u2502 \u2514\u2500\u2500 resource-limits/ \u251c\u2500\u2500 overlays/ \u2502 \u251c\u2500\u2500 region/ # Multi-dimensional overlays \u2502 \u2502 \u251c\u2500\u2500 us-west/ \u2502 \u2502 \u2514\u2500\u2500 eu-central/ \u2502 \u251c\u2500\u2500 environment/ \u2502 \u2502 \u251c\u2500\u2500 dev/ \u2502 \u2502 \u2502 \u251c\u2500\u2500 kustomization.yaml # Environment config \u2502 \u2502 \u2502 \u251c\u2500\u2500 namespace.yaml \u2502 \u2502 \u2502 \u2514\u2500\u2500 patch-deployment.yaml \u2502 \u2502 \u251c\u2500\u2500 staging/ \u2502 \u2502 \u2514\u2500\u2500 production/ \u2502 \u2514\u2500\u2500 tenant/ # Multi-tenancy \u2502 \u251c\u2500\u2500 team-a/ \u2502 \u2514\u2500\u2500 team-b/ \u251c\u2500\u2500 clusters/ # Cluster-specific configs \u2502 \u251c\u2500\u2500 cluster-01/ \u2502 \u2514\u2500\u2500 cluster-02/ \u2514\u2500\u2500 generators/ # Dynamic resource generation \u251c\u2500\u2500 helm-charts/ \u2514\u2500\u2500 jsonnet/","title":"\ud83d\udcc1 Advanced Project Structure"},{"location":"03-kustomize/notes/#modern-kustomizationyaml-features","text":"","title":"\u26a1 Modern kustomization.yaml Features"},{"location":"03-kustomize/notes/#components-kustomize-v4","text":"# components/pod-security/kustomization.yaml apiVersion : kustomize.config.k8s.io/v1alpha1 kind : Component patches : - target : kind : Pod patch : | - op: add path: /spec/securityContext value: runAsNonRoot: true seccompProfile: type: RuntimeDefault - target : kind : Deployment patch : | - op: add path: /spec/template/spec/containers/0/securityContext value: allowPrivilegeEscalation: false capabilities: drop: [\"ALL\"]","title":"Components (Kustomize v4+)"},{"location":"03-kustomize/notes/#replacements-kustomize-v45-advanced-variable-substitution","text":"# Base configmap apiVersion : v1 kind : ConfigMap metadata : name : app-config data : appVersion : \"2.1.0\" logLevel : \"INFO\" replicas : \"3\" # Overlay using replacements apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization resources : - deployment.yaml - configmap.yaml replacements : # Simple value copy - source : kind : ConfigMap name : app-config fieldPath : data.appVersion targets : - select : kind : Deployment name : app-deployment fieldPaths : - spec.template.metadata.labels.version # Multiple field mapping - source : kind : ConfigMap name : app-config fieldPath : data.replicas targets : - select : kind : Deployment fieldPaths : - spec.replicas # Complex transformations - source : kind : ConfigMap name : app-config fieldPath : data.logLevel targets : - select : kind : Deployment fieldPaths : - spec.template.spec.containers.[name=app].env.[name=LOG_LEVEL].value options : create : true # Create if doesn't exist","title":"Replacements (Kustomize v4.5+) - Advanced Variable Substitution"},{"location":"03-kustomize/notes/#generators-dynamic-resource-creation","text":"# generators/secrets/kustomization.yaml apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization secretGenerator : - name : tls-certs type : \"kubernetes.io/tls\" files : - tls.crt=./certs/server.crt - tls.key=./certs/server.key options : annotations : sealedsecrets.bitnami.com/managed : \"true\" labels : cert-manager.io/certificate-name : \"app-tls\" - name : dynamic-secret type : Opaque literals : - DB_PASSWORD=$(cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 32 | head -n 1) - API_KEY=$(openssl rand -base64 32) behavior : create # create, merge, or replace configMapGenerator : - name : app-config envs : - .env.production - .env.secrets files : - config.yaml - \"*.conf\" # Wildcard support options : immutable : true # Kubernetes 1.19+ feature generators : - | # Inline generators apiVersion: batch/v1 kind: Job metadata: name: db-migration spec: template: spec: containers: - name: migrator image: alpine:latest restartPolicy: Never","title":"Generators - Dynamic Resource Creation"},{"location":"03-kustomize/notes/#advanced-patching-techniques","text":"","title":"\ud83c\udfaf Advanced Patching Techniques"},{"location":"03-kustomize/notes/#json-patch-rfc-6902-most-powerful","text":"patches : - target : kind : Deployment name : \".*\" # Regex support labelSelector : \"app=frontend\" patch : | [ { \"op\": \"add\", \"path\": \"/spec/template/spec/tolerations\", \"value\": [ { \"key\": \"gpu\", \"operator\": \"Equal\", \"value\": \"nvidia\", \"effect\": \"NoSchedule\" } ] }, { \"op\": \"replace\", \"path\": \"/spec/template/spec/containers/0/resources/limits/memory\", \"value\": \"2Gi\" }, { \"op\": \"copy\", \"from\": \"/spec/replicas\", \"path\": \"/spec/template/metadata/annotations/initialReplicas\" }, { \"op\": \"test\", # Conditional - only apply if condition matches \"path\": \"/metadata/namespace\", \"value\": \"production\" } ] # Inline YAML patch - patch : | apiVersion: apps/v1 kind: Deployment metadata: name: app spec: template: spec: containers: - name: main envFrom: - configMapRef: name: dynamic-config-$(CONFIG_HASH) target : name : app","title":"JSON Patch (RFC 6902) - Most Powerful"},{"location":"03-kustomize/notes/#strategic-merge-patch-kubernetes-specific","text":"patchesStrategicMerge : # Add sidecar container (K8s knows how to merge arrays) - | apiVersion: apps/v1 kind: Deployment metadata: name: app spec: template: spec: containers: - name: istio-proxy image: istio/proxyv2:1.16 resources: requests: cpu: 100m memory: 128Mi # Add volume (merged by name) - | apiVersion: apps/v1 kind: Deployment metadata: name: app spec: template: spec: volumes: - name: nginx-config configMap: name: nginx-config","title":"Strategic Merge Patch - Kubernetes-Specific"},{"location":"03-kustomize/notes/#multi-base-composition-cross-cutting","text":"","title":"\ud83d\udd17 Multi-Base Composition &amp; Cross-Cutting"},{"location":"03-kustomize/notes/#cross-cutting-concerns-as-components","text":"# components/istio-sidecar-injection/kustomization.yaml (Component) apiVersion : kustomize.config.k8s.io/v1alpha1 kind : Component namespace : istio-system patches : - target : kind : Namespace patch : | - op: add path: /metadata/labels/istio-injection value: enabled # components/linkerd-proxy/kustomization.yaml apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization resources : - https://raw.githubusercontent.com/linkerd/linkerd2/main/manifests/linkerd-crds.yaml - https://raw.githubusercontent.com/linkerd/linkerd2/main/manifests/linkerd-control-plane.yaml transformers : - | # Inline transformer apiVersion: builtin kind: LabelTransformer metadata: name: linkerd-injection labels: linkerd.io/inject: enabled fieldSpecs: - path: metadata/labels create: true","title":"Cross-Cutting Concerns as Components"},{"location":"03-kustomize/notes/#dynamic-resource-inclusion","text":"apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization # Include remote bases resources : - github.com/kubernetes-sigs/cluster-api//config/default?ref=v1.4.0 - https://raw.githubusercontent.com/prometheus-operator/kube-prometheus/main/manifests/setup/prometheus-operator-0servicemonitorCustomResourceDefinition.yaml # Include local generated manifests resources : - ../manifests-generated/ # Helm output - ../jsonnet-output/ - ./dynamic/ # Generated by scripts # Conditional inclusion using build args vars : - name : FEATURE_FLAG objref : kind : ConfigMap name : feature-flags fieldref : fieldpath : data.enableMonitoring resources : - name : monitoring resource : ../monitoring/ if : $(FEATURE_FLAG) == \"true\"","title":"Dynamic Resource Inclusion"},{"location":"03-kustomize/notes/#cicd-integration-patterns","text":"","title":"\ud83d\udd04 CI/CD Integration Patterns"},{"location":"03-kustomize/notes/#github-actions-workflow","text":"name : Kustomize CI/CD on : [ push , pull_request ] jobs : kustomize-validation : runs-on : ubuntu-latest steps : - uses : actions/checkout@v3 - name : Setup Kustomize uses : imranismail/setup-kustomize@v2 with : kustomize-version : 'v5.0.0' - name : Validate All Overlays run : | for overlay in overlays/*/; do echo \"Validating $overlay\" kustomize build $overlay --load-restrictor=LoadRestrictionsNone | \\ kubectl apply --dry-run=server --validate=true -f - done kustomize-diff : runs-on : ubuntu-latest steps : - uses : actions/checkout@v3 with : fetch-depth : 0 - name : Generate Kustomize Diff run : | git diff --name-only HEAD^ HEAD | grep -E '(kustomization\\.yaml|\\.yaml$|\\.yml$)' | \\ while read file; do dir=$(dirname \"$file\") if [ -f \"$dir/kustomization.yaml\" ]; then echo \"Changes detected in $dir\" kustomize build \"$dir\" --load-restrictor=LoadRestrictionsNone > /tmp/new.yaml git checkout HEAD^ -- \"$dir\" kustomize build \"$dir\" --load-restrictor=LoadRestrictionsNone > /tmp/old.yaml diff -u /tmp/old.yaml /tmp/new.yaml || true fi done","title":"GitHub Actions Workflow"},{"location":"03-kustomize/notes/#argocd-integration","text":"# argocd-application.yaml apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : production-app spec : project : default source : repoURL : https://github.com/org/repo.git targetRevision : main path : k8s/overlays/production kustomize : # ArgoCD-specific kustomize options namePrefix : prod- nameSuffix : -v1 images : - nginx:1.21.0 commonAnnotations : deployed-by : argocd commonLabels : environment : production # Force common labels/annotations on all resources forceCommonLabels : true forceCommonAnnotations : true # Replicas override replicas : - name : app-deployment count : 5 destination : server : https://kubernetes.default.svc namespace : production syncPolicy : automated : prune : true selfHeal : true syncOptions : - CreateNamespace=true - PruneLast=true","title":"ArgoCD Integration"},{"location":"03-kustomize/notes/#security-compliance-patterns","text":"","title":"\ud83d\udd10 Security &amp; Compliance Patterns"},{"location":"03-kustomize/notes/#pod-security-standards-k8s-123","text":"# components/psa-baseline/kustomization.yaml apiVersion : kustomize.config.k8s.io/v1alpha1 kind : Component patches : - target : kind : Namespace patch : | - op: add path: /metadata/labels value: pod-security.kubernetes.io/enforce: baseline pod-security.kubernetes.io/enforce-version: latest pod-security.kubernetes.io/audit: restricted - target : kind : Pod patch : | - op: add path: /spec/securityContext value: seccompProfile: type: RuntimeDefault runAsNonRoot: true","title":"Pod Security Standards (K8s 1.23+)"},{"location":"03-kustomize/notes/#secrets-management-with-external-secrets","text":"# overlays/production/secrets/kustomization.yaml apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization generators : - | # ExternalSecret (external-secrets.io) apiVersion: external-secrets.io/v1beta1 kind: ExternalSecret metadata: name: database-credentials spec: refreshInterval: 1h secretStoreRef: name: vault-backend kind: SecretStore target: name: database-secret creationPolicy: Owner data: - secretKey: password remoteRef: key: /secrets/production/db property: password - secretKey: username remoteRef: key: /secrets/production/db property: username configMapGenerator : - name : sealed-secret-params literals : - namespace=production - scope=strict","title":"Secrets Management with External Secrets"},{"location":"03-kustomize/notes/#production-grade-patterns","text":"","title":"\ud83d\udea8 Production-Grade Patterns"},{"location":"03-kustomize/notes/#multi-cluster-management-with-kustomize","text":"# clusters/aws-us-west-2/production/kustomization.yaml apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization # Import environment overlay resources : - ../../../overlays/production # Cluster-specific patches patches : - target : kind : Ingress patch : | - op: replace path: /metadata/annotations/nginx.ingress.kubernetes.io~1load-balancer-id value: alb-1234567890 - target : kind : PersistentVolumeClaim patch : | - op: replace path: /spec/storageClassName value: gp3 # Cluster-specific generators configMapGenerator : - name : cluster-info literals : - CLUSTER_NAME=aws-us-west-2 - REGION=us-west-2 - PROVIDER=aws - VPC_ID=vpc-123456","title":"Multi-Cluster Management with Kustomize"},{"location":"03-kustomize/notes/#feature-flag-management","text":"# kustomization.yaml with feature flags apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization # Feature flag ConfigMap configMapGenerator : - name : feature-flags literals : - enableNewUI=true - enableBetaFeatures=false - maintenanceMode=false # Conditional resources based on feature flags transformers : - target : kind : Deployment name : app patch : | - op: add path: /spec/template/spec/containers/0/env value: - name: FEATURE_NEW_UI valueFrom: configMapKeyRef: name: feature-flags key: enableNewUI - name: BETA_FEATURES valueFrom: configMapKeyRef: name: feature-flags key: enableBetaFeatures # Remove resources if feature is disabled patches : - target : kind : Deployment name : beta-service patch : | - op: remove path: /spec options : allowMissingTarget : true","title":"Feature Flag Management"},{"location":"03-kustomize/notes/#testing-validation","text":"","title":"\ud83e\uddea Testing &amp; Validation"},{"location":"03-kustomize/notes/#kustomize-test-framework","text":"# kustomization.yaml apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization resources : - deployment.yaml - service.yaml # tests/test.yaml apiVersion : kustomize.config.k8s.io/v1beta1 kind : KustomizationTest tests : - name : deployment-has-correct-replicas resources : - ../deployment.yaml asserts : - equals : fieldPath : spec.replicas value : 3 - name : service-selector-matches-deployment resources : - ../deployment.yaml - ../service.yaml asserts : - and : - equals : fieldPath : spec.selector.matchLabels.app value : myapp - exists : fieldPath : metadata.labels.app","title":"Kustomize Test Framework"},{"location":"03-kustomize/notes/#conftest-integration-open-policy-agent","text":"# conftest-policy.rego package main deny [ msg ] { input.kind == \"Deployment\" not input.spec.template.spec.securityContext.runAsNonRoot msg : = \"Deployment must set runAsNonRoot\" } # Test with kustomize build overlays/production | conftest test -","title":"Conftest Integration (Open Policy Agent)"},{"location":"03-kustomize/notes/#performance-optimizations","text":"# .kustomizerc (global config) apiVersion : kustomize.config.k8s.io/v1beta1 kind : KustomizationConfig buildMetadata : [ originAnnotations , transformerAnnotations ] loadRestrictor : LoadRestrictionsNone enableExec : true enableStar : true # Use caching for remote resources reorder : legacy # In production builds kustomize build \\ --enable-alpha-plugins \\ --load-restrictor=LoadRestrictionsNone \\ --reorder=legacy \\ --enable-exec \\ overlays/production","title":"\u2699\ufe0f Performance Optimizations"},{"location":"03-kustomize/notes/#custom-transformers-generators-go-plugins","text":"// main.go package main import ( \"sigs.k8s.io/kustomize/api/types\" \"sigs.k8s.io/kustomize/kyaml/fn/framework\" \"sigs.k8s.io/kustomize/kyaml/yaml\" ) type AnnotationTransformer struct { Annotations map [ string ] string `yaml:\"annotations,omitempty\"` } func ( at * AnnotationTransformer ) Filter ( objects [] * yaml . RNode ) ([] * yaml . RNode , error ) { for _ , obj := range objects { meta , err := obj . GetMeta () if err != nil { return nil , err } if meta . Annotations == nil { meta . Annotations = make ( map [ string ] string ) } for k , v := range at . Annotations { meta . Annotations [ k ] = v } err = obj . SetAnnotations ( meta . Annotations ) if err != nil { return nil , err } } return objects , nil } func main () { resource := & framework . ResourceList { FunctionConfig : & AnnotationTransformer {}, } framework . Command ( resource , func () error { return resource . Filter ( & AnnotationTransformer {}) }). Execute () } # Use custom transformer apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization transformers : - kind : AnnotationTransformer annotations : managed-by : kustomize git-sha : $(git rev-parse HEAD)","title":"\ud83d\udd27 Custom Transformers &amp; Generators (Go Plugins)"},{"location":"03-kustomize/notes/#comparison-with-modern-alternatives","text":"Feature Kustomize Helm Carvel ytt CUE Paradigm Overlay/Patch Templating Templating Configuration Language Learning Curve Low Medium High Very High GitOps Ready \u2705 Excellent \u26a0\ufe0f Needs Tillerless \u2705 Good \u2705 Good Multi-Environment \u2705 Native \u26a0\ufe0f Values files \u2705 Good \u2705 Excellent Type Safety \u274c \u274c \u26a0\ufe0f Limited \u2705 Excellent Secret Management \u26a0\ufe0f Basic \u26a0\ufe0f Basic \u2705 Good \u2705 Good Community Adoption \u2705 High \u2705 Very High \u26a0\ufe0f Moderate \u26a0\ufe0f Moderate","title":"\ud83d\udcca Comparison with Modern Alternatives"},{"location":"03-kustomize/notes/#best-practices-summary","text":"Use components for cross-cutting concerns (v4+) Leverage replacements over vars (deprecated) Structure overlays by concern: environment \u00d7 region \u00d7 tenant Always validate with kustomize build --load-restrictor=LoadRestrictionsNone Use generators for dynamic content Implement testing with kustomize test framework Cache remote resources in CI/CD Seal secrets before committing Use JSON patches for complex transformations Monitor kustomize releases - rapid evolution This master guide covers advanced modern patterns for enterprise-grade Kubernetes management with Kustomize. The key is embracing its declarative, patch-based philosophy while leveraging new features like components, replacements, and generators for maximum power and flexibility. When you use: yaml secretGenerator: - name: app-secret behavior: merge envs: - prod-secret.txt The behavior field controls how Kustomize handles the existing secret in your manifests, NOT the source file. Here's what happens: If the secret DOES NOT exist in base manifests: With behavior: merge \u2192 Will create the secret With behavior: create \u2192 Will create the secret With behavior: replace \u2192 Will fail (no secret to replace) If the secret DOES exist in base manifests: With behavior: merge \u2192 Merge values from generator with base secret With behavior: create \u2192 Error (can't create, already exists) With behavior: replace \u2192 Replace base secret completely","title":"\ud83c\udfaf Best Practices Summary"},{"location":"03-kustomize/question1/","text":"I'll provide a complete Kustomize exercise with step-by-step problem statements first, then the full solutions with YAML. Kustomize Learning Path: From Zero to Production Application Overview We'll deploy a Node.js API with Redis cache and monitoring sidecar . Components: - node:18-alpine for API (port 3000) - redis:7-alpine for caching - busybox:latest for monitoring sidecar - nginx:1.25 for ingress PROBLEM STATEMENTS ONLY Step 1 \u2013 Base Application Create a minimal deployable base: 1. Create base/ directory 2. Create deployment.yaml for node:18-alpine with 1 replica 3. Create service.yaml ClusterIP on port 80\u21923000 4. Create kustomization.yaml with these 2 resources Step 2 \u2013 Add ConfigMap and Redis Extend base with configuration: 1. Add configmap.yaml with APP_NAME , LOG_LEVEL , REDIS_HOST 2. Add Redis deployment and service (port 6379) 3. Mount ConfigMap as env vars in API deployment 4. Update base kustomization to include all resources Step 3 \u2013 Environment Overlays Create dev/prod overlays: 1. Create overlays/dev/ and overlays/prod/ 2. Each overlay references ../../base 3. In dev: add namePrefix: dev- and label env: dev 4. In prod: add namePrefix: prod- and label env: prod 5. Test both overlays Step 4 \u2013 Replica Scaling Set different replica counts: 1. In dev overlay: set replicas to 2 2. In prod overlay: set replicas to 4 3. Use patchesStrategicMerge Step 5 \u2013 Image Tag Management Use different image tags: 1. In dev: use node:18-alpine 2. In prod: use specific version node:18.20.0-alpine 3. Use images transformer Step 6 \u2013 Environment-Specific ConfigMaps Override ConfigMap values: 1. In dev: set LOG_LEVEL: \"debug\" , REDIS_HOST: \"dev-redis\" 2. In prod: set LOG_LEVEL: \"warn\" , add ENABLE_CACHE: \"true\" 3. Use ConfigMapGenerator with behavior: merge Step 7 \u2013 Add Secrets Add database secrets: 1. Add secret to base with placeholder values 2. In dev: use literal secrets (plain text) 3. In prod: use file-based secrets (simulate secure) 4. Mount secrets as env vars Step 8 \u2013 Resource Limits Set CPU/memory limits: 1. Base: requests 100m CPU, 128Mi RAM; limits 200m CPU, 256Mi RAM 2. Dev: reduce to 50m/64Mi requests, 100m/128Mi limits 3. Prod: increase to 200m/256Mi requests, 500m/512Mi limits 4. Use JSON patches Step 9 \u2013 Add Ingress Add ingress only in overlays: 1. In dev: ingress with host dev-api.example.com 2. In prod: ingress with host api.example.com , TLS, and annotations 3. Use different ingress classes Step 10 \u2013 Monitoring Sidecar (Prod Only) Add sidecar container: 1. Only in prod overlay 2. Add busybox:latest sidecar that runs [\"sh\", \"-c\", \"while true; do echo 'Monitoring...'; sleep 30; done\"] 3. Use JSON patch to add container Step 11 \u2013 Affinity and Tolerations Add production-specific scheduling: 1. In prod: add podAntiAffinity to spread across nodes 2. Add toleration for dedicated=prod:NoSchedule 3. Use strategic merge patch Step 12 \u2013 Common Labels and Annotations Add metadata across all resources: 1. Base: label app: node-api , managed-by: kustomize 2. Dev: annotation environment: development , team: devops 3. Prod: annotation environment: production , team: platform 4. Use commonLabels and commonAnnotations Step 13 \u2013 Namespace Isolation Deploy to different namespaces: 1. Create overlays/prod-us and overlays/prod-eu 2. Each sets different namespace 3. Add region-specific ConfigMap patches 4. Use namespace transformer Step 14 \u2013 Volume Mounts Add persistent configuration: 1. Base: add emptyDir volume 2. Dev: mount as /tmp/logs 3. Prod: mount as /var/log/app with read-only 4. Use volume mounts and patches Step 15 \u2013 Job for Database Migration Add initialization job: 1. Only in prod overlay 2. Create Job that runs before deployment 3. Use kustomize.config.k8s.io/behavior: create annotation 4. Job should simulate DB migration Step 16 \u2013 Multiple Environments with Bases Create staging from prod: 1. Create overlays/staging/ that uses prod as base 2. Override only specific values (replicas=3, different hostname) 3. Demonstrate inheritance chain Step 17 \u2013 Variables Replacement Use Kustomize variables: 1. Define variables for image tags 2. Use in deployment 3. Override in overlays Step 18 \u2013 CRD Patching Add custom resource (simulate): 1. Add CustomResourceDefinition to base 2. Create instance of CRD 3. Patch fields in overlays Now here are the SOLUTIONS WITH YAML : SOLUTIONS Step 1 \u2013 Base Application # base/deployment.yaml apiVersion : apps/v1 kind : Deployment metadata : name : api-deployment spec : replicas : 1 selector : matchLabels : app : api template : metadata : labels : app : api spec : containers : - name : api image : node:18-alpine command : [ \"node\" , \"-e\" , \"console.log('API starting'); require('http').createServer((req, res) => { res.end('Hello from Base API') }).listen(3000)\" ] ports : - containerPort : 3000 # base/service.yaml apiVersion : v1 kind : Service metadata : name : api-service spec : selector : app : api ports : - port : 80 targetPort : 3000 # base/kustomization.yaml apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization resources : - deployment.yaml - service.yaml Step 2 \u2013 Add ConfigMap and Redis # base/configmap.yaml apiVersion : v1 kind : ConfigMap metadata : name : app-config data : APP_NAME : \"node-api\" LOG_LEVEL : \"info\" REDIS_HOST : \"redis-service\" # base/redis-deployment.yaml apiVersion : apps/v1 kind : Deployment metadata : name : redis-deployment spec : replicas : 1 selector : matchLabels : app : redis template : metadata : labels : app : redis spec : containers : - name : redis image : redis:7-alpine ports : - containerPort : 6379 # base/redis-service.yaml apiVersion : v1 kind : Service metadata : name : redis-service spec : selector : app : redis ports : - port : 6379 targetPort : 6379 Update deployment to use ConfigMap: # Update base/deployment.yaml (containers section) containers : - name : api image : node:18-alpine command : [ \"node\" , \"-e\" , \"console.log('API starting'); require('http').createServer((req, res) => { res.end('Hello from Base API') }).listen(3000)\" ] ports : - containerPort : 3000 env : - name : APP_NAME valueFrom : configMapKeyRef : name : app-config key : APP_NAME - name : LOG_LEVEL valueFrom : configMapKeyRef : name : app-config key : LOG_LEVEL - name : REDIS_HOST valueFrom : configMapKeyRef : name : app-config key : REDIS_HOST # Update base/kustomization.yaml apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization resources : - deployment.yaml - service.yaml - configmap.yaml - redis-deployment.yaml - redis-service.yaml Step 3 \u2013 Environment Overlays # overlays/dev/kustomization.yaml apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization namePrefix : dev- commonLabels : env : dev resources : - ../../base # overlays/prod/kustomization.yaml apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization namePrefix : prod- commonLabels : env : prod resources : - ../../base Step 4 \u2013 Replica Scaling # overlays/dev/patch-replicas.yaml apiVersion : apps/v1 kind : Deployment metadata : name : api-deployment spec : replicas : 2 # overlays/prod/patch-replicas.yaml apiVersion : apps/v1 kind : Deployment metadata : name : api-deployment spec : replicas : 4 # Update overlays/dev/kustomization.yaml apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization namePrefix : dev- commonLabels : env : dev resources : - ../../base patchesStrategicMerge : - patch-replicas.yaml # Update overlays/prod/kustomization.yaml apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization namePrefix : prod- commonLabels : env : prod resources : - ../../base patchesStrategicMerge : - patch-replicas.yaml Step 5 \u2013 Image Tag Management # Update overlays/dev/kustomization.yaml apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization namePrefix : dev- commonLabels : env : dev resources : - ../../base patchesStrategicMerge : - patch-replicas.yaml images : - name : node:18-alpine newTag : 18-alpine # Update overlays/prod/kustomization.yaml apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization namePrefix : prod- commonLabels : env : prod resources : - ../../base patchesStrategicMerge : - patch-replicas.yaml images : - name : node:18-alpine newTag : 18.20.0-alpine - name : redis:7-alpine newTag : 7.2.4-alpine Step 6 \u2013 Environment-Specific ConfigMaps # overlays/dev/configmap-patch.yaml apiVersion : v1 kind : ConfigMap metadata : name : app-config data : LOG_LEVEL : \"debug\" REDIS_HOST : \"dev-redis-service\" ENV_TYPE : \"development\" # overlays/prod/configmap-patch.yaml apiVersion : v1 kind : ConfigMap metadata : name : app-config data : LOG_LEVEL : \"warn\" ENV_TYPE : \"production\" ENABLE_CACHE : \"true\" MAX_CONNECTIONS : \"1000\" # Update overlays/dev/kustomization.yaml apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization namePrefix : dev- commonLabels : env : dev resources : - ../../base patchesStrategicMerge : - patch-replicas.yaml - configmap-patch.yaml images : - name : node:18-alpine newTag : 18-alpine # Update overlays/prod/kustomization.yaml apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization namePrefix : prod- commonLabels : env : prod resources : - ../../base patchesStrategicMerge : - patch-replicas.yaml - configmap-patch.yaml images : - name : node:18-alpine newTag : 18.20.0-alpine - name : redis:7-alpine newTag : 7.2.4-alpine Step 7 \u2013 Add Secrets # base/secret.yaml apiVersion : v1 kind : Secret metadata : name : app-secret type : Opaque data : # Base64 encoded placeholder values DB_PASSWORD : cGxhY2Vob2xkZXI= # \"placeholder\" API_KEY : cGxhY2Vob2xkZXI= # \"placeholder\" Update deployment to use secrets: # Add to base/deployment.yaml containers.env section env : # ... existing env vars ... - name : DB_PASSWORD valueFrom : secretKeyRef : name : app-secret key : DB_PASSWORD - name : API_KEY valueFrom : secretKeyRef : name : app-secret key : API_KEY # Update base/kustomization.yaml to include secret.yaml resources : - deployment.yaml - service.yaml - configmap.yaml - redis-deployment.yaml - redis-service.yaml - secret.yaml # overlays/dev/kustomization.yaml with secretGenerator apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization namePrefix : dev- commonLabels : env : dev resources : - ../../base patchesStrategicMerge : - patch-replicas.yaml - configmap-patch.yaml images : - name : node:18-alpine newTag : 18-alpine secretGenerator : - name : app-secret behavior : merge literals : - DB_PASSWORD=dev-password-123 - API_KEY=dev-api-key-abc # Create prod-secret.txt DB_PASSWORD=prod-strong-password-!@#456 API_KEY=prod-secure-api-key-xyz789 # overlays/prod/kustomization.yaml with file-based secret apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization namePrefix : prod- commonLabels : env : prod resources : - ../../base patchesStrategicMerge : - patch-replicas.yaml - configmap-patch.yaml images : - name : node:18-alpine newTag : 18.20.0-alpine - name : redis:7-alpine newTag : 7.2.4-alpine secretGenerator : - name : app-secret behavior : merge envs : - prod-secret.txt Step 8 \u2013 Resource Limits # base/deployment.yaml - add resources section containers : - name : api image : node:18-alpine # ... existing config ... resources : requests : memory : \"128Mi\" cpu : \"100m\" limits : memory : \"256Mi\" cpu : \"200m\" - name : redis image : redis:7-alpine resources : requests : memory : \"64Mi\" cpu : \"50m\" limits : memory : \"128Mi\" cpu : \"100m\" # overlays/dev/patch-resources.yaml apiVersion : apps/v1 kind : Deployment metadata : name : api-deployment spec : template : spec : containers : - name : api resources : requests : memory : \"64Mi\" cpu : \"50m\" limits : memory : \"128Mi\" cpu : \"100m\" - name : redis resources : requests : memory : \"32Mi\" cpu : \"25m\" limits : memory : \"64Mi\" cpu : \"50m\" # overlays/prod/patch-resources.yaml apiVersion : apps/v1 kind : Deployment metadata : name : api-deployment spec : template : spec : containers : - name : api resources : requests : memory : \"256Mi\" cpu : \"200m\" limits : memory : \"512Mi\" cpu : \"500m\" - name : redis resources : requests : memory : \"128Mi\" cpu : \"100m\" limits : memory : \"256Mi\" cpu : \"200m\" Update kustomization files to include these patches. Step 9 \u2013 Add Ingress # overlays/dev/ingress.yaml apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : api-ingress annotations : nginx.ingress.kubernetes.io/rewrite-target : / spec : ingressClassName : nginx rules : - host : dev-api.example.com http : paths : - path : / pathType : Prefix backend : service : name : api-service port : number : 80 # overlays/prod/ingress.yaml apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : api-ingress annotations : nginx.ingress.kubernetes.io/rewrite-target : / nginx.ingress.kubernetes.io/ssl-redirect : \"true\" cert-manager.io/cluster-issuer : \"letsencrypt-prod\" spec : ingressClassName : nginx tls : - hosts : - api.example.com secretName : api-tls-secret rules : - host : api.example.com http : paths : - path : / pathType : Prefix backend : service : name : api-service port : number : 80 # Update overlays/dev/kustomization.yaml apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization namePrefix : dev- commonLabels : env : dev resources : - ../../base - ingress.yaml # ... rest remains ... # Update overlays/prod/kustomization.yaml apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization namePrefix : prod- commonLabels : env : prod resources : - ../../base - ingress.yaml # ... rest remains ... Step 10 \u2013 Monitoring Sidecar (Prod Only) # overlays/prod/patch-sidecar.yaml apiVersion : apps/v1 kind : Deployment metadata : name : api-deployment spec : template : spec : containers : - name : monitor image : busybox:latest command : [ \"sh\" , \"-c\" , \"while true; do echo 'Monitoring API...'; sleep 30; done\" ] resources : requests : memory : \"16Mi\" cpu : \"10m\" limits : memory : \"32Mi\" cpu : \"20m\" Add to prod kustomization patches. Step 11 \u2013 Affinity and Tolerations # overlays/prod/patch-affinity.yaml apiVersion : apps/v1 kind : Deployment metadata : name : api-deployment spec : template : spec : affinity : podAntiAffinity : preferredDuringSchedulingIgnoredDuringExecution : - weight : 100 podAffinityTerm : labelSelector : matchExpressions : - key : app operator : In values : - api topologyKey : kubernetes.io/hostname tolerations : - key : \"dedicated\" operator : \"Equal\" value : \"prod\" effect : \"NoSchedule\" Step 12 \u2013 Common Labels and Annotations # Update base/kustomization.yaml apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization resources : - deployment.yaml - service.yaml - configmap.yaml - redis-deployment.yaml - redis-service.yaml - secret.yaml commonLabels : app : node-api managed-by : kustomize version : v1.0.0 # Update overlays/dev/kustomization.yaml apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization namePrefix : dev- commonLabels : env : dev commonAnnotations : environment : development team : devops deploy-timestamp : \"2024-01-15\" # ... rest remains ... # Update overlays/prod/kustomization.yaml apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization namePrefix : prod- commonLabels : env : prod commonAnnotations : environment : production team : platform owner : platform-team@company.com sla-tier : \"gold\" # ... rest remains ... Step 13 \u2013 Namespace Isolation # overlays/prod-us/kustomization.yaml apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization namespace : prod-us namePrefix : us- commonLabels : region : us-east resources : - ../prod patchesStrategicMerge : - configmap-region.yaml configMapGenerator : - name : app-config behavior : merge literals : - REGION=us-east - DATACENTER=dc1 # overlays/prod-eu/kustomization.yaml apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization namespace : prod-eu namePrefix : eu- commonLabels : region : eu-west resources : - ../prod patchesStrategicMerge : - configmap-region.yaml configMapGenerator : - name : app-config behavior : merge literals : - REGION=eu-west - DATACENTER=dc2 Step 14 \u2013 Volume Mounts # Update base/deployment.yaml - add volumes spec : template : spec : volumes : - name : app-logs emptyDir : {} containers : - name : api # ... existing config ... volumeMounts : - name : app-logs mountPath : /tmp/logs # overlays/prod/patch-volumes.yaml apiVersion : apps/v1 kind : Deployment metadata : name : api-deployment spec : template : spec : containers : - name : api volumeMounts : - name : app-logs mountPath : /var/log/app readOnly : false Step 15 \u2013 Job for Database Migration # overlays/prod/migration-job.yaml apiVersion : batch/v1 kind : Job metadata : name : db-migration annotations : kustomize.config.k8s.io/behavior : create spec : template : spec : restartPolicy : Never containers : - name : migration image : node:18-alpine command : [ \"sh\" , \"-c\" , \"echo 'Running database migrations...'; sleep 5; echo 'Migrations completed successfully'\" ] env : - name : DB_HOST valueFrom : configMapKeyRef : name : app-config key : REDIS_HOST Step 16 \u2013 Multiple Environments with Bases # overlays/staging/kustomization.yaml apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization namePrefix : staging- commonLabels : env : staging commonAnnotations : environment : staging team : qa resources : - ../prod patchesStrategicMerge : - patch-staging.yaml images : - name : node:18-alpine newTag : 18-alpine # overlays/staging/patch-staging.yaml apiVersion : apps/v1 kind : Deployment metadata : name : api-deployment spec : replicas : 3 --- apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : api-ingress spec : rules : - host : staging-api.example.com http : paths : - path : / pathType : Prefix backend : service : name : api-service port : number : 80 Step 17 \u2013 Variables Replacement # Update base/kustomization.yaml with vars apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization resources : - deployment.yaml - service.yaml - configmap.yaml - redis-deployment.yaml - redis-service.yaml - secret.yaml commonLabels : app : node-api managed-by : kustomize version : v1.0.0 vars : - name : API_IMAGE objref : kind : Deployment name : api-deployment apiVersion : apps/v1 fieldref : fieldpath : spec.template.spec.containers[0].image - name : REDIS_IMAGE objref : kind : Deployment name : redis-deployment apiVersion : apps/v1 fieldref : fieldpath : spec.template.spec.containers[0].image # Add to base/configmap.yaml data : # ... existing data ... API_IMAGE : $(API_IMAGE) REDIS_IMAGE : $(REDIS_IMAGE) Step 18 \u2013 CRD Patching # base/custom-resource.yaml apiVersion : apiextensions.k8s.io/v1 kind : CustomResourceDefinition metadata : name : monitors.monitoring.example.com spec : group : monitoring.example.com versions : - name : v1 served : true storage : true schema : openAPIV3Schema : type : object properties : spec : type : object properties : interval : type : string endpoints : type : array items : type : string scope : Namespaced names : plural : monitors singular : monitor kind : Monitor --- apiVersion : monitoring.example.com/v1 kind : Monitor metadata : name : api-monitor spec : interval : \"30s\" endpoints : - \"/health\" - \"/metrics\" # overlays/prod/patch-crd.yaml apiVersion : monitoring.example.com/v1 kind : Monitor metadata : name : api-monitor spec : interval : \"15s\" endpoints : - \"/health\" - \"/metrics\" - \"/debug\" alertThreshold : \"95\" Testing Commands # Test each step kubectl kustomize ./base kubectl kustomize ./overlays/dev kubectl kustomize ./overlays/prod kubectl kustomize ./overlays/prod-us kubectl kustomize ./overlays/prod-eu kubectl kustomize ./overlays/staging # Apply to cluster kubectl apply -k ./overlays/dev kubectl apply -k ./overlays/prod This complete solution covers 15+ Kustomize transformers including: - resources , patchesStrategicMerge , patchesJson6902 - namePrefix , nameSuffix , namespace - commonLabels , commonAnnotations - images transformer - configMapGenerator , secretGenerator - vars for variable substitution - replicas field in patches - affinity , tolerations , volumes - CRD support - Job creation with annotations - Multi-environment inheritance","title":"Question1"},{"location":"03-kustomize/question1/#kustomize-learning-path-from-zero-to-production","text":"","title":"Kustomize Learning Path: From Zero to Production"},{"location":"03-kustomize/question1/#application-overview","text":"We'll deploy a Node.js API with Redis cache and monitoring sidecar . Components: - node:18-alpine for API (port 3000) - redis:7-alpine for caching - busybox:latest for monitoring sidecar - nginx:1.25 for ingress","title":"Application Overview"},{"location":"03-kustomize/question1/#problem-statements-only","text":"","title":"PROBLEM STATEMENTS ONLY"},{"location":"03-kustomize/question1/#step-1-base-application","text":"Create a minimal deployable base: 1. Create base/ directory 2. Create deployment.yaml for node:18-alpine with 1 replica 3. Create service.yaml ClusterIP on port 80\u21923000 4. Create kustomization.yaml with these 2 resources","title":"Step 1 \u2013 Base Application"},{"location":"03-kustomize/question1/#step-2-add-configmap-and-redis","text":"Extend base with configuration: 1. Add configmap.yaml with APP_NAME , LOG_LEVEL , REDIS_HOST 2. Add Redis deployment and service (port 6379) 3. Mount ConfigMap as env vars in API deployment 4. Update base kustomization to include all resources","title":"Step 2 \u2013 Add ConfigMap and Redis"},{"location":"03-kustomize/question1/#step-3-environment-overlays","text":"Create dev/prod overlays: 1. Create overlays/dev/ and overlays/prod/ 2. Each overlay references ../../base 3. In dev: add namePrefix: dev- and label env: dev 4. In prod: add namePrefix: prod- and label env: prod 5. Test both overlays","title":"Step 3 \u2013 Environment Overlays"},{"location":"03-kustomize/question1/#step-4-replica-scaling","text":"Set different replica counts: 1. In dev overlay: set replicas to 2 2. In prod overlay: set replicas to 4 3. Use patchesStrategicMerge","title":"Step 4 \u2013 Replica Scaling"},{"location":"03-kustomize/question1/#step-5-image-tag-management","text":"Use different image tags: 1. In dev: use node:18-alpine 2. In prod: use specific version node:18.20.0-alpine 3. Use images transformer","title":"Step 5 \u2013 Image Tag Management"},{"location":"03-kustomize/question1/#step-6-environment-specific-configmaps","text":"Override ConfigMap values: 1. In dev: set LOG_LEVEL: \"debug\" , REDIS_HOST: \"dev-redis\" 2. In prod: set LOG_LEVEL: \"warn\" , add ENABLE_CACHE: \"true\" 3. Use ConfigMapGenerator with behavior: merge","title":"Step 6 \u2013 Environment-Specific ConfigMaps"},{"location":"03-kustomize/question1/#step-7-add-secrets","text":"Add database secrets: 1. Add secret to base with placeholder values 2. In dev: use literal secrets (plain text) 3. In prod: use file-based secrets (simulate secure) 4. Mount secrets as env vars","title":"Step 7 \u2013 Add Secrets"},{"location":"03-kustomize/question1/#step-8-resource-limits","text":"Set CPU/memory limits: 1. Base: requests 100m CPU, 128Mi RAM; limits 200m CPU, 256Mi RAM 2. Dev: reduce to 50m/64Mi requests, 100m/128Mi limits 3. Prod: increase to 200m/256Mi requests, 500m/512Mi limits 4. Use JSON patches","title":"Step 8 \u2013 Resource Limits"},{"location":"03-kustomize/question1/#step-9-add-ingress","text":"Add ingress only in overlays: 1. In dev: ingress with host dev-api.example.com 2. In prod: ingress with host api.example.com , TLS, and annotations 3. Use different ingress classes","title":"Step 9 \u2013 Add Ingress"},{"location":"03-kustomize/question1/#step-10-monitoring-sidecar-prod-only","text":"Add sidecar container: 1. Only in prod overlay 2. Add busybox:latest sidecar that runs [\"sh\", \"-c\", \"while true; do echo 'Monitoring...'; sleep 30; done\"] 3. Use JSON patch to add container","title":"Step 10 \u2013 Monitoring Sidecar (Prod Only)"},{"location":"03-kustomize/question1/#step-11-affinity-and-tolerations","text":"Add production-specific scheduling: 1. In prod: add podAntiAffinity to spread across nodes 2. Add toleration for dedicated=prod:NoSchedule 3. Use strategic merge patch","title":"Step 11 \u2013 Affinity and Tolerations"},{"location":"03-kustomize/question1/#step-12-common-labels-and-annotations","text":"Add metadata across all resources: 1. Base: label app: node-api , managed-by: kustomize 2. Dev: annotation environment: development , team: devops 3. Prod: annotation environment: production , team: platform 4. Use commonLabels and commonAnnotations","title":"Step 12 \u2013 Common Labels and Annotations"},{"location":"03-kustomize/question1/#step-13-namespace-isolation","text":"Deploy to different namespaces: 1. Create overlays/prod-us and overlays/prod-eu 2. Each sets different namespace 3. Add region-specific ConfigMap patches 4. Use namespace transformer","title":"Step 13 \u2013 Namespace Isolation"},{"location":"03-kustomize/question1/#step-14-volume-mounts","text":"Add persistent configuration: 1. Base: add emptyDir volume 2. Dev: mount as /tmp/logs 3. Prod: mount as /var/log/app with read-only 4. Use volume mounts and patches","title":"Step 14 \u2013 Volume Mounts"},{"location":"03-kustomize/question1/#step-15-job-for-database-migration","text":"Add initialization job: 1. Only in prod overlay 2. Create Job that runs before deployment 3. Use kustomize.config.k8s.io/behavior: create annotation 4. Job should simulate DB migration","title":"Step 15 \u2013 Job for Database Migration"},{"location":"03-kustomize/question1/#step-16-multiple-environments-with-bases","text":"Create staging from prod: 1. Create overlays/staging/ that uses prod as base 2. Override only specific values (replicas=3, different hostname) 3. Demonstrate inheritance chain","title":"Step 16 \u2013 Multiple Environments with Bases"},{"location":"03-kustomize/question1/#step-17-variables-replacement","text":"Use Kustomize variables: 1. Define variables for image tags 2. Use in deployment 3. Override in overlays","title":"Step 17 \u2013 Variables Replacement"},{"location":"03-kustomize/question1/#step-18-crd-patching","text":"Add custom resource (simulate): 1. Add CustomResourceDefinition to base 2. Create instance of CRD 3. Patch fields in overlays Now here are the SOLUTIONS WITH YAML :","title":"Step 18 \u2013 CRD Patching"},{"location":"03-kustomize/question1/#solutions","text":"","title":"SOLUTIONS"},{"location":"03-kustomize/question1/#step-1-base-application_1","text":"# base/deployment.yaml apiVersion : apps/v1 kind : Deployment metadata : name : api-deployment spec : replicas : 1 selector : matchLabels : app : api template : metadata : labels : app : api spec : containers : - name : api image : node:18-alpine command : [ \"node\" , \"-e\" , \"console.log('API starting'); require('http').createServer((req, res) => { res.end('Hello from Base API') }).listen(3000)\" ] ports : - containerPort : 3000 # base/service.yaml apiVersion : v1 kind : Service metadata : name : api-service spec : selector : app : api ports : - port : 80 targetPort : 3000 # base/kustomization.yaml apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization resources : - deployment.yaml - service.yaml","title":"Step 1 \u2013 Base Application"},{"location":"03-kustomize/question1/#step-2-add-configmap-and-redis_1","text":"# base/configmap.yaml apiVersion : v1 kind : ConfigMap metadata : name : app-config data : APP_NAME : \"node-api\" LOG_LEVEL : \"info\" REDIS_HOST : \"redis-service\" # base/redis-deployment.yaml apiVersion : apps/v1 kind : Deployment metadata : name : redis-deployment spec : replicas : 1 selector : matchLabels : app : redis template : metadata : labels : app : redis spec : containers : - name : redis image : redis:7-alpine ports : - containerPort : 6379 # base/redis-service.yaml apiVersion : v1 kind : Service metadata : name : redis-service spec : selector : app : redis ports : - port : 6379 targetPort : 6379 Update deployment to use ConfigMap: # Update base/deployment.yaml (containers section) containers : - name : api image : node:18-alpine command : [ \"node\" , \"-e\" , \"console.log('API starting'); require('http').createServer((req, res) => { res.end('Hello from Base API') }).listen(3000)\" ] ports : - containerPort : 3000 env : - name : APP_NAME valueFrom : configMapKeyRef : name : app-config key : APP_NAME - name : LOG_LEVEL valueFrom : configMapKeyRef : name : app-config key : LOG_LEVEL - name : REDIS_HOST valueFrom : configMapKeyRef : name : app-config key : REDIS_HOST # Update base/kustomization.yaml apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization resources : - deployment.yaml - service.yaml - configmap.yaml - redis-deployment.yaml - redis-service.yaml","title":"Step 2 \u2013 Add ConfigMap and Redis"},{"location":"03-kustomize/question1/#step-3-environment-overlays_1","text":"# overlays/dev/kustomization.yaml apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization namePrefix : dev- commonLabels : env : dev resources : - ../../base # overlays/prod/kustomization.yaml apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization namePrefix : prod- commonLabels : env : prod resources : - ../../base","title":"Step 3 \u2013 Environment Overlays"},{"location":"03-kustomize/question1/#step-4-replica-scaling_1","text":"# overlays/dev/patch-replicas.yaml apiVersion : apps/v1 kind : Deployment metadata : name : api-deployment spec : replicas : 2 # overlays/prod/patch-replicas.yaml apiVersion : apps/v1 kind : Deployment metadata : name : api-deployment spec : replicas : 4 # Update overlays/dev/kustomization.yaml apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization namePrefix : dev- commonLabels : env : dev resources : - ../../base patchesStrategicMerge : - patch-replicas.yaml # Update overlays/prod/kustomization.yaml apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization namePrefix : prod- commonLabels : env : prod resources : - ../../base patchesStrategicMerge : - patch-replicas.yaml","title":"Step 4 \u2013 Replica Scaling"},{"location":"03-kustomize/question1/#step-5-image-tag-management_1","text":"# Update overlays/dev/kustomization.yaml apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization namePrefix : dev- commonLabels : env : dev resources : - ../../base patchesStrategicMerge : - patch-replicas.yaml images : - name : node:18-alpine newTag : 18-alpine # Update overlays/prod/kustomization.yaml apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization namePrefix : prod- commonLabels : env : prod resources : - ../../base patchesStrategicMerge : - patch-replicas.yaml images : - name : node:18-alpine newTag : 18.20.0-alpine - name : redis:7-alpine newTag : 7.2.4-alpine","title":"Step 5 \u2013 Image Tag Management"},{"location":"03-kustomize/question1/#step-6-environment-specific-configmaps_1","text":"# overlays/dev/configmap-patch.yaml apiVersion : v1 kind : ConfigMap metadata : name : app-config data : LOG_LEVEL : \"debug\" REDIS_HOST : \"dev-redis-service\" ENV_TYPE : \"development\" # overlays/prod/configmap-patch.yaml apiVersion : v1 kind : ConfigMap metadata : name : app-config data : LOG_LEVEL : \"warn\" ENV_TYPE : \"production\" ENABLE_CACHE : \"true\" MAX_CONNECTIONS : \"1000\" # Update overlays/dev/kustomization.yaml apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization namePrefix : dev- commonLabels : env : dev resources : - ../../base patchesStrategicMerge : - patch-replicas.yaml - configmap-patch.yaml images : - name : node:18-alpine newTag : 18-alpine # Update overlays/prod/kustomization.yaml apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization namePrefix : prod- commonLabels : env : prod resources : - ../../base patchesStrategicMerge : - patch-replicas.yaml - configmap-patch.yaml images : - name : node:18-alpine newTag : 18.20.0-alpine - name : redis:7-alpine newTag : 7.2.4-alpine","title":"Step 6 \u2013 Environment-Specific ConfigMaps"},{"location":"03-kustomize/question1/#step-7-add-secrets_1","text":"# base/secret.yaml apiVersion : v1 kind : Secret metadata : name : app-secret type : Opaque data : # Base64 encoded placeholder values DB_PASSWORD : cGxhY2Vob2xkZXI= # \"placeholder\" API_KEY : cGxhY2Vob2xkZXI= # \"placeholder\" Update deployment to use secrets: # Add to base/deployment.yaml containers.env section env : # ... existing env vars ... - name : DB_PASSWORD valueFrom : secretKeyRef : name : app-secret key : DB_PASSWORD - name : API_KEY valueFrom : secretKeyRef : name : app-secret key : API_KEY # Update base/kustomization.yaml to include secret.yaml resources : - deployment.yaml - service.yaml - configmap.yaml - redis-deployment.yaml - redis-service.yaml - secret.yaml # overlays/dev/kustomization.yaml with secretGenerator apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization namePrefix : dev- commonLabels : env : dev resources : - ../../base patchesStrategicMerge : - patch-replicas.yaml - configmap-patch.yaml images : - name : node:18-alpine newTag : 18-alpine secretGenerator : - name : app-secret behavior : merge literals : - DB_PASSWORD=dev-password-123 - API_KEY=dev-api-key-abc # Create prod-secret.txt DB_PASSWORD=prod-strong-password-!@#456 API_KEY=prod-secure-api-key-xyz789 # overlays/prod/kustomization.yaml with file-based secret apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization namePrefix : prod- commonLabels : env : prod resources : - ../../base patchesStrategicMerge : - patch-replicas.yaml - configmap-patch.yaml images : - name : node:18-alpine newTag : 18.20.0-alpine - name : redis:7-alpine newTag : 7.2.4-alpine secretGenerator : - name : app-secret behavior : merge envs : - prod-secret.txt","title":"Step 7 \u2013 Add Secrets"},{"location":"03-kustomize/question1/#step-8-resource-limits_1","text":"# base/deployment.yaml - add resources section containers : - name : api image : node:18-alpine # ... existing config ... resources : requests : memory : \"128Mi\" cpu : \"100m\" limits : memory : \"256Mi\" cpu : \"200m\" - name : redis image : redis:7-alpine resources : requests : memory : \"64Mi\" cpu : \"50m\" limits : memory : \"128Mi\" cpu : \"100m\" # overlays/dev/patch-resources.yaml apiVersion : apps/v1 kind : Deployment metadata : name : api-deployment spec : template : spec : containers : - name : api resources : requests : memory : \"64Mi\" cpu : \"50m\" limits : memory : \"128Mi\" cpu : \"100m\" - name : redis resources : requests : memory : \"32Mi\" cpu : \"25m\" limits : memory : \"64Mi\" cpu : \"50m\" # overlays/prod/patch-resources.yaml apiVersion : apps/v1 kind : Deployment metadata : name : api-deployment spec : template : spec : containers : - name : api resources : requests : memory : \"256Mi\" cpu : \"200m\" limits : memory : \"512Mi\" cpu : \"500m\" - name : redis resources : requests : memory : \"128Mi\" cpu : \"100m\" limits : memory : \"256Mi\" cpu : \"200m\" Update kustomization files to include these patches.","title":"Step 8 \u2013 Resource Limits"},{"location":"03-kustomize/question1/#step-9-add-ingress_1","text":"# overlays/dev/ingress.yaml apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : api-ingress annotations : nginx.ingress.kubernetes.io/rewrite-target : / spec : ingressClassName : nginx rules : - host : dev-api.example.com http : paths : - path : / pathType : Prefix backend : service : name : api-service port : number : 80 # overlays/prod/ingress.yaml apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : api-ingress annotations : nginx.ingress.kubernetes.io/rewrite-target : / nginx.ingress.kubernetes.io/ssl-redirect : \"true\" cert-manager.io/cluster-issuer : \"letsencrypt-prod\" spec : ingressClassName : nginx tls : - hosts : - api.example.com secretName : api-tls-secret rules : - host : api.example.com http : paths : - path : / pathType : Prefix backend : service : name : api-service port : number : 80 # Update overlays/dev/kustomization.yaml apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization namePrefix : dev- commonLabels : env : dev resources : - ../../base - ingress.yaml # ... rest remains ... # Update overlays/prod/kustomization.yaml apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization namePrefix : prod- commonLabels : env : prod resources : - ../../base - ingress.yaml # ... rest remains ...","title":"Step 9 \u2013 Add Ingress"},{"location":"03-kustomize/question1/#step-10-monitoring-sidecar-prod-only_1","text":"# overlays/prod/patch-sidecar.yaml apiVersion : apps/v1 kind : Deployment metadata : name : api-deployment spec : template : spec : containers : - name : monitor image : busybox:latest command : [ \"sh\" , \"-c\" , \"while true; do echo 'Monitoring API...'; sleep 30; done\" ] resources : requests : memory : \"16Mi\" cpu : \"10m\" limits : memory : \"32Mi\" cpu : \"20m\" Add to prod kustomization patches.","title":"Step 10 \u2013 Monitoring Sidecar (Prod Only)"},{"location":"03-kustomize/question1/#step-11-affinity-and-tolerations_1","text":"# overlays/prod/patch-affinity.yaml apiVersion : apps/v1 kind : Deployment metadata : name : api-deployment spec : template : spec : affinity : podAntiAffinity : preferredDuringSchedulingIgnoredDuringExecution : - weight : 100 podAffinityTerm : labelSelector : matchExpressions : - key : app operator : In values : - api topologyKey : kubernetes.io/hostname tolerations : - key : \"dedicated\" operator : \"Equal\" value : \"prod\" effect : \"NoSchedule\"","title":"Step 11 \u2013 Affinity and Tolerations"},{"location":"03-kustomize/question1/#step-12-common-labels-and-annotations_1","text":"# Update base/kustomization.yaml apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization resources : - deployment.yaml - service.yaml - configmap.yaml - redis-deployment.yaml - redis-service.yaml - secret.yaml commonLabels : app : node-api managed-by : kustomize version : v1.0.0 # Update overlays/dev/kustomization.yaml apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization namePrefix : dev- commonLabels : env : dev commonAnnotations : environment : development team : devops deploy-timestamp : \"2024-01-15\" # ... rest remains ... # Update overlays/prod/kustomization.yaml apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization namePrefix : prod- commonLabels : env : prod commonAnnotations : environment : production team : platform owner : platform-team@company.com sla-tier : \"gold\" # ... rest remains ...","title":"Step 12 \u2013 Common Labels and Annotations"},{"location":"03-kustomize/question1/#step-13-namespace-isolation_1","text":"# overlays/prod-us/kustomization.yaml apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization namespace : prod-us namePrefix : us- commonLabels : region : us-east resources : - ../prod patchesStrategicMerge : - configmap-region.yaml configMapGenerator : - name : app-config behavior : merge literals : - REGION=us-east - DATACENTER=dc1 # overlays/prod-eu/kustomization.yaml apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization namespace : prod-eu namePrefix : eu- commonLabels : region : eu-west resources : - ../prod patchesStrategicMerge : - configmap-region.yaml configMapGenerator : - name : app-config behavior : merge literals : - REGION=eu-west - DATACENTER=dc2","title":"Step 13 \u2013 Namespace Isolation"},{"location":"03-kustomize/question1/#step-14-volume-mounts_1","text":"# Update base/deployment.yaml - add volumes spec : template : spec : volumes : - name : app-logs emptyDir : {} containers : - name : api # ... existing config ... volumeMounts : - name : app-logs mountPath : /tmp/logs # overlays/prod/patch-volumes.yaml apiVersion : apps/v1 kind : Deployment metadata : name : api-deployment spec : template : spec : containers : - name : api volumeMounts : - name : app-logs mountPath : /var/log/app readOnly : false","title":"Step 14 \u2013 Volume Mounts"},{"location":"03-kustomize/question1/#step-15-job-for-database-migration_1","text":"# overlays/prod/migration-job.yaml apiVersion : batch/v1 kind : Job metadata : name : db-migration annotations : kustomize.config.k8s.io/behavior : create spec : template : spec : restartPolicy : Never containers : - name : migration image : node:18-alpine command : [ \"sh\" , \"-c\" , \"echo 'Running database migrations...'; sleep 5; echo 'Migrations completed successfully'\" ] env : - name : DB_HOST valueFrom : configMapKeyRef : name : app-config key : REDIS_HOST","title":"Step 15 \u2013 Job for Database Migration"},{"location":"03-kustomize/question1/#step-16-multiple-environments-with-bases_1","text":"# overlays/staging/kustomization.yaml apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization namePrefix : staging- commonLabels : env : staging commonAnnotations : environment : staging team : qa resources : - ../prod patchesStrategicMerge : - patch-staging.yaml images : - name : node:18-alpine newTag : 18-alpine # overlays/staging/patch-staging.yaml apiVersion : apps/v1 kind : Deployment metadata : name : api-deployment spec : replicas : 3 --- apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : api-ingress spec : rules : - host : staging-api.example.com http : paths : - path : / pathType : Prefix backend : service : name : api-service port : number : 80","title":"Step 16 \u2013 Multiple Environments with Bases"},{"location":"03-kustomize/question1/#step-17-variables-replacement_1","text":"# Update base/kustomization.yaml with vars apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization resources : - deployment.yaml - service.yaml - configmap.yaml - redis-deployment.yaml - redis-service.yaml - secret.yaml commonLabels : app : node-api managed-by : kustomize version : v1.0.0 vars : - name : API_IMAGE objref : kind : Deployment name : api-deployment apiVersion : apps/v1 fieldref : fieldpath : spec.template.spec.containers[0].image - name : REDIS_IMAGE objref : kind : Deployment name : redis-deployment apiVersion : apps/v1 fieldref : fieldpath : spec.template.spec.containers[0].image # Add to base/configmap.yaml data : # ... existing data ... API_IMAGE : $(API_IMAGE) REDIS_IMAGE : $(REDIS_IMAGE)","title":"Step 17 \u2013 Variables Replacement"},{"location":"03-kustomize/question1/#step-18-crd-patching_1","text":"# base/custom-resource.yaml apiVersion : apiextensions.k8s.io/v1 kind : CustomResourceDefinition metadata : name : monitors.monitoring.example.com spec : group : monitoring.example.com versions : - name : v1 served : true storage : true schema : openAPIV3Schema : type : object properties : spec : type : object properties : interval : type : string endpoints : type : array items : type : string scope : Namespaced names : plural : monitors singular : monitor kind : Monitor --- apiVersion : monitoring.example.com/v1 kind : Monitor metadata : name : api-monitor spec : interval : \"30s\" endpoints : - \"/health\" - \"/metrics\" # overlays/prod/patch-crd.yaml apiVersion : monitoring.example.com/v1 kind : Monitor metadata : name : api-monitor spec : interval : \"15s\" endpoints : - \"/health\" - \"/metrics\" - \"/debug\" alertThreshold : \"95\"","title":"Step 18 \u2013 CRD Patching"},{"location":"03-kustomize/question1/#testing-commands","text":"# Test each step kubectl kustomize ./base kubectl kustomize ./overlays/dev kubectl kustomize ./overlays/prod kubectl kustomize ./overlays/prod-us kubectl kustomize ./overlays/prod-eu kubectl kustomize ./overlays/staging # Apply to cluster kubectl apply -k ./overlays/dev kubectl apply -k ./overlays/prod This complete solution covers 15+ Kustomize transformers including: - resources , patchesStrategicMerge , patchesJson6902 - namePrefix , nameSuffix , namespace - commonLabels , commonAnnotations - images transformer - configMapGenerator , secretGenerator - vars for variable substitution - replicas field in patches - affinity , tolerations , volumes - CRD support - Job creation with annotations - Multi-environment inheritance","title":"Testing Commands"},{"location":"04-admission-webhook/commands/","text":"Bootstrap a kind cluster kind create cluster --name webhook --image kindest/node:v1.29.2 Generating Certificates and Shit!! cd C:\\Users\\VikashKumar\\Desktop\\dev3\\ops\\kubequest\\compose\\04-admission-webhook\\controllers cd /mnt/c/Users/VikashKumar/Desktop/dev3/ops/kubequest/compose/04-admission-webhook/controllers mkdir -p tls docker run -it --rm -v ${PWD}:/work -w /work debian bash apt-get update && apt-get install -y curl && curl -L https://github.com/cloudflare/cfssl/releases/download/v1.5.0/cfssl_1.5.0_linux_amd64 -o /usr/local/bin/cfssl && \\ curl -L https://github.com/cloudflare/cfssl/releases/download/v1.5.0/cfssljson_1.5.0_linux_amd64 -o /usr/local/bin/cfssljson && \\ chmod +x /usr/local/bin/cfssl && \\ chmod +x /usr/local/bin/cfssljson cat < ./tls/ca-csr.json { \"hosts\": [ \"cluster.local\" ], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"AU\", \"L\": \"Melbourne\", \"O\": \"Example\", \"OU\": \"CA\", \"ST\": \"Example\" } ] } EOF cat < tls/ca-config.json { \"signing\": { \"default\": { \"expiry\": \"175200h\" }, \"profiles\": { \"default\": { \"usages\": [\"signing\", \"key encipherment\", \"server auth\", \"client auth\"], \"expiry\": \"175200h\" } } } } EOF generate ca in /tmp cfssl gencert -initca ./tls/ca-csr.json | cfssljson -bare /tmp/ca generate certificate in /tmp cfssl gencert \\ -ca=/tmp/ca.pem \\ -ca-key=/tmp/ca-key.pem \\ -config=./tls/ca-config.json \\ -hostname=\"example-webhook,example-webhook.default.svc.cluster.local,example-webhook.default.svc,localhost,127.0.0.1\" \\ -profile=default \\ ./tls/ca-csr.json | cfssljson -bare /tmp/example-webhook make a secret cat < ./tls/example-webhook-tls.yaml apiVersion: v1 kind: Secret metadata: name: example-webhook-tls type: Opaque data: tls.crt: $(cat /tmp/example-webhook.pem | base64 | tr -d '\\n') tls.key: $(cat /tmp/example-webhook-key.pem | base64 | tr -d '\\n') EOF generate CA Bundle + inject into template ca_pem_b64=\"$(openssl base64 -A <\"/tmp/ca.pem\")\" sed -e 's@${CA_PEM_B64}@'\"$ca_pem_b64\"'@g' <\"webhook-template.yaml\" \\ > webhook.yaml mkdir -p tls/ca cp /tmp/ca.pem tls/ca/ca.pem cp /tmp/ca-key.pem tls/ca/ca-key.pem mkdir -p tls/webhook cp /tmp/example-webhook.pem tls/webhook/tls.crt cp /tmp/example-webhook-key.pem tls/webhook/tls.key Go coding starting maaeen cd C:\\Users\\VikashKumar\\Desktop\\dev3\\ops\\kubequest\\compose\\04-admission-webhook\\controllers\\src cd /mnt/c/Users/VikashKumar/Desktop/dev3/ops/kubequest/compose/04-admission-webhook/controllers/src docker build . -t webhook docker run -it --rm -p 8081:80 -v ${PWD}:/app webhook sh go mod init example-webhook export CGO_ENABLED=0 go build -o webhook ./webhook docker run -it --rm --net host -v ${HOME}/.kube/:/root/.kube/ -v ${PWD}:/app webhook sh apk add --no-cache curl curl -LO https://storage.googleapis.com/kubernetes-release/release/ curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt /bin/linux/amd64/kubectl chmod +x ./kubectl mv ./kubectl /usr/local/bin/kubectl apk add --no-cache make cat <<'EOF' > Makefile .PHONY: build run docker-build docker-push clean help BINARY_NAME=webhook GO_BUILD_FLAGS=CGO_ENABLED=0 GOOS=linux DOCKER_IMAGE=webhook DOCKER_TAG=latest REGISTRY=your-registry.example.com build: $(GO_BUILD_FLAGS) go build -o $(BINARY_NAME) . run: build ./$(BINARY_NAME) docker-build: docker build . -t $(DOCKER_IMAGE):$(DOCKER_TAG) docker tag $(DOCKER_IMAGE):$(DOCKER_TAG) $(REGISTRY)/$(DOCKER_IMAGE):$(DOCKER_TAG) docker-push: docker-build docker push $(REGISTRY)/$(DOCKER_IMAGE):$(DOCKER_TAG) clean: rm -f $(BINARY_NAME) help: @echo \"make build\" @echo \"make run\" @echo \"make docker-build\" @echo \"make docker-push\" @echo \"make clean\" EOF go get k8s.io/apimachinery@v0.29.0 go get k8s.io/client-go@v0.29.0 go get k8s.io/api@v0.29.0","title":"Commands"},{"location":"04-admission-webhook/commands/#bootstrap-a-kind-cluster","text":"kind create cluster --name webhook --image kindest/node:v1.29.2","title":"Bootstrap a kind cluster"},{"location":"04-admission-webhook/commands/#generating-certificates-and-shit","text":"cd C:\\Users\\VikashKumar\\Desktop\\dev3\\ops\\kubequest\\compose\\04-admission-webhook\\controllers cd /mnt/c/Users/VikashKumar/Desktop/dev3/ops/kubequest/compose/04-admission-webhook/controllers mkdir -p tls docker run -it --rm -v ${PWD}:/work -w /work debian bash apt-get update && apt-get install -y curl && curl -L https://github.com/cloudflare/cfssl/releases/download/v1.5.0/cfssl_1.5.0_linux_amd64 -o /usr/local/bin/cfssl && \\ curl -L https://github.com/cloudflare/cfssl/releases/download/v1.5.0/cfssljson_1.5.0_linux_amd64 -o /usr/local/bin/cfssljson && \\ chmod +x /usr/local/bin/cfssl && \\ chmod +x /usr/local/bin/cfssljson cat < ./tls/ca-csr.json { \"hosts\": [ \"cluster.local\" ], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"AU\", \"L\": \"Melbourne\", \"O\": \"Example\", \"OU\": \"CA\", \"ST\": \"Example\" } ] } EOF cat < tls/ca-config.json { \"signing\": { \"default\": { \"expiry\": \"175200h\" }, \"profiles\": { \"default\": { \"usages\": [\"signing\", \"key encipherment\", \"server auth\", \"client auth\"], \"expiry\": \"175200h\" } } } } EOF","title":"Generating Certificates and Shit!!"},{"location":"04-admission-webhook/commands/#generate-ca-in-tmp","text":"cfssl gencert -initca ./tls/ca-csr.json | cfssljson -bare /tmp/ca","title":"generate ca in /tmp"},{"location":"04-admission-webhook/commands/#generate-certificate-in-tmp","text":"cfssl gencert \\ -ca=/tmp/ca.pem \\ -ca-key=/tmp/ca-key.pem \\ -config=./tls/ca-config.json \\ -hostname=\"example-webhook,example-webhook.default.svc.cluster.local,example-webhook.default.svc,localhost,127.0.0.1\" \\ -profile=default \\ ./tls/ca-csr.json | cfssljson -bare /tmp/example-webhook","title":"generate certificate in /tmp"},{"location":"04-admission-webhook/commands/#make-a-secret","text":"cat < ./tls/example-webhook-tls.yaml apiVersion: v1 kind: Secret metadata: name: example-webhook-tls type: Opaque data: tls.crt: $(cat /tmp/example-webhook.pem | base64 | tr -d '\\n') tls.key: $(cat /tmp/example-webhook-key.pem | base64 | tr -d '\\n') EOF","title":"make a secret"},{"location":"04-admission-webhook/commands/#generate-ca-bundle-inject-into-template","text":"ca_pem_b64=\"$(openssl base64 -A <\"/tmp/ca.pem\")\" sed -e 's@${CA_PEM_B64}@'\"$ca_pem_b64\"'@g' <\"webhook-template.yaml\" \\ > webhook.yaml mkdir -p tls/ca cp /tmp/ca.pem tls/ca/ca.pem cp /tmp/ca-key.pem tls/ca/ca-key.pem mkdir -p tls/webhook cp /tmp/example-webhook.pem tls/webhook/tls.crt cp /tmp/example-webhook-key.pem tls/webhook/tls.key","title":"generate CA Bundle + inject into template"},{"location":"04-admission-webhook/commands/#go-coding-starting-maaeen","text":"cd C:\\Users\\VikashKumar\\Desktop\\dev3\\ops\\kubequest\\compose\\04-admission-webhook\\controllers\\src cd /mnt/c/Users/VikashKumar/Desktop/dev3/ops/kubequest/compose/04-admission-webhook/controllers/src docker build . -t webhook docker run -it --rm -p 8081:80 -v ${PWD}:/app webhook sh go mod init example-webhook export CGO_ENABLED=0 go build -o webhook ./webhook docker run -it --rm --net host -v ${HOME}/.kube/:/root/.kube/ -v ${PWD}:/app webhook sh apk add --no-cache curl curl -LO https://storage.googleapis.com/kubernetes-release/release/ curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt /bin/linux/amd64/kubectl chmod +x ./kubectl mv ./kubectl /usr/local/bin/kubectl apk add --no-cache make cat <<'EOF' > Makefile .PHONY: build run docker-build docker-push clean help BINARY_NAME=webhook GO_BUILD_FLAGS=CGO_ENABLED=0 GOOS=linux DOCKER_IMAGE=webhook DOCKER_TAG=latest REGISTRY=your-registry.example.com build: $(GO_BUILD_FLAGS) go build -o $(BINARY_NAME) . run: build ./$(BINARY_NAME) docker-build: docker build . -t $(DOCKER_IMAGE):$(DOCKER_TAG) docker tag $(DOCKER_IMAGE):$(DOCKER_TAG) $(REGISTRY)/$(DOCKER_IMAGE):$(DOCKER_TAG) docker-push: docker-build docker push $(REGISTRY)/$(DOCKER_IMAGE):$(DOCKER_TAG) clean: rm -f $(BINARY_NAME) help: @echo \"make build\" @echo \"make run\" @echo \"make docker-build\" @echo \"make docker-push\" @echo \"make clean\" EOF go get k8s.io/apimachinery@v0.29.0 go get k8s.io/client-go@v0.29.0 go get k8s.io/api@v0.29.0","title":"Go coding starting maaeen"},{"location":"04-admission-webhook/notes/","text":"* Admission Webhooks (Kubernetes) \u2014 Practical Notes Admission webhooks sit between authentication/authorization and object persistence . They are API-server extensions , not runtime components, and they execute on every matching API request (create, update, delete, connect). 1. Where Admission Webhooks Execute (Critical Mental Model) kubectl / client \u2193 Authentication \u2193 Authorization (RBAC) \u2193 Admission Controllers \u251c\u2500 MutatingAdmissionWebhook \u2514\u2500 ValidatingAdmissionWebhook \u2193 etcd (object persisted) Key implication If an admission webhook fails or times out (depending on failurePolicy), the API request may be rejected even if RBAC allows it . 2. Mutating vs Validating (Operational Difference) MutatingAdmissionWebhook Can modify the incoming object Runs first Typical use: Inject sidecars Add labels/annotations Set defaults not handled by API schema ValidatingAdmissionWebhook Cannot modify the object Runs after mutation Typical use: Enforce policies Block non-compliant resources Golden rule Mutation makes objects convenient . Validation makes clusters safe . 3. What Actually Triggers a Webhook A webhook triggers when ALL of these match: API group (e.g., apps , \"\" for core) API version (e.g., v1 ) Resource (e.g., pods , deployments ) Operation ( CREATE , UPDATE , DELETE , CONNECT ) Namespace selector (optional) Object selector (optional) This is why webhooks can be surgically precise or dangerously broad . 4. Real-World Use Cases (Practical, Not Theoretical) 4.1 Enforce \u201cNo :latest Image\u201d Type : Validating Why : Prevents non-reproducible deployments Trigger: CREATE , UPDATE on pods Logic: Reject if any container image ends with :latest Outcome: kubectl apply fails immediately with a validation error. 4.2 Auto-Inject Sidecar Type : Mutating Why : Avoid developers manually adding boilerplate Trigger: CREATE on pods Condition: Namespace has label mesh=enabled Mutation: Append sidecar container spec Outcome: Pod spec stored in etcd already includes the sidecar. 4.3 Enforce Required Labels Type : Validating Why : Governance, cost allocation, ownership Require labels like: app owner environment Outcome: Objects without labels never reach the cluster state. 4.4 Block Privileged Containers Type : Validating Why : Security hardening Reject if: securityContext.privileged: true hostPath volumes used Outcome: Security enforced before workload starts. 5. Failure Modes You Must Understand failurePolicy Fail API request fails if webhook is unreachable Use for security-critical controls Ignore API request continues if webhook fails Use for non-critical mutation Production guidance Validating webhooks enforcing security should almost always be Fail . timeoutSeconds Default: 10 seconds API server blocks waiting for response Anti-pattern Slow webhook = cluster-wide deployment slowdown. 6. Operational Risks (Exam + Real World) 6.1 Self-Inflicted Cluster Outage Webhook applies to: pods deployments Webhook service is down failurePolicy = Fail Result: No pods can be created, including webhook itself. Mitigation Narrow selectors Use namespace exclusion Careful bootstrapping order 6.2 Infinite Mutation Loops Webhook mutates object Mutation changes something webhook re-triggers on Mitigation Add idempotent markers (e.g., annotation mutated=true ) Exit early if already processed 7. How Admission Webhooks Are Deployed (Practically) Components Webhook server Runs as a Pod Exposes HTTPS endpoint Service Stable DNS for API server TLS Mandatory (API server only talks HTTPS) WebhookConfiguration MutatingWebhookConfiguration or ValidatingWebhookConfiguration 8. AdmissionReview Request / Response (Conceptual) Webhook receives: Full object ( oldObject for updates) User info Namespace Operation Webhook returns: allowed: true | false Optional status message Optional JSON patch (mutation only) This is synchronous and blocking . 9. Admission Webhooks vs Other Controls (Exam-Relevant) Mechanism When it Acts Can Modify Can Block RBAC Before admission No Yes Admission Webhook Before persistence Mutating: Yes Yes Pod Security Admission Admission No Yes OPA / Gatekeeper Admission (via webhook) No Yes Runtime security After pod runs N/A No 10. Key Takeaways to Remember Admission webhooks operate before objects exist They are synchronous and blocking Mutating runs before validating A broken webhook can break the cluster Always scope selectors tightly Prefer validation for safety, mutation for convenience 11. One-Line Exam Summary Admission webhooks intercept API requests after RBAC but before persistence, allowing mutation or rejection of Kubernetes objects based on custom logic. If you want, the next extension can cover: Step-by-step webhook deployment Common CKA traps Admission vs Pod Security Admission Debugging webhook failures in production 12. Minimal, Practical Code Examples (Incremental Add-on) The sections below extend the existing notes by adding working YAML and logic fragments that correspond directly to the use cases already described. Nothing earlier is replaced. 12.1 Validating Webhook \u2014 Block :latest Images Webhook Configuration (Validating) apiVersion : admissionregistration.k8s.io/v1 kind : ValidatingWebhookConfiguration metadata : name : deny-latest-tag webhooks : - name : deny-latest.images.example.com admissionReviewVersions : [ \"v1\" ] sideEffects : None failurePolicy : Fail timeoutSeconds : 5 rules : - apiGroups : [ \"\" ] apiVersions : [ \"v1\" ] operations : [ \"CREATE\" , \"UPDATE\" ] resources : [ \"pods\" ] clientConfig : service : name : image-policy-webhook namespace : admission path : /validate-images caBundle : <BASE64_CA_CERT> Admission Logic (Pseudo-code) for each container in pod.spec.containers: if image ends with \":latest\": deny(\"image tag ':latest' is not allowed\") allow() Practical effect kubectl apply fails immediately if any container uses :latest . 12.2 Mutating Webhook \u2014 Sidecar Injection Webhook Configuration (Mutating) apiVersion : admissionregistration.k8s.io/v1 kind : MutatingWebhookConfiguration metadata : name : inject-sidecar webhooks : - name : sidecar.inject.example.com admissionReviewVersions : [ \"v1\" ] sideEffects : None failurePolicy : Ignore rules : - apiGroups : [ \"\" ] apiVersions : [ \"v1\" ] operations : [ \"CREATE\" ] resources : [ \"pods\" ] namespaceSelector : matchLabels : mesh : enabled clientConfig : service : name : sidecar-webhook namespace : admission path : /mutate-pod caBundle : <BASE64_CA_CERT> JSON Patch Returned by Webhook [ { \"op\" : \"add\" , \"path\" : \"/spec/containers/-\" , \"value\" : { \"name\" : \"mesh-proxy\" , \"image\" : \"envoyproxy/envoy:v1.30.0\" } } ] Practical effect Developers deploy normal pods; sidecars appear automatically in stored objects. 12.3 Validating Webhook \u2014 Required Labels Webhook Rule rules : - apiGroups : [ \"apps\" ] apiVersions : [ \"v1\" ] operations : [ \"CREATE\" , \"UPDATE\" ] resources : [ \"deployments\" ] Validation Logic required = [\"app\", \"owner\", \"environment\"] for key in required: if key not in metadata.labels: deny(\"missing required label: \" + key) allow() Practical effect Governance enforced before workloads exist. 12.4 Validating Webhook \u2014 Block Privileged Pods Validation Logic for container in pod.spec.containers: if container.securityContext.privileged == true: deny(\"privileged containers are not allowed\") for volume in pod.spec.volumes: if volume.hostPath exists: deny(\"hostPath volumes are not allowed\") allow() Practical effect Security policy enforced independently of RBAC. 12.5 Webhook Server (Skeleton) HTTPS Server (Conceptual) POST /validate decode AdmissionReview inspect request.object build AdmissionReview response: allowed: true | false status.message (if denied) return response Key requirement HTTPS only Valid certificate trusted by API server 12.6 Bootstrap-Safe Selector Example (Outage Prevention) namespaceSelector : matchExpressions : - key : kubernetes.io/metadata.name operator : NotIn values : - kube-system - admission Why this matters Prevents the webhook from blocking its own Pods or system components. 12.7 Debugging a Failing Webhook (Operational) kubectl get validatingwebhookconfigurations kubectl describe validatingwebhookconfiguration deny-latest-tag kubectl logs -n admission deploy/image-policy-webhook Typical symptoms: context deadline exceeded no endpoints available for service TLS trust errors 12.8 Exam-Focused Code Recognition Checklist MutatingWebhookConfiguration \u2192 JSON Patch present ValidatingWebhookConfiguration \u2192 allow/deny only failurePolicy: Fail \u2192 cluster safety risk if misused namespaceSelector \u2192 blast-radius control HTTPS + CA bundle mandatory 12.9 One-Line Extension Summary Admission webhooks are implemented as HTTPS services registered via webhook configurations that synchronously mutate or reject Kubernetes API objects before persistence, with selectors and failure policies determining their operational safety.","title":"Notes"},{"location":"04-admission-webhook/notes/#admission-webhooks-kubernetes-practical-notes","text":"Admission webhooks sit between authentication/authorization and object persistence . They are API-server extensions , not runtime components, and they execute on every matching API request (create, update, delete, connect).","title":"Admission Webhooks (Kubernetes) \u2014 Practical Notes"},{"location":"04-admission-webhook/notes/#1-where-admission-webhooks-execute-critical-mental-model","text":"kubectl / client \u2193 Authentication \u2193 Authorization (RBAC) \u2193 Admission Controllers \u251c\u2500 MutatingAdmissionWebhook \u2514\u2500 ValidatingAdmissionWebhook \u2193 etcd (object persisted) Key implication If an admission webhook fails or times out (depending on failurePolicy), the API request may be rejected even if RBAC allows it .","title":"1. Where Admission Webhooks Execute (Critical Mental Model)"},{"location":"04-admission-webhook/notes/#2-mutating-vs-validating-operational-difference","text":"","title":"2. Mutating vs Validating (Operational Difference)"},{"location":"04-admission-webhook/notes/#mutatingadmissionwebhook","text":"Can modify the incoming object Runs first Typical use: Inject sidecars Add labels/annotations Set defaults not handled by API schema","title":"MutatingAdmissionWebhook"},{"location":"04-admission-webhook/notes/#validatingadmissionwebhook","text":"Cannot modify the object Runs after mutation Typical use: Enforce policies Block non-compliant resources Golden rule Mutation makes objects convenient . Validation makes clusters safe .","title":"ValidatingAdmissionWebhook"},{"location":"04-admission-webhook/notes/#3-what-actually-triggers-a-webhook","text":"A webhook triggers when ALL of these match: API group (e.g., apps , \"\" for core) API version (e.g., v1 ) Resource (e.g., pods , deployments ) Operation ( CREATE , UPDATE , DELETE , CONNECT ) Namespace selector (optional) Object selector (optional) This is why webhooks can be surgically precise or dangerously broad .","title":"3. What Actually Triggers a Webhook"},{"location":"04-admission-webhook/notes/#4-real-world-use-cases-practical-not-theoretical","text":"","title":"4. Real-World Use Cases (Practical, Not Theoretical)"},{"location":"04-admission-webhook/notes/#41-enforce-no-latest-image","text":"Type : Validating Why : Prevents non-reproducible deployments Trigger: CREATE , UPDATE on pods Logic: Reject if any container image ends with :latest Outcome: kubectl apply fails immediately with a validation error.","title":"4.1 Enforce \u201cNo :latest Image\u201d"},{"location":"04-admission-webhook/notes/#42-auto-inject-sidecar","text":"Type : Mutating Why : Avoid developers manually adding boilerplate Trigger: CREATE on pods Condition: Namespace has label mesh=enabled Mutation: Append sidecar container spec Outcome: Pod spec stored in etcd already includes the sidecar.","title":"4.2 Auto-Inject Sidecar"},{"location":"04-admission-webhook/notes/#43-enforce-required-labels","text":"Type : Validating Why : Governance, cost allocation, ownership Require labels like: app owner environment Outcome: Objects without labels never reach the cluster state.","title":"4.3 Enforce Required Labels"},{"location":"04-admission-webhook/notes/#44-block-privileged-containers","text":"Type : Validating Why : Security hardening Reject if: securityContext.privileged: true hostPath volumes used Outcome: Security enforced before workload starts.","title":"4.4 Block Privileged Containers"},{"location":"04-admission-webhook/notes/#5-failure-modes-you-must-understand","text":"","title":"5. Failure Modes You Must Understand"},{"location":"04-admission-webhook/notes/#failurepolicy","text":"Fail API request fails if webhook is unreachable Use for security-critical controls Ignore API request continues if webhook fails Use for non-critical mutation Production guidance Validating webhooks enforcing security should almost always be Fail .","title":"failurePolicy"},{"location":"04-admission-webhook/notes/#timeoutseconds","text":"Default: 10 seconds API server blocks waiting for response Anti-pattern Slow webhook = cluster-wide deployment slowdown.","title":"timeoutSeconds"},{"location":"04-admission-webhook/notes/#6-operational-risks-exam-real-world","text":"","title":"6. Operational Risks (Exam + Real World)"},{"location":"04-admission-webhook/notes/#61-self-inflicted-cluster-outage","text":"Webhook applies to: pods deployments Webhook service is down failurePolicy = Fail Result: No pods can be created, including webhook itself. Mitigation Narrow selectors Use namespace exclusion Careful bootstrapping order","title":"6.1 Self-Inflicted Cluster Outage"},{"location":"04-admission-webhook/notes/#62-infinite-mutation-loops","text":"Webhook mutates object Mutation changes something webhook re-triggers on Mitigation Add idempotent markers (e.g., annotation mutated=true ) Exit early if already processed","title":"6.2 Infinite Mutation Loops"},{"location":"04-admission-webhook/notes/#7-how-admission-webhooks-are-deployed-practically","text":"","title":"7. How Admission Webhooks Are Deployed (Practically)"},{"location":"04-admission-webhook/notes/#components","text":"Webhook server Runs as a Pod Exposes HTTPS endpoint Service Stable DNS for API server TLS Mandatory (API server only talks HTTPS) WebhookConfiguration MutatingWebhookConfiguration or ValidatingWebhookConfiguration","title":"Components"},{"location":"04-admission-webhook/notes/#8-admissionreview-request-response-conceptual","text":"Webhook receives: Full object ( oldObject for updates) User info Namespace Operation Webhook returns: allowed: true | false Optional status message Optional JSON patch (mutation only) This is synchronous and blocking .","title":"8. AdmissionReview Request / Response (Conceptual)"},{"location":"04-admission-webhook/notes/#9-admission-webhooks-vs-other-controls-exam-relevant","text":"Mechanism When it Acts Can Modify Can Block RBAC Before admission No Yes Admission Webhook Before persistence Mutating: Yes Yes Pod Security Admission Admission No Yes OPA / Gatekeeper Admission (via webhook) No Yes Runtime security After pod runs N/A No","title":"9. Admission Webhooks vs Other Controls (Exam-Relevant)"},{"location":"04-admission-webhook/notes/#10-key-takeaways-to-remember","text":"Admission webhooks operate before objects exist They are synchronous and blocking Mutating runs before validating A broken webhook can break the cluster Always scope selectors tightly Prefer validation for safety, mutation for convenience","title":"10. Key Takeaways to Remember"},{"location":"04-admission-webhook/notes/#11-one-line-exam-summary","text":"Admission webhooks intercept API requests after RBAC but before persistence, allowing mutation or rejection of Kubernetes objects based on custom logic. If you want, the next extension can cover: Step-by-step webhook deployment Common CKA traps Admission vs Pod Security Admission Debugging webhook failures in production","title":"11. One-Line Exam Summary"},{"location":"04-admission-webhook/notes/#12-minimal-practical-code-examples-incremental-add-on","text":"The sections below extend the existing notes by adding working YAML and logic fragments that correspond directly to the use cases already described. Nothing earlier is replaced.","title":"12. Minimal, Practical Code Examples (Incremental Add-on)"},{"location":"04-admission-webhook/notes/#121-validating-webhook-block-latest-images","text":"","title":"12.1 Validating Webhook \u2014 Block :latest Images"},{"location":"04-admission-webhook/notes/#webhook-configuration-validating","text":"apiVersion : admissionregistration.k8s.io/v1 kind : ValidatingWebhookConfiguration metadata : name : deny-latest-tag webhooks : - name : deny-latest.images.example.com admissionReviewVersions : [ \"v1\" ] sideEffects : None failurePolicy : Fail timeoutSeconds : 5 rules : - apiGroups : [ \"\" ] apiVersions : [ \"v1\" ] operations : [ \"CREATE\" , \"UPDATE\" ] resources : [ \"pods\" ] clientConfig : service : name : image-policy-webhook namespace : admission path : /validate-images caBundle : <BASE64_CA_CERT>","title":"Webhook Configuration (Validating)"},{"location":"04-admission-webhook/notes/#admission-logic-pseudo-code","text":"for each container in pod.spec.containers: if image ends with \":latest\": deny(\"image tag ':latest' is not allowed\") allow() Practical effect kubectl apply fails immediately if any container uses :latest .","title":"Admission Logic (Pseudo-code)"},{"location":"04-admission-webhook/notes/#122-mutating-webhook-sidecar-injection","text":"","title":"12.2 Mutating Webhook \u2014 Sidecar Injection"},{"location":"04-admission-webhook/notes/#webhook-configuration-mutating","text":"apiVersion : admissionregistration.k8s.io/v1 kind : MutatingWebhookConfiguration metadata : name : inject-sidecar webhooks : - name : sidecar.inject.example.com admissionReviewVersions : [ \"v1\" ] sideEffects : None failurePolicy : Ignore rules : - apiGroups : [ \"\" ] apiVersions : [ \"v1\" ] operations : [ \"CREATE\" ] resources : [ \"pods\" ] namespaceSelector : matchLabels : mesh : enabled clientConfig : service : name : sidecar-webhook namespace : admission path : /mutate-pod caBundle : <BASE64_CA_CERT>","title":"Webhook Configuration (Mutating)"},{"location":"04-admission-webhook/notes/#json-patch-returned-by-webhook","text":"[ { \"op\" : \"add\" , \"path\" : \"/spec/containers/-\" , \"value\" : { \"name\" : \"mesh-proxy\" , \"image\" : \"envoyproxy/envoy:v1.30.0\" } } ] Practical effect Developers deploy normal pods; sidecars appear automatically in stored objects.","title":"JSON Patch Returned by Webhook"},{"location":"04-admission-webhook/notes/#123-validating-webhook-required-labels","text":"","title":"12.3 Validating Webhook \u2014 Required Labels"},{"location":"04-admission-webhook/notes/#webhook-rule","text":"rules : - apiGroups : [ \"apps\" ] apiVersions : [ \"v1\" ] operations : [ \"CREATE\" , \"UPDATE\" ] resources : [ \"deployments\" ]","title":"Webhook Rule"},{"location":"04-admission-webhook/notes/#validation-logic","text":"required = [\"app\", \"owner\", \"environment\"] for key in required: if key not in metadata.labels: deny(\"missing required label: \" + key) allow() Practical effect Governance enforced before workloads exist.","title":"Validation Logic"},{"location":"04-admission-webhook/notes/#124-validating-webhook-block-privileged-pods","text":"","title":"12.4 Validating Webhook \u2014 Block Privileged Pods"},{"location":"04-admission-webhook/notes/#validation-logic_1","text":"for container in pod.spec.containers: if container.securityContext.privileged == true: deny(\"privileged containers are not allowed\") for volume in pod.spec.volumes: if volume.hostPath exists: deny(\"hostPath volumes are not allowed\") allow() Practical effect Security policy enforced independently of RBAC.","title":"Validation Logic"},{"location":"04-admission-webhook/notes/#125-webhook-server-skeleton","text":"","title":"12.5 Webhook Server (Skeleton)"},{"location":"04-admission-webhook/notes/#https-server-conceptual","text":"POST /validate decode AdmissionReview inspect request.object build AdmissionReview response: allowed: true | false status.message (if denied) return response Key requirement HTTPS only Valid certificate trusted by API server","title":"HTTPS Server (Conceptual)"},{"location":"04-admission-webhook/notes/#126-bootstrap-safe-selector-example-outage-prevention","text":"namespaceSelector : matchExpressions : - key : kubernetes.io/metadata.name operator : NotIn values : - kube-system - admission Why this matters Prevents the webhook from blocking its own Pods or system components.","title":"12.6 Bootstrap-Safe Selector Example (Outage Prevention)"},{"location":"04-admission-webhook/notes/#127-debugging-a-failing-webhook-operational","text":"kubectl get validatingwebhookconfigurations kubectl describe validatingwebhookconfiguration deny-latest-tag kubectl logs -n admission deploy/image-policy-webhook Typical symptoms: context deadline exceeded no endpoints available for service TLS trust errors","title":"12.7 Debugging a Failing Webhook (Operational)"},{"location":"04-admission-webhook/notes/#128-exam-focused-code-recognition-checklist","text":"MutatingWebhookConfiguration \u2192 JSON Patch present ValidatingWebhookConfiguration \u2192 allow/deny only failurePolicy: Fail \u2192 cluster safety risk if misused namespaceSelector \u2192 blast-radius control HTTPS + CA bundle mandatory","title":"12.8 Exam-Focused Code Recognition Checklist"},{"location":"04-admission-webhook/notes/#129-one-line-extension-summary","text":"Admission webhooks are implemented as HTTPS services registered via webhook configurations that synchronously mutate or reject Kubernetes API objects before persistence, with selectors and failure policies determining their operational safety.","title":"12.9 One-Line Extension Summary"},{"location":"05-kubeconfig/how-to-setup/","text":"kubectl config set-cluster kubernetes --server=https://172.30.1.2:6443 --certificate-authority=/etc/kubernetes/pki/ca.crt kubectl config set-credentials kubelet --token=m3qmx4.qa9c83ju82ru6njq kubectl config set-context default --cluster=kubernetes --user=kubelet kubectl config use-context default","title":"How to setup"},{"location":"05-kubeconfig/notes/","text":"~/.kube 14:23:22 \u276f kind get cluster Gets one of [clusters, nodes, kubeconfig] Usage: kind get [flags] kind get [command] Available Commands: clusters Lists existing kind clusters by their name kubeconfig Prints cluster kubeconfig nodes Lists existing kind nodes by their name Flags: -h, --help help for get Global Flags: -q, --quiet silence all stderr output -v, --verbosity int32 info log verbosity, higher value produces more output Use \"kind get [command] --help\" for more information about a command. ERROR: Subcommand is required ~/.kube 14:24:45 \u276f kind get clusters gatewayapi ~/.kube 14:24:53 \u276f # Get kubeconfig for gatewayapi cluster kind get kubeconfig --name=gatewayapi > /tmp/gatewayapi-kubeconfig.yaml Show the structure cat /tmp/gatewayapi-kubeconfig.yaml zsh: command not found: # zsh: command not found: # apiVersion: v1 clusters: - cluster: certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURCVENDQWUyZ0F3SUJBZ0lJS0RYandhOW1KNll3RFFZSktvWklodmNOQVFFTEJRQXdGVEVUTUJFR0ExVUUKQXhNS2EzVmlaWEp1WlhSbGN6QWVGdzB5TmpBeE1ERXdOekE0TlRoYUZ3MHpOVEV5TXpBd056RXpOVGhhTUJVeApFekFSQmdOVkJBTVRDbXQxWW1WeWJtVjBaWE13Z2dFaU1BMEdDU3FHU0liM0RRRUJBUVVBQTRJQkR3QXdnZ0VLCkFvSUJBUUM1OHpsZnJGRUJRZHZuRzJ6MzBvQ09zODJnTldnUlNsWW5ZNGgwUkVtSE5DOVVYWmFNUm1hVHFCQ1kKT2pDbW9nMVhaYlFQcTE5MHFGTkV1ZDk2MDRjS3FhTFhHbGhuR20wZEQ1dWFtN3NsQngrQURmMzErbEhXYTNsSgpGU3A1WFlIMGxpb3dWMHpQVmN5RlpDZjk0UTIxejVMS1lvRmdXWHdTMG5oZUZ5WWZPdjFXWUV3cVExZVFlS0RGCjljbFNiQ0VpbEtUbDgzZng1MXZ1YThLcGExTmt3TG1tdzFBSHZkZXRBUnZBUDk2Zm5zTm94SFY0YVM3YVNTM1kKQkJUeC9SWExPaFVJRXFjbFZSSzNyeEF4T2NUYnVqaCtZRzh3Nmwrc3VBY296QmtNZmNzcGY4UjdxMm4wTkdkcAp4eGYvZG9WeUQxK200N3dKVzIyTDcxalN6OXNUQWdNQkFBR2pXVEJYTUE0R0ExVWREd0VCL3dRRUF3SUNwREFQCkJnTlZIUk1CQWY4RUJUQURBUUgvTUIwR0ExVWREZ1FXQkJTRXByTko1dmVOQU1kb1BWUlJ4cjJvUUpqUytqQVYKQmdOVkhSRUVEakFNZ2dwcmRXSmxjbTVsZEdWek1BMEdDU3FHU0liM0RRRUJDd1VBQTRJQkFRQmM5RnpOSEhTaAoyZnhJZ0l0azZEUVZRc3BpN0ZZcURKd3pKdFczZmx6ZHBnckF0STM4VTdmc0NhcnpFaFpQVmY3ODR2OTdFWHBDCmZabHRGVXlXL2xvWVlDZ1hnSGo1Sjh6MjNQSGxVVklJOUU0c1BYM1dKWlBjWmtsSWpyNFdDOFY1dzNpSms2aHoKZS9xdDhpb291QkFGNVlHM24ycTlSSUFuN0h3N3M0UCs5ZXJMOGlKbC80T0VOQWpNemRYMzJvZmJUenNzQlcyVwppelU4YWpVVnE0RkVRZW9zT3JmcWQvNURwcEJqb3lpZlJ1SXNLaDJNdG1vcFBtV3VLcTVLemI3M3BNcTJSeE5OClVWRDdXZWFzdTAxWVQrdEZTMlRqTFZyNGZxL1dYRjhDM0JUUHltZ3lUQWVxcXk4SzhVQlVFTnVBMG1FS1VONEcKcnVqbFZ1dEM1M0VXCi0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K server: https://127.0.0.1:39619 name: kind-gatewayapi contexts: - context: cluster: kind-gatewayapi user: kind-gatewayapi name: kind-gatewayapi current-context: kind-gatewayapi kind: Config users: - name: kind-gatewayapi user: client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURLVENDQWhHZ0F3SUJBZ0lJTGh0ajRISmR5Nkl3RFFZSktvWklodmNOQVFFTEJRQXdGVEVUTUJFR0ExVUUKQXhNS2EzVmlaWEp1WlhSbGN6QWVGdzB5TmpBeE1ERXdOekE0TlRoYUZ3MHlOekF4TURFd056RXpOVGhhTUR3eApIekFkQmdOVkJBb1RGbXQxWW1WaFpHMDZZMngxYzNSbGNpMWhaRzFwYm5NeEdUQVhCZ05WQkFNVEVHdDFZbVZ5CmJtVjBaWE10WVdSdGFXNHdnZ0VpTUEwR0NTcUdTSWIzRFFFQkFRVUFBNElCRHdBd2dnRUtBb0lCQVFDcWtwcEMKbndWQnNmMjFId2dGQ1Rod0dQR2RpcUtSVk42RVF3Z0Z3WnkzWjlNb0llRWZNa1MvYjlQOGxEY0d2Vk0vR3d3Rwp5RG4rcFJMK3ZYT2JObVNnblMveHNRK3NVamZnTERLb2V3ZHFBYXJCUG0vaHIxTXpPSXErdUdvMWRBeHZpcHNvCjNPQjc0Y0VsQmIxd2pxb3U1akwxRzNjNW9pbnRUSmFjcjJPcjZ3Z2JSaUsxaEF0ZjlPL2tFbndNRkQvdnV2NmMKSXZwVVB0aUtwUUJGZnhLRWdQYjZYOVZhc2J5d1Jab3JsS2pvSWlpVTNZM3ZBQ1FqSlExYVlEUm1JYW12bVNURQoyKzl4RDUweGY5MDNRSnp5UG45Umx0RzhPYnlHRTkvVW5IL1g2VER3UGRydFpwMjlXWHAxZDIrd3VyMld4a01PCnREZDZUbGNuV01kMStKL3JBZ01CQUFHalZqQlVNQTRHQTFVZER3RUIvd1FFQXdJRm9EQVRCZ05WSFNVRUREQUsKQmdnckJnRUZCUWNEQWpBTUJnTlZIUk1CQWY4RUFqQUFNQjhHQTFVZEl3UVlNQmFBRklTbXMwbm05NDBBeDJnOQpWRkhHdmFoQW1OTDZNQTBHQ1NxR1NJYjNEUUVCQ3dVQUE0SUJBUUJlUXk2N3YvMVdFUVB5bXV6L2V4NHlNdHgwCjY3S3BWQzkxUVZ1aEFjNUpZMWwwSnp3bWxjRzJrUk1lK2VFM3Erd1pEMTRmYVhKcjdrbGFjQkcxMXRsZUZKRXoKelZCVVArU3Jpbk9yN05wS3VlMGdmZ2FzTTJ4UkFRVWkyOXQvem5FTjZ3a1BzRzkvbUFBSGZZWTAzVjlxSmovMwp2N3NFcElrR3lxNzMzek5qRC81bHEvTFgvUWppeHc4MzE1b0h1WHMzekhmM3pONGkrZFhRQnAxTlk0TGVCQzFYCnVUa3lhbXJRR3ZQRTBsT2llTUlLQ3pkQ1k0cXlpb04yL2Jqd21KZStXYTBpNEorQ3VKQ0MyWjVKYVJpVUM1QUQKdW94Z1lYSXFSazlkekZ2SkU2WnRvWG9VMDVlS0oyNHFBaEd4UEViV2ptMDZJVEtwc3J3MCs0aUVLY09mCi0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K client-key-data: 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 ~/.kube 14:25:21 \u276f sed -n '/certificate-authority-data:/,/^[[:space:]]*server:/p' /tmp/gatewayapi-kubeconfig.yaml | \\ grep 'certificate-authority-data:' | \\ cut -d':' -f2- | \\ tr -d ' ' | \\ base64 -d > /tmp/decoded-ca.crt ~/.kube 14:28:18 \u276f cat /tmp/decoded-ca.crt -----BEGIN CERTIFICATE----- MIIDBTCCAe2gAwIBAgIIKDXjwa9mJ6YwDQYJKoZIhvcNAQELBQAwFTETMBEGA1UE AxMKa3ViZXJuZXRlczAeFw0yNjAxMDEwNzA4NThaFw0zNTEyMzAwNzEzNThaMBUx EzARBgNVBAMTCmt1YmVybmV0ZXMwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEK AoIBAQC58zlfrFEBQdvnG2z30oCOs82gNWgRSlYnY4h0REmHNC9UXZaMRmaTqBCY OjCmog1XZbQPq190qFNEud9604cKqaLXGlhnGm0dD5uam7slBx+ADf31+lHWa3lJ FSp5XYH0liowV0zPVcyFZCf94Q21z5LKYoFgWXwS0nheFyYfOv1WYEwqQ1eQeKDF 9clSbCEilKTl83fx51vua8Kpa1NkwLmmw1AHvdetARvAP96fnsNoxHV4aS7aSS3Y BBTx/RXLOhUIEqclVRK3rxAxOcTbujh+YG8w6l+suAcozBkMfcspf8R7q2n0NGdp xxf/doVyD1+m47wJW22L71jSz9sTAgMBAAGjWTBXMA4GA1UdDwEB/wQEAwICpDAP BgNVHRMBAf8EBTADAQH/MB0GA1UdDgQWBBSEprNJ5veNAMdoPVRRxr2oQJjS+jAV BgNVHREEDjAMggprdWJlcm5ldGVzMA0GCSqGSIb3DQEBCwUAA4IBAQBc9FzNHHSh 2fxIgItk6DQVQspi7FYqDJwzJtW3flzdpgrAtI38U7fsCarzEhZPVf784v97EXpC fZltFUyW/loYYCgXgHj5J8z23PHlUVII9E4sPX3WJZPcZklIjr4WC8V5w3iJk6hz e/qt8ioouBAF5YG3n2q9RIAn7Hw7s4P+9erL8iJl/4OENAjMzdX32ofbTzssBW2W izU8ajUVq4FEQeosOrfqd/5DppBjoyifRuIsKh2MtmopPmWuKq5Kzb73pMq2RxNN UVD7Weasu01YT+tFS2TjLVr4fq/WXF8C3BTPymgyTAeqqy8K8UBUENuA0mEKUN4G rujlVutC53EW -----END CERTIFICATE----- ~/.kube 14:28:22 \u276f openssl x509 -in /tmp/decoded-ca.crt -text -noout | head -30 Certificate: Data: Version: 3 (0x2) Serial Number: 2897472356293683110 (0x2835e3c1af6627a6) Signature Algorithm: sha256WithRSAEncryption Issuer: CN = kubernetes Validity Not Before: Jan 1 07:08:58 2026 GMT Not After : Dec 30 07:13:58 2035 GMT Subject: CN = kubernetes Subject Public Key Info: Public Key Algorithm: rsaEncryption Public-Key: (2048 bit) Modulus: 00:b9:f3:39:5f 51:01:41:db:e7:1b:6c:f7:d2: 80:8e:b3 a0:35:68:11:4a:56:27:63:88:74:44: 49:87:34:2f:54:5d:96:8c:46:66:93:a8:10:98:3a: 30:a6:a2:0d:57:65:b4:0f:ab:5f:74:a8:53:44:b9: df:7a:d3:87:0a:a9:a2:d7:1a:58:67:1a:6d:1d:0f: 9b:9a:9b 25:07:1f:80:0d:fd:f5:fa:51:d6:6b: 79:49:15:2a:79:5d:81:f4:96:2a:30:57:4c 55: cc:85:64:27:fd:e1:0d:b5:cf:92:ca:62:81:60:59: 7c:12:d2:78:5e:17:26:1f:3a:fd:56:60:4c:2a:43: 57:90:78:a0:c5:f5:c9:52:6c:21:22:94:a4:e5:f3: 77:f1:e7:5b:ee:6b:c2:a9:6b:53:64:c0:b9:a6:c3: 50:07:bd:d7:ad:01:1b:c0:3f 9f:9e:c3:68:c4: 75:78:69:2e:da:49:2d:d8:04:14:f1:fd:15:cb:3a: 15:08:12:a7:25:55:12:b7:af:10:31:39:c4:db:ba: 38:7e:60:6f:30 5f b8:07:28 19:0c:7d: cb:29:7f:c4:7b 69:f4:34:67:69:c7:17:ff:76: ~/.kube 14:28:36 \u276f awk '/client-certificate-data:/{flag=1} flag && /^[[:space:]] client-key-data:/{flag=0} flag' /tmp/gatewayapi-kubeconfig.yaml | \\ grep 'client-certificate-data:' | \\ cut -d':' -f2- | \\ sed 's/^[[:space:]] //' | \\ base64 -d > /tmp/decoded-client.crt ~/.kube 14:28:57 \u276f openssl x509 -in /tmp/decoded-client.crt -text -noout | head -30 Certificate: Data: Version: 3 (0x2) Serial Number: 3322358965758446498 (0x2e1b63e0725dcba2) Signature Algorithm: sha256WithRSAEncryption Issuer: CN = kubernetes Validity Not Before: Jan 1 07:08:58 2026 GMT Not After : Jan 1 07:13:58 2027 GMT Subject: O = kubeadm:cluster-admins, CN = kubernetes-admin Subject Public Key Info: Public Key Algorithm: rsaEncryption Public-Key: (2048 bit) Modulus: 00:aa:92:9a:42:9f:05:41:b1:fd:b5:1f:08:05:09: 38:70:18:f1:9d:8a:a2:91:54 84:43:08:05:c1: 9c:b7:67:d3:28:21:e1:1f:32:44:bf:6f:d3:fc:94: 37:06:bd:53:3f:1b:0c:06:c8:39:fe:a5:12:fe:bd: 73:9b:36:64:a0:9d:2f:f1:b1:0f:ac:52:37:e0:2c: 32:a8:7b:07:6a:01:aa:c1:3e:6f:e1 53:33:38: 8a b8:6a:35:74:0c:6f:8a:9b:28:dc:e0:7b:e1: c1:25:05 70:8e:aa:2e:e6:32:f5:1b:77:39:a2: 29:ed:4c:96:9c 63 eb:08:1b:46:22:b5:84: 0b:5f:f4:ef:e4:12:7c:0c:14:3f:ef fe:9c:22: fa:54:3e:d8:8a:a5:00:45:7f:12:84:80:f6:fa:5f: d5:5a:b1:bc:b0:45:9a:2b:94:a8:e8:22:28:94:dd: 8d:ef:00:24:23:25:0d:5a:60:34:66:21:a9 99: 24:c4:db:ef:71:0f:9d:31:7f:dd:37:40:9c:f2:3e: 7f:51:96:d1:bc:39:bc:86:13:df:d4:9c:7f:d7:e9: 30:f0:3d:da:ed:66:9d 59:7a:75:77:6f:b0:ba: ~/.kube 14:29:02 \u276f awk '/client-key-data:/{flag=1} flag && /^[[:space:]] token:/{flag=0} flag' /tmp/gatewayapi-kubeconfig.yaml | \\ grep 'client-key-data:' | \\ cut -d':' -f2- | \\ sed 's/^[[:space:]] //' | \\ base64 -d > /tmp/decoded-client.key ~/.kube 14:29:18 \u276f file /tmp/decoded-client.key /tmp/decoded-client.key: PEM RSA private key ~/.kube 14:29:22 \u276f # Using yq (if installed) cat /tmp/gatewayapi-kubeconfig.yaml | yq '.clusters[0].cluster.certificate-authority-data' | base64 -d > /tmp/ca.crt cat /tmp/gatewayapi-kubeconfig.yaml | yq '.users[0].user.client-certificate-data' | base64 -d > /tmp/client.crt cat /tmp/gatewayapi-kubeconfig.yaml | yq '.users[0].user.client-key-data' | base64 -d > /tmp/client.key Using grep/sed only cat /tmp/gatewayapi-kubeconfig.yaml | grep -A1 -B1 'certificate-authority-data:' | tail -1 | base64 -d > /tmp/ca2.crt zsh: unknown file attribute: i jq: error: authority/0 is not defined at , line 1: .clusters[0].cluster.certificate-authority-data jq: error: data/0 is not defined at , line 1: .clusters[0].cluster.certificate-authority-data jq: 2 compile errors jq: error: certificate/0 is not defined at , line 1: .users[0].user.client-certificate-data jq: error: data/0 is not defined at , line 1: .users[0].user.client-certificate-data jq: 2 compile errors jq: error: key/0 is not defined at , line 1: .users[0].user.client-key-data jq: error: data/0 is not defined at , line 1: .users[0].user.client-key-data jq: 2 compile errors zsh: command not found: # base64: invalid input ~/.kube 14:29:36 \u276f","title":"Notes"},{"location":"05-kubeconfig/notes/#show-the-structure","text":"cat /tmp/gatewayapi-kubeconfig.yaml zsh: command not found: # zsh: command not found: # apiVersion: v1 clusters: - cluster: certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURCVENDQWUyZ0F3SUJBZ0lJS0RYandhOW1KNll3RFFZSktvWklodmNOQVFFTEJRQXdGVEVUTUJFR0ExVUUKQXhNS2EzVmlaWEp1WlhSbGN6QWVGdzB5TmpBeE1ERXdOekE0TlRoYUZ3MHpOVEV5TXpBd056RXpOVGhhTUJVeApFekFSQmdOVkJBTVRDbXQxWW1WeWJtVjBaWE13Z2dFaU1BMEdDU3FHU0liM0RRRUJBUVVBQTRJQkR3QXdnZ0VLCkFvSUJBUUM1OHpsZnJGRUJRZHZuRzJ6MzBvQ09zODJnTldnUlNsWW5ZNGgwUkVtSE5DOVVYWmFNUm1hVHFCQ1kKT2pDbW9nMVhaYlFQcTE5MHFGTkV1ZDk2MDRjS3FhTFhHbGhuR20wZEQ1dWFtN3NsQngrQURmMzErbEhXYTNsSgpGU3A1WFlIMGxpb3dWMHpQVmN5RlpDZjk0UTIxejVMS1lvRmdXWHdTMG5oZUZ5WWZPdjFXWUV3cVExZVFlS0RGCjljbFNiQ0VpbEtUbDgzZng1MXZ1YThLcGExTmt3TG1tdzFBSHZkZXRBUnZBUDk2Zm5zTm94SFY0YVM3YVNTM1kKQkJUeC9SWExPaFVJRXFjbFZSSzNyeEF4T2NUYnVqaCtZRzh3Nmwrc3VBY296QmtNZmNzcGY4UjdxMm4wTkdkcAp4eGYvZG9WeUQxK200N3dKVzIyTDcxalN6OXNUQWdNQkFBR2pXVEJYTUE0R0ExVWREd0VCL3dRRUF3SUNwREFQCkJnTlZIUk1CQWY4RUJUQURBUUgvTUIwR0ExVWREZ1FXQkJTRXByTko1dmVOQU1kb1BWUlJ4cjJvUUpqUytqQVYKQmdOVkhSRUVEakFNZ2dwcmRXSmxjbTVsZEdWek1BMEdDU3FHU0liM0RRRUJDd1VBQTRJQkFRQmM5RnpOSEhTaAoyZnhJZ0l0azZEUVZRc3BpN0ZZcURKd3pKdFczZmx6ZHBnckF0STM4VTdmc0NhcnpFaFpQVmY3ODR2OTdFWHBDCmZabHRGVXlXL2xvWVlDZ1hnSGo1Sjh6MjNQSGxVVklJOUU0c1BYM1dKWlBjWmtsSWpyNFdDOFY1dzNpSms2aHoKZS9xdDhpb291QkFGNVlHM24ycTlSSUFuN0h3N3M0UCs5ZXJMOGlKbC80T0VOQWpNemRYMzJvZmJUenNzQlcyVwppelU4YWpVVnE0RkVRZW9zT3JmcWQvNURwcEJqb3lpZlJ1SXNLaDJNdG1vcFBtV3VLcTVLemI3M3BNcTJSeE5OClVWRDdXZWFzdTAxWVQrdEZTMlRqTFZyNGZxL1dYRjhDM0JUUHltZ3lUQWVxcXk4SzhVQlVFTnVBMG1FS1VONEcKcnVqbFZ1dEM1M0VXCi0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K server: https://127.0.0.1:39619 name: kind-gatewayapi contexts: - context: cluster: kind-gatewayapi user: kind-gatewayapi name: kind-gatewayapi current-context: kind-gatewayapi kind: Config users: - name: kind-gatewayapi user: client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURLVENDQWhHZ0F3SUJBZ0lJTGh0ajRISmR5Nkl3RFFZSktvWklodmNOQVFFTEJRQXdGVEVUTUJFR0ExVUUKQXhNS2EzVmlaWEp1WlhSbGN6QWVGdzB5TmpBeE1ERXdOekE0TlRoYUZ3MHlOekF4TURFd056RXpOVGhhTUR3eApIekFkQmdOVkJBb1RGbXQxWW1WaFpHMDZZMngxYzNSbGNpMWhaRzFwYm5NeEdUQVhCZ05WQkFNVEVHdDFZbVZ5CmJtVjBaWE10WVdSdGFXNHdnZ0VpTUEwR0NTcUdTSWIzRFFFQkFRVUFBNElCRHdBd2dnRUtBb0lCQVFDcWtwcEMKbndWQnNmMjFId2dGQ1Rod0dQR2RpcUtSVk42RVF3Z0Z3WnkzWjlNb0llRWZNa1MvYjlQOGxEY0d2Vk0vR3d3Rwp5RG4rcFJMK3ZYT2JObVNnblMveHNRK3NVamZnTERLb2V3ZHFBYXJCUG0vaHIxTXpPSXErdUdvMWRBeHZpcHNvCjNPQjc0Y0VsQmIxd2pxb3U1akwxRzNjNW9pbnRUSmFjcjJPcjZ3Z2JSaUsxaEF0ZjlPL2tFbndNRkQvdnV2NmMKSXZwVVB0aUtwUUJGZnhLRWdQYjZYOVZhc2J5d1Jab3JsS2pvSWlpVTNZM3ZBQ1FqSlExYVlEUm1JYW12bVNURQoyKzl4RDUweGY5MDNRSnp5UG45Umx0RzhPYnlHRTkvVW5IL1g2VER3UGRydFpwMjlXWHAxZDIrd3VyMld4a01PCnREZDZUbGNuV01kMStKL3JBZ01CQUFHalZqQlVNQTRHQTFVZER3RUIvd1FFQXdJRm9EQVRCZ05WSFNVRUREQUsKQmdnckJnRUZCUWNEQWpBTUJnTlZIUk1CQWY4RUFqQUFNQjhHQTFVZEl3UVlNQmFBRklTbXMwbm05NDBBeDJnOQpWRkhHdmFoQW1OTDZNQTBHQ1NxR1NJYjNEUUVCQ3dVQUE0SUJBUUJlUXk2N3YvMVdFUVB5bXV6L2V4NHlNdHgwCjY3S3BWQzkxUVZ1aEFjNUpZMWwwSnp3bWxjRzJrUk1lK2VFM3Erd1pEMTRmYVhKcjdrbGFjQkcxMXRsZUZKRXoKelZCVVArU3Jpbk9yN05wS3VlMGdmZ2FzTTJ4UkFRVWkyOXQvem5FTjZ3a1BzRzkvbUFBSGZZWTAzVjlxSmovMwp2N3NFcElrR3lxNzMzek5qRC81bHEvTFgvUWppeHc4MzE1b0h1WHMzekhmM3pONGkrZFhRQnAxTlk0TGVCQzFYCnVUa3lhbXJRR3ZQRTBsT2llTUlLQ3pkQ1k0cXlpb04yL2Jqd21KZStXYTBpNEorQ3VKQ0MyWjVKYVJpVUM1QUQKdW94Z1lYSXFSazlkekZ2SkU2WnRvWG9VMDVlS0oyNHFBaEd4UEViV2ptMDZJVEtwc3J3MCs0aUVLY09mCi0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K client-key-data: LS0tLS1CRUdJTiBSU0EgUFJJVkFURSBLRVktLS0tLQpNSUlFcEFJQkFBS0NBUUVBcXBLYVFwOEZRYkg5dFI4SUJRazRjQmp4bllxaWtWVGVoRU1JQmNHY3QyZlRLQ0hoCkh6SkV2Mi9UL0pRM0JyMVRQeHNNQnNnNS9xVVMvcjF6bXpaa29KMHY4YkVQckZJMzRDd3lxSHNIYWdHcXdUNXYKNGE5VE16aUt2cmhxTlhRTWI0cWJLTnpnZStIQkpRVzljSTZxTHVZeTlSdDNPYUlwN1V5V25LOWpxK3NJRzBZaQp0WVFMWC9UdjVCSjhEQlEvNzdyK25DTDZWRDdZaXFVQVJYOFNoSUQyK2wvVldyRzhzRVdhSzVTbzZDSW9sTjJOCjd3QWtJeVVOV21BMFppR3ByNWtreE52dmNRK2RNWC9kTjBDYzhqNS9VWmJSdkRtOGhoUGYxSngvMStrdzhEM2EKN1dhZHZWbDZkWGR2c0xxOWxzWkREclEzZWs1WEoxakhkZmlmNndJREFRQUJBb0lCQUJJcUpPY0ZycFoxbG4rVQpkajJzWXQxcExVaC9nWVY1ajIxd0FxbUk3MHF4Y2Z3Rm5rYm5ZRjZFcHdOd1kwQnRVVndETjRUU0MzOEhPWjUxCmlCdFdMN2JTVm5ubUhHQlhySFNoTUU4MGlZWEJUTEVNbUV5UDU3WitNSG1wV096U3JCcGF4K0E4K1dPbjdISW8KQ2ltemxRTUQ1OEdtSGFHK2JRN3Zjalh0dXU4TlNXWFJRKzhtMHVYN2k1dGRUa3BXUkNKdEVYZzY5bmtLazF4LwpscHVHNS9RT2J3cDdrSVBYS0tuZTdYL1hRTnB6TCtSaHh3UVFxVllDUzhUMzZMVkVnMVZIbGV6OEpSQlVGNENvClRDUUM4SWgvVWY0TTBTWHd2Z1RJRXAyQlNWVThESWQzUHFhdk1RU2E2UmxZcnFmUHNFcXgwZ0tLSFhGM2ZjTDcKSXZadVFNRUNnWUVBM0cvQ21qZlJBY0lCVGJwYklYcXo2MEpjVEpZdDVDaEF3cEtYVjN5T3NmNGxDNG15MG1hcApNT2t5MWxkVXRMWnBaWTN2ejVqSE5rOUYyY3BiRjhRbHNyb3dPUzFjZ05XM1JVVExBaWZHRGpmak9uOTFiNHk2CjZ3T1hZSFlyRWszVXVrcm1tcTlRNlI1ek10bTNlNlNlRmlQSmovTzQzVUx1R2Z3MXNyanVmS3NDZ1lFQXhoZHEKZ0d5Z2wyellOSTc3ZWVmOVJRNTB2UGUwRFBKUGZWLzlXMldZWG1rMVQwNWVscWhEWXowRnhEWGo4aG9pZG5kTQpzZW42UUdueUdaMFQ3S0Uwc1pLWjlmaEZWWlhrYnM5dncxS3BsSlpCaG9uek1nVkVqeEw4ZGlwVVpTQmpuZHRjCmZmS3YvcncxdnpJdzZ0RDJ6Vk4vc21iQTFkT3hWTlFaQTZZZjZjRUNnWUVBZ0tBUi9HenZYMGcxL0lYdUlSWDUKSUNDanZPaXd0SDRzYzV5WUJLdWdsQW5JMGZleVNZVXYybU5vajV0N3lNcmJxeTlzTEVWb2tLOG5BaE5Lbmc2TgpOTUhoMjZzMVc5UFkwZWwzVDdXbm9xcEh3OTJWeDlabFJ6YmNRS1FUTStZSVovL0dtYUlNNDBvcVRCU3dOTXgwCmxsU2hpNGJhYXZsZjkvZXIyYktCTG1zQ2dZRUFsSWJzS1B6SjhLQUJBRytRNlJma0ZBcEJ4NHBtNnlvb0pkWjYKVGpRLzZkSXkwWkx1WTBJb3ZOajlZT0FUV096MW1DUGRVcTBnSVhvT3Q5dktHNnZIcWJsRlRXTnBBVUlSZEhCKwoyVkk2cXBsNjZoaTNTM01kczdWRnJJZ1NuWHlLbE1yc2I5Y3UxTzVqMGtjYzNJUHYrWVk1QWhmL1VKU1lxd1VZCitGNXdJVUVDZ1lCdE9FNGhzQXJteDgrbFRHQUJKMHRSdjZkYzE5WFJlUFFYY2NUV3gyQWt1alM5b1dmSHduUmIKWHVZQkI3Yi9TV05jcVJiNTZjMTZRZWYrZzFPb1RObGpaeHZmblg2WUZoUnNOeXNYeUtxUnRZdHZUcXhUM2VjbQpYdTlKWXVpUmx1enBJSUc5S1QrVS95VU1qcjFVa0x3Nmw5dU9mcGtSVko1elF2VVFSeXdvcXc9PQotLS0tLUVORCBSU0EgUFJJVkFURSBLRVktLS0tLQo= ~/.kube 14:25:21 \u276f sed -n '/certificate-authority-data:/,/^[[:space:]]*server:/p' /tmp/gatewayapi-kubeconfig.yaml | \\ grep 'certificate-authority-data:' | \\ cut -d':' -f2- | \\ tr -d ' ' | \\ base64 -d > /tmp/decoded-ca.crt ~/.kube 14:28:18 \u276f cat /tmp/decoded-ca.crt -----BEGIN CERTIFICATE----- MIIDBTCCAe2gAwIBAgIIKDXjwa9mJ6YwDQYJKoZIhvcNAQELBQAwFTETMBEGA1UE AxMKa3ViZXJuZXRlczAeFw0yNjAxMDEwNzA4NThaFw0zNTEyMzAwNzEzNThaMBUx EzARBgNVBAMTCmt1YmVybmV0ZXMwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEK AoIBAQC58zlfrFEBQdvnG2z30oCOs82gNWgRSlYnY4h0REmHNC9UXZaMRmaTqBCY OjCmog1XZbQPq190qFNEud9604cKqaLXGlhnGm0dD5uam7slBx+ADf31+lHWa3lJ FSp5XYH0liowV0zPVcyFZCf94Q21z5LKYoFgWXwS0nheFyYfOv1WYEwqQ1eQeKDF 9clSbCEilKTl83fx51vua8Kpa1NkwLmmw1AHvdetARvAP96fnsNoxHV4aS7aSS3Y BBTx/RXLOhUIEqclVRK3rxAxOcTbujh+YG8w6l+suAcozBkMfcspf8R7q2n0NGdp xxf/doVyD1+m47wJW22L71jSz9sTAgMBAAGjWTBXMA4GA1UdDwEB/wQEAwICpDAP BgNVHRMBAf8EBTADAQH/MB0GA1UdDgQWBBSEprNJ5veNAMdoPVRRxr2oQJjS+jAV BgNVHREEDjAMggprdWJlcm5ldGVzMA0GCSqGSIb3DQEBCwUAA4IBAQBc9FzNHHSh 2fxIgItk6DQVQspi7FYqDJwzJtW3flzdpgrAtI38U7fsCarzEhZPVf784v97EXpC fZltFUyW/loYYCgXgHj5J8z23PHlUVII9E4sPX3WJZPcZklIjr4WC8V5w3iJk6hz e/qt8ioouBAF5YG3n2q9RIAn7Hw7s4P+9erL8iJl/4OENAjMzdX32ofbTzssBW2W izU8ajUVq4FEQeosOrfqd/5DppBjoyifRuIsKh2MtmopPmWuKq5Kzb73pMq2RxNN UVD7Weasu01YT+tFS2TjLVr4fq/WXF8C3BTPymgyTAeqqy8K8UBUENuA0mEKUN4G rujlVutC53EW -----END CERTIFICATE----- ~/.kube 14:28:22 \u276f openssl x509 -in /tmp/decoded-ca.crt -text -noout | head -30 Certificate: Data: Version: 3 (0x2) Serial Number: 2897472356293683110 (0x2835e3c1af6627a6) Signature Algorithm: sha256WithRSAEncryption Issuer: CN = kubernetes Validity Not Before: Jan 1 07:08:58 2026 GMT Not After : Dec 30 07:13:58 2035 GMT Subject: CN = kubernetes Subject Public Key Info: Public Key Algorithm: rsaEncryption Public-Key: (2048 bit) Modulus: 00:b9:f3:39:5f 51:01:41:db:e7:1b:6c:f7:d2: 80:8e:b3 a0:35:68:11:4a:56:27:63:88:74:44: 49:87:34:2f:54:5d:96:8c:46:66:93:a8:10:98:3a: 30:a6:a2:0d:57:65:b4:0f:ab:5f:74:a8:53:44:b9: df:7a:d3:87:0a:a9:a2:d7:1a:58:67:1a:6d:1d:0f: 9b:9a:9b 25:07:1f:80:0d:fd:f5:fa:51:d6:6b: 79:49:15:2a:79:5d:81:f4:96:2a:30:57:4c 55: cc:85:64:27:fd:e1:0d:b5:cf:92:ca:62:81:60:59: 7c:12:d2:78:5e:17:26:1f:3a:fd:56:60:4c:2a:43: 57:90:78:a0:c5:f5:c9:52:6c:21:22:94:a4:e5:f3: 77:f1:e7:5b:ee:6b:c2:a9:6b:53:64:c0:b9:a6:c3: 50:07:bd:d7:ad:01:1b:c0:3f 9f:9e:c3:68:c4: 75:78:69:2e:da:49:2d:d8:04:14:f1:fd:15:cb:3a: 15:08:12:a7:25:55:12:b7:af:10:31:39:c4:db:ba: 38:7e:60:6f:30 5f b8:07:28 19:0c:7d: cb:29:7f:c4:7b 69:f4:34:67:69:c7:17:ff:76: ~/.kube 14:28:36 \u276f awk '/client-certificate-data:/{flag=1} flag && /^[[:space:]] client-key-data:/{flag=0} flag' /tmp/gatewayapi-kubeconfig.yaml | \\ grep 'client-certificate-data:' | \\ cut -d':' -f2- | \\ sed 's/^[[:space:]] //' | \\ base64 -d > /tmp/decoded-client.crt ~/.kube 14:28:57 \u276f openssl x509 -in /tmp/decoded-client.crt -text -noout | head -30 Certificate: Data: Version: 3 (0x2) Serial Number: 3322358965758446498 (0x2e1b63e0725dcba2) Signature Algorithm: sha256WithRSAEncryption Issuer: CN = kubernetes Validity Not Before: Jan 1 07:08:58 2026 GMT Not After : Jan 1 07:13:58 2027 GMT Subject: O = kubeadm:cluster-admins, CN = kubernetes-admin Subject Public Key Info: Public Key Algorithm: rsaEncryption Public-Key: (2048 bit) Modulus: 00:aa:92:9a:42:9f:05:41:b1:fd:b5:1f:08:05:09: 38:70:18:f1:9d:8a:a2:91:54 84:43:08:05:c1: 9c:b7:67:d3:28:21:e1:1f:32:44:bf:6f:d3:fc:94: 37:06:bd:53:3f:1b:0c:06:c8:39:fe:a5:12:fe:bd: 73:9b:36:64:a0:9d:2f:f1:b1:0f:ac:52:37:e0:2c: 32:a8:7b:07:6a:01:aa:c1:3e:6f:e1 53:33:38: 8a b8:6a:35:74:0c:6f:8a:9b:28:dc:e0:7b:e1: c1:25:05 70:8e:aa:2e:e6:32:f5:1b:77:39:a2: 29:ed:4c:96:9c 63 eb:08:1b:46:22:b5:84: 0b:5f:f4:ef:e4:12:7c:0c:14:3f:ef fe:9c:22: fa:54:3e:d8:8a:a5:00:45:7f:12:84:80:f6:fa:5f: d5:5a:b1:bc:b0:45:9a:2b:94:a8:e8:22:28:94:dd: 8d:ef:00:24:23:25:0d:5a:60:34:66:21:a9 99: 24:c4:db:ef:71:0f:9d:31:7f:dd:37:40:9c:f2:3e: 7f:51:96:d1:bc:39:bc:86:13:df:d4:9c:7f:d7:e9: 30:f0:3d:da:ed:66:9d 59:7a:75:77:6f:b0:ba: ~/.kube 14:29:02 \u276f awk '/client-key-data:/{flag=1} flag && /^[[:space:]] token:/{flag=0} flag' /tmp/gatewayapi-kubeconfig.yaml | \\ grep 'client-key-data:' | \\ cut -d':' -f2- | \\ sed 's/^[[:space:]] //' | \\ base64 -d > /tmp/decoded-client.key ~/.kube 14:29:18 \u276f file /tmp/decoded-client.key /tmp/decoded-client.key: PEM RSA private key ~/.kube 14:29:22 \u276f # Using yq (if installed) cat /tmp/gatewayapi-kubeconfig.yaml | yq '.clusters[0].cluster.certificate-authority-data' | base64 -d > /tmp/ca.crt cat /tmp/gatewayapi-kubeconfig.yaml | yq '.users[0].user.client-certificate-data' | base64 -d > /tmp/client.crt cat /tmp/gatewayapi-kubeconfig.yaml | yq '.users[0].user.client-key-data' | base64 -d > /tmp/client.key","title":"Show the structure"},{"location":"05-kubeconfig/notes/#using-grepsed-only","text":"cat /tmp/gatewayapi-kubeconfig.yaml | grep -A1 -B1 'certificate-authority-data:' | tail -1 | base64 -d > /tmp/ca2.crt zsh: unknown file attribute: i jq: error: authority/0 is not defined at , line 1: .clusters[0].cluster.certificate-authority-data jq: error: data/0 is not defined at , line 1: .clusters[0].cluster.certificate-authority-data jq: 2 compile errors jq: error: certificate/0 is not defined at , line 1: .users[0].user.client-certificate-data jq: error: data/0 is not defined at , line 1: .users[0].user.client-certificate-data jq: 2 compile errors jq: error: key/0 is not defined at , line 1: .users[0].user.client-key-data jq: error: data/0 is not defined at , line 1: .users[0].user.client-key-data jq: 2 compile errors zsh: command not found: # base64: invalid input ~/.kube 14:29:36 \u276f","title":"Using grep/sed only"},{"location":"06-cluster-certificates-and-kube-system%20%5Bimportant%5D/kubeadm-certs/","text":"kubeadm Certificate Checks & Renewal Check Certificate Expiration kubeadm certs check-expiration kubeadm certs check-expiration -v = 5 Renew Certificates kubeadm certs renew all kubeadm certs renew all --dry-run Renew Specific Certificate kubeadm certs renew apiserver","title":"kubeadm Certificate Checks &amp; Renewal"},{"location":"06-cluster-certificates-and-kube-system%20%5Bimportant%5D/kubeadm-certs/#kubeadm-certificate-checks-renewal","text":"","title":"kubeadm Certificate Checks &amp; Renewal"},{"location":"06-cluster-certificates-and-kube-system%20%5Bimportant%5D/kubeadm-certs/#check-certificate-expiration","text":"kubeadm certs check-expiration kubeadm certs check-expiration -v = 5","title":"Check Certificate Expiration"},{"location":"06-cluster-certificates-and-kube-system%20%5Bimportant%5D/kubeadm-certs/#renew-certificates","text":"kubeadm certs renew all kubeadm certs renew all --dry-run","title":"Renew Certificates"},{"location":"06-cluster-certificates-and-kube-system%20%5Bimportant%5D/kubeadm-certs/#renew-specific-certificate","text":"kubeadm certs renew apiserver","title":"Renew Specific Certificate"},{"location":"07-crd%27s/crd/","text":"Custom Resource Definitions (CRDs) and Custom Resources (CRs) What CRDs and CRs are CustomResourceDefinition (CRD) extends the Kubernetes API by defining a new resource type Custom Resource (CR) is an instance of that resource type CRDs are created once CRs are created many times A CR cannot exist unless its CRD already exists CRDs are always cluster-scoped CRs can be namespaced or cluster-scoped , depending on the CRD Analogy: Deployment API \u2192 Deployment objects CRD \u2192 Custom API CR \u2192 Objects using that API Core identity of any Kubernetes resource Every Kubernetes resource is uniquely identified by: group + version + kind Examples: apiVersion : apps/v1 kind : Deployment apiVersion : storage.example.com/v1 kind : Backup Minimal CRD structure (exam-ready, v1) With apiextensions.k8s.io/v1 , a schema is mandatory . apiVersion : apiextensions.k8s.io/v1 kind : CustomResourceDefinition metadata : name : <plural>.<group> spec : group : <group> scope : Namespaced | Cluster names : plural : <plural> singular : <singular> kind : <Kind> versions : - name : v1 served : true storage : true schema : openAPIV3Schema : type : object Critical rules: metadata.name = plural.group Exactly one version must have storage: true At least one version must have served: true Meaning of CRD fields group Logical API grouping Used in CRD and CR apiVersion version API version Appears after / in apiVersion kind Singular, capitalized resource name Used in CR YAML plural Used in kubectl commands singular Optional kubectl usage scope Determines whether CRs are namespaced or cluster-scoped served and storage (exam-critical) served served : true Determines whether this version is accessible via the API If false , users cannot create or read CRs using this version Mental model: served = can users use this version? storage storage : true Determines which version is used to store objects in etcd Exactly one version must be storage: true Other served versions are automatically converted to this version Mental model: storage = how Kubernetes stores the object internally Namespaced vs Cluster scope Namespaced CRD CR must include metadata.namespace kubectl supports -n and -A Example CR: metadata : name : daily-backup namespace : default Commands: kubectl get backups -n default kubectl get backups -A Cluster-scoped CRD CR must NOT include metadata.namespace kubectl does NOT support -n or -A Example CR: metadata : name : sunday-window Example 1: Namespaced CRD (Backup) CustomResourceDefinition apiVersion : apiextensions.k8s.io/v1 kind : CustomResourceDefinition metadata : name : backups.storage.example.com spec : group : storage.example.com scope : Namespaced names : plural : backups singular : backup kind : Backup versions : - name : v1 served : true storage : true schema : openAPIV3Schema : type : object Create and verify: kubectl apply -f backup-crd.yaml kubectl get crd backups.storage.example.com Custom Resource apiVersion : storage.example.com/v1 kind : Backup metadata : name : daily-backup namespace : default Example 2: Cluster-scoped CRD (MaintenanceWindow) CustomResourceDefinition apiVersion : apiextensions.k8s.io/v1 kind : CustomResourceDefinition metadata : name : maintenancewindows.ops.example.com spec : group : ops.example.com scope : Cluster names : plural : maintenancewindows singular : maintenancewindow kind : MaintenanceWindow versions : - name : v1 served : true storage : true schema : openAPIV3Schema : type : object Custom Resource apiVersion : ops.example.com/v1 kind : MaintenanceWindow metadata : name : example-maintenancewindow Example 3: Namespaced CRD with operational intent (ScheduledBackup) This example models scheduled backups with retention and demonstrates validation. CustomResourceDefinition apiVersion : apiextensions.k8s.io/v1 kind : CustomResourceDefinition metadata : name : scheduledbackups.storage.example.com spec : group : storage.example.com scope : Namespaced names : plural : scheduledbackups singular : scheduledbackup kind : ScheduledBackup versions : - name : v1 served : true storage : true schema : openAPIV3Schema : type : object properties : spec : type : object required : - schedule - retention properties : schedule : type : string retention : type : object required : - days properties : days : type : integer minimum : 1 Custom Resource apiVersion : storage.example.com/v1 kind : ScheduledBackup metadata : name : daily-backup namespace : default spec : schedule : \"0 2 * * *\" retention : days : 7 Adding shortcuts with shortNames Shortcuts make kubectl usage faster. How to add CR names : plural : featuretoggles singular : featuretoggle kind : FeatureToggle shortNames : - ft Usage kubectl get ft kubectl describe ft generic -n default kubectl delete ft generic -n default Short names are: Defined only in the CRD Optional Very useful in the exam Creating Custom Resources (CRs) Rules: CRD must exist first CR apiVersion must match CRD group/version CR kind must match CRD kind exactly Namespace must match scope Command: kubectl apply -f cr.yaml Listing, describing, deleting CRs kubectl get <plural> kubectl describe <singular> <name> -n <namespace> kubectl delete <singular> <name> -n <namespace> Order of operations (exam critical) Correct: Create CRD Verify CRD exists Create CR Incorrect: Creating CR before CRD Error: no matches for kind \"<Kind>\" in version \"<group>/<version>\" kubectl mental model kubectl get \u2192 plural YAML kind \u2192 Kind YAML apiVersion \u2192 group/version CRD name \u2192 plural.group Common CKA mistakes Missing schema in v1 CRDs Wrong CRD name (not plural.group) apiVersion mismatch in CR Namespace used on cluster-scoped CR Missing namespace on namespaced CR Using singular with kubectl get Creating CR before CRD Final exam checklist CRD applied successfully Schema present for v1 CRDs metadata.name = plural.group Exactly one storage: true At least one served: true Scope respected Correct plural used in kubectl commands","title":"Custom Resource Definitions (CRDs) and Custom Resources (CRs)"},{"location":"07-crd%27s/crd/#custom-resource-definitions-crds-and-custom-resources-crs","text":"","title":"Custom Resource Definitions (CRDs) and Custom Resources (CRs)"},{"location":"07-crd%27s/crd/#what-crds-and-crs-are","text":"CustomResourceDefinition (CRD) extends the Kubernetes API by defining a new resource type Custom Resource (CR) is an instance of that resource type CRDs are created once CRs are created many times A CR cannot exist unless its CRD already exists CRDs are always cluster-scoped CRs can be namespaced or cluster-scoped , depending on the CRD Analogy: Deployment API \u2192 Deployment objects CRD \u2192 Custom API CR \u2192 Objects using that API","title":"What CRDs and CRs are"},{"location":"07-crd%27s/crd/#core-identity-of-any-kubernetes-resource","text":"Every Kubernetes resource is uniquely identified by: group + version + kind Examples: apiVersion : apps/v1 kind : Deployment apiVersion : storage.example.com/v1 kind : Backup","title":"Core identity of any Kubernetes resource"},{"location":"07-crd%27s/crd/#minimal-crd-structure-exam-ready-v1","text":"With apiextensions.k8s.io/v1 , a schema is mandatory . apiVersion : apiextensions.k8s.io/v1 kind : CustomResourceDefinition metadata : name : <plural>.<group> spec : group : <group> scope : Namespaced | Cluster names : plural : <plural> singular : <singular> kind : <Kind> versions : - name : v1 served : true storage : true schema : openAPIV3Schema : type : object Critical rules: metadata.name = plural.group Exactly one version must have storage: true At least one version must have served: true","title":"Minimal CRD structure (exam-ready, v1)"},{"location":"07-crd%27s/crd/#meaning-of-crd-fields","text":"group Logical API grouping Used in CRD and CR apiVersion version API version Appears after / in apiVersion kind Singular, capitalized resource name Used in CR YAML plural Used in kubectl commands singular Optional kubectl usage scope Determines whether CRs are namespaced or cluster-scoped","title":"Meaning of CRD fields"},{"location":"07-crd%27s/crd/#served-and-storage-exam-critical","text":"","title":"served and storage (exam-critical)"},{"location":"07-crd%27s/crd/#served","text":"served : true Determines whether this version is accessible via the API If false , users cannot create or read CRs using this version Mental model: served = can users use this version?","title":"served"},{"location":"07-crd%27s/crd/#storage","text":"storage : true Determines which version is used to store objects in etcd Exactly one version must be storage: true Other served versions are automatically converted to this version Mental model: storage = how Kubernetes stores the object internally","title":"storage"},{"location":"07-crd%27s/crd/#namespaced-vs-cluster-scope","text":"","title":"Namespaced vs Cluster scope"},{"location":"07-crd%27s/crd/#namespaced-crd","text":"CR must include metadata.namespace kubectl supports -n and -A Example CR: metadata : name : daily-backup namespace : default Commands: kubectl get backups -n default kubectl get backups -A","title":"Namespaced CRD"},{"location":"07-crd%27s/crd/#cluster-scoped-crd","text":"CR must NOT include metadata.namespace kubectl does NOT support -n or -A Example CR: metadata : name : sunday-window","title":"Cluster-scoped CRD"},{"location":"07-crd%27s/crd/#example-1-namespaced-crd-backup","text":"","title":"Example 1: Namespaced CRD (Backup)"},{"location":"07-crd%27s/crd/#customresourcedefinition","text":"apiVersion : apiextensions.k8s.io/v1 kind : CustomResourceDefinition metadata : name : backups.storage.example.com spec : group : storage.example.com scope : Namespaced names : plural : backups singular : backup kind : Backup versions : - name : v1 served : true storage : true schema : openAPIV3Schema : type : object Create and verify: kubectl apply -f backup-crd.yaml kubectl get crd backups.storage.example.com Custom Resource apiVersion : storage.example.com/v1 kind : Backup metadata : name : daily-backup namespace : default","title":"CustomResourceDefinition"},{"location":"07-crd%27s/crd/#example-2-cluster-scoped-crd-maintenancewindow","text":"","title":"Example 2: Cluster-scoped CRD (MaintenanceWindow)"},{"location":"07-crd%27s/crd/#customresourcedefinition_1","text":"apiVersion : apiextensions.k8s.io/v1 kind : CustomResourceDefinition metadata : name : maintenancewindows.ops.example.com spec : group : ops.example.com scope : Cluster names : plural : maintenancewindows singular : maintenancewindow kind : MaintenanceWindow versions : - name : v1 served : true storage : true schema : openAPIV3Schema : type : object Custom Resource","title":"CustomResourceDefinition"},{"location":"07-crd%27s/crd/#apiversion-opsexamplecomv1-kind-maintenancewindow-metadata-name-example-maintenancewindow","text":"","title":"apiVersion: ops.example.com/v1 kind: MaintenanceWindow metadata: name: example-maintenancewindow"},{"location":"07-crd%27s/crd/#example-3-namespaced-crd-with-operational-intent-scheduledbackup","text":"This example models scheduled backups with retention and demonstrates validation.","title":"Example 3: Namespaced CRD with operational intent (ScheduledBackup)"},{"location":"07-crd%27s/crd/#customresourcedefinition_2","text":"apiVersion : apiextensions.k8s.io/v1 kind : CustomResourceDefinition metadata : name : scheduledbackups.storage.example.com spec : group : storage.example.com scope : Namespaced names : plural : scheduledbackups singular : scheduledbackup kind : ScheduledBackup versions : - name : v1 served : true storage : true schema : openAPIV3Schema : type : object properties : spec : type : object required : - schedule - retention properties : schedule : type : string retention : type : object required : - days properties : days : type : integer minimum : 1 Custom Resource","title":"CustomResourceDefinition"},{"location":"07-crd%27s/crd/#apiversion-storageexamplecomv1-kind-scheduledbackup-metadata-name-daily-backup-namespace-default-spec-schedule-0-2-retention-days-7","text":"","title":"apiVersion: storage.example.com/v1 kind: ScheduledBackup metadata: name: daily-backup namespace: default spec: schedule: &quot;0 2 * * *&quot; retention: days: 7"},{"location":"07-crd%27s/crd/#adding-shortcuts-with-shortnames","text":"Shortcuts make kubectl usage faster.","title":"Adding shortcuts with shortNames"},{"location":"07-crd%27s/crd/#how-to-add","text":"CR names : plural : featuretoggles singular : featuretoggle kind : FeatureToggle shortNames : - ft","title":"How to add"},{"location":"07-crd%27s/crd/#usage","text":"kubectl get ft kubectl describe ft generic -n default kubectl delete ft generic -n default Short names are: Defined only in the CRD Optional Very useful in the exam","title":"Usage"},{"location":"07-crd%27s/crd/#creating-custom-resources-crs","text":"Rules: CRD must exist first CR apiVersion must match CRD group/version CR kind must match CRD kind exactly Namespace must match scope Command: kubectl apply -f cr.yaml","title":"Creating Custom Resources (CRs)"},{"location":"07-crd%27s/crd/#listing-describing-deleting-crs","text":"kubectl get <plural> kubectl describe <singular> <name> -n <namespace> kubectl delete <singular> <name> -n <namespace>","title":"Listing, describing, deleting CRs"},{"location":"07-crd%27s/crd/#order-of-operations-exam-critical","text":"Correct: Create CRD Verify CRD exists Create CR Incorrect: Creating CR before CRD Error: no matches for kind \"<Kind>\" in version \"<group>/<version>\"","title":"Order of operations (exam critical)"},{"location":"07-crd%27s/crd/#kubectl-mental-model","text":"kubectl get \u2192 plural YAML kind \u2192 Kind YAML apiVersion \u2192 group/version CRD name \u2192 plural.group","title":"kubectl mental model"},{"location":"07-crd%27s/crd/#common-cka-mistakes","text":"Missing schema in v1 CRDs Wrong CRD name (not plural.group) apiVersion mismatch in CR Namespace used on cluster-scoped CR Missing namespace on namespaced CR Using singular with kubectl get Creating CR before CRD","title":"Common CKA mistakes"},{"location":"07-crd%27s/crd/#final-exam-checklist","text":"CRD applied successfully Schema present for v1 CRDs metadata.name = plural.group Exactly one storage: true At least one served: true Scope respected Correct plural used in kubectl commands","title":"Final exam checklist"},{"location":"07-crd%27s/question/","text":"Question 1 \u2014 Reading and reasoning You are given the following CRD: apiVersion : apiextensions.k8s.io/v1 kind : CustomResourceDefinition metadata : name : accesspolicies.security.example.com spec : group : security.example.com scope : Namespaced names : plural : accesspolicies singular : accesspolicy kind : AccessPolicy versions : - name : v1 served : true storage : true Tasks: Write the correct apiVersion for a Custom Resource Write the correct kubectl get command State whether a namespace is required when creating a CR State the correct CRD name format Question 2 \u2014 Fix the broken CRD The following CRD fails to apply: apiVersion : apiextensions.k8s.io/v1 kind : CustomResourceDefinition metadata : name : policies.security.example.com spec : group : security.example.com scope : Namespaced names : plural : accesspolicies singular : accesspolicy kind : AccessPolicy versions : - name : v1 served : true storage : true Tasks: Identify what is wrong Fix the CRD Explain why Kubernetes rejects the original Question 3 \u2014 FeatureToggle CRD Create a namespaced CRD with: Kind: FeatureToggle Group: config.example.com Version: v1 Plural: featuretoggles Spec fields: enabled (boolean) description (string) Then create a Custom Resource: Name: new-ui Namespace: default enabled: true Question 4 \u2014 Schema validation failure You apply the following CR: apiVersion : config.example.com/v1 kind : FeatureToggle metadata : name : broken-toggle namespace : default spec : enabled : \"true\" Tasks: Explain why this CR is rejected Identify the exact field causing the issue State the correct value type Question 5 \u2014 kubectl explain Assume the CRD featuretoggles.config.example.com exists. Tasks: Inspect the Custom Resource using kubectl explain Inspect the spec.enabled field Explain why this is useful in the exam Question 6 \u2014 Cluster vs Namespaced trap You are given a CRD with: spec : scope : Cluster But the CR YAML contains: metadata : namespace : default Tasks: State what will go wrong Explain how to fix it Identify which object is incorrect Question 7 \u2014 Fast recognition drill Given: CRD name: backups.storage.example.com Answer: Group Plural Kind kubectl get command","title":"Question"},{"location":"07-crd%27s/question/#question-1-reading-and-reasoning","text":"You are given the following CRD: apiVersion : apiextensions.k8s.io/v1 kind : CustomResourceDefinition metadata : name : accesspolicies.security.example.com spec : group : security.example.com scope : Namespaced names : plural : accesspolicies singular : accesspolicy kind : AccessPolicy versions : - name : v1 served : true storage : true Tasks: Write the correct apiVersion for a Custom Resource Write the correct kubectl get command State whether a namespace is required when creating a CR State the correct CRD name format","title":"Question 1 \u2014 Reading and reasoning"},{"location":"07-crd%27s/question/#question-2-fix-the-broken-crd","text":"The following CRD fails to apply: apiVersion : apiextensions.k8s.io/v1 kind : CustomResourceDefinition metadata : name : policies.security.example.com spec : group : security.example.com scope : Namespaced names : plural : accesspolicies singular : accesspolicy kind : AccessPolicy versions : - name : v1 served : true storage : true Tasks: Identify what is wrong Fix the CRD Explain why Kubernetes rejects the original","title":"Question 2 \u2014 Fix the broken CRD"},{"location":"07-crd%27s/question/#question-3-featuretoggle-crd","text":"Create a namespaced CRD with: Kind: FeatureToggle Group: config.example.com Version: v1 Plural: featuretoggles Spec fields: enabled (boolean) description (string) Then create a Custom Resource: Name: new-ui Namespace: default enabled: true","title":"Question 3 \u2014 FeatureToggle CRD"},{"location":"07-crd%27s/question/#question-4-schema-validation-failure","text":"You apply the following CR: apiVersion : config.example.com/v1 kind : FeatureToggle metadata : name : broken-toggle namespace : default spec : enabled : \"true\" Tasks: Explain why this CR is rejected Identify the exact field causing the issue State the correct value type","title":"Question 4 \u2014 Schema validation failure"},{"location":"07-crd%27s/question/#question-5-kubectl-explain","text":"Assume the CRD featuretoggles.config.example.com exists. Tasks: Inspect the Custom Resource using kubectl explain Inspect the spec.enabled field Explain why this is useful in the exam","title":"Question 5 \u2014 kubectl explain"},{"location":"07-crd%27s/question/#question-6-cluster-vs-namespaced-trap","text":"You are given a CRD with: spec : scope : Cluster But the CR YAML contains: metadata : namespace : default Tasks: State what will go wrong Explain how to fix it Identify which object is incorrect","title":"Question 6 \u2014 Cluster vs Namespaced trap"},{"location":"07-crd%27s/question/#question-7-fast-recognition-drill","text":"Given: CRD name: backups.storage.example.com Answer: Group Plural Kind kubectl get command","title":"Question 7 \u2014 Fast recognition drill"},{"location":"07-crd%27s/solutions/","text":"Solutions Solution 1 \u2014 Reading and reasoning Custom Resource apiVersion : apiVersion : security.example.com/v1 kubectl get command: kubectl get accesspolicies Namespace requirement: Yes. The CRD scope is Namespaced , so metadata.namespace is required. Correct CRD name format: <plural>.<group> \u2192 accesspolicies.security.example.com Solution 2 \u2014 Fix the broken CRD What is wrong: metadata.name does not match spec.names.plural + \".\" + spec.group . Fixed CRD: metadata : name : accesspolicies.security.example.com Why Kubernetes rejects the original: Kubernetes requires the CRD name to be exactly plural.group ; otherwise the API cannot be registered. Solution 3 \u2014 FeatureToggle CRD and CR CustomResourceDefinition apiVersion : apiextensions.k8s.io/v1 kind : CustomResourceDefinition metadata : name : featuretoggles.config.example.com spec : group : config.example.com scope : Namespaced names : plural : featuretoggles singular : featuretoggle kind : FeatureToggle versions : - name : v1 served : true storage : true schema : openAPIV3Schema : type : object properties : spec : type : object properties : enabled : type : boolean description : type : string Custom Resource apiVersion : config.example.com/v1 kind : FeatureToggle metadata : name : new-ui namespace : default spec : enabled : true Solution 4 \u2014 Schema validation failure Why the CR is rejected: The value type does not match the CRD schema. Field causing the issue: spec.enabled Correct value type: boolean (true or false) Solution 5 \u2014 kubectl explain Inspect the Custom Resource: kubectl explain featuretoggles Inspect the spec.enabled field: kubectl explain featuretoggles.spec.enabled Why this is useful in the exam: It shows field names, types, and descriptions without opening YAML files. Solution 6 \u2014 Cluster vs Namespaced trap What will go wrong: The CR will be rejected because cluster-scoped resources cannot have a namespace. How to fix it: Remove metadata.namespace from the CR. Which object is incorrect: The Custom Resource (CR), not the CRD. Solution 7 \u2014 Fast recognition drill Given: CRD name: backups.storage.example.com Group: storage.example.com Plural: backups Kind: Backup kubectl get command: kubectl get backups","title":"Solutions"},{"location":"07-crd%27s/solutions/#solutions","text":"","title":"Solutions"},{"location":"07-crd%27s/solutions/#solution-1-reading-and-reasoning","text":"Custom Resource apiVersion : apiVersion : security.example.com/v1 kubectl get command: kubectl get accesspolicies Namespace requirement: Yes. The CRD scope is Namespaced , so metadata.namespace is required. Correct CRD name format: <plural>.<group> \u2192 accesspolicies.security.example.com","title":"Solution 1 \u2014 Reading and reasoning"},{"location":"07-crd%27s/solutions/#solution-2-fix-the-broken-crd","text":"What is wrong: metadata.name does not match spec.names.plural + \".\" + spec.group . Fixed CRD: metadata : name : accesspolicies.security.example.com Why Kubernetes rejects the original: Kubernetes requires the CRD name to be exactly plural.group ; otherwise the API cannot be registered.","title":"Solution 2 \u2014 Fix the broken CRD"},{"location":"07-crd%27s/solutions/#solution-3-featuretoggle-crd-and-cr","text":"","title":"Solution 3 \u2014 FeatureToggle CRD and CR"},{"location":"07-crd%27s/solutions/#customresourcedefinition","text":"apiVersion : apiextensions.k8s.io/v1 kind : CustomResourceDefinition metadata : name : featuretoggles.config.example.com spec : group : config.example.com scope : Namespaced names : plural : featuretoggles singular : featuretoggle kind : FeatureToggle versions : - name : v1 served : true storage : true schema : openAPIV3Schema : type : object properties : spec : type : object properties : enabled : type : boolean description : type : string","title":"CustomResourceDefinition"},{"location":"07-crd%27s/solutions/#custom-resource","text":"apiVersion : config.example.com/v1 kind : FeatureToggle metadata : name : new-ui namespace : default spec : enabled : true","title":"Custom Resource"},{"location":"07-crd%27s/solutions/#solution-4-schema-validation-failure","text":"Why the CR is rejected: The value type does not match the CRD schema. Field causing the issue: spec.enabled Correct value type: boolean (true or false)","title":"Solution 4 \u2014 Schema validation failure"},{"location":"07-crd%27s/solutions/#solution-5-kubectl-explain","text":"Inspect the Custom Resource: kubectl explain featuretoggles Inspect the spec.enabled field: kubectl explain featuretoggles.spec.enabled Why this is useful in the exam: It shows field names, types, and descriptions without opening YAML files.","title":"Solution 5 \u2014 kubectl explain"},{"location":"07-crd%27s/solutions/#solution-6-cluster-vs-namespaced-trap","text":"What will go wrong: The CR will be rejected because cluster-scoped resources cannot have a namespace. How to fix it: Remove metadata.namespace from the CR. Which object is incorrect: The Custom Resource (CR), not the CRD.","title":"Solution 6 \u2014 Cluster vs Namespaced trap"},{"location":"07-crd%27s/solutions/#solution-7-fast-recognition-drill","text":"Given: CRD name: backups.storage.example.com Group: storage.example.com Plural: backups Kind: Backup kubectl get command: kubectl get backups","title":"Solution 7 \u2014 Fast recognition drill"},{"location":"08-gateway-api/gateway-api/","text":"Kubernetes Gateway API Core Architecture Three-Tier Model: GatewayClass \u2192 Which controller implementation to use Gateway \u2192 Actual load balancer with listeners HTTPRoute/TCPRoute/TLSRoute \u2192 Routing rules and logic Complete YAML Examples (All Fields Explained) GatewayClass - All Possible Fields: apiVersion : gateway.networking.k8s.io/v1 kind : GatewayClass metadata : name : traefik spec : # REQUIRED: Which controller implements this controllerName : \"traefik.io/gateway-controller\" # OPTIONAL: Description description : \"Traefik GatewayClass for production\" # OPTIONAL: Controller-specific parameters parametersRef : name : traefik-config group : traefik.io kind : GatewayClassConfig Gateway - Complete with All Listeners: apiVersion : gateway.networking.k8s.io/v1 kind : Gateway metadata : name : traefik spec : # REQUIRED: Which GatewayClass to use gatewayClassName : traefik # REQUIRED: At least one listener listeners : - name : http-public port : 80 protocol : HTTP # OPTIONAL: Restrict hostnames hostname : \"*.example-app.com\" # OPTIONAL: TLS (for HTTPS/TLS listeners only) # tls: # mode: Terminate # certificateRefs: [] # OPTIONAL: Which namespaces can attach routes allowedRoutes : namespaces : from : Same # Same, All, or Selector # selector: # When from: Selector # matchLabels: # shared-gateway: \"true\" # HTTPS Listener Example - name : https-secure port : 443 protocol : HTTPS hostname : \"example-app.com\" # REQUIRED for HTTPS: TLS configuration tls : mode : Terminate # or Passthrough certificateRefs : - name : secret-tls kind : Secret group : \"\" # options: # Optional TLS options # cipherSuites: [] # minVersion: \"TLSv1.2\" allowedRoutes : namespaces : from : All # TCP Listener Example (Experimental) - name : tcp-database port : 5432 protocol : TCP allowedRoutes : namespaces : from : Same Listener Protocol Types: - HTTP \u2192 For HTTPRoute - HTTPS \u2192 For HTTPRoute with TLS - TLS \u2192 For TLSRoute (passthrough) - TCP \u2192 For TCPRoute - UDP \u2192 For UDPRoute (experimental) TLS Modes: - Terminate \u2192 TLS ends at Gateway (decrypts) - Passthrough \u2192 TLS continues to backend HTTPRoute - Complete with All Features Route by Hostname (From README): apiVersion : gateway.networking.k8s.io/v1 kind : HTTPRoute metadata : name : go spec : # REQUIRED: Attach to which Gateway parentRefs : - name : traefik # namespace: default # If Gateway in different namespace # sectionName: web # Target specific listener # port: 80 # Target specific port # OPTIONAL: Which hostnames to match hostnames : - \"example-app-go.com\" - \"*.test.example-app-go.com\" # Wildcard subdomains # REQUIRED: At least one rule rules : - matches : # Path matching (choose one type) - path : type : PathPrefix # Exact, PathPrefix, or RegularExpression value : \"/\" # OPTIONAL: Header matching # headers: # - type: Exact # or RegularExpression # name: \"X-API-Version\" # value: \"v2\" # OPTIONAL: Query parameter matching # queryParams: # - type: Exact # name: \"debug\" # value: \"true\" # OPTIONAL: HTTP method matching # method: \"GET\" # GET, POST, PUT, DELETE, etc. # OPTIONAL: Request filters (applied in order) filters : # - type: RequestHeaderModifier # requestHeaderModifier: # set: # Overwrite if exists # - name: \"X-Request-ID\" # value: \"{{uuid}}\" # add: # Add if not exists # - name: \"X-Forwarded-For\" # value: \"$remote_addr\" # remove: # Remove headers # - \"X-Secret-Header\" # - type: ResponseHeaderModifier # responseHeaderModifier: # add: # - name: \"Cache-Control\" # value: \"max-age=3600\" # - type: RequestRedirect # requestRedirect: # scheme: \"https\" # hostname: \"secure.example.com\" # port: 443 # statusCode: 301 # - type: URLRewrite # urlRewrite: # path: # type: ReplacePrefixMatch # or ReplaceFullPath # replacePrefixMatch: \"/v2\" # - type: RequestMirror # requestMirror: # backendRef: # name: audit-service # port: 8080 # - type: ExtensionRef # extensionRef: # name: custom-filter # kind: CustomFilter # group: example.com # REQUIRED: Where to send traffic backendRefs : - name : go-svc port : 5000 # weight: 100 # For traffic splitting (0-100) # filters: [] # Backend-specific filters # Multiple backends for traffic splitting # - name: go-svc-v2 # port: 5000 # weight: 20 Route by Path - Exact Match: apiVersion : gateway.networking.k8s.io/v1 kind : HTTPRoute metadata : name : python-exact spec : parentRefs : - name : traefik hostnames : - \"example-app-python.com\" rules : - matches : - path : type : Exact # Only \"/\" matches value : \"/\" backendRefs : - name : python-svc port : 5000 Route by Path - Prefix Match: apiVersion : gateway.networking.k8s.io/v1 kind : HTTPRoute metadata : name : python-prefix spec : parentRefs : - name : traefik hostnames : - \"example-app-python.com\" rules : - matches : - path : type : PathPrefix # \"/\", \"/api\", \"/api/users\" all match value : \"/\" backendRefs : - name : python-svc port : 5000 URL Rewrite Pattern (API Gateway): apiVersion : gateway.networking.k8s.io/v1 kind : HTTPRoute metadata : name : api-gateway spec : parentRefs : - name : traefik hostnames : - \"example-app.com\" rules : # Python service - matches : - path : type : PathPrefix value : \"/api/python\" filters : - type : URLRewrite urlRewrite : path : type : ReplacePrefixMatch replacePrefixMatch : \"/\" backendRefs : - name : python-svc port : 5000 # Go service - matches : - path : type : PathPrefix value : \"/api/go\" filters : - type : URLRewrite urlRewrite : path : type : ReplacePrefixMatch replacePrefixMatch : \"/\" backendRefs : - name : go-svc port : 5000 URL Rewrite Types: - ReplacePrefixMatch \u2192 Replace matched prefix only - ReplaceFullPath \u2192 Replace entire path Header Modification (CORS Example): apiVersion : gateway.networking.k8s.io/v1 kind : HTTPRoute metadata : name : go-cors spec : parentRefs : - name : traefik hostnames : - \"example-app.com\" rules : - matches : - path : type : PathPrefix value : \"/api/go\" filters : - type : ResponseHeaderModifier responseHeaderModifier : add : - name : \"Access-Control-Allow-Origin\" value : \"*\" - name : \"Access-Control-Allow-Methods\" value : \"GET, POST, PUT, DELETE, OPTIONS\" - name : \"Access-Control-Allow-Headers\" value : \"Content-Type, Authorization\" - name : \"Access-Control-Max-Age\" value : \"86400\" - type : URLRewrite urlRewrite : path : type : ReplacePrefixMatch replacePrefixMatch : \"/\" backendRefs : - name : go-svc port : 5000 HTTPS/TLS Configuration: # Updated Gateway with TLS apiVersion : gateway.networking.k8s.io/v1 kind : Gateway metadata : name : traefik spec : gatewayClassName : traefik listeners : - name : web port : 80 protocol : HTTP hostname : \"*.example-app.com\" - name : web-secure port : 443 protocol : HTTPS hostname : \"*.example-app.com\" tls : mode : Terminate certificateRefs : - name : secret-tls # HTTPRoute targeting TLS listener apiVersion : gateway.networking.k8s.io/v1 kind : HTTPRoute metadata : name : go-tls spec : parentRefs : - name : traefik sectionName : web-secure # Target HTTPS listener hostnames : - \"example-app.com\" rules : - matches : - path : type : PathPrefix value : \"/api/go\" filters : - type : URLRewrite urlRewrite : path : type : ReplacePrefixMatch replacePrefixMatch : \"/\" backendRefs : - name : go-svc port : 5000 Other Route Types (Experimental) TCPRoute: apiVersion : gateway.networking.k8s.io/v1alpha2 kind : TCPRoute metadata : name : postgres-route spec : parentRefs : - name : traefik rules : - backendRefs : - name : postgres-service port : 5432 TLSRoute: apiVersion : gateway.networking.k8s.io/v1alpha2 kind : TLSRoute metadata : name : tls-passthrough spec : parentRefs : - name : traefik hostnames : - \"secure.example-app.com\" rules : - backendRefs : - name : backend-service port : 8443 Supporting Resources TLS Secret: apiVersion : v1 kind : Secret metadata : name : secret-tls type : kubernetes.io/tls data : tls.crt : BASE64_ENCODED_CERT tls.key : BASE64_ENCODED_KEY ReferenceGrant (Cross-Namespace): apiVersion : gateway.networking.k8s.io/v1beta1 kind : ReferenceGrant metadata : name : allow-routes namespace : gateway-namespace spec : from : - group : gateway.networking.k8s.io kind : HTTPRoute namespace : app-namespace to : - group : gateway.networking.k8s.io kind : Gateway name : shared-gateway Testing Commands Local Testing with kind: # Update /etc/hosts 127 .0.0.1 example-app.com 127 .0.0.1 example-app-go.com 127 .0.0.1 example-app-python.com # Port forward kubectl -n traefik port-forward svc/traefik 80 kubectl -n traefik port-forward svc/traefik 443 # Test HTTP curl -H \"Host: example-app-go.com\" http://localhost curl -H \"Host: example-app-python.com\" http://localhost curl -H \"Host: example-app.com\" http://localhost/api/go # Test HTTPS curl -k -H \"Host: example-app.com\" https://localhost:443/api/go Key Concepts Traffic Flow: Request \u2192 Gateway (listener) \u2192 HTTPRoute (match) \u2192 Filters \u2192 Backend Service Required Fields: GatewayClass : controllerName Gateway : gatewayClassName , listeners (with name , port , protocol ) HTTPRoute : parentRefs , rules (with backendRefs ) Optional But Important: hostnames \u2192 Virtual hosting tls \u2192 HTTPS/TLS allowedRoutes \u2192 Security boundaries filters \u2192 Transformations sectionName \u2192 Target specific listener Path Matching Types: Exact \u2192 Only exact path PathPrefix \u2192 Path and everything under it RegularExpression \u2192 Regex pattern Filter Types: RequestHeaderModifier \u2192 Modify request headers ResponseHeaderModifier \u2192 Modify response headers URLRewrite \u2192 Rewrite URL path RequestRedirect \u2192 Redirect to different URL RequestMirror \u2192 Mirror traffic ExtensionRef \u2192 Custom filters Infrastructure Labels apiVersion : gateway.networking.k8s.io/v1 kind : Gateway metadata : name : cloud-gateway labels : gateway.networking.k8s.io/infrastructure : \"aws-nlb\" environment : \"production\" annotations : service.beta.kubernetes.io/aws-load-balancer-type : \"nlb\" service.beta.kubernetes.io/aws-load-balancer-scheme : \"internet-facing\" Purpose: Propagate labels/annotations to cloud infrastructure. Available Controllers Traefik \u2192 Used in README, simple setup Envoy \u2192 High-performance proxy Istio \u2192 Service mesh integration NGINX Fabric \u2192 NGINX-based implementation","title":"Kubernetes Gateway API"},{"location":"08-gateway-api/gateway-api/#kubernetes-gateway-api","text":"","title":"Kubernetes Gateway API"},{"location":"08-gateway-api/gateway-api/#core-architecture","text":"","title":"Core Architecture"},{"location":"08-gateway-api/gateway-api/#three-tier-model","text":"GatewayClass \u2192 Which controller implementation to use Gateway \u2192 Actual load balancer with listeners HTTPRoute/TCPRoute/TLSRoute \u2192 Routing rules and logic","title":"Three-Tier Model:"},{"location":"08-gateway-api/gateway-api/#complete-yaml-examples-all-fields-explained","text":"","title":"Complete YAML Examples (All Fields Explained)"},{"location":"08-gateway-api/gateway-api/#gatewayclass-all-possible-fields","text":"apiVersion : gateway.networking.k8s.io/v1 kind : GatewayClass metadata : name : traefik spec : # REQUIRED: Which controller implements this controllerName : \"traefik.io/gateway-controller\" # OPTIONAL: Description description : \"Traefik GatewayClass for production\" # OPTIONAL: Controller-specific parameters parametersRef : name : traefik-config group : traefik.io kind : GatewayClassConfig","title":"GatewayClass - All Possible Fields:"},{"location":"08-gateway-api/gateway-api/#gateway-complete-with-all-listeners","text":"apiVersion : gateway.networking.k8s.io/v1 kind : Gateway metadata : name : traefik spec : # REQUIRED: Which GatewayClass to use gatewayClassName : traefik # REQUIRED: At least one listener listeners : - name : http-public port : 80 protocol : HTTP # OPTIONAL: Restrict hostnames hostname : \"*.example-app.com\" # OPTIONAL: TLS (for HTTPS/TLS listeners only) # tls: # mode: Terminate # certificateRefs: [] # OPTIONAL: Which namespaces can attach routes allowedRoutes : namespaces : from : Same # Same, All, or Selector # selector: # When from: Selector # matchLabels: # shared-gateway: \"true\" # HTTPS Listener Example - name : https-secure port : 443 protocol : HTTPS hostname : \"example-app.com\" # REQUIRED for HTTPS: TLS configuration tls : mode : Terminate # or Passthrough certificateRefs : - name : secret-tls kind : Secret group : \"\" # options: # Optional TLS options # cipherSuites: [] # minVersion: \"TLSv1.2\" allowedRoutes : namespaces : from : All # TCP Listener Example (Experimental) - name : tcp-database port : 5432 protocol : TCP allowedRoutes : namespaces : from : Same Listener Protocol Types: - HTTP \u2192 For HTTPRoute - HTTPS \u2192 For HTTPRoute with TLS - TLS \u2192 For TLSRoute (passthrough) - TCP \u2192 For TCPRoute - UDP \u2192 For UDPRoute (experimental) TLS Modes: - Terminate \u2192 TLS ends at Gateway (decrypts) - Passthrough \u2192 TLS continues to backend","title":"Gateway - Complete with All Listeners:"},{"location":"08-gateway-api/gateway-api/#httproute-complete-with-all-features","text":"","title":"HTTPRoute - Complete with All Features"},{"location":"08-gateway-api/gateway-api/#route-by-hostname-from-readme","text":"apiVersion : gateway.networking.k8s.io/v1 kind : HTTPRoute metadata : name : go spec : # REQUIRED: Attach to which Gateway parentRefs : - name : traefik # namespace: default # If Gateway in different namespace # sectionName: web # Target specific listener # port: 80 # Target specific port # OPTIONAL: Which hostnames to match hostnames : - \"example-app-go.com\" - \"*.test.example-app-go.com\" # Wildcard subdomains # REQUIRED: At least one rule rules : - matches : # Path matching (choose one type) - path : type : PathPrefix # Exact, PathPrefix, or RegularExpression value : \"/\" # OPTIONAL: Header matching # headers: # - type: Exact # or RegularExpression # name: \"X-API-Version\" # value: \"v2\" # OPTIONAL: Query parameter matching # queryParams: # - type: Exact # name: \"debug\" # value: \"true\" # OPTIONAL: HTTP method matching # method: \"GET\" # GET, POST, PUT, DELETE, etc. # OPTIONAL: Request filters (applied in order) filters : # - type: RequestHeaderModifier # requestHeaderModifier: # set: # Overwrite if exists # - name: \"X-Request-ID\" # value: \"{{uuid}}\" # add: # Add if not exists # - name: \"X-Forwarded-For\" # value: \"$remote_addr\" # remove: # Remove headers # - \"X-Secret-Header\" # - type: ResponseHeaderModifier # responseHeaderModifier: # add: # - name: \"Cache-Control\" # value: \"max-age=3600\" # - type: RequestRedirect # requestRedirect: # scheme: \"https\" # hostname: \"secure.example.com\" # port: 443 # statusCode: 301 # - type: URLRewrite # urlRewrite: # path: # type: ReplacePrefixMatch # or ReplaceFullPath # replacePrefixMatch: \"/v2\" # - type: RequestMirror # requestMirror: # backendRef: # name: audit-service # port: 8080 # - type: ExtensionRef # extensionRef: # name: custom-filter # kind: CustomFilter # group: example.com # REQUIRED: Where to send traffic backendRefs : - name : go-svc port : 5000 # weight: 100 # For traffic splitting (0-100) # filters: [] # Backend-specific filters # Multiple backends for traffic splitting # - name: go-svc-v2 # port: 5000 # weight: 20","title":"Route by Hostname (From README):"},{"location":"08-gateway-api/gateway-api/#route-by-path-exact-match","text":"apiVersion : gateway.networking.k8s.io/v1 kind : HTTPRoute metadata : name : python-exact spec : parentRefs : - name : traefik hostnames : - \"example-app-python.com\" rules : - matches : - path : type : Exact # Only \"/\" matches value : \"/\" backendRefs : - name : python-svc port : 5000","title":"Route by Path - Exact Match:"},{"location":"08-gateway-api/gateway-api/#route-by-path-prefix-match","text":"apiVersion : gateway.networking.k8s.io/v1 kind : HTTPRoute metadata : name : python-prefix spec : parentRefs : - name : traefik hostnames : - \"example-app-python.com\" rules : - matches : - path : type : PathPrefix # \"/\", \"/api\", \"/api/users\" all match value : \"/\" backendRefs : - name : python-svc port : 5000","title":"Route by Path - Prefix Match:"},{"location":"08-gateway-api/gateway-api/#url-rewrite-pattern-api-gateway","text":"apiVersion : gateway.networking.k8s.io/v1 kind : HTTPRoute metadata : name : api-gateway spec : parentRefs : - name : traefik hostnames : - \"example-app.com\" rules : # Python service - matches : - path : type : PathPrefix value : \"/api/python\" filters : - type : URLRewrite urlRewrite : path : type : ReplacePrefixMatch replacePrefixMatch : \"/\" backendRefs : - name : python-svc port : 5000 # Go service - matches : - path : type : PathPrefix value : \"/api/go\" filters : - type : URLRewrite urlRewrite : path : type : ReplacePrefixMatch replacePrefixMatch : \"/\" backendRefs : - name : go-svc port : 5000 URL Rewrite Types: - ReplacePrefixMatch \u2192 Replace matched prefix only - ReplaceFullPath \u2192 Replace entire path","title":"URL Rewrite Pattern (API Gateway):"},{"location":"08-gateway-api/gateway-api/#header-modification-cors-example","text":"apiVersion : gateway.networking.k8s.io/v1 kind : HTTPRoute metadata : name : go-cors spec : parentRefs : - name : traefik hostnames : - \"example-app.com\" rules : - matches : - path : type : PathPrefix value : \"/api/go\" filters : - type : ResponseHeaderModifier responseHeaderModifier : add : - name : \"Access-Control-Allow-Origin\" value : \"*\" - name : \"Access-Control-Allow-Methods\" value : \"GET, POST, PUT, DELETE, OPTIONS\" - name : \"Access-Control-Allow-Headers\" value : \"Content-Type, Authorization\" - name : \"Access-Control-Max-Age\" value : \"86400\" - type : URLRewrite urlRewrite : path : type : ReplacePrefixMatch replacePrefixMatch : \"/\" backendRefs : - name : go-svc port : 5000","title":"Header Modification (CORS Example):"},{"location":"08-gateway-api/gateway-api/#httpstls-configuration","text":"# Updated Gateway with TLS apiVersion : gateway.networking.k8s.io/v1 kind : Gateway metadata : name : traefik spec : gatewayClassName : traefik listeners : - name : web port : 80 protocol : HTTP hostname : \"*.example-app.com\" - name : web-secure port : 443 protocol : HTTPS hostname : \"*.example-app.com\" tls : mode : Terminate certificateRefs : - name : secret-tls # HTTPRoute targeting TLS listener apiVersion : gateway.networking.k8s.io/v1 kind : HTTPRoute metadata : name : go-tls spec : parentRefs : - name : traefik sectionName : web-secure # Target HTTPS listener hostnames : - \"example-app.com\" rules : - matches : - path : type : PathPrefix value : \"/api/go\" filters : - type : URLRewrite urlRewrite : path : type : ReplacePrefixMatch replacePrefixMatch : \"/\" backendRefs : - name : go-svc port : 5000","title":"HTTPS/TLS Configuration:"},{"location":"08-gateway-api/gateway-api/#other-route-types-experimental","text":"","title":"Other Route Types (Experimental)"},{"location":"08-gateway-api/gateway-api/#tcproute","text":"apiVersion : gateway.networking.k8s.io/v1alpha2 kind : TCPRoute metadata : name : postgres-route spec : parentRefs : - name : traefik rules : - backendRefs : - name : postgres-service port : 5432","title":"TCPRoute:"},{"location":"08-gateway-api/gateway-api/#tlsroute","text":"apiVersion : gateway.networking.k8s.io/v1alpha2 kind : TLSRoute metadata : name : tls-passthrough spec : parentRefs : - name : traefik hostnames : - \"secure.example-app.com\" rules : - backendRefs : - name : backend-service port : 8443","title":"TLSRoute:"},{"location":"08-gateway-api/gateway-api/#supporting-resources","text":"","title":"Supporting Resources"},{"location":"08-gateway-api/gateway-api/#tls-secret","text":"apiVersion : v1 kind : Secret metadata : name : secret-tls type : kubernetes.io/tls data : tls.crt : BASE64_ENCODED_CERT tls.key : BASE64_ENCODED_KEY","title":"TLS Secret:"},{"location":"08-gateway-api/gateway-api/#referencegrant-cross-namespace","text":"apiVersion : gateway.networking.k8s.io/v1beta1 kind : ReferenceGrant metadata : name : allow-routes namespace : gateway-namespace spec : from : - group : gateway.networking.k8s.io kind : HTTPRoute namespace : app-namespace to : - group : gateway.networking.k8s.io kind : Gateway name : shared-gateway","title":"ReferenceGrant (Cross-Namespace):"},{"location":"08-gateway-api/gateway-api/#testing-commands","text":"","title":"Testing Commands"},{"location":"08-gateway-api/gateway-api/#local-testing-with-kind","text":"# Update /etc/hosts 127 .0.0.1 example-app.com 127 .0.0.1 example-app-go.com 127 .0.0.1 example-app-python.com # Port forward kubectl -n traefik port-forward svc/traefik 80 kubectl -n traefik port-forward svc/traefik 443 # Test HTTP curl -H \"Host: example-app-go.com\" http://localhost curl -H \"Host: example-app-python.com\" http://localhost curl -H \"Host: example-app.com\" http://localhost/api/go # Test HTTPS curl -k -H \"Host: example-app.com\" https://localhost:443/api/go","title":"Local Testing with kind:"},{"location":"08-gateway-api/gateway-api/#key-concepts","text":"","title":"Key Concepts"},{"location":"08-gateway-api/gateway-api/#traffic-flow","text":"Request \u2192 Gateway (listener) \u2192 HTTPRoute (match) \u2192 Filters \u2192 Backend Service","title":"Traffic Flow:"},{"location":"08-gateway-api/gateway-api/#required-fields","text":"GatewayClass : controllerName Gateway : gatewayClassName , listeners (with name , port , protocol ) HTTPRoute : parentRefs , rules (with backendRefs )","title":"Required Fields:"},{"location":"08-gateway-api/gateway-api/#optional-but-important","text":"hostnames \u2192 Virtual hosting tls \u2192 HTTPS/TLS allowedRoutes \u2192 Security boundaries filters \u2192 Transformations sectionName \u2192 Target specific listener","title":"Optional But Important:"},{"location":"08-gateway-api/gateway-api/#path-matching-types","text":"Exact \u2192 Only exact path PathPrefix \u2192 Path and everything under it RegularExpression \u2192 Regex pattern","title":"Path Matching Types:"},{"location":"08-gateway-api/gateway-api/#filter-types","text":"RequestHeaderModifier \u2192 Modify request headers ResponseHeaderModifier \u2192 Modify response headers URLRewrite \u2192 Rewrite URL path RequestRedirect \u2192 Redirect to different URL RequestMirror \u2192 Mirror traffic ExtensionRef \u2192 Custom filters","title":"Filter Types:"},{"location":"08-gateway-api/gateway-api/#infrastructure-labels","text":"apiVersion : gateway.networking.k8s.io/v1 kind : Gateway metadata : name : cloud-gateway labels : gateway.networking.k8s.io/infrastructure : \"aws-nlb\" environment : \"production\" annotations : service.beta.kubernetes.io/aws-load-balancer-type : \"nlb\" service.beta.kubernetes.io/aws-load-balancer-scheme : \"internet-facing\" Purpose: Propagate labels/annotations to cloud infrastructure.","title":"Infrastructure Labels"},{"location":"08-gateway-api/gateway-api/#available-controllers","text":"Traefik \u2192 Used in README, simple setup Envoy \u2192 High-performance proxy Istio \u2192 Service mesh integration NGINX Fabric \u2192 NGINX-based implementation","title":"Available Controllers"},{"location":"08-gateway-api/question/","text":"Gateway API Practice Questions 1. Simple Host-Based Routing You need to route traffic based on hostnames to different services: - app.example.com \u2192 frontend-service:8080 - api.example.com \u2192 backend-service:3000 Both should use the existing Gateway named main-gateway on port 80. Task: Write the HTTPRoute YAML(s) to implement this. 2. Path Rewriting (API Gateway Pattern) You have a single domain services.company.com with these requirements: - /shop/products \u2192 product-service:8080 (should receive /products ) - /shop/cart \u2192 cart-service:9090 (should receive /cart ) - /auth/login \u2192 auth-service:7070 (should receive /login ) Task: Create a single HTTPRoute that rewrites paths to remove the /shop and /auth prefixes. 3. Path Redirects (Migration) You're migrating from an old path structure to a new one: - /old/dashboard \u2192 redirect to /new/dashboard (301 permanent) - /old/api/v1 \u2192 redirect to /api/v2 (301 permanent) - /temp/redirect \u2192 redirect to /new/temp (302 temporary) Task: Write an HTTPRoute that handles these redirects. 4. TLS Configuration with Multiple Hostnames You have a Gateway secure-gateway with an HTTPS listener on port 443. You need to: 1. Configure TLS with a certificate secret named wildcard-tls 2. Route traffic for app.company.com and api.company.com to their respective services 3. Ensure only HTTPS traffic is accepted (no HTTP) Task: Write both the Gateway update and HTTPRoute(s) for TLS configuration. 5. Exact vs Prefix Path Matching You have these specific routing requirements: - /api/health (exact) \u2192 health-check:8080 - /api/users (prefix) \u2192 user-service:3000 (matches /api/users , /api/users/123 , etc.) - /admin (exact) \u2192 admin-panel:9000 - /admin/config (exact) \u2192 config-service:4000 (higher priority than /admin prefix) Task: Create an HTTPRoute with proper path matching types and rule ordering.","title":"Gateway API Practice Questions"},{"location":"08-gateway-api/question/#gateway-api-practice-questions","text":"","title":"Gateway API Practice Questions"},{"location":"08-gateway-api/question/#1-simple-host-based-routing","text":"You need to route traffic based on hostnames to different services: - app.example.com \u2192 frontend-service:8080 - api.example.com \u2192 backend-service:3000 Both should use the existing Gateway named main-gateway on port 80. Task: Write the HTTPRoute YAML(s) to implement this.","title":"1. Simple Host-Based Routing"},{"location":"08-gateway-api/question/#2-path-rewriting-api-gateway-pattern","text":"You have a single domain services.company.com with these requirements: - /shop/products \u2192 product-service:8080 (should receive /products ) - /shop/cart \u2192 cart-service:9090 (should receive /cart ) - /auth/login \u2192 auth-service:7070 (should receive /login ) Task: Create a single HTTPRoute that rewrites paths to remove the /shop and /auth prefixes.","title":"2. Path Rewriting (API Gateway Pattern)"},{"location":"08-gateway-api/question/#3-path-redirects-migration","text":"You're migrating from an old path structure to a new one: - /old/dashboard \u2192 redirect to /new/dashboard (301 permanent) - /old/api/v1 \u2192 redirect to /api/v2 (301 permanent) - /temp/redirect \u2192 redirect to /new/temp (302 temporary) Task: Write an HTTPRoute that handles these redirects.","title":"3. Path Redirects (Migration)"},{"location":"08-gateway-api/question/#4-tls-configuration-with-multiple-hostnames","text":"You have a Gateway secure-gateway with an HTTPS listener on port 443. You need to: 1. Configure TLS with a certificate secret named wildcard-tls 2. Route traffic for app.company.com and api.company.com to their respective services 3. Ensure only HTTPS traffic is accepted (no HTTP) Task: Write both the Gateway update and HTTPRoute(s) for TLS configuration.","title":"4. TLS Configuration with Multiple Hostnames"},{"location":"08-gateway-api/question/#5-exact-vs-prefix-path-matching","text":"You have these specific routing requirements: - /api/health (exact) \u2192 health-check:8080 - /api/users (prefix) \u2192 user-service:3000 (matches /api/users , /api/users/123 , etc.) - /admin (exact) \u2192 admin-panel:9000 - /admin/config (exact) \u2192 config-service:4000 (higher priority than /admin prefix) Task: Create an HTTPRoute with proper path matching types and rule ordering.","title":"5. Exact vs Prefix Path Matching"},{"location":"10-scheduling/nodename/","text":"NodeName Notes on nodeName What it does: - Forces pod to specific node by name - Skips normal scheduler logic - Direct assignment only Example: apiVersion : v1 kind : Pod metadata : name : example-pod spec : nodeName : target-node-1 containers : - name : main image : nginx Use cases: - Hardware-specific nodes - Local storage access - Testing purposes Limitations: - No failover if node fails - Pod stays Pending if node missing - No resource validation Check placement: kubectl get pod node-monitor -o wide Question You have a 3-node Kubernetes cluster: - control-plane - node-01 - node-02 You need to deploy a monitoring pod that must run on node-01 because it has special monitoring tools installed. Write a Pod YAML manifest that: 1. Pod name: node-monitor 2. Image: ubuntu:latest 3. Command: [\"sleep\", \"infinity\"] 4. Label: monitor: system 5. Force to node: node-01","title":"NodeName"},{"location":"10-scheduling/nodename/#nodename","text":"","title":"NodeName"},{"location":"10-scheduling/nodename/#notes-on-nodename","text":"What it does: - Forces pod to specific node by name - Skips normal scheduler logic - Direct assignment only Example: apiVersion : v1 kind : Pod metadata : name : example-pod spec : nodeName : target-node-1 containers : - name : main image : nginx Use cases: - Hardware-specific nodes - Local storage access - Testing purposes Limitations: - No failover if node fails - Pod stays Pending if node missing - No resource validation Check placement: kubectl get pod node-monitor -o wide","title":"Notes on nodeName"},{"location":"10-scheduling/nodename/#question","text":"You have a 3-node Kubernetes cluster: - control-plane - node-01 - node-02 You need to deploy a monitoring pod that must run on node-01 because it has special monitoring tools installed. Write a Pod YAML manifest that: 1. Pod name: node-monitor 2. Image: ubuntu:latest 3. Command: [\"sleep\", \"infinity\"] 4. Label: monitor: system 5. Force to node: node-01","title":"Question"},{"location":"10-scheduling/nodeselector/","text":"NodeSelector What is nodeSelector? nodeSelector is a field in a Pod spec that selects nodes based on their labels . Unlike nodeName which bypasses the scheduler, nodeSelector works with the scheduler to match pods to nodes with specific labels. How It Works apiVersion : v1 kind : Pod metadata : name : example-pod spec : nodeSelector : # \u2190 This is the key field disktype : ssd # Node must have label disktype=ssd region : us-east # Node must have label region=us-east containers : - name : main image : nginx The scheduler will only place this pod on nodes that have ALL the labels specified in nodeSelector . Key Differences: nodeSelector vs nodeName Feature nodeName nodeSelector Scheduler Bypasses scheduler Works with scheduler Flexibility Hardcoded node name Uses labels (more flexible) Failover No failover Can reschedule to other matching nodes Validation No resource checking Normal scheduler validation Production Use Rarely used Commonly used Step-by-Step Usage 1. Label Your Nodes # Add labels to nodes kubectl label nodes node-01 disktype = ssd kubectl label nodes node-01 region = us-east kubectl label nodes node-02 disktype = hdd kubectl label nodes node-02 region = us-west # Verify labels kubectl get nodes --show-labels 2. Create Pod with nodeSelector apiVersion : v1 kind : Pod metadata : name : web-app spec : nodeSelector : disktype : ssd region : us-east containers : - name : app image : nginx:alpine 3. Verify Scheduling # Check which node the pod landed on kubectl get pod web-app -o wide # Describe the pod to see scheduling details kubectl describe pod web-app Common Use Cases Hardware Requirements nodeSelector : gpu : \"true\" memory : \"high\" Environment/Region nodeSelector : environment : production zone : zone-a Team/Project Isolation nodeSelector : team : data-science project : ml-training Limitations AND Logic Only : All labels must match (cannot do OR logic) No Preferences : Cannot say \"prefer SSD, but HDD is okay\" No Complex Rules : Cannot specify \"not equal to\" or \"exists\" operators Best Practices Use Meaningful Labels : disktype:ssd instead of type:1 Document Labels : Keep a list of node labels your team uses Combine with Resources : Use with resource requests/limits Test Label Matching : Verify labels exist before deploying Troubleshooting Pod Stays Pending? # Check if any nodes have the required labels kubectl get nodes -l disktype = ssd,region = us-east # Check pod events kubectl describe pod [ pod-name ] # Check node capacity kubectl describe node [ node-name ] Remove Labels: kubectl label node node-01 disktype- Practice Question 2 You have a Kubernetes cluster with the following nodes and labels: Node: worker-1 Labels: environment=production, storage=ssd, zone=us-east-1a Node: worker-2 Labels: environment=staging, storage=hdd, zone=us-east-1b Node: worker-3 Labels: environment=production, storage=ssd, zone=us-east-1b Create a Pod YAML for a database application that: 1. Pod name : production-db 2. Image : postgres:14 3. Label the pod : app: database 4. Environment variable : POSTGRES_PASSWORD: secret123 5. Must run only on nodes with: - environment=production - storage=ssd 6. Container port : 5432 Write your YAML solution below:","title":"NodeSelector"},{"location":"10-scheduling/nodeselector/#nodeselector","text":"","title":"NodeSelector"},{"location":"10-scheduling/nodeselector/#what-is-nodeselector","text":"nodeSelector is a field in a Pod spec that selects nodes based on their labels . Unlike nodeName which bypasses the scheduler, nodeSelector works with the scheduler to match pods to nodes with specific labels.","title":"What is nodeSelector?"},{"location":"10-scheduling/nodeselector/#how-it-works","text":"apiVersion : v1 kind : Pod metadata : name : example-pod spec : nodeSelector : # \u2190 This is the key field disktype : ssd # Node must have label disktype=ssd region : us-east # Node must have label region=us-east containers : - name : main image : nginx The scheduler will only place this pod on nodes that have ALL the labels specified in nodeSelector .","title":"How It Works"},{"location":"10-scheduling/nodeselector/#key-differences-nodeselector-vs-nodename","text":"Feature nodeName nodeSelector Scheduler Bypasses scheduler Works with scheduler Flexibility Hardcoded node name Uses labels (more flexible) Failover No failover Can reschedule to other matching nodes Validation No resource checking Normal scheduler validation Production Use Rarely used Commonly used","title":"Key Differences: nodeSelector vs nodeName"},{"location":"10-scheduling/nodeselector/#step-by-step-usage","text":"","title":"Step-by-Step Usage"},{"location":"10-scheduling/nodeselector/#1-label-your-nodes","text":"# Add labels to nodes kubectl label nodes node-01 disktype = ssd kubectl label nodes node-01 region = us-east kubectl label nodes node-02 disktype = hdd kubectl label nodes node-02 region = us-west # Verify labels kubectl get nodes --show-labels","title":"1. Label Your Nodes"},{"location":"10-scheduling/nodeselector/#2-create-pod-with-nodeselector","text":"apiVersion : v1 kind : Pod metadata : name : web-app spec : nodeSelector : disktype : ssd region : us-east containers : - name : app image : nginx:alpine","title":"2. Create Pod with nodeSelector"},{"location":"10-scheduling/nodeselector/#3-verify-scheduling","text":"# Check which node the pod landed on kubectl get pod web-app -o wide # Describe the pod to see scheduling details kubectl describe pod web-app","title":"3. Verify Scheduling"},{"location":"10-scheduling/nodeselector/#common-use-cases","text":"Hardware Requirements nodeSelector : gpu : \"true\" memory : \"high\" Environment/Region nodeSelector : environment : production zone : zone-a Team/Project Isolation nodeSelector : team : data-science project : ml-training","title":"Common Use Cases"},{"location":"10-scheduling/nodeselector/#limitations","text":"AND Logic Only : All labels must match (cannot do OR logic) No Preferences : Cannot say \"prefer SSD, but HDD is okay\" No Complex Rules : Cannot specify \"not equal to\" or \"exists\" operators","title":"Limitations"},{"location":"10-scheduling/nodeselector/#best-practices","text":"Use Meaningful Labels : disktype:ssd instead of type:1 Document Labels : Keep a list of node labels your team uses Combine with Resources : Use with resource requests/limits Test Label Matching : Verify labels exist before deploying","title":"Best Practices"},{"location":"10-scheduling/nodeselector/#troubleshooting","text":"Pod Stays Pending? # Check if any nodes have the required labels kubectl get nodes -l disktype = ssd,region = us-east # Check pod events kubectl describe pod [ pod-name ] # Check node capacity kubectl describe node [ node-name ] Remove Labels: kubectl label node node-01 disktype-","title":"Troubleshooting"},{"location":"10-scheduling/nodeselector/#practice-question-2","text":"You have a Kubernetes cluster with the following nodes and labels: Node: worker-1 Labels: environment=production, storage=ssd, zone=us-east-1a Node: worker-2 Labels: environment=staging, storage=hdd, zone=us-east-1b Node: worker-3 Labels: environment=production, storage=ssd, zone=us-east-1b Create a Pod YAML for a database application that: 1. Pod name : production-db 2. Image : postgres:14 3. Label the pod : app: database 4. Environment variable : POSTGRES_PASSWORD: secret123 5. Must run only on nodes with: - environment=production - storage=ssd 6. Container port : 5432 Write your YAML solution below:","title":"Practice Question 2"},{"location":"10-scheduling/podAffinity-nodeAffinity-deployments/notes/","text":"Kubernetes Scheduling: Pod & Node Affinity - Theory & Practice 1. Theoretical Foundation: How Affinity Works Key Concepts: Labels : Key-value pairs attached to Kubernetes objects (nodes, pods) Node Affinity : Rules based on NODE LABELS Pod Affinity : Rules based on EXISTING POD LABELS Node Affinity Operators Explained: Operator What It Does Example When to Use In Value must be in list zone in [\"us-east-1a\", \"us-east-1b\"] Specific values allowed NotIn Value must NOT be in list node-type not in [\"spot\", \"preemptible\"] Exclude certain node types Exists Label key must exist gpu exists Check for feature presence DoesNotExist Label key must NOT exist maintenance does not exist Avoid problematic nodes Gt , Lt Numeric comparison memory-gb > 16 Resource-based selection Important Theory Points: NodeSelectorTerms Logic : Within a term : All conditions must be true (AND logic) Between terms : Any term can be true (OR logic) Required vs Preferred : requiredDuringScheduling : MUST be satisfied (hard rule) preferredDuringScheduling : SHOULD be satisfied (soft rule, weighted 1-100) IgnoredDuringExecution : Once scheduled, rules aren't re-evaluated if labels change 2. Pod Affinity Theory: Relationships Between Pods Three Types of Pod Relationships: Pod Affinity : \"Run near these pods\" (attraction) Pod Anti-Affinity : \"Don't run near these pods\" (repulsion) Topology : Defines what \"near\" means Topology Keys Theory: A topology key is a node label that defines a grouping domain: kubernetes.io/hostname = Different physical/virtual machines topology.kubernetes.io/zone = Different availability zones topology.kubernetes.io/region = Different geographic regions Custom keys = Your own grouping logic (e.g., rack , row , datacenter ) How Pod Affinity Works: podAntiAffinity : requiredDuringSchedulingIgnoredDuringExecution : - labelSelector : matchLabels : app : web # Look for pods with this label topologyKey : kubernetes.io/hostname # Group by node Translation : \"Don't schedule this pod on any node that already has a pod with label app=web \" 3. Complete Practical Example with Theory Applied apiVersion : apps/v1 kind : Deployment metadata : name : example-app spec : replicas : 3 template : metadata : labels : app : example tier : backend spec : affinity : # NODE AFFINITY: Where can pods go? nodeAffinity : # HARD REQUIREMENT: Must be satisfied requiredDuringSchedulingIgnoredDuringExecution : nodeSelectorTerms : # Term 1: Production environment - matchExpressions : - key : environment operator : In values : [ \"production\" ] - key : available operator : Exists # Term 2: OR staging with SSD - matchExpressions : - key : environment operator : In values : [ \"staging\" ] - key : storage-type operator : In values : [ \"ssd\" ] # SOFT PREFERENCE: Try to satisfy preferredDuringSchedulingIgnoredDuringExecution : - weight : 90 # Higher weight = more important preference : matchExpressions : - key : zone operator : In values : [ \"us-east-1a\" ] # POD ANTI-AFFINITY: How pods relate to each other podAntiAffinity : # HARD: Don't put pods on same node requiredDuringSchedulingIgnoredDuringExecution : - labelSelector : matchLabels : app : example # Look for our own pods topologyKey : kubernetes.io/hostname # Different nodes # SOFT: Try to spread across zones preferredDuringSchedulingIgnoredDuringExecution : - weight : 70 podAffinityTerm : labelSelector : matchLabels : app : example topologyKey : topology.kubernetes.io/zone # Different zones containers : - name : app image : nginx Theory Applied: 1. Node Selection : Pods can run in production (any storage) OR staging (only SSD) 2. Preference : Prefer zone us-east-1a if possible 3. Pod Placement : Never on same node, try for different zones 4. Result : High availability across failure domains 4. When to Use Each Feature - Decision Guide Use Node Affinity When: Pod needs specific hardware (GPU, SSD, memory) Pod needs to run in specific region/zone You want to dedicate nodes to certain workloads You need to exclude certain node types Use Pod Affinity When: Related pods should be co-located (app + cache) You need data locality (pod near its data) Communication latency matters Use Pod Anti-Affinity When: You need high availability (spread across nodes/zones) You want to avoid resource contention You have stateful applications that shouldn't share nodes Use Topology Spread Constraints (Alternative): For simpler \"spread evenly\" requirements When you want built-in balancing For zone/region spreading without complex rules 5. Practice Questions Question 1: Basic Node Affinity Create a Deployment that: 1. Name: cache-app 2. Replicas: 2 3. Image: redis:alpine 4. Must run on nodes with label memory-type=high 5. Should prefer nodes with label storage=ssd (weight: 80) 6. Pods should not be on the same node Question 2: Complex Node Selection Create a StatefulSet that: 1. Name: postgres-cluster 2. Replicas: 3 3. Image: postgres:15 4. Requirements: - Must be in production environment - Must have fast storage (any label with \"fast\" in key) - Must NOT be on spot instances - Must NOT be under maintenance 5. Each pod should have 10Gi persistent storage Question 3: Pod Anti-Affinity for HA Create a Deployment that: 1. Name: web-frontend 2. Replicas: 5 3. Image: nginx:latest 4. Requirements: - Hard rule: Pods must be on different nodes - Soft rule: Try to spread across zones (weight: 100) - Pods should prefer to run near cache pods (weight: 60) 5. Add readiness and liveness probes Question 4: Complete Production Setup Create a complete application with: 1. Deployment: api-server (3 replicas) 2. Deployment: redis-cache (2 replicas) 3. Requirements: - Both must run in environment=production - API pods should be on same nodes as cache pods - API pods should be spread across zones - Cache pods should be on different nodes - Prefer nodes with label instance-type=large 4. Include all necessary: resources, probes, environment variables","title":"Kubernetes Scheduling: Pod &amp; Node Affinity - Theory &amp; Practice"},{"location":"10-scheduling/podAffinity-nodeAffinity-deployments/notes/#kubernetes-scheduling-pod-node-affinity-theory-practice","text":"","title":"Kubernetes Scheduling: Pod &amp; Node Affinity - Theory &amp; Practice"},{"location":"10-scheduling/podAffinity-nodeAffinity-deployments/notes/#1-theoretical-foundation-how-affinity-works","text":"","title":"1. Theoretical Foundation: How Affinity Works"},{"location":"10-scheduling/podAffinity-nodeAffinity-deployments/notes/#key-concepts","text":"Labels : Key-value pairs attached to Kubernetes objects (nodes, pods) Node Affinity : Rules based on NODE LABELS Pod Affinity : Rules based on EXISTING POD LABELS","title":"Key Concepts:"},{"location":"10-scheduling/podAffinity-nodeAffinity-deployments/notes/#node-affinity-operators-explained","text":"Operator What It Does Example When to Use In Value must be in list zone in [\"us-east-1a\", \"us-east-1b\"] Specific values allowed NotIn Value must NOT be in list node-type not in [\"spot\", \"preemptible\"] Exclude certain node types Exists Label key must exist gpu exists Check for feature presence DoesNotExist Label key must NOT exist maintenance does not exist Avoid problematic nodes Gt , Lt Numeric comparison memory-gb > 16 Resource-based selection","title":"Node Affinity Operators Explained:"},{"location":"10-scheduling/podAffinity-nodeAffinity-deployments/notes/#important-theory-points","text":"NodeSelectorTerms Logic : Within a term : All conditions must be true (AND logic) Between terms : Any term can be true (OR logic) Required vs Preferred : requiredDuringScheduling : MUST be satisfied (hard rule) preferredDuringScheduling : SHOULD be satisfied (soft rule, weighted 1-100) IgnoredDuringExecution : Once scheduled, rules aren't re-evaluated if labels change","title":"Important Theory Points:"},{"location":"10-scheduling/podAffinity-nodeAffinity-deployments/notes/#2-pod-affinity-theory-relationships-between-pods","text":"","title":"2. Pod Affinity Theory: Relationships Between Pods"},{"location":"10-scheduling/podAffinity-nodeAffinity-deployments/notes/#three-types-of-pod-relationships","text":"Pod Affinity : \"Run near these pods\" (attraction) Pod Anti-Affinity : \"Don't run near these pods\" (repulsion) Topology : Defines what \"near\" means","title":"Three Types of Pod Relationships:"},{"location":"10-scheduling/podAffinity-nodeAffinity-deployments/notes/#topology-keys-theory","text":"A topology key is a node label that defines a grouping domain: kubernetes.io/hostname = Different physical/virtual machines topology.kubernetes.io/zone = Different availability zones topology.kubernetes.io/region = Different geographic regions Custom keys = Your own grouping logic (e.g., rack , row , datacenter )","title":"Topology Keys Theory:"},{"location":"10-scheduling/podAffinity-nodeAffinity-deployments/notes/#how-pod-affinity-works","text":"podAntiAffinity : requiredDuringSchedulingIgnoredDuringExecution : - labelSelector : matchLabels : app : web # Look for pods with this label topologyKey : kubernetes.io/hostname # Group by node Translation : \"Don't schedule this pod on any node that already has a pod with label app=web \"","title":"How Pod Affinity Works:"},{"location":"10-scheduling/podAffinity-nodeAffinity-deployments/notes/#3-complete-practical-example-with-theory-applied","text":"apiVersion : apps/v1 kind : Deployment metadata : name : example-app spec : replicas : 3 template : metadata : labels : app : example tier : backend spec : affinity : # NODE AFFINITY: Where can pods go? nodeAffinity : # HARD REQUIREMENT: Must be satisfied requiredDuringSchedulingIgnoredDuringExecution : nodeSelectorTerms : # Term 1: Production environment - matchExpressions : - key : environment operator : In values : [ \"production\" ] - key : available operator : Exists # Term 2: OR staging with SSD - matchExpressions : - key : environment operator : In values : [ \"staging\" ] - key : storage-type operator : In values : [ \"ssd\" ] # SOFT PREFERENCE: Try to satisfy preferredDuringSchedulingIgnoredDuringExecution : - weight : 90 # Higher weight = more important preference : matchExpressions : - key : zone operator : In values : [ \"us-east-1a\" ] # POD ANTI-AFFINITY: How pods relate to each other podAntiAffinity : # HARD: Don't put pods on same node requiredDuringSchedulingIgnoredDuringExecution : - labelSelector : matchLabels : app : example # Look for our own pods topologyKey : kubernetes.io/hostname # Different nodes # SOFT: Try to spread across zones preferredDuringSchedulingIgnoredDuringExecution : - weight : 70 podAffinityTerm : labelSelector : matchLabels : app : example topologyKey : topology.kubernetes.io/zone # Different zones containers : - name : app image : nginx Theory Applied: 1. Node Selection : Pods can run in production (any storage) OR staging (only SSD) 2. Preference : Prefer zone us-east-1a if possible 3. Pod Placement : Never on same node, try for different zones 4. Result : High availability across failure domains","title":"3. Complete Practical Example with Theory Applied"},{"location":"10-scheduling/podAffinity-nodeAffinity-deployments/notes/#4-when-to-use-each-feature-decision-guide","text":"","title":"4. When to Use Each Feature - Decision Guide"},{"location":"10-scheduling/podAffinity-nodeAffinity-deployments/notes/#use-node-affinity-when","text":"Pod needs specific hardware (GPU, SSD, memory) Pod needs to run in specific region/zone You want to dedicate nodes to certain workloads You need to exclude certain node types","title":"Use Node Affinity When:"},{"location":"10-scheduling/podAffinity-nodeAffinity-deployments/notes/#use-pod-affinity-when","text":"Related pods should be co-located (app + cache) You need data locality (pod near its data) Communication latency matters","title":"Use Pod Affinity When:"},{"location":"10-scheduling/podAffinity-nodeAffinity-deployments/notes/#use-pod-anti-affinity-when","text":"You need high availability (spread across nodes/zones) You want to avoid resource contention You have stateful applications that shouldn't share nodes","title":"Use Pod Anti-Affinity When:"},{"location":"10-scheduling/podAffinity-nodeAffinity-deployments/notes/#use-topology-spread-constraints-alternative","text":"For simpler \"spread evenly\" requirements When you want built-in balancing For zone/region spreading without complex rules","title":"Use Topology Spread Constraints (Alternative):"},{"location":"10-scheduling/podAffinity-nodeAffinity-deployments/notes/#5-practice-questions","text":"","title":"5. Practice Questions"},{"location":"10-scheduling/podAffinity-nodeAffinity-deployments/notes/#question-1-basic-node-affinity","text":"Create a Deployment that: 1. Name: cache-app 2. Replicas: 2 3. Image: redis:alpine 4. Must run on nodes with label memory-type=high 5. Should prefer nodes with label storage=ssd (weight: 80) 6. Pods should not be on the same node","title":"Question 1: Basic Node Affinity"},{"location":"10-scheduling/podAffinity-nodeAffinity-deployments/notes/#question-2-complex-node-selection","text":"Create a StatefulSet that: 1. Name: postgres-cluster 2. Replicas: 3 3. Image: postgres:15 4. Requirements: - Must be in production environment - Must have fast storage (any label with \"fast\" in key) - Must NOT be on spot instances - Must NOT be under maintenance 5. Each pod should have 10Gi persistent storage","title":"Question 2: Complex Node Selection"},{"location":"10-scheduling/podAffinity-nodeAffinity-deployments/notes/#question-3-pod-anti-affinity-for-ha","text":"Create a Deployment that: 1. Name: web-frontend 2. Replicas: 5 3. Image: nginx:latest 4. Requirements: - Hard rule: Pods must be on different nodes - Soft rule: Try to spread across zones (weight: 100) - Pods should prefer to run near cache pods (weight: 60) 5. Add readiness and liveness probes","title":"Question 3: Pod Anti-Affinity for HA"},{"location":"10-scheduling/podAffinity-nodeAffinity-deployments/notes/#question-4-complete-production-setup","text":"Create a complete application with: 1. Deployment: api-server (3 replicas) 2. Deployment: redis-cache (2 replicas) 3. Requirements: - Both must run in environment=production - API pods should be on same nodes as cache pods - API pods should be spread across zones - Cache pods should be on different nodes - Prefer nodes with label instance-type=large 4. Include all necessary: resources, probes, environment variables","title":"Question 4: Complete Production Setup"},{"location":"11-coredns/","text":"it's creazy there is none quetion in 70 question worth doing!!","title":"Index"},{"location":"12-deployments/container-lifecycle-events/","text":"Container Lifecycle Events Container lifecycle events track the state transitions of a container from creation to termination. Container States Waiting Container is not yet running. Reasons include: - PullBackOff - Failed to pull image - CrashLoopBackOff - Container keeps crashing - CreateContainerConfigError - Invalid config - ImagePullBackOff - Can't access image registry kubectl describe pod <pod-name> # Shows: State: Waiting, Reason: ImagePullBackOff Running Container is executing and healthy. State : Running Started : 2026-01-03T11:00:00Z Terminated Container has finished execution. Can be due to: - Successful exit (exit code 0) - Failure (non-zero exit code) - Signal (SIGKILL, SIGTERM) - Resource limit exceeded State : Terminated Exit Code : 137 # SIGKILL Reason : OOMKilled Message : Out of memory Lifecycle Hooks (Related) postStart Runs right after container starts (not guaranteed before ENTRYPOINT). lifecycle : postStart : exec : command : [ \"/bin/sh\" , \"-c\" , \"echo 'Started'\" ] preStop Runs before container is terminated. lifecycle : preStop : exec : command : [ \"/bin/sh\" , \"-c\" , \"sleep 15\" ] Monitoring Container Lifecycle # View detailed container state kubectl describe pod <pod-name> # Watch state transitions in real-time kubectl get pods -w # Check container logs kubectl logs <pod-name> <container-name> # View previous logs (if container restarted) kubectl logs <pod-name> <container-name> --previous Exit Codes and Signals Exit Code Meaning 0 Success 1-127 Application error 128+N Killed by signal N 137 SIGKILL (OOMKilled) 143 SIGTERM (Graceful shutdown) Example: Tracking Lifecycle # Pod starts kubectl get pods # NAME READY STATUS # myapp-abc123 0/1 Pending # Container is pulling image # STATUS: PullImage # Container is running # READY: 1/1, STATUS: Running # Container crashes # READY: 0/1, STATUS: CrashLoopBackOff # Check logs: kubectl logs <pod-name> Restarting Container vs Pod Container restart - Only the container restarts, increment restart count Pod restart - Entire Pod is terminated and recreated Container restarts (controlled by RestartPolicy) are less disruptive than Pod restarts.","title":"Container Lifecycle Events"},{"location":"12-deployments/container-lifecycle-events/#container-lifecycle-events","text":"Container lifecycle events track the state transitions of a container from creation to termination.","title":"Container Lifecycle Events"},{"location":"12-deployments/container-lifecycle-events/#container-states","text":"","title":"Container States"},{"location":"12-deployments/container-lifecycle-events/#waiting","text":"Container is not yet running. Reasons include: - PullBackOff - Failed to pull image - CrashLoopBackOff - Container keeps crashing - CreateContainerConfigError - Invalid config - ImagePullBackOff - Can't access image registry kubectl describe pod <pod-name> # Shows: State: Waiting, Reason: ImagePullBackOff","title":"Waiting"},{"location":"12-deployments/container-lifecycle-events/#running","text":"Container is executing and healthy. State : Running Started : 2026-01-03T11:00:00Z","title":"Running"},{"location":"12-deployments/container-lifecycle-events/#terminated","text":"Container has finished execution. Can be due to: - Successful exit (exit code 0) - Failure (non-zero exit code) - Signal (SIGKILL, SIGTERM) - Resource limit exceeded State : Terminated Exit Code : 137 # SIGKILL Reason : OOMKilled Message : Out of memory","title":"Terminated"},{"location":"12-deployments/container-lifecycle-events/#lifecycle-hooks-related","text":"","title":"Lifecycle Hooks (Related)"},{"location":"12-deployments/container-lifecycle-events/#poststart","text":"Runs right after container starts (not guaranteed before ENTRYPOINT). lifecycle : postStart : exec : command : [ \"/bin/sh\" , \"-c\" , \"echo 'Started'\" ]","title":"postStart"},{"location":"12-deployments/container-lifecycle-events/#prestop","text":"Runs before container is terminated. lifecycle : preStop : exec : command : [ \"/bin/sh\" , \"-c\" , \"sleep 15\" ]","title":"preStop"},{"location":"12-deployments/container-lifecycle-events/#monitoring-container-lifecycle","text":"# View detailed container state kubectl describe pod <pod-name> # Watch state transitions in real-time kubectl get pods -w # Check container logs kubectl logs <pod-name> <container-name> # View previous logs (if container restarted) kubectl logs <pod-name> <container-name> --previous","title":"Monitoring Container Lifecycle"},{"location":"12-deployments/container-lifecycle-events/#exit-codes-and-signals","text":"Exit Code Meaning 0 Success 1-127 Application error 128+N Killed by signal N 137 SIGKILL (OOMKilled) 143 SIGTERM (Graceful shutdown)","title":"Exit Codes and Signals"},{"location":"12-deployments/container-lifecycle-events/#example-tracking-lifecycle","text":"# Pod starts kubectl get pods # NAME READY STATUS # myapp-abc123 0/1 Pending # Container is pulling image # STATUS: PullImage # Container is running # READY: 1/1, STATUS: Running # Container crashes # READY: 0/1, STATUS: CrashLoopBackOff # Check logs: kubectl logs <pod-name>","title":"Example: Tracking Lifecycle"},{"location":"12-deployments/container-lifecycle-events/#restarting-container-vs-pod","text":"Container restart - Only the container restarts, increment restart count Pod restart - Entire Pod is terminated and recreated Container restarts (controlled by RestartPolicy) are less disruptive than Pod restarts.","title":"Restarting Container vs Pod"},{"location":"12-deployments/daemonsets/","text":"DaemonSet Purpose Ensure that one Pod instance runs on each eligible node in a cluster, automatically tracking node membership changes. DaemonSets solve the problem of node-scoped workloads that must exist wherever compute exists. Core Concept A DaemonSet is a workload API that instructs the control plane to maintain exactly one Pod per eligible node . It is implemented as an apps/v1 API object. It is reconciled by the DaemonSet controller. Pod placement is node-driven , not replica-driven. Pods are created and bound directly to nodes as they appear. It does not manage horizontal scaling, traffic routing, or service-level availability. DaemonSets are intended for infrastructure agents , not application replicas. Mental Model Control plane responsibility : Ensure Pod presence per node. Data plane behavior : Pod runs continuously on its node. Declarative : Desired state = \u201cone Pod on every eligible node\u201d. Reconciliation loop : Node added \u2192 Pod created Node removed \u2192 Pod deleted Node becomes ineligible \u2192 Pod removed Scaling equals number of nodes , not user-defined replicas. Key Components DaemonSet (apps/v1) DaemonSet Controller Pod Template Node Eligibility Controls nodeSelector nodeAffinity tolerations Lifecycle / Flow User creates a DaemonSet. API server persists it. Controller evaluates all nodes. One Pod is created per eligible node. Continuous reconciliation on node or spec changes. Configuration Structure Required spec.selector spec.template Behavior-altering nodeSelector , nodeAffinity tolerations updateStrategy Host access fields ( hostNetwork , hostPath , privileged ) Full YAML Examples Example 1: Basic DaemonSet (All Nodes) apiVersion : apps/v1 kind : DaemonSet metadata : name : simple-agent namespace : default spec : selector : matchLabels : app : simple-agent template : metadata : labels : app : simple-agent spec : containers : - name : agent image : busybox command : [ \"sh\" , \"-c\" , \"while true; do echo running; sleep 60; done\" ] Example 2: Run Only on Worker Nodes (nodeSelector) apiVersion : apps/v1 kind : DaemonSet metadata : name : worker-only-agent namespace : default spec : selector : matchLabels : app : worker-agent template : metadata : labels : app : worker-agent spec : nodeSelector : node-role.kubernetes.io/worker : \"true\" containers : - name : agent image : busybox command : [ \"sh\" , \"-c\" , \"sleep infinity\" ] Example 3: Run on Control Plane Nodes (Tolerations) apiVersion : apps/v1 kind : DaemonSet metadata : name : control-plane-agent namespace : kube-system spec : selector : matchLabels : app : cp-agent template : metadata : labels : app : cp-agent spec : tolerations : - key : node-role.kubernetes.io/control-plane operator : Exists effect : NoSchedule containers : - name : agent image : busybox command : [ \"sh\" , \"-c\" , \"sleep infinity\" ] Example 4: DaemonSet with Node Affinity (Zone Restricted) apiVersion : apps/v1 kind : DaemonSet metadata : name : zone-agent namespace : default spec : selector : matchLabels : app : zone-agent template : metadata : labels : app : zone-agent spec : affinity : nodeAffinity : requiredDuringSchedulingIgnoredDuringExecution : nodeSelectorTerms : - matchExpressions : - key : topology.kubernetes.io/zone operator : In values : - us-east-1a containers : - name : agent image : busybox command : [ \"sh\" , \"-c\" , \"sleep infinity\" ] Example 5: Privileged DaemonSet with Host Access (Security / Monitoring) apiVersion : apps/v1 kind : DaemonSet metadata : name : privileged-agent namespace : monitoring spec : selector : matchLabels : app : privileged-agent template : metadata : labels : app : privileged-agent spec : hostNetwork : true hostPID : true containers : - name : agent image : busybox securityContext : privileged : true volumeMounts : - name : rootfs mountPath : /host readOnly : true command : [ \"sh\" , \"-c\" , \"sleep infinity\" ] volumes : - name : rootfs hostPath : path : / Example 6: DaemonSet with Rolling Update Control apiVersion : apps/v1 kind : DaemonSet metadata : name : rolling-agent namespace : default spec : updateStrategy : type : RollingUpdate rollingUpdate : maxUnavailable : 1 selector : matchLabels : app : rolling-agent template : metadata : labels : app : rolling-agent spec : containers : - name : agent image : busybox command : [ \"sh\" , \"-c\" , \"sleep infinity\" ] Behavior and Guarantees Guaranteed One Pod per eligible node. Automatic reconciliation on node add/remove. Controlled rollout via updateStrategy . Not guaranteed Ordering across nodes. Zero-downtime updates. Traffic exposure or service availability. Common Variants System-level DaemonSets Worker-only DaemonSets Privileged security agents Zone- or hardware-specific DaemonSets Failure Modes and Debugging Pod missing \u2192 node taints or affinity mismatch Update stuck \u2192 readiness or maxUnavailable kubectl describe daemonset <name> kubectl get pods -o wide kubectl describe node <node> Constraints and Limitations No replicas field. Namespace-scoped. Not suitable for application workloads. Host access increases security risk. Exam-Relevant Notes (CKA/CKAD/CKS) One Pod per node is enforced by controller logic. Tolerations are mandatory for tainted nodes. Selector and template labels must match exactly. DaemonSets fully support rolling updates. Related Concepts Deployment Static Pods Node Affinity Taints and Tolerations CNI / CSI plugins","title":"DaemonSet"},{"location":"12-deployments/daemonsets/#daemonset","text":"","title":"DaemonSet"},{"location":"12-deployments/daemonsets/#purpose","text":"Ensure that one Pod instance runs on each eligible node in a cluster, automatically tracking node membership changes. DaemonSets solve the problem of node-scoped workloads that must exist wherever compute exists.","title":"Purpose"},{"location":"12-deployments/daemonsets/#core-concept","text":"A DaemonSet is a workload API that instructs the control plane to maintain exactly one Pod per eligible node . It is implemented as an apps/v1 API object. It is reconciled by the DaemonSet controller. Pod placement is node-driven , not replica-driven. Pods are created and bound directly to nodes as they appear. It does not manage horizontal scaling, traffic routing, or service-level availability. DaemonSets are intended for infrastructure agents , not application replicas.","title":"Core Concept"},{"location":"12-deployments/daemonsets/#mental-model","text":"Control plane responsibility : Ensure Pod presence per node. Data plane behavior : Pod runs continuously on its node. Declarative : Desired state = \u201cone Pod on every eligible node\u201d. Reconciliation loop : Node added \u2192 Pod created Node removed \u2192 Pod deleted Node becomes ineligible \u2192 Pod removed Scaling equals number of nodes , not user-defined replicas.","title":"Mental Model"},{"location":"12-deployments/daemonsets/#key-components","text":"DaemonSet (apps/v1) DaemonSet Controller Pod Template Node Eligibility Controls nodeSelector nodeAffinity tolerations","title":"Key Components"},{"location":"12-deployments/daemonsets/#lifecycle-flow","text":"User creates a DaemonSet. API server persists it. Controller evaluates all nodes. One Pod is created per eligible node. Continuous reconciliation on node or spec changes.","title":"Lifecycle / Flow"},{"location":"12-deployments/daemonsets/#configuration-structure","text":"Required spec.selector spec.template Behavior-altering nodeSelector , nodeAffinity tolerations updateStrategy Host access fields ( hostNetwork , hostPath , privileged )","title":"Configuration Structure"},{"location":"12-deployments/daemonsets/#full-yaml-examples","text":"","title":"Full YAML Examples"},{"location":"12-deployments/daemonsets/#example-1-basic-daemonset-all-nodes","text":"apiVersion : apps/v1 kind : DaemonSet metadata : name : simple-agent namespace : default spec : selector : matchLabels : app : simple-agent template : metadata : labels : app : simple-agent spec : containers : - name : agent image : busybox command : [ \"sh\" , \"-c\" , \"while true; do echo running; sleep 60; done\" ]","title":"Example 1: Basic DaemonSet (All Nodes)"},{"location":"12-deployments/daemonsets/#example-2-run-only-on-worker-nodes-nodeselector","text":"apiVersion : apps/v1 kind : DaemonSet metadata : name : worker-only-agent namespace : default spec : selector : matchLabels : app : worker-agent template : metadata : labels : app : worker-agent spec : nodeSelector : node-role.kubernetes.io/worker : \"true\" containers : - name : agent image : busybox command : [ \"sh\" , \"-c\" , \"sleep infinity\" ]","title":"Example 2: Run Only on Worker Nodes (nodeSelector)"},{"location":"12-deployments/daemonsets/#example-3-run-on-control-plane-nodes-tolerations","text":"apiVersion : apps/v1 kind : DaemonSet metadata : name : control-plane-agent namespace : kube-system spec : selector : matchLabels : app : cp-agent template : metadata : labels : app : cp-agent spec : tolerations : - key : node-role.kubernetes.io/control-plane operator : Exists effect : NoSchedule containers : - name : agent image : busybox command : [ \"sh\" , \"-c\" , \"sleep infinity\" ]","title":"Example 3: Run on Control Plane Nodes (Tolerations)"},{"location":"12-deployments/daemonsets/#example-4-daemonset-with-node-affinity-zone-restricted","text":"apiVersion : apps/v1 kind : DaemonSet metadata : name : zone-agent namespace : default spec : selector : matchLabels : app : zone-agent template : metadata : labels : app : zone-agent spec : affinity : nodeAffinity : requiredDuringSchedulingIgnoredDuringExecution : nodeSelectorTerms : - matchExpressions : - key : topology.kubernetes.io/zone operator : In values : - us-east-1a containers : - name : agent image : busybox command : [ \"sh\" , \"-c\" , \"sleep infinity\" ]","title":"Example 4: DaemonSet with Node Affinity (Zone Restricted)"},{"location":"12-deployments/daemonsets/#example-5-privileged-daemonset-with-host-access-security-monitoring","text":"apiVersion : apps/v1 kind : DaemonSet metadata : name : privileged-agent namespace : monitoring spec : selector : matchLabels : app : privileged-agent template : metadata : labels : app : privileged-agent spec : hostNetwork : true hostPID : true containers : - name : agent image : busybox securityContext : privileged : true volumeMounts : - name : rootfs mountPath : /host readOnly : true command : [ \"sh\" , \"-c\" , \"sleep infinity\" ] volumes : - name : rootfs hostPath : path : /","title":"Example 5: Privileged DaemonSet with Host Access (Security / Monitoring)"},{"location":"12-deployments/daemonsets/#example-6-daemonset-with-rolling-update-control","text":"apiVersion : apps/v1 kind : DaemonSet metadata : name : rolling-agent namespace : default spec : updateStrategy : type : RollingUpdate rollingUpdate : maxUnavailable : 1 selector : matchLabels : app : rolling-agent template : metadata : labels : app : rolling-agent spec : containers : - name : agent image : busybox command : [ \"sh\" , \"-c\" , \"sleep infinity\" ]","title":"Example 6: DaemonSet with Rolling Update Control"},{"location":"12-deployments/daemonsets/#behavior-and-guarantees","text":"Guaranteed One Pod per eligible node. Automatic reconciliation on node add/remove. Controlled rollout via updateStrategy . Not guaranteed Ordering across nodes. Zero-downtime updates. Traffic exposure or service availability.","title":"Behavior and Guarantees"},{"location":"12-deployments/daemonsets/#common-variants","text":"System-level DaemonSets Worker-only DaemonSets Privileged security agents Zone- or hardware-specific DaemonSets","title":"Common Variants"},{"location":"12-deployments/daemonsets/#failure-modes-and-debugging","text":"Pod missing \u2192 node taints or affinity mismatch Update stuck \u2192 readiness or maxUnavailable kubectl describe daemonset <name> kubectl get pods -o wide kubectl describe node <node>","title":"Failure Modes and Debugging"},{"location":"12-deployments/daemonsets/#constraints-and-limitations","text":"No replicas field. Namespace-scoped. Not suitable for application workloads. Host access increases security risk.","title":"Constraints and Limitations"},{"location":"12-deployments/daemonsets/#exam-relevant-notes-ckackadcks","text":"One Pod per node is enforced by controller logic. Tolerations are mandatory for tainted nodes. Selector and template labels must match exactly. DaemonSets fully support rolling updates.","title":"Exam-Relevant Notes (CKA/CKAD/CKS)"},{"location":"12-deployments/daemonsets/#related-concepts","text":"Deployment Static Pods Node Affinity Taints and Tolerations CNI / CSI plugins","title":"Related Concepts"},{"location":"12-deployments/deployment-stragety/","text":"","title":"Deployment stragety"},{"location":"12-deployments/lifecycle-hooks/","text":"Pod Lifecycle Hooks Lifecycle hooks allow containers to be aware of events in their management lifecycle and run code when they occur. Types of Hooks 1. postStart Executed immediately after a container is created. Does NOT wait for the container process to start. 2. preStop Executed just before a container is terminated. Gives containers time to gracefully shut down. Hook Handler Types Exec Executes a specific command inside the container. postStart : exec : command : [ \"/bin/sh\" , \"-c\" , \"echo 'Container started'\" ] HTTPGet Makes an HTTP request to a specified endpoint. postStart : httpGet : host : localhost port : 8080 path : /init TCPSocket Performs a TCP check against a specific port. preStop : tcpSocket : port : 5432 Important Notes postStart is not guaranteed to execute before ENTRYPOINT - use init containers for setup preStop is synchronous - Pod termination waits for preStop to complete Failure in preStop hook will cause Pod to immediately terminate preStop is useful for cleanup: closing database connections, draining requests, etc. postStart failures will restart the container (if RestartPolicy allows) Example: Graceful Shutdown with preStop lifecycle : preStop : exec : command : [ \"/bin/sh\" , \"-c\" , \"sleep 15\" ] This gives the application 15 seconds to finish ongoing requests before termination.","title":"Pod Lifecycle Hooks"},{"location":"12-deployments/lifecycle-hooks/#pod-lifecycle-hooks","text":"Lifecycle hooks allow containers to be aware of events in their management lifecycle and run code when they occur.","title":"Pod Lifecycle Hooks"},{"location":"12-deployments/lifecycle-hooks/#types-of-hooks","text":"","title":"Types of Hooks"},{"location":"12-deployments/lifecycle-hooks/#1-poststart","text":"Executed immediately after a container is created. Does NOT wait for the container process to start.","title":"1. postStart"},{"location":"12-deployments/lifecycle-hooks/#2-prestop","text":"Executed just before a container is terminated. Gives containers time to gracefully shut down.","title":"2. preStop"},{"location":"12-deployments/lifecycle-hooks/#hook-handler-types","text":"","title":"Hook Handler Types"},{"location":"12-deployments/lifecycle-hooks/#exec","text":"Executes a specific command inside the container. postStart : exec : command : [ \"/bin/sh\" , \"-c\" , \"echo 'Container started'\" ]","title":"Exec"},{"location":"12-deployments/lifecycle-hooks/#httpget","text":"Makes an HTTP request to a specified endpoint. postStart : httpGet : host : localhost port : 8080 path : /init","title":"HTTPGet"},{"location":"12-deployments/lifecycle-hooks/#tcpsocket","text":"Performs a TCP check against a specific port. preStop : tcpSocket : port : 5432","title":"TCPSocket"},{"location":"12-deployments/lifecycle-hooks/#important-notes","text":"postStart is not guaranteed to execute before ENTRYPOINT - use init containers for setup preStop is synchronous - Pod termination waits for preStop to complete Failure in preStop hook will cause Pod to immediately terminate preStop is useful for cleanup: closing database connections, draining requests, etc. postStart failures will restart the container (if RestartPolicy allows)","title":"Important Notes"},{"location":"12-deployments/lifecycle-hooks/#example-graceful-shutdown-with-prestop","text":"lifecycle : preStop : exec : command : [ \"/bin/sh\" , \"-c\" , \"sleep 15\" ] This gives the application 15 seconds to finish ongoing requests before termination.","title":"Example: Graceful Shutdown with preStop"},{"location":"12-deployments/pod-disruption-budgets/","text":"Pod Disruption Budgets (PDB) Pod Disruption Budgets are policies to maintain application availability during cluster maintenance or node failures. What is a Disruption? A voluntary disruption is when cluster operations intentionally remove or drain Pods: - Node maintenance - Cluster upgrades - Manual Pod deletion - Horizontal Pod Autoscaler scaling down Non-voluntary disruptions (hardware failures, network partitions) are NOT covered by PDB. PDB Specification minAvailable Minimum number of Pods that must remain available. apiVersion : policy/v1 kind : PodDisruptionBudget metadata : name : web-pdb spec : minAvailable : 2 selector : matchLabels : app : web-server Ensures at least 2 web-server Pods remain available. maxUnavailable Maximum number of Pods that can be unavailable. apiVersion : policy/v1 kind : PodDisruptionBudget metadata : name : database-pdb spec : maxUnavailable : 1 selector : matchLabels : app : postgres Allows only 1 database Pod to be unavailable at a time. Percentage-Based PDB apiVersion : policy/v1 kind : PodDisruptionBudget metadata : name : app-pdb spec : minAvailable : \"50%\" selector : matchLabels : app : myapp Maintains at least 50% of Pods available. Real-World Example apiVersion : policy/v1 kind : PodDisruptionBudget metadata : name : critical-app-pdb namespace : production spec : minAvailable : 3 selector : matchLabels : tier : critical app : api-server Ensures a critical API server always has at least 3 Pods running during maintenance. Best Practices Use minAvailable for critical apps - API servers, databases Use maxUnavailable for less critical - Batch processors, workers Set realistic values - Too strict (minAvailable: 10 out of 10) prevents maintenance Monitor PDB status - Check if Pods are blocked from disruption Checking PDB Status kubectl get pdb kubectl describe pdb <name> # Output shows: # Allowed Disruptions: X (how many Pods can be disrupted) Limitations PDB only prevents voluntary disruptions Does NOT protect against node hardware failures Does NOT work with StaticPods Requires at least minAvailable/maxUnavailable replicas running","title":"Pod Disruption Budgets (PDB)"},{"location":"12-deployments/pod-disruption-budgets/#pod-disruption-budgets-pdb","text":"Pod Disruption Budgets are policies to maintain application availability during cluster maintenance or node failures.","title":"Pod Disruption Budgets (PDB)"},{"location":"12-deployments/pod-disruption-budgets/#what-is-a-disruption","text":"A voluntary disruption is when cluster operations intentionally remove or drain Pods: - Node maintenance - Cluster upgrades - Manual Pod deletion - Horizontal Pod Autoscaler scaling down Non-voluntary disruptions (hardware failures, network partitions) are NOT covered by PDB.","title":"What is a Disruption?"},{"location":"12-deployments/pod-disruption-budgets/#pdb-specification","text":"","title":"PDB Specification"},{"location":"12-deployments/pod-disruption-budgets/#minavailable","text":"Minimum number of Pods that must remain available. apiVersion : policy/v1 kind : PodDisruptionBudget metadata : name : web-pdb spec : minAvailable : 2 selector : matchLabels : app : web-server Ensures at least 2 web-server Pods remain available.","title":"minAvailable"},{"location":"12-deployments/pod-disruption-budgets/#maxunavailable","text":"Maximum number of Pods that can be unavailable. apiVersion : policy/v1 kind : PodDisruptionBudget metadata : name : database-pdb spec : maxUnavailable : 1 selector : matchLabels : app : postgres Allows only 1 database Pod to be unavailable at a time.","title":"maxUnavailable"},{"location":"12-deployments/pod-disruption-budgets/#percentage-based-pdb","text":"apiVersion : policy/v1 kind : PodDisruptionBudget metadata : name : app-pdb spec : minAvailable : \"50%\" selector : matchLabels : app : myapp Maintains at least 50% of Pods available.","title":"Percentage-Based PDB"},{"location":"12-deployments/pod-disruption-budgets/#real-world-example","text":"apiVersion : policy/v1 kind : PodDisruptionBudget metadata : name : critical-app-pdb namespace : production spec : minAvailable : 3 selector : matchLabels : tier : critical app : api-server Ensures a critical API server always has at least 3 Pods running during maintenance.","title":"Real-World Example"},{"location":"12-deployments/pod-disruption-budgets/#best-practices","text":"Use minAvailable for critical apps - API servers, databases Use maxUnavailable for less critical - Batch processors, workers Set realistic values - Too strict (minAvailable: 10 out of 10) prevents maintenance Monitor PDB status - Check if Pods are blocked from disruption","title":"Best Practices"},{"location":"12-deployments/pod-disruption-budgets/#checking-pdb-status","text":"kubectl get pdb kubectl describe pdb <name> # Output shows: # Allowed Disruptions: X (how many Pods can be disrupted)","title":"Checking PDB Status"},{"location":"12-deployments/pod-disruption-budgets/#limitations","text":"PDB only prevents voluntary disruptions Does NOT protect against node hardware failures Does NOT work with StaticPods Requires at least minAvailable/maxUnavailable replicas running","title":"Limitations"},{"location":"12-deployments/questions/","text":"DaemonSets Question 1 Namespace: ops Name: node-config-writer Image: busybox:1.36 Command: sh -c \"echo $NODE_NAME > /host/node.txt && sleep 3600\" Specifications Nodes: - All worker nodes Environment Variables: - NODE_NAME from Downward API Volumes: - HostPath: /etc/node-info - Mount path: /host Scheduling: - One Pod per node Task Create a DaemonSet that writes the node name to a file on each worker node. Question 2 Namespace: infra Name: zone-aware-daemon Image: busybox:1.36 Command: printenv ZONE && sleep 3600 Specifications Nodes: - Only nodes with label topology.kubernetes.io/zone Environment Variables: - Set ZONE environment variable from the node label topology.kubernetes.io/zone using Downward API Scheduling: - One Pod per eligible node Task Create a DaemonSet that reads a node label and exposes it as an environment variable inside the Pod. Use nodeAffinity to schedule only on nodes that have the topology.kubernetes.io/zone label. Question 3 Namespace: monitoring Name: log-collector Image: busybox:1.36 Command: sh -c \"while true; do echo '[SYSTEM] Node: '$HOSTNAME' - Memory: $(free -m | grep Mem | awk '{print $3}')MB' >> /var/log/node-metrics.log; sleep 30; done\" Specifications Nodes: - All nodes Environment Variables: - HOSTNAME from Downward API (pod.spec.nodeName) Volumes: - HostPath: /var/log - Mount path: /var/log - Type: DirectoryOrCreate Resources: - Requests: CPU 100m, Memory 64Mi - Limits: CPU 200m, Memory 128Mi Scheduling: - One Pod per node - Tolerate all taints Task Create a DaemonSet that collects and logs node metrics to a shared log file on each node. The Pod should be able to run on all nodes including master nodes by using tolerations. Question 4 Namespace: system Name: network-monitor Image: busybox:1.36 Command: sh -c \"while true; do echo '[TIMESTAMP]' $(date '+%Y-%m-%d %H:%M:%S') '[INTERFACE] eth0:' $(ip addr show eth0 | grep 'inet ' | awk '{print $2}') '[GATEWAY]' $(ip route | grep default | awk '{print $3}') >> /var/log/network-info.log; sleep 60; done\" Specifications Nodes: - All nodes Environment Variables: - NODE_IP from pod.status.podIP Volumes: - HostPath: /var/log - Mount path: /var/log - Type: DirectoryOrCreate Init Container: - Image: busybox:1.36 - Command: Create /var/log/network-info.log if it doesn't exist Resources: - Requests: CPU 50m, Memory 32Mi - Limits: CPU 100m, Memory 64Mi Scheduling: - One Pod per node - Skip control-plane nodes (use nodeSelector or taints/tolerations) Task Create a DaemonSet that monitors network information on each worker node and logs it periodically. Use an init container to ensure the log file exists, and verify that Pods do NOT run on control-plane nodes.","title":"DaemonSets"},{"location":"12-deployments/questions/#daemonsets","text":"","title":"DaemonSets"},{"location":"12-deployments/questions/#question-1","text":"Namespace: ops Name: node-config-writer Image: busybox:1.36 Command: sh -c \"echo $NODE_NAME > /host/node.txt && sleep 3600\"","title":"Question 1"},{"location":"12-deployments/questions/#specifications","text":"Nodes: - All worker nodes Environment Variables: - NODE_NAME from Downward API Volumes: - HostPath: /etc/node-info - Mount path: /host Scheduling: - One Pod per node","title":"Specifications"},{"location":"12-deployments/questions/#task","text":"Create a DaemonSet that writes the node name to a file on each worker node.","title":"Task"},{"location":"12-deployments/questions/#question-2","text":"Namespace: infra Name: zone-aware-daemon Image: busybox:1.36 Command: printenv ZONE && sleep 3600","title":"Question 2"},{"location":"12-deployments/questions/#specifications_1","text":"Nodes: - Only nodes with label topology.kubernetes.io/zone Environment Variables: - Set ZONE environment variable from the node label topology.kubernetes.io/zone using Downward API Scheduling: - One Pod per eligible node","title":"Specifications"},{"location":"12-deployments/questions/#task_1","text":"Create a DaemonSet that reads a node label and exposes it as an environment variable inside the Pod. Use nodeAffinity to schedule only on nodes that have the topology.kubernetes.io/zone label.","title":"Task"},{"location":"12-deployments/questions/#question-3","text":"Namespace: monitoring Name: log-collector Image: busybox:1.36 Command: sh -c \"while true; do echo '[SYSTEM] Node: '$HOSTNAME' - Memory: $(free -m | grep Mem | awk '{print $3}')MB' >> /var/log/node-metrics.log; sleep 30; done\"","title":"Question 3"},{"location":"12-deployments/questions/#specifications_2","text":"Nodes: - All nodes Environment Variables: - HOSTNAME from Downward API (pod.spec.nodeName) Volumes: - HostPath: /var/log - Mount path: /var/log - Type: DirectoryOrCreate Resources: - Requests: CPU 100m, Memory 64Mi - Limits: CPU 200m, Memory 128Mi Scheduling: - One Pod per node - Tolerate all taints","title":"Specifications"},{"location":"12-deployments/questions/#task_2","text":"Create a DaemonSet that collects and logs node metrics to a shared log file on each node. The Pod should be able to run on all nodes including master nodes by using tolerations.","title":"Task"},{"location":"12-deployments/questions/#question-4","text":"Namespace: system Name: network-monitor Image: busybox:1.36 Command: sh -c \"while true; do echo '[TIMESTAMP]' $(date '+%Y-%m-%d %H:%M:%S') '[INTERFACE] eth0:' $(ip addr show eth0 | grep 'inet ' | awk '{print $2}') '[GATEWAY]' $(ip route | grep default | awk '{print $3}') >> /var/log/network-info.log; sleep 60; done\"","title":"Question 4"},{"location":"12-deployments/questions/#specifications_3","text":"Nodes: - All nodes Environment Variables: - NODE_IP from pod.status.podIP Volumes: - HostPath: /var/log - Mount path: /var/log - Type: DirectoryOrCreate Init Container: - Image: busybox:1.36 - Command: Create /var/log/network-info.log if it doesn't exist Resources: - Requests: CPU 50m, Memory 32Mi - Limits: CPU 100m, Memory 64Mi Scheduling: - One Pod per node - Skip control-plane nodes (use nodeSelector or taints/tolerations)","title":"Specifications"},{"location":"12-deployments/questions/#task_3","text":"Create a DaemonSet that monitors network information on each worker node and logs it periodically. Use an init container to ensure the log file exists, and verify that Pods do NOT run on control-plane nodes.","title":"Task"},{"location":"12-deployments/restart-policies/","text":"Restart Policies RestartPolicy determines what happens to a Pod when its containers exit or are terminated. Available Restart Policies Always (Default) Container is always restarted if it exits, regardless of exit code. spec : restartPolicy : Always containers : - name : app image : myapp:1.0 Use Case: Deployments, Services, web servers that should always be running. OnFailure Container is restarted only if it exits with a non-zero exit code. spec : restartPolicy : OnFailure containers : - name : job image : batch-job:1.0 Use Case: Jobs that should retry on failure but not restart on successful completion. Never Container is never restarted, even if it fails. spec : restartPolicy : Never containers : - name : one-time-task image : init-task:1.0 Use Case: Jobs, one-time scripts, initialization tasks. Restart Behavior Details Restart delay increases exponentially - 100ms, 200ms, 400ms, 800ms, 1.6s, 3.2s, etc. Max restart delay is capped at 5 minutes Restarts count toward resource quotas CrashLoopBackOff state occurs when container keeps crashing Pod-Level vs Container-Level RestartPolicy is specified at Pod level and applies to all containers in the Pod. apiVersion : v1 kind : Pod metadata : name : multi-container-pod spec : restartPolicy : OnFailure containers : - name : container1 image : image1:1.0 - name : container2 image : image2:1.0 Both containers follow the same restart policy. Important Notes If a container exit code is 0, it's considered successful (no restart with OnFailure) Deployments override RestartPolicy for individual container restarts StatefulSets and DaemonSets typically use Always CronJobs and Jobs typically use Never or OnFailure Checking Restart Count kubectl get pods kubectl describe pod <pod-name> # Shows 'Restarts' column","title":"Restart Policies"},{"location":"12-deployments/restart-policies/#restart-policies","text":"RestartPolicy determines what happens to a Pod when its containers exit or are terminated.","title":"Restart Policies"},{"location":"12-deployments/restart-policies/#available-restart-policies","text":"","title":"Available Restart Policies"},{"location":"12-deployments/restart-policies/#always-default","text":"Container is always restarted if it exits, regardless of exit code. spec : restartPolicy : Always containers : - name : app image : myapp:1.0 Use Case: Deployments, Services, web servers that should always be running.","title":"Always (Default)"},{"location":"12-deployments/restart-policies/#onfailure","text":"Container is restarted only if it exits with a non-zero exit code. spec : restartPolicy : OnFailure containers : - name : job image : batch-job:1.0 Use Case: Jobs that should retry on failure but not restart on successful completion.","title":"OnFailure"},{"location":"12-deployments/restart-policies/#never","text":"Container is never restarted, even if it fails. spec : restartPolicy : Never containers : - name : one-time-task image : init-task:1.0 Use Case: Jobs, one-time scripts, initialization tasks.","title":"Never"},{"location":"12-deployments/restart-policies/#restart-behavior-details","text":"Restart delay increases exponentially - 100ms, 200ms, 400ms, 800ms, 1.6s, 3.2s, etc. Max restart delay is capped at 5 minutes Restarts count toward resource quotas CrashLoopBackOff state occurs when container keeps crashing","title":"Restart Behavior Details"},{"location":"12-deployments/restart-policies/#pod-level-vs-container-level","text":"RestartPolicy is specified at Pod level and applies to all containers in the Pod. apiVersion : v1 kind : Pod metadata : name : multi-container-pod spec : restartPolicy : OnFailure containers : - name : container1 image : image1:1.0 - name : container2 image : image2:1.0 Both containers follow the same restart policy.","title":"Pod-Level vs Container-Level"},{"location":"12-deployments/restart-policies/#important-notes","text":"If a container exit code is 0, it's considered successful (no restart with OnFailure) Deployments override RestartPolicy for individual container restarts StatefulSets and DaemonSets typically use Always CronJobs and Jobs typically use Never or OnFailure","title":"Important Notes"},{"location":"12-deployments/restart-policies/#checking-restart-count","text":"kubectl get pods kubectl describe pod <pod-name> # Shows 'Restarts' column","title":"Checking Restart Count"},{"location":"12-deployments/termination-grace-period/","text":"Termination Grace Period The amount of time Kubernetes waits for a container to gracefully shut down before forcefully terminating it. Default Value 30 seconds How It Works Pod receives SIGTERM signal Container has terminationGracePeriodSeconds to shut down gracefully If container doesn't exit, SIGKILL is sent to force termination Pod is removed Setting Termination Grace Period spec : terminationGracePeriodSeconds : 60 containers : - name : app image : myapp:1.0 Typical Use Cases Web Application Need time to drain connection pools and close active requests. terminationGracePeriodSeconds : 30 Database Needs time to flush writes and close gracefully. terminationGracePeriodSeconds : 120 Long-Running Job Needs significant time to save state. terminationGracePeriodSeconds : 300 Best Practices Combine with preStop hook - preStop should complete before grace period expires Match with app shutdown time - set grace period longer than app's shutdown time Default 30s is usually sufficient - increase only if needed Monitor actual shutdown times - adjust based on real behavior Example: Web Server with Graceful Shutdown apiVersion : v1 kind : Pod metadata : name : web-server spec : terminationGracePeriodSeconds : 45 containers : - name : app image : nginx:latest lifecycle : preStop : exec : command : [ \"/bin/sh\" , \"-c\" , \"nginx -s quit; while killall -0 nginx 2>/dev/null; do sleep 1; done\" ] restartPolicy : Always This gives nginx up to 45 seconds to gracefully shut down before being killed.","title":"Termination Grace Period"},{"location":"12-deployments/termination-grace-period/#termination-grace-period","text":"The amount of time Kubernetes waits for a container to gracefully shut down before forcefully terminating it.","title":"Termination Grace Period"},{"location":"12-deployments/termination-grace-period/#default-value","text":"30 seconds","title":"Default Value"},{"location":"12-deployments/termination-grace-period/#how-it-works","text":"Pod receives SIGTERM signal Container has terminationGracePeriodSeconds to shut down gracefully If container doesn't exit, SIGKILL is sent to force termination Pod is removed","title":"How It Works"},{"location":"12-deployments/termination-grace-period/#setting-termination-grace-period","text":"spec : terminationGracePeriodSeconds : 60 containers : - name : app image : myapp:1.0","title":"Setting Termination Grace Period"},{"location":"12-deployments/termination-grace-period/#typical-use-cases","text":"","title":"Typical Use Cases"},{"location":"12-deployments/termination-grace-period/#web-application","text":"Need time to drain connection pools and close active requests. terminationGracePeriodSeconds : 30","title":"Web Application"},{"location":"12-deployments/termination-grace-period/#database","text":"Needs time to flush writes and close gracefully. terminationGracePeriodSeconds : 120","title":"Database"},{"location":"12-deployments/termination-grace-period/#long-running-job","text":"Needs significant time to save state. terminationGracePeriodSeconds : 300","title":"Long-Running Job"},{"location":"12-deployments/termination-grace-period/#best-practices","text":"Combine with preStop hook - preStop should complete before grace period expires Match with app shutdown time - set grace period longer than app's shutdown time Default 30s is usually sufficient - increase only if needed Monitor actual shutdown times - adjust based on real behavior","title":"Best Practices"},{"location":"12-deployments/termination-grace-period/#example-web-server-with-graceful-shutdown","text":"apiVersion : v1 kind : Pod metadata : name : web-server spec : terminationGracePeriodSeconds : 45 containers : - name : app image : nginx:latest lifecycle : preStop : exec : command : [ \"/bin/sh\" , \"-c\" , \"nginx -s quit; while killall -0 nginx 2>/dev/null; do sleep 1; done\" ] restartPolicy : Always This gives nginx up to 45 seconds to gracefully shut down before being killed.","title":"Example: Web Server with Graceful Shutdown"},{"location":"12-deployments/update-strategies/","text":"Update Strategies Update strategies define how Pods are updated when a Deployment changes (e.g., new image version). RollingUpdate (Default) Gradually replaces old Pods with new ones to maintain availability. spec : strategy : type : RollingUpdate rollingUpdate : maxSurge : 1 maxUnavailable : 0 Parameters maxSurge - Maximum number of Pods above the desired replica count - Default: 1 - Allows creating additional Pods during update for testing # Desired: 3 Pods # maxSurge: 2 # During update: up to 5 Pods running temporarily maxSurge : 2 maxUnavailable - Maximum number of Pods that can be unavailable - Default: 1 - Higher value = faster update, lower availability - Set to 0 for zero-downtime deployments # Zero-downtime update maxUnavailable : 0 maxSurge : 1 Recreate Terminates all old Pods before starting new ones. Causes downtime. spec : strategy : type : Recreate Use Case: Development/testing environments where downtime is acceptable. Behavior: 1. All existing Pods are terminated 2. All new Pods are created 3. Brief service unavailability Update Strategy Selection RollingUpdate - Production spec : replicas : 3 strategy : type : RollingUpdate rollingUpdate : maxSurge : 1 maxUnavailable : 0 Ensures at least 3 Pods always available. Recreate - Development spec : replicas : 1 strategy : type : Recreate Simple and fast for non-critical environments. Aggressive RollingUpdate - Fast Updates spec : replicas : 4 strategy : type : RollingUpdate rollingUpdate : maxSurge : 2 maxUnavailable : 1 Balances speed and availability. Blue-Green Deployment Pattern Not a built-in strategy, but achievable with manual Deployment management: Create \"blue\" Deployment with current version Create \"green\" Deployment with new version Switch Service selector from blue to green Delete old blue Deployment # Blue Deployment apiVersion : apps/v1 kind : Deployment metadata : name : myapp-blue spec : template : metadata : labels : version : blue --- # Service routes to current version apiVersion : v1 kind : Service metadata : name : myapp spec : selector : version : blue # Points to blue or green Canary Deployment Pattern Gradually roll out new version to small percentage of users: Keep most replicas on old version Add few replicas with new version Monitor metrics Gradually increase new version replicas Remove old version # Initial: 9 old Pods, 1 new Pod # Monitor: Check metrics on new Pod # 9 old, 2 new \u2192 8 old, 3 new \u2192 ... until fully updated Monitoring Updates # Watch update progress kubectl rollout status deployment <deployment-name> # View update history kubectl rollout history deployment <deployment-name> # Rollback to previous version kubectl rollout undo deployment <deployment-name>","title":"Update Strategies"},{"location":"12-deployments/update-strategies/#update-strategies","text":"Update strategies define how Pods are updated when a Deployment changes (e.g., new image version).","title":"Update Strategies"},{"location":"12-deployments/update-strategies/#rollingupdate-default","text":"Gradually replaces old Pods with new ones to maintain availability. spec : strategy : type : RollingUpdate rollingUpdate : maxSurge : 1 maxUnavailable : 0","title":"RollingUpdate (Default)"},{"location":"12-deployments/update-strategies/#parameters","text":"maxSurge - Maximum number of Pods above the desired replica count - Default: 1 - Allows creating additional Pods during update for testing # Desired: 3 Pods # maxSurge: 2 # During update: up to 5 Pods running temporarily maxSurge : 2 maxUnavailable - Maximum number of Pods that can be unavailable - Default: 1 - Higher value = faster update, lower availability - Set to 0 for zero-downtime deployments # Zero-downtime update maxUnavailable : 0 maxSurge : 1","title":"Parameters"},{"location":"12-deployments/update-strategies/#recreate","text":"Terminates all old Pods before starting new ones. Causes downtime. spec : strategy : type : Recreate Use Case: Development/testing environments where downtime is acceptable. Behavior: 1. All existing Pods are terminated 2. All new Pods are created 3. Brief service unavailability","title":"Recreate"},{"location":"12-deployments/update-strategies/#update-strategy-selection","text":"","title":"Update Strategy Selection"},{"location":"12-deployments/update-strategies/#rollingupdate-production","text":"spec : replicas : 3 strategy : type : RollingUpdate rollingUpdate : maxSurge : 1 maxUnavailable : 0 Ensures at least 3 Pods always available.","title":"RollingUpdate - Production"},{"location":"12-deployments/update-strategies/#recreate-development","text":"spec : replicas : 1 strategy : type : Recreate Simple and fast for non-critical environments.","title":"Recreate - Development"},{"location":"12-deployments/update-strategies/#aggressive-rollingupdate-fast-updates","text":"spec : replicas : 4 strategy : type : RollingUpdate rollingUpdate : maxSurge : 2 maxUnavailable : 1 Balances speed and availability.","title":"Aggressive RollingUpdate - Fast Updates"},{"location":"12-deployments/update-strategies/#blue-green-deployment-pattern","text":"Not a built-in strategy, but achievable with manual Deployment management: Create \"blue\" Deployment with current version Create \"green\" Deployment with new version Switch Service selector from blue to green Delete old blue Deployment # Blue Deployment apiVersion : apps/v1 kind : Deployment metadata : name : myapp-blue spec : template : metadata : labels : version : blue --- # Service routes to current version apiVersion : v1 kind : Service metadata : name : myapp spec : selector : version : blue # Points to blue or green","title":"Blue-Green Deployment Pattern"},{"location":"12-deployments/update-strategies/#canary-deployment-pattern","text":"Gradually roll out new version to small percentage of users: Keep most replicas on old version Add few replicas with new version Monitor metrics Gradually increase new version replicas Remove old version # Initial: 9 old Pods, 1 new Pod # Monitor: Check metrics on new Pod # 9 old, 2 new \u2192 8 old, 3 new \u2192 ... until fully updated","title":"Canary Deployment Pattern"},{"location":"12-deployments/update-strategies/#monitoring-updates","text":"# Watch update progress kubectl rollout status deployment <deployment-name> # View update history kubectl rollout history deployment <deployment-name> # Rollback to previous version kubectl rollout undo deployment <deployment-name>","title":"Monitoring Updates"},{"location":"12-deployments/probing/probing-psotgres/","text":"apiVersion: v1 kind: Pod metadata: name: postgres-pod spec: containers: - name: postgres image: postgres:latest env: - name: POSTGRES_PASSWORD value: dbpassword - name: POSTGRES_DB value: database ports: - containerPort: 5432 livenessProbe: tcpSocket: port: 5432 initialDelaySeconds: 30 periodSeconds: 10 readinessProbe: exec: command: - psql - -h - localhost - -U - postgres - -c - SELECT 1 initialDelaySeconds: 5 periodSeconds: 5","title":"Probing psotgres"},{"location":"13-manage-cluster-via-kubeadm/cluster-create/","text":"kubeadm Cluster Create & Upgrade Upgrade Order (Must Follow) Always upgrade kubeadm first Upgrade control plane nodes before worker nodes kubelet must be upgraded manually on every node Upgrade kubeadm Check Available Versions apt-cache show kubeadm Install Specific kubeadm Version apt-get install kubeadm = 1 .34.3-1.1 kubeadm version Upgrade the Control Plane See Upgrade Plan kubeadm upgrade plan Apply Upgrade (Example Version) kubeadm upgrade apply v1.34.1 Upgrade kubelet and kubectl apt-get install kubelet = 1 .34.1-1.1 kubectl = 1 .34.1-1.1 systemctl restart kubelet Create a Cluster (kubeadm init) Basic Init kubeadm init --kubernetes-version = 1 .34.1 Init with Pod Network CIDR kubeadm init \\ --kubernetes-version = 1 .34.1 \\ --pod-network-cidr = 192 .168.0.0/16 \\ --ignore-preflight-errors = NumCPU,Mem Pod Network CIDR Notes Required for some CNIs (Calico, Flannel) Must be valid CIDR (0\u2013255 per octet) Must not conflict with host or service CIDR Configure kubectl Access mkdir -p /root/.kube cp /etc/kubernetes/admin.conf /root/.kube/config If copying to another node: scp /etc/kubernetes/admin.conf node-summer:/root/.kube/config Join Worker Nodes Generate Join Command kubeadm token create --print-join-command Example Join kubeadm join 172 .30.1.2:6443 \\ --token m3qmx4.qa9c83ju82ru6njq \\ --discovery-token-ca-cert-hash sha256:4b0e1b1109e852de2f92bbb4bf0bdeae7616dd94ec1a35da21ba7ba83c0bb441","title":"kubeadm Cluster Create &amp; Upgrade"},{"location":"13-manage-cluster-via-kubeadm/cluster-create/#kubeadm-cluster-create-upgrade","text":"","title":"kubeadm Cluster Create &amp; Upgrade"},{"location":"13-manage-cluster-via-kubeadm/cluster-create/#upgrade-order-must-follow","text":"Always upgrade kubeadm first Upgrade control plane nodes before worker nodes kubelet must be upgraded manually on every node","title":"Upgrade Order (Must Follow)"},{"location":"13-manage-cluster-via-kubeadm/cluster-create/#upgrade-kubeadm","text":"","title":"Upgrade kubeadm"},{"location":"13-manage-cluster-via-kubeadm/cluster-create/#check-available-versions","text":"apt-cache show kubeadm","title":"Check Available Versions"},{"location":"13-manage-cluster-via-kubeadm/cluster-create/#install-specific-kubeadm-version","text":"apt-get install kubeadm = 1 .34.3-1.1 kubeadm version","title":"Install Specific kubeadm Version"},{"location":"13-manage-cluster-via-kubeadm/cluster-create/#upgrade-the-control-plane","text":"","title":"Upgrade the Control Plane"},{"location":"13-manage-cluster-via-kubeadm/cluster-create/#see-upgrade-plan","text":"kubeadm upgrade plan","title":"See Upgrade Plan"},{"location":"13-manage-cluster-via-kubeadm/cluster-create/#apply-upgrade-example-version","text":"kubeadm upgrade apply v1.34.1","title":"Apply Upgrade (Example Version)"},{"location":"13-manage-cluster-via-kubeadm/cluster-create/#upgrade-kubelet-and-kubectl","text":"apt-get install kubelet = 1 .34.1-1.1 kubectl = 1 .34.1-1.1 systemctl restart kubelet","title":"Upgrade kubelet and kubectl"},{"location":"13-manage-cluster-via-kubeadm/cluster-create/#create-a-cluster-kubeadm-init","text":"","title":"Create a Cluster (kubeadm init)"},{"location":"13-manage-cluster-via-kubeadm/cluster-create/#basic-init","text":"kubeadm init --kubernetes-version = 1 .34.1","title":"Basic Init"},{"location":"13-manage-cluster-via-kubeadm/cluster-create/#init-with-pod-network-cidr","text":"kubeadm init \\ --kubernetes-version = 1 .34.1 \\ --pod-network-cidr = 192 .168.0.0/16 \\ --ignore-preflight-errors = NumCPU,Mem","title":"Init with Pod Network CIDR"},{"location":"13-manage-cluster-via-kubeadm/cluster-create/#pod-network-cidr-notes","text":"Required for some CNIs (Calico, Flannel) Must be valid CIDR (0\u2013255 per octet) Must not conflict with host or service CIDR","title":"Pod Network CIDR Notes"},{"location":"13-manage-cluster-via-kubeadm/cluster-create/#configure-kubectl-access","text":"mkdir -p /root/.kube cp /etc/kubernetes/admin.conf /root/.kube/config If copying to another node: scp /etc/kubernetes/admin.conf node-summer:/root/.kube/config","title":"Configure kubectl Access"},{"location":"13-manage-cluster-via-kubeadm/cluster-create/#join-worker-nodes","text":"","title":"Join Worker Nodes"},{"location":"13-manage-cluster-via-kubeadm/cluster-create/#generate-join-command","text":"kubeadm token create --print-join-command","title":"Generate Join Command"},{"location":"13-manage-cluster-via-kubeadm/cluster-create/#example-join","text":"kubeadm join 172 .30.1.2:6443 \\ --token m3qmx4.qa9c83ju82ru6njq \\ --discovery-token-ca-cert-hash sha256:4b0e1b1109e852de2f92bbb4bf0bdeae7616dd94ec1a35da21ba7ba83c0bb441","title":"Example Join"},{"location":"13-manage-cluster-via-kubeadm/etcd/","text":"","title":"Etcd"},{"location":"13-manage-cluster-via-kubeadm/high-availablity/","text":"","title":"High availablity"},{"location":"14-ingress-controller-and-resources/north-south-traffic-management/","text":"Kubernetes Ingress Core Mental Model Ingress does only two things , always in this order: MATCH \u2192 ACTION MATCH decides whether a rule applies to a request ACTION decides what happens after the rule matches Ingress never : Creates traffic Modifies traffic implicitly Chooses backends randomly Everything you see in Ingress YAML is either: A matching constraint , or An explicit instruction If something changes (path, protocol, destination), there is always a line in YAML causing it . 1. MATCHING THEORY (WHEN A RULE APPLIES) Ingress matching happens in two layers : Host match Path match If either fails , the rule is ignored. 1.1 Host Matching (HTTP Host Header) Ingress matches against the HTTP Host header. apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : host-match spec : rules : - host : api.example.com http : paths : - path : / pathType : Prefix backend : service : name : app port : number : 80 Theory Host matching is exact Case-insensitive No wildcards unless controller-specific (not tested in CKA) Implications api.example.com \u2260 example.com Requests without the correct Host header skip this rule entirely On the exam, missing Host is a common silent failure 2. PATH MATCHING THEORY (CKA CRITICAL) After host match succeeds, Ingress evaluates path rules . Ingress supports three pathTypes , each with different semantics and priority . 2.1 Exact Path Match (Highest Priority) apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : exact-path spec : rules : - host : example.com http : paths : - path : /login pathType : Exact backend : service : name : auth-service port : number : 80 Theory Exact means byte-for-byte equality No normalization No implicit trailing slash handling Matches /login Does NOT match /login/ /login/admin Why it exists Guarantees deterministic routing Used for security-sensitive or fixed endpoints Exam Insight Exact paths always override Prefix paths with the same value. 2.2 Prefix Path Match (MOST USED) apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : prefix-path spec : rules : - host : example.com http : paths : - path : /api pathType : Prefix backend : service : name : api-service port : number : 80 Theory Prefix matches if the request path starts with the prefix Path is not modified This is hierarchical routing Matches /api /api/users /api/v1/orders Does NOT match /ap /myapi Why it is dominant Natural fit for APIs and microservices Minimal ambiguity Best performance Exam Insight If unsure which pathType to use \u2192 Prefix is almost always correct. 2.3 ImplementationSpecific (Regex) apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : regex-path annotations : nginx.ingress.kubernetes.io/use-regex : \"true\" spec : rules : - host : example.com http : paths : - path : ^/users/[0-9]+$ pathType : ImplementationSpecific backend : service : name : user-service port : number : 80 Theory Meaning depends on Ingress controller NGINX interprets this as a regular expression Kubernetes does not validate regex correctness Matches /users/123 Does NOT match /users/abc /users/123/profile Mandatory Requirements use-regex: \"true\" pathType: ImplementationSpecific Exam Insight Regex is tested only at a basic level : Numeric IDs Simple anchors ( ^ , $ ) Single capture groups 3. ACTION THEORY (WHAT HAPPENS AFTER MATCH) Once a rule matches, exactly one action occurs . 3.1 Forward (DEFAULT BEHAVIOR) apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : forward-only spec : rules : - host : example.com http : paths : - path : /app pathType : Prefix backend : service : name : app-service port : number : 80 Theory Forwarding is implicit No annotation required Request is proxied as-is What does NOT change URL in browser Path HTTP method Exam Insight If no annotation is present, the action is always forward . 3.2 Rewrite (SERVER-SIDE PATH TRANSFORMATION) Rewrite changes the path sent to the backend , not what the client sees. Strip Prefix (MOST IMPORTANT REWRITE) apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : rewrite-strip annotations : nginx.ingress.kubernetes.io/rewrite-target : /$2 spec : rules : - host : example.com http : paths : - path : /api(/|$)(.*) pathType : Prefix backend : service : name : backend port : number : 80 Theory Regex captures parts of the path Rewrite replaces the matched path before proxying Flow Client \u2192 /api/users Match \u2192 /api(/|$)(.*) $2 = users Backend receives /users Why this pattern exists Backends often do not know public URL structure API gateway behavior without changing services Add Prefix Rewrite apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : rewrite-add annotations : nginx.ingress.kubernetes.io/rewrite-target : /v1/$1 spec : rules : - host : example.com http : paths : - path : /(.*) pathType : Prefix backend : service : name : api port : number : 80 Theory Entire path is captured New prefix is injected Flow /users \u2192 backend sees /v1/users 3.3 Redirect (CLIENT-SIDE ACTION) Redirect tells the client to issue a new request . Permanent Redirect (301) apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : permanent-redirect annotations : nginx.ingress.kubernetes.io/permanent-redirect : https://new.example.com spec : rules : - host : old.example.com http : paths : - path : / pathType : Prefix backend : service : name : dummy port : number : 80 Theory Client receives 301 Browser updates address bar Search engines cache this Temporary Redirect (302) apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : temporary-redirect annotations : nginx.ingress.kubernetes.io/temporary-redirect : /maintenance spec : rules : - host : example.com http : paths : - path : /app pathType : Prefix backend : service : name : dummy port : number : 80 Theory Client retries at new location No caching assumption 4. PRIORITY THEORY (EXAM FAVORITE) Ingress chooses one rule only , based on specificity. apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : priority spec : rules : - host : example.com http : paths : - path : /api pathType : Exact backend : service : name : exact-api port : number : 80 - path : /api pathType : Prefix backend : service : name : prefix-api port : number : 80 - path : / pathType : Prefix backend : service : name : default port : number : 80 Priority Order Exact Longer Prefix Shorter Prefix / catch-all Exam Insight YAML order does not override specificity. 5. MULTI-PATH THEORY apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : multi-path spec : rules : - host : example.com http : paths : - path : /web pathType : Prefix backend : service : name : frontend port : number : 80 - path : /app pathType : Prefix backend : service : name : frontend port : number : 80 Theory Multiple public entry points Single backend No conflict because prefixes differ 6. CATCH-ALL THEORY apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : catch-all spec : rules : - host : example.com http : paths : - path : / pathType : Prefix backend : service : name : default-app port : number : 80 Theory Matches everything Used as fallback Lowest priority possible 7. TLS THEORY (CKA LEVEL) TLS Termination apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : tls-ingress spec : tls : - hosts : - example.com secretName : example-tls rules : - host : example.com http : paths : - path : / pathType : Prefix backend : service : name : web port : number : 80 Theory TLS ends at Ingress Backend sees plain HTTP Secret must be in same namespace HTTP \u2192 HTTPS Redirect apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : ssl-redirect annotations : nginx.ingress.kubernetes.io/ssl-redirect : \"true\" spec : rules : - host : example.com http : paths : - path : / pathType : Prefix backend : service : name : web port : number : 80 Theory HTTP requests receive redirect HTTPS requests are forwarded 8. CKA FAILURE MODES (WHY THINGS BREAK) Host mismatch \u2192 rule ignored Exact vs Prefix confusion \u2192 wrong backend Regex without use-regex \u2192 no match Wrong capture group \u2192 broken rewrite TLS secret missing \u2192 HTTPS fails FINAL CKA MENTAL MODEL (MEMORIZE) Ingress = MATCH \u2192 ACTION Match = Host + Path PathTypes = Exact | Prefix | ImplementationSpecific Priority = Exact > Longer Prefix > Shorter Prefix > / Forward is default Rewrite changes backend path only Redirect changes client behavior Regex requires annotation + ImplementationSpecific This is the entire CKA Ingress surface area .","title":"Kubernetes Ingress"},{"location":"14-ingress-controller-and-resources/north-south-traffic-management/#kubernetes-ingress","text":"","title":"Kubernetes Ingress"},{"location":"14-ingress-controller-and-resources/north-south-traffic-management/#core-mental-model","text":"Ingress does only two things , always in this order: MATCH \u2192 ACTION MATCH decides whether a rule applies to a request ACTION decides what happens after the rule matches Ingress never : Creates traffic Modifies traffic implicitly Chooses backends randomly Everything you see in Ingress YAML is either: A matching constraint , or An explicit instruction If something changes (path, protocol, destination), there is always a line in YAML causing it .","title":"Core Mental Model"},{"location":"14-ingress-controller-and-resources/north-south-traffic-management/#1-matching-theory-when-a-rule-applies","text":"Ingress matching happens in two layers : Host match Path match If either fails , the rule is ignored.","title":"1. MATCHING THEORY (WHEN A RULE APPLIES)"},{"location":"14-ingress-controller-and-resources/north-south-traffic-management/#11-host-matching-http-host-header","text":"Ingress matches against the HTTP Host header. apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : host-match spec : rules : - host : api.example.com http : paths : - path : / pathType : Prefix backend : service : name : app port : number : 80 Theory Host matching is exact Case-insensitive No wildcards unless controller-specific (not tested in CKA) Implications api.example.com \u2260 example.com Requests without the correct Host header skip this rule entirely On the exam, missing Host is a common silent failure","title":"1.1 Host Matching (HTTP Host Header)"},{"location":"14-ingress-controller-and-resources/north-south-traffic-management/#2-path-matching-theory-cka-critical","text":"After host match succeeds, Ingress evaluates path rules . Ingress supports three pathTypes , each with different semantics and priority .","title":"2. PATH MATCHING THEORY (CKA CRITICAL)"},{"location":"14-ingress-controller-and-resources/north-south-traffic-management/#21-exact-path-match-highest-priority","text":"apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : exact-path spec : rules : - host : example.com http : paths : - path : /login pathType : Exact backend : service : name : auth-service port : number : 80 Theory Exact means byte-for-byte equality No normalization No implicit trailing slash handling Matches /login Does NOT match /login/ /login/admin Why it exists Guarantees deterministic routing Used for security-sensitive or fixed endpoints Exam Insight Exact paths always override Prefix paths with the same value.","title":"2.1 Exact Path Match (Highest Priority)"},{"location":"14-ingress-controller-and-resources/north-south-traffic-management/#22-prefix-path-match-most-used","text":"apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : prefix-path spec : rules : - host : example.com http : paths : - path : /api pathType : Prefix backend : service : name : api-service port : number : 80 Theory Prefix matches if the request path starts with the prefix Path is not modified This is hierarchical routing Matches /api /api/users /api/v1/orders Does NOT match /ap /myapi Why it is dominant Natural fit for APIs and microservices Minimal ambiguity Best performance Exam Insight If unsure which pathType to use \u2192 Prefix is almost always correct.","title":"2.2 Prefix Path Match (MOST USED)"},{"location":"14-ingress-controller-and-resources/north-south-traffic-management/#23-implementationspecific-regex","text":"apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : regex-path annotations : nginx.ingress.kubernetes.io/use-regex : \"true\" spec : rules : - host : example.com http : paths : - path : ^/users/[0-9]+$ pathType : ImplementationSpecific backend : service : name : user-service port : number : 80 Theory Meaning depends on Ingress controller NGINX interprets this as a regular expression Kubernetes does not validate regex correctness Matches /users/123 Does NOT match /users/abc /users/123/profile Mandatory Requirements use-regex: \"true\" pathType: ImplementationSpecific Exam Insight Regex is tested only at a basic level : Numeric IDs Simple anchors ( ^ , $ ) Single capture groups","title":"2.3 ImplementationSpecific (Regex)"},{"location":"14-ingress-controller-and-resources/north-south-traffic-management/#3-action-theory-what-happens-after-match","text":"Once a rule matches, exactly one action occurs .","title":"3. ACTION THEORY (WHAT HAPPENS AFTER MATCH)"},{"location":"14-ingress-controller-and-resources/north-south-traffic-management/#31-forward-default-behavior","text":"apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : forward-only spec : rules : - host : example.com http : paths : - path : /app pathType : Prefix backend : service : name : app-service port : number : 80 Theory Forwarding is implicit No annotation required Request is proxied as-is What does NOT change URL in browser Path HTTP method Exam Insight If no annotation is present, the action is always forward .","title":"3.1 Forward (DEFAULT BEHAVIOR)"},{"location":"14-ingress-controller-and-resources/north-south-traffic-management/#32-rewrite-server-side-path-transformation","text":"Rewrite changes the path sent to the backend , not what the client sees.","title":"3.2 Rewrite (SERVER-SIDE PATH TRANSFORMATION)"},{"location":"14-ingress-controller-and-resources/north-south-traffic-management/#strip-prefix-most-important-rewrite","text":"apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : rewrite-strip annotations : nginx.ingress.kubernetes.io/rewrite-target : /$2 spec : rules : - host : example.com http : paths : - path : /api(/|$)(.*) pathType : Prefix backend : service : name : backend port : number : 80 Theory Regex captures parts of the path Rewrite replaces the matched path before proxying Flow Client \u2192 /api/users Match \u2192 /api(/|$)(.*) $2 = users Backend receives /users Why this pattern exists Backends often do not know public URL structure API gateway behavior without changing services","title":"Strip Prefix (MOST IMPORTANT REWRITE)"},{"location":"14-ingress-controller-and-resources/north-south-traffic-management/#add-prefix-rewrite","text":"apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : rewrite-add annotations : nginx.ingress.kubernetes.io/rewrite-target : /v1/$1 spec : rules : - host : example.com http : paths : - path : /(.*) pathType : Prefix backend : service : name : api port : number : 80 Theory Entire path is captured New prefix is injected Flow /users \u2192 backend sees /v1/users","title":"Add Prefix Rewrite"},{"location":"14-ingress-controller-and-resources/north-south-traffic-management/#33-redirect-client-side-action","text":"Redirect tells the client to issue a new request .","title":"3.3 Redirect (CLIENT-SIDE ACTION)"},{"location":"14-ingress-controller-and-resources/north-south-traffic-management/#permanent-redirect-301","text":"apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : permanent-redirect annotations : nginx.ingress.kubernetes.io/permanent-redirect : https://new.example.com spec : rules : - host : old.example.com http : paths : - path : / pathType : Prefix backend : service : name : dummy port : number : 80 Theory Client receives 301 Browser updates address bar Search engines cache this","title":"Permanent Redirect (301)"},{"location":"14-ingress-controller-and-resources/north-south-traffic-management/#temporary-redirect-302","text":"apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : temporary-redirect annotations : nginx.ingress.kubernetes.io/temporary-redirect : /maintenance spec : rules : - host : example.com http : paths : - path : /app pathType : Prefix backend : service : name : dummy port : number : 80 Theory Client retries at new location No caching assumption","title":"Temporary Redirect (302)"},{"location":"14-ingress-controller-and-resources/north-south-traffic-management/#4-priority-theory-exam-favorite","text":"Ingress chooses one rule only , based on specificity. apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : priority spec : rules : - host : example.com http : paths : - path : /api pathType : Exact backend : service : name : exact-api port : number : 80 - path : /api pathType : Prefix backend : service : name : prefix-api port : number : 80 - path : / pathType : Prefix backend : service : name : default port : number : 80 Priority Order Exact Longer Prefix Shorter Prefix / catch-all Exam Insight YAML order does not override specificity.","title":"4. PRIORITY THEORY (EXAM FAVORITE)"},{"location":"14-ingress-controller-and-resources/north-south-traffic-management/#5-multi-path-theory","text":"apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : multi-path spec : rules : - host : example.com http : paths : - path : /web pathType : Prefix backend : service : name : frontend port : number : 80 - path : /app pathType : Prefix backend : service : name : frontend port : number : 80 Theory Multiple public entry points Single backend No conflict because prefixes differ","title":"5. MULTI-PATH THEORY"},{"location":"14-ingress-controller-and-resources/north-south-traffic-management/#6-catch-all-theory","text":"apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : catch-all spec : rules : - host : example.com http : paths : - path : / pathType : Prefix backend : service : name : default-app port : number : 80 Theory Matches everything Used as fallback Lowest priority possible","title":"6. CATCH-ALL THEORY"},{"location":"14-ingress-controller-and-resources/north-south-traffic-management/#7-tls-theory-cka-level","text":"","title":"7. TLS THEORY (CKA LEVEL)"},{"location":"14-ingress-controller-and-resources/north-south-traffic-management/#tls-termination","text":"apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : tls-ingress spec : tls : - hosts : - example.com secretName : example-tls rules : - host : example.com http : paths : - path : / pathType : Prefix backend : service : name : web port : number : 80 Theory TLS ends at Ingress Backend sees plain HTTP Secret must be in same namespace","title":"TLS Termination"},{"location":"14-ingress-controller-and-resources/north-south-traffic-management/#http-https-redirect","text":"apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : ssl-redirect annotations : nginx.ingress.kubernetes.io/ssl-redirect : \"true\" spec : rules : - host : example.com http : paths : - path : / pathType : Prefix backend : service : name : web port : number : 80 Theory HTTP requests receive redirect HTTPS requests are forwarded","title":"HTTP \u2192 HTTPS Redirect"},{"location":"14-ingress-controller-and-resources/north-south-traffic-management/#8-cka-failure-modes-why-things-break","text":"Host mismatch \u2192 rule ignored Exact vs Prefix confusion \u2192 wrong backend Regex without use-regex \u2192 no match Wrong capture group \u2192 broken rewrite TLS secret missing \u2192 HTTPS fails","title":"8. CKA FAILURE MODES (WHY THINGS BREAK)"},{"location":"14-ingress-controller-and-resources/north-south-traffic-management/#final-cka-mental-model-memorize","text":"Ingress = MATCH \u2192 ACTION Match = Host + Path PathTypes = Exact | Prefix | ImplementationSpecific Priority = Exact > Longer Prefix > Shorter Prefix > / Forward is default Rewrite changes backend path only Redirect changes client behavior Regex requires annotation + ImplementationSpecific This is the entire CKA Ingress surface area .","title":"FINAL CKA MENTAL MODEL (MEMORIZE)"},{"location":"14-ingress-controller-and-resources/questions/","text":"NGINX Ingress Controller Practice Questions 1. Simple Host-Based Routing with NGINX Ingress You need to route traffic based on hostnames to different services using NGINX Ingress Controller: - app.example.com \u2192 frontend-service:8080 - api.example.com \u2192 backend-service:3000 - Both should be served on port 80 - Use the NGINX Ingress Controller (already installed) 2. Path Rewriting with NGINX Annotations You have a single domain services.company.com with these requirements using NGINX Ingress: - /shop/products \u2192 product-service:8080 (should receive /products ) - /shop/cart \u2192 cart-service:9090 (should receive /cart ) - /auth/login \u2192 auth-service:7070 (should receive /login ) - You must use NGINX Ingress annotations for path rewriting 3. Path Redirects with NGINX Configuration You're migrating from an old path structure to a new one using NGINX Ingress: - /old/dashboard \u2192 redirect to /new/dashboard (301 permanent) - /old/api/v1 \u2192 redirect to /api/v2 (301 permanent) - /temp/redirect \u2192 redirect to /new/temp (302 temporary) - Domain: redirect.example.com - Use NGINX Ingress annotations for redirects 4. TLS Configuration with NGINX Ingress You need to configure TLS for NGINX Ingress with: 1. TLS certificate stored in secret named wildcard-tls 2. Serve app.company.com and api.company.com over HTTPS 3. Redirect HTTP to HTTPS automatically 4. Use appropriate NGINX Ingress annotations 5. Exact vs Prefix Path Matching with NGINX You have these specific routing requirements using NGINX Ingress: - /api/health (exact match only) \u2192 health-check:8080 - /api/users (prefix match) \u2192 user-service:3000 (matches /api/users , /api/users/123 , etc.) - /admin (exact match) \u2192 admin-panel:9000 - /admin/config (exact match, higher priority than /admin prefix) \u2192 config-service:4000 - Implement proper path matching using NGINX Ingress path types and annotations if needed","title":"NGINX Ingress Controller Practice Questions"},{"location":"14-ingress-controller-and-resources/questions/#nginx-ingress-controller-practice-questions","text":"","title":"NGINX Ingress Controller Practice Questions"},{"location":"14-ingress-controller-and-resources/questions/#1-simple-host-based-routing-with-nginx-ingress","text":"You need to route traffic based on hostnames to different services using NGINX Ingress Controller: - app.example.com \u2192 frontend-service:8080 - api.example.com \u2192 backend-service:3000 - Both should be served on port 80 - Use the NGINX Ingress Controller (already installed)","title":"1. Simple Host-Based Routing with NGINX Ingress"},{"location":"14-ingress-controller-and-resources/questions/#2-path-rewriting-with-nginx-annotations","text":"You have a single domain services.company.com with these requirements using NGINX Ingress: - /shop/products \u2192 product-service:8080 (should receive /products ) - /shop/cart \u2192 cart-service:9090 (should receive /cart ) - /auth/login \u2192 auth-service:7070 (should receive /login ) - You must use NGINX Ingress annotations for path rewriting","title":"2. Path Rewriting with NGINX Annotations"},{"location":"14-ingress-controller-and-resources/questions/#3-path-redirects-with-nginx-configuration","text":"You're migrating from an old path structure to a new one using NGINX Ingress: - /old/dashboard \u2192 redirect to /new/dashboard (301 permanent) - /old/api/v1 \u2192 redirect to /api/v2 (301 permanent) - /temp/redirect \u2192 redirect to /new/temp (302 temporary) - Domain: redirect.example.com - Use NGINX Ingress annotations for redirects","title":"3. Path Redirects with NGINX Configuration"},{"location":"14-ingress-controller-and-resources/questions/#4-tls-configuration-with-nginx-ingress","text":"You need to configure TLS for NGINX Ingress with: 1. TLS certificate stored in secret named wildcard-tls 2. Serve app.company.com and api.company.com over HTTPS 3. Redirect HTTP to HTTPS automatically 4. Use appropriate NGINX Ingress annotations","title":"4. TLS Configuration with NGINX Ingress"},{"location":"14-ingress-controller-and-resources/questions/#5-exact-vs-prefix-path-matching-with-nginx","text":"You have these specific routing requirements using NGINX Ingress: - /api/health (exact match only) \u2192 health-check:8080 - /api/users (prefix match) \u2192 user-service:3000 (matches /api/users , /api/users/123 , etc.) - /admin (exact match) \u2192 admin-panel:9000 - /admin/config (exact match, higher priority than /admin prefix) \u2192 config-service:4000 - Implement proper path matching using NGINX Ingress path types and annotations if needed","title":"5. Exact vs Prefix Path Matching with NGINX"},{"location":"15-troubleshoot-controlplane/prerequisite-and-approach/","text":"How to Approach Control Plane Issues Prerequisite Knowledge /etc/kubernetes/manifests Location of control plane static pod manifests. Places to Look for Logs Kubelet Logs Use these when the kubelet itself is failing due to syntax errors, invalid flags, or startup issues. journalctl -u kubelet journalctl -u kubelet -n 100 journalctl -u kubelet -f journalctl -u kubelet -b journalctl -u kubelet -p err journalctl -u kubelet --since \"10 minutes ago\" To inspect kubelet service flags and configuration location: systemctl cat kubelet System logs (if applicable): /var/log/syslog | grep <keyword> Static Pod Logs Use these when the kubelet is running and able to create static pods, but the pods are crashing. Pod logs: /var/log/pods/ 0.log \u2192 current log 1.log \u2192 rotated log Container logs: /var/log/containers/ This directory contains symlinks to pod logs. Container Runtime Use these when the kubelet created the static pod but the container is failing. crictl ps crictl ps -a crictl logs <container-id> Health and Probe Endpoints kube-apiserver Probe Type Endpoint Port Startup /livez 6443 Liveness /livez 6443 Readiness /readyz 6443 etcd Probe Type Endpoint Port Liveness /health 2379 Readiness /health 2379 kube-controller-manager Probe Type Endpoint Port Liveness /healthz 10257 Readiness /healthz 10257 kube-scheduler Probe Type Endpoint Port Liveness /healthz 10259 Readiness /healthz 10259 kubelet Endpoint Address /healthz 127.0.0.1:10248 Key Component Ports Component Purpose Port kube-apiserver API + health 6443 etcd Client API 2379 kubelet Local health 10248 kube-apiserver Minimal Required Flags --advertise-address=<node-ip> --secure-port=6443 --etcd-servers=https://127.0.0.1:2379 TLS Certificate Paths (Do Not Invent) API Server /etc/kubernetes/pki/apiserver.crt /etc/kubernetes/pki/apiserver.key /etc/kubernetes/pki/ca.crt etcd /etc/kubernetes/pki/etcd/ca.crt /etc/kubernetes/pki/etcd/server.crt /etc/kubernetes/pki/etcd/server.key If these files exist, do not modify them.","title":"How to Approach Control Plane Issues"},{"location":"15-troubleshoot-controlplane/prerequisite-and-approach/#how-to-approach-control-plane-issues","text":"","title":"How to Approach Control Plane Issues"},{"location":"15-troubleshoot-controlplane/prerequisite-and-approach/#prerequisite-knowledge","text":"/etc/kubernetes/manifests Location of control plane static pod manifests.","title":"Prerequisite Knowledge"},{"location":"15-troubleshoot-controlplane/prerequisite-and-approach/#places-to-look-for-logs","text":"","title":"Places to Look for Logs"},{"location":"15-troubleshoot-controlplane/prerequisite-and-approach/#kubelet-logs","text":"Use these when the kubelet itself is failing due to syntax errors, invalid flags, or startup issues. journalctl -u kubelet journalctl -u kubelet -n 100 journalctl -u kubelet -f journalctl -u kubelet -b journalctl -u kubelet -p err journalctl -u kubelet --since \"10 minutes ago\" To inspect kubelet service flags and configuration location: systemctl cat kubelet System logs (if applicable): /var/log/syslog | grep <keyword>","title":"Kubelet Logs"},{"location":"15-troubleshoot-controlplane/prerequisite-and-approach/#static-pod-logs","text":"Use these when the kubelet is running and able to create static pods, but the pods are crashing. Pod logs: /var/log/pods/ 0.log \u2192 current log 1.log \u2192 rotated log Container logs: /var/log/containers/ This directory contains symlinks to pod logs.","title":"Static Pod Logs"},{"location":"15-troubleshoot-controlplane/prerequisite-and-approach/#container-runtime","text":"Use these when the kubelet created the static pod but the container is failing. crictl ps crictl ps -a crictl logs <container-id>","title":"Container Runtime"},{"location":"15-troubleshoot-controlplane/prerequisite-and-approach/#health-and-probe-endpoints","text":"","title":"Health and Probe Endpoints"},{"location":"15-troubleshoot-controlplane/prerequisite-and-approach/#kube-apiserver","text":"Probe Type Endpoint Port Startup /livez 6443 Liveness /livez 6443 Readiness /readyz 6443","title":"kube-apiserver"},{"location":"15-troubleshoot-controlplane/prerequisite-and-approach/#etcd","text":"Probe Type Endpoint Port Liveness /health 2379 Readiness /health 2379","title":"etcd"},{"location":"15-troubleshoot-controlplane/prerequisite-and-approach/#kube-controller-manager","text":"Probe Type Endpoint Port Liveness /healthz 10257 Readiness /healthz 10257","title":"kube-controller-manager"},{"location":"15-troubleshoot-controlplane/prerequisite-and-approach/#kube-scheduler","text":"Probe Type Endpoint Port Liveness /healthz 10259 Readiness /healthz 10259","title":"kube-scheduler"},{"location":"15-troubleshoot-controlplane/prerequisite-and-approach/#kubelet","text":"Endpoint Address /healthz 127.0.0.1:10248","title":"kubelet"},{"location":"15-troubleshoot-controlplane/prerequisite-and-approach/#key-component-ports","text":"Component Purpose Port kube-apiserver API + health 6443 etcd Client API 2379 kubelet Local health 10248","title":"Key Component Ports"},{"location":"15-troubleshoot-controlplane/prerequisite-and-approach/#kube-apiserver-minimal-required-flags","text":"--advertise-address=<node-ip> --secure-port=6443 --etcd-servers=https://127.0.0.1:2379","title":"kube-apiserver Minimal Required Flags"},{"location":"15-troubleshoot-controlplane/prerequisite-and-approach/#tls-certificate-paths-do-not-invent","text":"","title":"TLS Certificate Paths (Do Not Invent)"},{"location":"15-troubleshoot-controlplane/prerequisite-and-approach/#api-server","text":"/etc/kubernetes/pki/apiserver.crt /etc/kubernetes/pki/apiserver.key /etc/kubernetes/pki/ca.crt","title":"API Server"},{"location":"15-troubleshoot-controlplane/prerequisite-and-approach/#etcd_1","text":"/etc/kubernetes/pki/etcd/ca.crt /etc/kubernetes/pki/etcd/server.crt /etc/kubernetes/pki/etcd/server.key If these files exist, do not modify them.","title":"etcd"},{"location":"17-authorization/cani/","text":"Below is SECTION 5 only. Focused on kubectl auth can-i \u2014 complete, correct usage with full examples and explanations . This is an exam-critical debugging tool . Section 5 \u2014 kubectl auth can-i (Deep, Correct, Practical) What kubectl auth can-i Actually Does kubectl auth can-i answers one question only : Does RBAC allow this subject to perform this action? It checks: Roles ClusterRoles RoleBindings ClusterRoleBindings It does NOT check: Admission controllers PodSecurity Resource existence Runtime errors If can-i says yes and action fails \u2192 RBAC is not the issue. 5.1 Basic Usage (Current User) Example 1 \u2014 Can I read pods? kubectl auth can-i get pods Explanation: Uses current kubeconfig identity Checks default namespace Example 2 \u2014 Explicit namespace kubectl auth can-i list pods -n dev Explanation: RBAC evaluation is namespace-aware Missing -n often causes confusion 5.2 Acting As Another Identity ( --as ) Example 3 \u2014 Acting as a User kubectl auth can-i delete deployments \\ --as alice \\ -n prod Explanation: alice must match exact user name from auth provider Namespace is mandatory for namespaced resources Example 4 \u2014 Acting as a ServiceAccount (CORRECT FORMAT) kubectl auth can-i get pods \\ --as system:serviceaccount:default:my-sa \\ -n default Explanation: ServiceAccount identity is always expanded Short names ( --as my-sa ) will not match 5.3 Common Mistake \u2014 Wrong ServiceAccount Identity \u274c Wrong kubectl auth can-i get pods --as prometheus \u2705 Correct kubectl auth can-i get pods \\ --as system:serviceaccount:monitoring:prometheus \\ -n prod Explanation: RBAC matches full identity string Namespace is part of identity 5.4 Checking Cluster-Scoped Resources Example 5 \u2014 Nodes (cluster-scoped) kubectl auth can-i list nodes Explanation: No namespace flag Requires ClusterRole + ClusterRoleBinding Example 6 \u2014 Namespaces kubectl auth can-i get namespaces Explanation: Namespaces are cluster-scoped RoleBinding can never grant this 5.5 Verb + Resource Accuracy (Exam Trap) Example 7 \u2014 Wrong resource name kubectl auth can-i get deployment Result: no Correct kubectl auth can-i get deployments.apps Explanation: CLI uses fully qualified resource Same rule as kubectl create role 5.6 Subresource Checks Example 8 \u2014 Logs kubectl auth can-i get pods/log -n debug Explanation: pods permission \u2260 pods/log This explains kubectl logs failures Example 9 \u2014 Exec kubectl auth can-i create pods/exec -n dev Explanation: exec uses create verb Very common exam trap 5.7 Listing Effective Permissions Example 10 \u2014 What can I do? kubectl auth can-i --list Explanation: Shows all RBAC-allowed actions Only for current user Namespaced output if -n is provided Example 11 \u2014 Namespace-specific listing kubectl auth can-i --list -n prod Explanation: Extremely useful to debug RoleBindings 5.8 Wildcard Checks (Debug Only) Example 12 \u2014 Full access check kubectl auth can-i '*' '*' Explanation: Indicates cluster-admin-like access Never use as justification for least privilege 5.9 ResourceNames Behavior Example 13 \u2014 Specific object access kubectl auth can-i get secrets/db-creds -n finance Explanation: Tests resourceNames list will still fail Example 14 \u2014 List still denied kubectl auth can-i list secrets -n finance Explanation: resourceNames blocks list Very common misunderstanding 5.10 Debug Flow Using can-i (Exam Pattern) Typical Exam Debug Steps Test action: kubectl auth can-i create pods -n dev Test identity explicitly: kubectl auth can-i create pods \\ --as system:serviceaccount:ci:ci-bot \\ -n dev Test subresource if needed: kubectl auth can-i create pods/exec -n dev If all return yes \u2192 RBAC is correct. Section 5 \u2014 Hard Rules to Remember can-i checks RBAC only Always specify -n for namespaced resources ServiceAccount must use full identity string Subresources must be checked explicitly Cluster-scoped resources never use namespace --list is your RBAC truth view Say \u201csection 6\u201d for system users, nodes, and control-plane components or \u201cfinal cram\u201d for a one-page RBAC exam summary .","title":"Cani"},{"location":"17-authorization/cani/#section-5-kubectl-auth-can-i-deep-correct-practical","text":"","title":"Section 5 \u2014 kubectl auth can-i (Deep, Correct, Practical)"},{"location":"17-authorization/cani/#what-kubectl-auth-can-i-actually-does","text":"kubectl auth can-i answers one question only : Does RBAC allow this subject to perform this action? It checks: Roles ClusterRoles RoleBindings ClusterRoleBindings It does NOT check: Admission controllers PodSecurity Resource existence Runtime errors If can-i says yes and action fails \u2192 RBAC is not the issue.","title":"What kubectl auth can-i Actually Does"},{"location":"17-authorization/cani/#51-basic-usage-current-user","text":"","title":"5.1 Basic Usage (Current User)"},{"location":"17-authorization/cani/#example-1-can-i-read-pods","text":"kubectl auth can-i get pods Explanation: Uses current kubeconfig identity Checks default namespace","title":"Example 1 \u2014 Can I read pods?"},{"location":"17-authorization/cani/#example-2-explicit-namespace","text":"kubectl auth can-i list pods -n dev Explanation: RBAC evaluation is namespace-aware Missing -n often causes confusion","title":"Example 2 \u2014 Explicit namespace"},{"location":"17-authorization/cani/#52-acting-as-another-identity-as","text":"","title":"5.2 Acting As Another Identity (--as)"},{"location":"17-authorization/cani/#example-3-acting-as-a-user","text":"kubectl auth can-i delete deployments \\ --as alice \\ -n prod Explanation: alice must match exact user name from auth provider Namespace is mandatory for namespaced resources","title":"Example 3 \u2014 Acting as a User"},{"location":"17-authorization/cani/#example-4-acting-as-a-serviceaccount-correct-format","text":"kubectl auth can-i get pods \\ --as system:serviceaccount:default:my-sa \\ -n default Explanation: ServiceAccount identity is always expanded Short names ( --as my-sa ) will not match","title":"Example 4 \u2014 Acting as a ServiceAccount (CORRECT FORMAT)"},{"location":"17-authorization/cani/#53-common-mistake-wrong-serviceaccount-identity","text":"","title":"5.3 Common Mistake \u2014 Wrong ServiceAccount Identity"},{"location":"17-authorization/cani/#wrong","text":"kubectl auth can-i get pods --as prometheus","title":"\u274c Wrong"},{"location":"17-authorization/cani/#correct","text":"kubectl auth can-i get pods \\ --as system:serviceaccount:monitoring:prometheus \\ -n prod Explanation: RBAC matches full identity string Namespace is part of identity","title":"\u2705 Correct"},{"location":"17-authorization/cani/#54-checking-cluster-scoped-resources","text":"","title":"5.4 Checking Cluster-Scoped Resources"},{"location":"17-authorization/cani/#example-5-nodes-cluster-scoped","text":"kubectl auth can-i list nodes Explanation: No namespace flag Requires ClusterRole + ClusterRoleBinding","title":"Example 5 \u2014 Nodes (cluster-scoped)"},{"location":"17-authorization/cani/#example-6-namespaces","text":"kubectl auth can-i get namespaces Explanation: Namespaces are cluster-scoped RoleBinding can never grant this","title":"Example 6 \u2014 Namespaces"},{"location":"17-authorization/cani/#55-verb-resource-accuracy-exam-trap","text":"","title":"5.5 Verb + Resource Accuracy (Exam Trap)"},{"location":"17-authorization/cani/#example-7-wrong-resource-name","text":"kubectl auth can-i get deployment Result: no","title":"Example 7 \u2014 Wrong resource name"},{"location":"17-authorization/cani/#correct_1","text":"kubectl auth can-i get deployments.apps Explanation: CLI uses fully qualified resource Same rule as kubectl create role","title":"Correct"},{"location":"17-authorization/cani/#56-subresource-checks","text":"","title":"5.6 Subresource Checks"},{"location":"17-authorization/cani/#example-8-logs","text":"kubectl auth can-i get pods/log -n debug Explanation: pods permission \u2260 pods/log This explains kubectl logs failures","title":"Example 8 \u2014 Logs"},{"location":"17-authorization/cani/#example-9-exec","text":"kubectl auth can-i create pods/exec -n dev Explanation: exec uses create verb Very common exam trap","title":"Example 9 \u2014 Exec"},{"location":"17-authorization/cani/#57-listing-effective-permissions","text":"","title":"5.7 Listing Effective Permissions"},{"location":"17-authorization/cani/#example-10-what-can-i-do","text":"kubectl auth can-i --list Explanation: Shows all RBAC-allowed actions Only for current user Namespaced output if -n is provided","title":"Example 10 \u2014 What can I do?"},{"location":"17-authorization/cani/#example-11-namespace-specific-listing","text":"kubectl auth can-i --list -n prod Explanation: Extremely useful to debug RoleBindings","title":"Example 11 \u2014 Namespace-specific listing"},{"location":"17-authorization/cani/#58-wildcard-checks-debug-only","text":"","title":"5.8 Wildcard Checks (Debug Only)"},{"location":"17-authorization/cani/#example-12-full-access-check","text":"kubectl auth can-i '*' '*' Explanation: Indicates cluster-admin-like access Never use as justification for least privilege","title":"Example 12 \u2014 Full access check"},{"location":"17-authorization/cani/#59-resourcenames-behavior","text":"","title":"5.9 ResourceNames Behavior"},{"location":"17-authorization/cani/#example-13-specific-object-access","text":"kubectl auth can-i get secrets/db-creds -n finance Explanation: Tests resourceNames list will still fail","title":"Example 13 \u2014 Specific object access"},{"location":"17-authorization/cani/#example-14-list-still-denied","text":"kubectl auth can-i list secrets -n finance Explanation: resourceNames blocks list Very common misunderstanding","title":"Example 14 \u2014 List still denied"},{"location":"17-authorization/cani/#510-debug-flow-using-can-i-exam-pattern","text":"","title":"5.10 Debug Flow Using can-i (Exam Pattern)"},{"location":"17-authorization/cani/#typical-exam-debug-steps","text":"Test action: kubectl auth can-i create pods -n dev Test identity explicitly: kubectl auth can-i create pods \\ --as system:serviceaccount:ci:ci-bot \\ -n dev Test subresource if needed: kubectl auth can-i create pods/exec -n dev If all return yes \u2192 RBAC is correct.","title":"Typical Exam Debug Steps"},{"location":"17-authorization/cani/#section-5-hard-rules-to-remember","text":"can-i checks RBAC only Always specify -n for namespaced resources ServiceAccount must use full identity string Subresources must be checked explicitly Cluster-scoped resources never use namespace --list is your RBAC truth view Say \u201csection 6\u201d for system users, nodes, and control-plane components or \u201cfinal cram\u201d for a one-page RBAC exam summary .","title":"Section 5 \u2014 Hard Rules to Remember"},{"location":"17-authorization/examples/","text":"Here are 20 YAML examples with detailed explanations and 20 CLI examples with explanations, properly formatted. YAML Examples with Explanations Example 1: Basic Role for Pod Reading This creates a Role that allows reading pods in the default namespace. apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-reader namespace : default rules : - apiGroups : [ \"\" ] resources : [ \"pods\" ] verbs : [ \"get\" , \"list\" , \"watch\" ] The Role defines permissions but does not grant access. It specifies get, list, and watch verbs on pods in the core API group. Example 2: RoleBinding for User This binds the pod-reader Role to a user named alice. apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-reader-binding namespace : default subjects : - kind : User name : alice roleRef : kind : Role name : pod-reader apiGroup : rbac.authorization.k8s.io The RoleBinding grants alice the permissions defined in the pod-reader Role, but only in the default namespace. Example 3: ClusterRole for Node Reading This creates a ClusterRole that allows reading nodes cluster-wide. apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : node-reader rules : - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"get\" , \"list\" , \"watch\" ] ClusterRoles exist at cluster scope and can reference cluster-scoped resources like nodes. Example 4: ClusterRoleBinding for Group This grants node-reader permissions to all users in the monitoring group. apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : node-reader-binding subjects : - kind : Group name : monitoring roleRef : kind : ClusterRole name : node-reader apiGroup : rbac.authorization.k8s.io The binding is cluster-wide, so group members can read nodes in any namespace. Example 5: Role with Multiple Resources This Role allows management of both pods and services. apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : resource-manager namespace : production rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"services\" ] verbs : [ \"create\" , \"get\" , \"update\" , \"delete\" , \"list\" ] A single Role can contain rules for multiple resources in the same API group. Example 6: Role with Specific Resource Names This Role allows access only to specific ConfigMaps by name. apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : config-reader namespace : default rules : - apiGroups : [ \"\" ] resources : [ \"configmaps\" ] resourceNames : [ \"app-config\" , \"database-config\" ] verbs : [ \"get\" ] The resourceNames field restricts access to only those named resources, not all ConfigMaps. Example 7: Role for Subresources This Role allows accessing pod logs and execution. apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-debugger namespace : default rules : - apiGroups : [ \"\" ] resources : [ \"pods/log\" , \"pods/exec\" ] verbs : [ \"get\" , \"create\" ] Subresources like logs and exec require explicit permissions separate from the parent resource. Example 8: ServiceAccount Binding This binds a ServiceAccount to a Role in its namespace. apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : sa-binding namespace : default subjects : - kind : ServiceAccount name : my-app-sa namespace : default roleRef : kind : Role name : pod-reader apiGroup : rbac.authorization.k8s.io ServiceAccounts must specify their namespace when referenced in bindings. Example 9: Cross-Namespace ServiceAccount Access This allows a ServiceAccount from monitoring namespace to read pods in default namespace. apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : cross-ns-binding namespace : default subjects : - kind : ServiceAccount name : prometheus namespace : monitoring roleRef : kind : Role name : pod-reader apiGroup : rbac.authorization.k8s.io ServiceAccounts can be granted permissions in namespaces other than where they exist. Example 10: ClusterRole Bound to Namespace This uses a ClusterRole but limits it to a specific namespace via RoleBinding. apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : limited-clusterrole namespace : staging subjects : - kind : User name : tester roleRef : kind : ClusterRole name : admin apiGroup : rbac.authorization.k8s.io Even though admin is a ClusterRole, the RoleBinding limits its permissions to the staging namespace only. Example 11: Role with API Group Specific Resources This Role manages resources from the apps API group. apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : apps-manager namespace : default rules : - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" ] verbs : [ \"get\" , \"list\" , \"create\" , \"update\" , \"delete\" ] Different API groups require separate rules or comma-separated entries in the apiGroups array. Example 12: Role with Multiple API Groups This Role covers resources from multiple API groups. apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : multi-group-role namespace : default rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"services\" ] verbs : [ \"get\" , \"list\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" ] verbs : [ \"get\" , \"list\" ] Each distinct API group requires a separate rule entry in the rules array. Example 13: ClusterRole for Non-Resource URLs This ClusterRole allows access to health check endpoints. apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : health-checker rules : - nonResourceURLs : [ \"/healthz\" , \"/readyz\" , \"/livez\" ] verbs : [ \"get\" ] Non-resource URLs represent API endpoints that aren't tied to specific resources. Example 14: Role with All Verbs This Role grants all actions on pods within a namespace. apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-admin namespace : default rules : - apiGroups : [ \"\" ] resources : [ \"pods\" ] verbs : [ \"*\" ] The asterisk wildcard grants all available verbs for the specified resource. Example 15: Role with All Resources This Role grants read access to all resources in a namespace. apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : namespace-reader namespace : default rules : - apiGroups : [ \"*\" ] resources : [ \"*\" ] verbs : [ \"get\" , \"list\" , \"watch\" ] Wildcards in both apiGroups and resources grant access across all API groups and resources. Example 16: Binding Multiple Subjects This RoleBinding grants access to both a user and a ServiceAccount. apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : multi-subject-binding namespace : default subjects : - kind : User name : developer - kind : ServiceAccount name : automation namespace : default roleRef : kind : Role name : pod-reader apiGroup : rbac.authorization.k8s.io A single binding can reference multiple subjects of different kinds. Example 17: Role for PersistentVolumeClaims This Role manages PersistentVolumeClaims in a namespace. apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pvc-manager namespace : storage rules : - apiGroups : [ \"\" ] resources : [ \"persistentvolumeclaims\" ] verbs : [ \"create\" , \"delete\" , \"get\" , \"list\" ] PersistentVolumeClaims are namespaced resources in the core API group. Example 18: Role for Secrets Management This Role allows reading and creating secrets. apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : secret-manager namespace : default rules : - apiGroups : [ \"\" ] resources : [ \"secrets\" ] verbs : [ \"get\" , \"list\" , \"create\" ] Secret management requires explicit permissions as secrets are sensitive resources. Example 19: ClusterRole for Namespace Operations This ClusterRole allows creating and listing namespaces. apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : namespace-admin rules : - apiGroups : [ \"\" ] resources : [ \"namespaces\" ] verbs : [ \"create\" , \"get\" , \"list\" ] Namespace operations require cluster-wide permissions since namespaces are cluster-scoped. Example 20: Aggregated ClusterRole This ClusterRole uses aggregation to combine rules from other ClusterRoles. apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : monitoring-aggregated aggregationRule : clusterRoleSelectors : - matchLabels : rbac.monitoring.io/aggregate-to-monitoring : \"true\" rules : [] Aggregated ClusterRoles dynamically collect rules from other ClusterRoles with matching labels. CLI Examples with Explanations Example 1: Create Basic Role kubectl create role pod-reader --verb = get,list,watch --resource = pods --namespace = default Creates a Role named pod-reader in the default namespace with read-only pod permissions. Example 2: Create RoleBinding for User kubectl create rolebinding pod-reader-binding --role = pod-reader --user = alice --namespace = default Binds the pod-reader Role to user alice in the default namespace. Example 3: Create ClusterRole kubectl create clusterrole node-reader --verb = get,list,watch --resource = nodes Creates a ClusterRole that allows reading nodes across the entire cluster. Example 4: Create ClusterRoleBinding kubectl create clusterrolebinding node-reader-binding --clusterrole = node-reader --group = monitoring Grants node-reader permissions to all users in the monitoring group cluster-wide. Example 5: Create Role with Multiple Resources kubectl create role resource-manager --verb = create,get,update,delete,list --resource = pods,services --namespace = production Creates a Role that manages both pods and services in the production namespace. Example 6: Create Role with Specific Resource Names kubectl create role config-reader --verb = get --resource = configmaps --resource-name = app-config,database-config --namespace = default Creates a Role that only allows accessing specific ConfigMaps by name. Example 7: Create Role for Subresources kubectl create role pod-debugger --verb = get,create --resource = pods/log,pods/exec --namespace = default Creates a Role for accessing pod logs and exec subresources. Example 8: Create ServiceAccount Binding kubectl create rolebinding sa-binding --role = pod-reader --serviceaccount = default:my-app-sa --namespace = default Binds a ServiceAccount to a Role using the serviceaccount:namespace:name format. Example 9: Test Permissions kubectl auth can-i create pods --namespace = default Checks if the current user has permission to create pods in the default namespace. Example 10: Test Permissions as Different User kubectl auth can-i delete deployments --as = system:serviceaccount:default:my-app-sa --namespace = default Tests permissions for a specific ServiceAccount rather than the current user. Example 11: List All Permissions kubectl auth can-i --list --namespace = default Lists all permissions the current user has in the specified namespace. Example 12: Create Role for All Resources kubectl create role namespace-reader --verb = get,list,watch --resource = '*' --namespace = default Creates a Role with read access to all resources in the namespace. Example 13: Create ClusterRole for Non-Resource URLs kubectl create clusterrole health-checker --verb = get --non-resource-url = /healthz,/readyz,/livez Creates a ClusterRole for accessing health check endpoints. Example 14: Bind ClusterRole to Namespace kubectl create rolebinding admin-in-staging --clusterrole = admin --user = admin --namespace = staging Binds a ClusterRole to a user but limits it to a specific namespace via RoleBinding. Example 15: Create Role with API Group kubectl create role deployment-manager --verb = get,list,create,update,delete --resource = deployments.apps --namespace = production Specifies the API group (apps) for deployments resource. Example 16: Check Cross-Namespace Permissions kubectl auth can-i get pods --namespace = production --as = system:serviceaccount:monitoring:prometheus Checks if a ServiceAccount from one namespace has permissions in another namespace. Example 17: Create Role for Specific Verbs Only kubectl create role pod-lister --verb = list --resource = pods --namespace = default Creates a Role with only list permission, not get or watch. Example 18: Update Role Rules kubectl edit role pod-reader --namespace = default Opens the Role definition in an editor for modification. Example 19: Describe RBAC Resources kubectl describe role pod-reader --namespace = default Shows detailed information about a Role including its rules. Example 20: Delete RBAC Resources kubectl delete rolebinding pod-reader-binding --namespace = default Removes a RoleBinding, which revokes the permissions it was granting. Each CLI command corresponds to creating, testing, or managing RBAC resources with specific parameters and options. The explanations detail what each command does and when to use it.","title":"Examples"},{"location":"17-authorization/examples/#yaml-examples-with-explanations","text":"","title":"YAML Examples with Explanations"},{"location":"17-authorization/examples/#example-1-basic-role-for-pod-reading","text":"This creates a Role that allows reading pods in the default namespace. apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-reader namespace : default rules : - apiGroups : [ \"\" ] resources : [ \"pods\" ] verbs : [ \"get\" , \"list\" , \"watch\" ] The Role defines permissions but does not grant access. It specifies get, list, and watch verbs on pods in the core API group.","title":"Example 1: Basic Role for Pod Reading"},{"location":"17-authorization/examples/#example-2-rolebinding-for-user","text":"This binds the pod-reader Role to a user named alice. apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-reader-binding namespace : default subjects : - kind : User name : alice roleRef : kind : Role name : pod-reader apiGroup : rbac.authorization.k8s.io The RoleBinding grants alice the permissions defined in the pod-reader Role, but only in the default namespace.","title":"Example 2: RoleBinding for User"},{"location":"17-authorization/examples/#example-3-clusterrole-for-node-reading","text":"This creates a ClusterRole that allows reading nodes cluster-wide. apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : node-reader rules : - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"get\" , \"list\" , \"watch\" ] ClusterRoles exist at cluster scope and can reference cluster-scoped resources like nodes.","title":"Example 3: ClusterRole for Node Reading"},{"location":"17-authorization/examples/#example-4-clusterrolebinding-for-group","text":"This grants node-reader permissions to all users in the monitoring group. apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : node-reader-binding subjects : - kind : Group name : monitoring roleRef : kind : ClusterRole name : node-reader apiGroup : rbac.authorization.k8s.io The binding is cluster-wide, so group members can read nodes in any namespace.","title":"Example 4: ClusterRoleBinding for Group"},{"location":"17-authorization/examples/#example-5-role-with-multiple-resources","text":"This Role allows management of both pods and services. apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : resource-manager namespace : production rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"services\" ] verbs : [ \"create\" , \"get\" , \"update\" , \"delete\" , \"list\" ] A single Role can contain rules for multiple resources in the same API group.","title":"Example 5: Role with Multiple Resources"},{"location":"17-authorization/examples/#example-6-role-with-specific-resource-names","text":"This Role allows access only to specific ConfigMaps by name. apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : config-reader namespace : default rules : - apiGroups : [ \"\" ] resources : [ \"configmaps\" ] resourceNames : [ \"app-config\" , \"database-config\" ] verbs : [ \"get\" ] The resourceNames field restricts access to only those named resources, not all ConfigMaps.","title":"Example 6: Role with Specific Resource Names"},{"location":"17-authorization/examples/#example-7-role-for-subresources","text":"This Role allows accessing pod logs and execution. apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-debugger namespace : default rules : - apiGroups : [ \"\" ] resources : [ \"pods/log\" , \"pods/exec\" ] verbs : [ \"get\" , \"create\" ] Subresources like logs and exec require explicit permissions separate from the parent resource.","title":"Example 7: Role for Subresources"},{"location":"17-authorization/examples/#example-8-serviceaccount-binding","text":"This binds a ServiceAccount to a Role in its namespace. apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : sa-binding namespace : default subjects : - kind : ServiceAccount name : my-app-sa namespace : default roleRef : kind : Role name : pod-reader apiGroup : rbac.authorization.k8s.io ServiceAccounts must specify their namespace when referenced in bindings.","title":"Example 8: ServiceAccount Binding"},{"location":"17-authorization/examples/#example-9-cross-namespace-serviceaccount-access","text":"This allows a ServiceAccount from monitoring namespace to read pods in default namespace. apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : cross-ns-binding namespace : default subjects : - kind : ServiceAccount name : prometheus namespace : monitoring roleRef : kind : Role name : pod-reader apiGroup : rbac.authorization.k8s.io ServiceAccounts can be granted permissions in namespaces other than where they exist.","title":"Example 9: Cross-Namespace ServiceAccount Access"},{"location":"17-authorization/examples/#example-10-clusterrole-bound-to-namespace","text":"This uses a ClusterRole but limits it to a specific namespace via RoleBinding. apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : limited-clusterrole namespace : staging subjects : - kind : User name : tester roleRef : kind : ClusterRole name : admin apiGroup : rbac.authorization.k8s.io Even though admin is a ClusterRole, the RoleBinding limits its permissions to the staging namespace only.","title":"Example 10: ClusterRole Bound to Namespace"},{"location":"17-authorization/examples/#example-11-role-with-api-group-specific-resources","text":"This Role manages resources from the apps API group. apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : apps-manager namespace : default rules : - apiGroups : [ \"apps\" ] resources : [ \"deployments\" , \"statefulsets\" , \"daemonsets\" ] verbs : [ \"get\" , \"list\" , \"create\" , \"update\" , \"delete\" ] Different API groups require separate rules or comma-separated entries in the apiGroups array.","title":"Example 11: Role with API Group Specific Resources"},{"location":"17-authorization/examples/#example-12-role-with-multiple-api-groups","text":"This Role covers resources from multiple API groups. apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : multi-group-role namespace : default rules : - apiGroups : [ \"\" ] resources : [ \"pods\" , \"services\" ] verbs : [ \"get\" , \"list\" ] - apiGroups : [ \"apps\" ] resources : [ \"deployments\" ] verbs : [ \"get\" , \"list\" ] Each distinct API group requires a separate rule entry in the rules array.","title":"Example 12: Role with Multiple API Groups"},{"location":"17-authorization/examples/#example-13-clusterrole-for-non-resource-urls","text":"This ClusterRole allows access to health check endpoints. apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : health-checker rules : - nonResourceURLs : [ \"/healthz\" , \"/readyz\" , \"/livez\" ] verbs : [ \"get\" ] Non-resource URLs represent API endpoints that aren't tied to specific resources.","title":"Example 13: ClusterRole for Non-Resource URLs"},{"location":"17-authorization/examples/#example-14-role-with-all-verbs","text":"This Role grants all actions on pods within a namespace. apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-admin namespace : default rules : - apiGroups : [ \"\" ] resources : [ \"pods\" ] verbs : [ \"*\" ] The asterisk wildcard grants all available verbs for the specified resource.","title":"Example 14: Role with All Verbs"},{"location":"17-authorization/examples/#example-15-role-with-all-resources","text":"This Role grants read access to all resources in a namespace. apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : namespace-reader namespace : default rules : - apiGroups : [ \"*\" ] resources : [ \"*\" ] verbs : [ \"get\" , \"list\" , \"watch\" ] Wildcards in both apiGroups and resources grant access across all API groups and resources.","title":"Example 15: Role with All Resources"},{"location":"17-authorization/examples/#example-16-binding-multiple-subjects","text":"This RoleBinding grants access to both a user and a ServiceAccount. apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : multi-subject-binding namespace : default subjects : - kind : User name : developer - kind : ServiceAccount name : automation namespace : default roleRef : kind : Role name : pod-reader apiGroup : rbac.authorization.k8s.io A single binding can reference multiple subjects of different kinds.","title":"Example 16: Binding Multiple Subjects"},{"location":"17-authorization/examples/#example-17-role-for-persistentvolumeclaims","text":"This Role manages PersistentVolumeClaims in a namespace. apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pvc-manager namespace : storage rules : - apiGroups : [ \"\" ] resources : [ \"persistentvolumeclaims\" ] verbs : [ \"create\" , \"delete\" , \"get\" , \"list\" ] PersistentVolumeClaims are namespaced resources in the core API group.","title":"Example 17: Role for PersistentVolumeClaims"},{"location":"17-authorization/examples/#example-18-role-for-secrets-management","text":"This Role allows reading and creating secrets. apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : secret-manager namespace : default rules : - apiGroups : [ \"\" ] resources : [ \"secrets\" ] verbs : [ \"get\" , \"list\" , \"create\" ] Secret management requires explicit permissions as secrets are sensitive resources.","title":"Example 18: Role for Secrets Management"},{"location":"17-authorization/examples/#example-19-clusterrole-for-namespace-operations","text":"This ClusterRole allows creating and listing namespaces. apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : namespace-admin rules : - apiGroups : [ \"\" ] resources : [ \"namespaces\" ] verbs : [ \"create\" , \"get\" , \"list\" ] Namespace operations require cluster-wide permissions since namespaces are cluster-scoped.","title":"Example 19: ClusterRole for Namespace Operations"},{"location":"17-authorization/examples/#example-20-aggregated-clusterrole","text":"This ClusterRole uses aggregation to combine rules from other ClusterRoles. apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : monitoring-aggregated aggregationRule : clusterRoleSelectors : - matchLabels : rbac.monitoring.io/aggregate-to-monitoring : \"true\" rules : [] Aggregated ClusterRoles dynamically collect rules from other ClusterRoles with matching labels.","title":"Example 20: Aggregated ClusterRole"},{"location":"17-authorization/examples/#cli-examples-with-explanations","text":"","title":"CLI Examples with Explanations"},{"location":"17-authorization/examples/#example-1-create-basic-role","text":"kubectl create role pod-reader --verb = get,list,watch --resource = pods --namespace = default Creates a Role named pod-reader in the default namespace with read-only pod permissions.","title":"Example 1: Create Basic Role"},{"location":"17-authorization/examples/#example-2-create-rolebinding-for-user","text":"kubectl create rolebinding pod-reader-binding --role = pod-reader --user = alice --namespace = default Binds the pod-reader Role to user alice in the default namespace.","title":"Example 2: Create RoleBinding for User"},{"location":"17-authorization/examples/#example-3-create-clusterrole","text":"kubectl create clusterrole node-reader --verb = get,list,watch --resource = nodes Creates a ClusterRole that allows reading nodes across the entire cluster.","title":"Example 3: Create ClusterRole"},{"location":"17-authorization/examples/#example-4-create-clusterrolebinding","text":"kubectl create clusterrolebinding node-reader-binding --clusterrole = node-reader --group = monitoring Grants node-reader permissions to all users in the monitoring group cluster-wide.","title":"Example 4: Create ClusterRoleBinding"},{"location":"17-authorization/examples/#example-5-create-role-with-multiple-resources","text":"kubectl create role resource-manager --verb = create,get,update,delete,list --resource = pods,services --namespace = production Creates a Role that manages both pods and services in the production namespace.","title":"Example 5: Create Role with Multiple Resources"},{"location":"17-authorization/examples/#example-6-create-role-with-specific-resource-names","text":"kubectl create role config-reader --verb = get --resource = configmaps --resource-name = app-config,database-config --namespace = default Creates a Role that only allows accessing specific ConfigMaps by name.","title":"Example 6: Create Role with Specific Resource Names"},{"location":"17-authorization/examples/#example-7-create-role-for-subresources","text":"kubectl create role pod-debugger --verb = get,create --resource = pods/log,pods/exec --namespace = default Creates a Role for accessing pod logs and exec subresources.","title":"Example 7: Create Role for Subresources"},{"location":"17-authorization/examples/#example-8-create-serviceaccount-binding","text":"kubectl create rolebinding sa-binding --role = pod-reader --serviceaccount = default:my-app-sa --namespace = default Binds a ServiceAccount to a Role using the serviceaccount:namespace:name format.","title":"Example 8: Create ServiceAccount Binding"},{"location":"17-authorization/examples/#example-9-test-permissions","text":"kubectl auth can-i create pods --namespace = default Checks if the current user has permission to create pods in the default namespace.","title":"Example 9: Test Permissions"},{"location":"17-authorization/examples/#example-10-test-permissions-as-different-user","text":"kubectl auth can-i delete deployments --as = system:serviceaccount:default:my-app-sa --namespace = default Tests permissions for a specific ServiceAccount rather than the current user.","title":"Example 10: Test Permissions as Different User"},{"location":"17-authorization/examples/#example-11-list-all-permissions","text":"kubectl auth can-i --list --namespace = default Lists all permissions the current user has in the specified namespace.","title":"Example 11: List All Permissions"},{"location":"17-authorization/examples/#example-12-create-role-for-all-resources","text":"kubectl create role namespace-reader --verb = get,list,watch --resource = '*' --namespace = default Creates a Role with read access to all resources in the namespace.","title":"Example 12: Create Role for All Resources"},{"location":"17-authorization/examples/#example-13-create-clusterrole-for-non-resource-urls","text":"kubectl create clusterrole health-checker --verb = get --non-resource-url = /healthz,/readyz,/livez Creates a ClusterRole for accessing health check endpoints.","title":"Example 13: Create ClusterRole for Non-Resource URLs"},{"location":"17-authorization/examples/#example-14-bind-clusterrole-to-namespace","text":"kubectl create rolebinding admin-in-staging --clusterrole = admin --user = admin --namespace = staging Binds a ClusterRole to a user but limits it to a specific namespace via RoleBinding.","title":"Example 14: Bind ClusterRole to Namespace"},{"location":"17-authorization/examples/#example-15-create-role-with-api-group","text":"kubectl create role deployment-manager --verb = get,list,create,update,delete --resource = deployments.apps --namespace = production Specifies the API group (apps) for deployments resource.","title":"Example 15: Create Role with API Group"},{"location":"17-authorization/examples/#example-16-check-cross-namespace-permissions","text":"kubectl auth can-i get pods --namespace = production --as = system:serviceaccount:monitoring:prometheus Checks if a ServiceAccount from one namespace has permissions in another namespace.","title":"Example 16: Check Cross-Namespace Permissions"},{"location":"17-authorization/examples/#example-17-create-role-for-specific-verbs-only","text":"kubectl create role pod-lister --verb = list --resource = pods --namespace = default Creates a Role with only list permission, not get or watch.","title":"Example 17: Create Role for Specific Verbs Only"},{"location":"17-authorization/examples/#example-18-update-role-rules","text":"kubectl edit role pod-reader --namespace = default Opens the Role definition in an editor for modification.","title":"Example 18: Update Role Rules"},{"location":"17-authorization/examples/#example-19-describe-rbac-resources","text":"kubectl describe role pod-reader --namespace = default Shows detailed information about a Role including its rules.","title":"Example 19: Describe RBAC Resources"},{"location":"17-authorization/examples/#example-20-delete-rbac-resources","text":"kubectl delete rolebinding pod-reader-binding --namespace = default Removes a RoleBinding, which revokes the permissions it was granting. Each CLI command corresponds to creating, testing, or managing RBAC resources with specific parameters and options. The explanations detail what each command does and when to use it.","title":"Example 20: Delete RBAC Resources"},{"location":"17-authorization/gotchas/","text":"RBAC Gotchas and Common Mistakes 4.1 Core API Group Empty String Requirement Incorrect Implementation apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-access namespace : default rules : - apiGroups : [ \"v1\" ] resources : [ \"pods\" ] verbs : [ \"get\" , \"list\" ] Correct Implementation apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-access namespace : default rules : - apiGroups : [ \"\" ] resources : [ \"pods\" ] verbs : [ \"get\" , \"list\" ] Explanation: The empty string \"\" represents the core API group, which contains fundamental resources like pods, services, and configmaps. Using \"v1\" instead of \"\" will cause the RBAC rule to be ignored, resulting in access denial without clear error messages. 4.2 ServiceAccount Subject API Group Omission Incorrect Implementation apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : sa-binding namespace : default subjects : - kind : ServiceAccount name : my-app namespace : default apiGroup : rbac.authorization.k8s.io roleRef : kind : Role name : pod-reader apiGroup : rbac.authorization.k8s.io Correct Implementation apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : sa-binding namespace : default subjects : - kind : ServiceAccount name : my-app namespace : default roleRef : kind : Role name : pod-reader apiGroup : rbac.authorization.k8s.io Explanation: ServiceAccount resources exist in the core API group, not the RBAC API group. The apiGroup field must be omitted when referencing ServiceAccounts as subjects. For Users and Groups, the apiGroup must be rbac.authorization.k8s.io . 4.3 RoleBinding Scope Limitation Incorrect Expectation User creates a RoleBinding in the development namespace and expects access to production namespace. apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-access namespace : development subjects : - kind : User name : developer roleRef : kind : Role name : pod-reader apiGroup : rbac.authorization.k8s.io Result: The developer only has access in the development namespace, not in production or any other namespace. Correct Approach for Multi-Namespace Access apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : pod-reader rules : - apiGroups : [ \"\" ] resources : [ \"pods\" ] verbs : [ \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-access-dev namespace : development subjects : - kind : User name : developer roleRef : kind : ClusterRole name : pod-reader apiGroup : rbac.authorization.k8s.io --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-access-prod namespace : production subjects : - kind : User name : developer roleRef : kind : ClusterRole name : pod-reader apiGroup : rbac.authorization.k8s.io Explanation: RoleBindings are always namespace-scoped. A single RoleBinding cannot grant access across multiple namespaces. To provide access in multiple namespaces, either create multiple RoleBindings or use a ClusterRoleBinding for cluster-wide access. 4.4 ClusterRole with RoleBinding Scope Limitation Misunderstanding Assuming ClusterRole automatically provides cluster-wide access. apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : pod-manager rules : - apiGroups : [ \"\" ] resources : [ \"pods\" ] verbs : [ \"get\" , \"list\" , \"create\" , \"delete\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-manager-binding namespace : staging subjects : - kind : User name : operator roleRef : kind : ClusterRole name : pod-manager apiGroup : rbac.authorization.k8s.io Result: The operator only has pod management permissions in the staging namespace, not cluster-wide. Correct Implementation for Cluster-Wide Access apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : pod-manager-global subjects : - kind : User name : operator roleRef : kind : ClusterRole name : pod-manager apiGroup : rbac.authorization.k8s.io Explanation: The scope of permissions is determined by the binding type, not the role type. A ClusterRole bound with a RoleBinding becomes namespace-scoped. For cluster-wide access, a ClusterRoleBinding must be used. 4.5 Subresource Permission Requirements Incorrect Implementation Assuming pod access includes subresource access. apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-access namespace : default rules : - apiGroups : [ \"\" ] resources : [ \"pods\" ] verbs : [ \"get\" , \"list\" ] Result: Users can see pods but cannot view logs or execute commands inside containers. Correct Implementation with Subresources apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-access namespace : default rules : - apiGroups : [ \"\" ] resources : [ \"pods\" ] verbs : [ \"get\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"pods/log\" ] verbs : [ \"get\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" ] verbs : [ \"create\" ] Explanation: Subresources like logs, exec, port-forward, and attach require explicit permissions separate from the parent resource. Each subresource must be listed individually with appropriate verbs. 4.6 ResourceNames Limitation with List Verb Incorrect Implementation apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : secret-access namespace : default rules : - apiGroups : [ \"\" ] resources : [ \"secrets\" ] resourceNames : [ \"db-credentials\" , \"api-key\" ] verbs : [ \"get\" , \"list\" ] Result: Users can get specific secrets by name but cannot list all secrets. Correct Implementation Separating Actions apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : secret-access namespace : default rules : - apiGroups : [ \"\" ] resources : [ \"secrets\" ] verbs : [ \"list\" ] - apiGroups : [ \"\" ] resources : [ \"secrets\" ] resourceNames : [ \"db-credentials\" , \"api-key\" ] verbs : [ \"get\" ] Explanation: The list verb is incompatible with resourceNames. When resourceNames is specified, users can only access the named resources individually, not list all resources of that type. For list access, a separate rule without resourceNames is required. 4.7 Cross-Namespace Role Reference Prohibition Incorrect Setup Attempting to reference a Role from another namespace. # Role exists in development namespace apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-reader namespace : development rules : - apiGroups : [ \"\" ] resources : [ \"pods\" ] verbs : [ \"get\" , \"list\" ] # RoleBinding attempts to reference it from production namespace apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-reader-binding namespace : production subjects : - kind : User name : operator roleRef : kind : Role name : pod-reader apiGroup : rbac.authorization.k8s.io Result: The binding fails validation or becomes invalid. Correct Solution # Option 1: Create duplicate Role in production namespace apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-reader namespace : production rules : - apiGroups : [ \"\" ] resources : [ \"pods\" ] verbs : [ \"get\" , \"list\" ] # Option 2: Use ClusterRole and RoleBinding apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : pod-reader rules : - apiGroups : [ \"\" ] resources : [ \"pods\" ] verbs : [ \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-reader-binding namespace : production subjects : - kind : User name : operator roleRef : kind : ClusterRole name : pod-reader apiGroup : rbac.authorization.k8s.io Explanation: Role resources are namespace-scoped and cannot be referenced across namespace boundaries. A RoleBinding can only reference a Role in the same namespace. For cross-namespace permission reuse, ClusterRoles should be used. 4.8 Cluster-Scoped Resource Access with RoleBinding Incorrect Expectation Attempting to grant node access through a RoleBinding. apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : node-viewer rules : - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : node-access namespace : default subjects : - kind : User name : viewer roleRef : kind : ClusterRole name : node-viewer apiGroup : rbac.authorization.k8s.io Result: The user cannot access nodes despite the binding. Correct Implementation apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : node-access-global subjects : - kind : User name : viewer roleRef : kind : ClusterRole name : node-viewer apiGroup : rbac.authorization.k8s.io Explanation: Cluster-scoped resources like nodes, persistentvolumes, and namespaces cannot be accessed through namespace-scoped bindings. RoleBindings can only grant access to namespace-scoped resources, even when referencing a ClusterRole. For cluster-scoped resource access, ClusterRoleBinding must be used. 4.9 Missing Default ServiceAccount Permissions Common Misunderstanding Assuming ServiceAccounts have default permissions. apiVersion : v1 kind : Pod metadata : name : my-app namespace : default spec : serviceAccountName : my-app-sa containers : - name : app image : myapp:latest Result: The pod cannot access any Kubernetes API resources. Required RBAC Setup # First, create the ServiceAccount apiVersion : v1 kind : ServiceAccount metadata : name : my-app-sa namespace : default --- # Then, create permissions apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : my-app-role namespace : default rules : - apiGroups : [ \"\" ] resources : [ \"pods\" ] verbs : [ \"get\" , \"list\" ] --- # Finally, bind them apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : my-app-binding namespace : default subjects : - kind : ServiceAccount name : my-app-sa namespace : default roleRef : kind : Role name : my-app-role apiGroup : rbac.authorization.k8s.io Explanation: ServiceAccounts have no permissions by default. Unlike some cloud providers' IAM systems, Kubernetes RBAC requires explicit granting of all permissions. Each ServiceAccount starts with zero access and must be explicitly bound to Roles or ClusterRoles. 4.10 Verb Wildcard Incompatibility Incorrect Implementation apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : wildcard-role namespace : default rules : - apiGroups : [ \"\" ] resources : [ \"configmaps\" ] resourceNames : [ \"app-config\" ] verbs : [ \"*\" ] Result: The wildcard verb may not work as expected with resourceNames. Correct Implementation apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : config-access namespace : default rules : - apiGroups : [ \"\" ] resources : [ \"configmaps\" ] resourceNames : [ \"app-config\" ] verbs : [ \"get\" , \"update\" , \"patch\" , \"delete\" ] Explanation: When using resourceNames, explicitly list the required verbs instead of using wildcards. Some verbs like list and watch may not be compatible with resourceNames restrictions. 4.11 API Group Specification for Custom Resources Incorrect Implementation apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : crd-access rules : - apiGroups : [ \"\" ] resources : [ \"customresourcedefinitions\" ] verbs : [ \"get\" , \"list\" ] Result: Cannot access CRDs or custom resources. Correct Implementation apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : crd-access rules : - apiGroups : [ \"apiextensions.k8s.io\" ] resources : [ \"customresourcedefinitions\" ] verbs : [ \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : custom-resource-access rules : - apiGroups : [ \"mycompany.com\" ] resources : [ \"myresources\" ] verbs : [ \"get\" , \"list\" , \"create\" ] Explanation: CustomResourceDefinitions are in the apiextensions.k8s.io API group, not the core group. Custom resources themselves are in their own API groups defined by their CRD. Both the CRD access and the custom resource access require separate rules with correct API groups. 4.12 Role Binding Subject Namespace Requirement Incorrect Implementation apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : invalid-binding namespace : app-ns subjects : - kind : ServiceAccount name : monitor-sa roleRef : kind : Role name : pod-reader apiGroup : rbac.authorization.k8s.io Result: Binding fails validation or ServiceAccount reference is invalid. Correct Implementation apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : valid-binding namespace : app-ns subjects : - kind : ServiceAccount name : monitor-sa namespace : monitoring-ns roleRef : kind : Role name : pod-reader apiGroup : rbac.authorization.k8s.io Explanation: When referencing a ServiceAccount in a RoleBinding, the namespace field is required in the subject definition. This is because ServiceAccounts are namespace-scoped resources, and the binding needs to know which namespace contains the ServiceAccount being referenced. 4.13 Non-Resource URL Access Requirements Incorrect Implementation apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : health-checker namespace : default rules : - apiGroups : [ \"\" ] resources : [ \"/healthz\" ] verbs : [ \"get\" ] Result: Cannot access health check endpoints. Correct Implementation apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : health-checker rules : - nonResourceURLs : [ \"/healthz\" , \"/readyz\" , \"/livez\" ] verbs : [ \"get\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : health-check-binding subjects : - kind : User name : health-monitor roleRef : kind : ClusterRole name : health-checker apiGroup : rbac.authorization.k8s.io Explanation: Non-resource URLs like health check endpoints require ClusterRoles with nonResourceURLs field instead of resources field. They also require ClusterRoleBindings since these endpoints exist at the cluster level, not within namespaces. 4.14 Missing API Group for Apps Resources Incorrect Implementation apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : deployment-access namespace : default rules : - apiGroups : [ \"\" ] resources : [ \"deployments\" ] verbs : [ \"get\" , \"list\" ] Result: Cannot access deployments. Correct Implementation apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : deployment-access namespace : default rules : - apiGroups : [ \"apps\" ] resources : [ \"deployments\" ] verbs : [ \"get\" , \"list\" ] Explanation: Deployments, StatefulSets, and DaemonSets are in the apps API group, not the core API group. Many resources that were originally in extensions/v1beta1 have moved to stable API groups like apps/v1. 4.15 Aggregated ClusterRole Rule Inheritance Incorrect Expectation Assuming aggregated ClusterRoles automatically update. apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : aggregated-role labels : rbac.example.com/aggregate-to-monitoring : \"true\" rules : - apiGroups : [ \"\" ] resources : [ \"pods\" ] verbs : [ \"get\" , \"list\" ] Result: The rules are not aggregated to other ClusterRoles. Correct Aggregation Setup # Base ClusterRole that will receive aggregated rules apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : monitoring-role aggregationRule : clusterRoleSelectors : - matchLabels : rbac.example.com/aggregate-to-monitoring : \"true\" rules : [] # Rules will be aggregated here --- # ClusterRole that contributes rules apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : monitoring-pods labels : rbac.example.com/aggregate-to-monitoring : \"true\" rules : - apiGroups : [ \"\" ] resources : [ \"pods\" ] verbs : [ \"get\" , \"list\" ] --- # Another contributing ClusterRole apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : monitoring-services labels : rbac.example.com/aggregate-to-monitoring : \"true\" rules : - apiGroups : [ \"\" ] resources : [ \"services\" ] verbs : [ \"get\" , \"list\" ] Explanation: Aggregated ClusterRoles use the aggregationRule field to dynamically collect rules from other ClusterRoles with matching labels. The base ClusterRole has empty rules initially, and contributing ClusterRoles must have the matching label. The aggregation is not automatic for regular ClusterRoles. Summary of Key RBAC Principles Empty string represents the core API group ServiceAccount subjects do not use apiGroup field RoleBindings are always namespace-scoped ClusterRoles become namespace-scoped when bound with RoleBindings Subresources require explicit permissions ResourceNames is incompatible with list verb Roles cannot be referenced across namespaces Cluster-scoped resources require ClusterRoleBindings ServiceAccounts have no default permissions Custom resources have specific API groups Non-resource URLs require ClusterRoles with nonResourceURLs field Apps resources are in the apps API group, not core Each example demonstrates a specific gotcha with both incorrect and correct implementations, showing exactly what fails and how to fix it.","title":"RBAC Gotchas and Common Mistakes"},{"location":"17-authorization/gotchas/#rbac-gotchas-and-common-mistakes","text":"","title":"RBAC Gotchas and Common Mistakes"},{"location":"17-authorization/gotchas/#41-core-api-group-empty-string-requirement","text":"","title":"4.1 Core API Group Empty String Requirement"},{"location":"17-authorization/gotchas/#incorrect-implementation","text":"apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-access namespace : default rules : - apiGroups : [ \"v1\" ] resources : [ \"pods\" ] verbs : [ \"get\" , \"list\" ]","title":"Incorrect Implementation"},{"location":"17-authorization/gotchas/#correct-implementation","text":"apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-access namespace : default rules : - apiGroups : [ \"\" ] resources : [ \"pods\" ] verbs : [ \"get\" , \"list\" ] Explanation: The empty string \"\" represents the core API group, which contains fundamental resources like pods, services, and configmaps. Using \"v1\" instead of \"\" will cause the RBAC rule to be ignored, resulting in access denial without clear error messages.","title":"Correct Implementation"},{"location":"17-authorization/gotchas/#42-serviceaccount-subject-api-group-omission","text":"","title":"4.2 ServiceAccount Subject API Group Omission"},{"location":"17-authorization/gotchas/#incorrect-implementation_1","text":"apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : sa-binding namespace : default subjects : - kind : ServiceAccount name : my-app namespace : default apiGroup : rbac.authorization.k8s.io roleRef : kind : Role name : pod-reader apiGroup : rbac.authorization.k8s.io","title":"Incorrect Implementation"},{"location":"17-authorization/gotchas/#correct-implementation_1","text":"apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : sa-binding namespace : default subjects : - kind : ServiceAccount name : my-app namespace : default roleRef : kind : Role name : pod-reader apiGroup : rbac.authorization.k8s.io Explanation: ServiceAccount resources exist in the core API group, not the RBAC API group. The apiGroup field must be omitted when referencing ServiceAccounts as subjects. For Users and Groups, the apiGroup must be rbac.authorization.k8s.io .","title":"Correct Implementation"},{"location":"17-authorization/gotchas/#43-rolebinding-scope-limitation","text":"","title":"4.3 RoleBinding Scope Limitation"},{"location":"17-authorization/gotchas/#incorrect-expectation","text":"User creates a RoleBinding in the development namespace and expects access to production namespace. apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-access namespace : development subjects : - kind : User name : developer roleRef : kind : Role name : pod-reader apiGroup : rbac.authorization.k8s.io Result: The developer only has access in the development namespace, not in production or any other namespace.","title":"Incorrect Expectation"},{"location":"17-authorization/gotchas/#correct-approach-for-multi-namespace-access","text":"apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : pod-reader rules : - apiGroups : [ \"\" ] resources : [ \"pods\" ] verbs : [ \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-access-dev namespace : development subjects : - kind : User name : developer roleRef : kind : ClusterRole name : pod-reader apiGroup : rbac.authorization.k8s.io --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-access-prod namespace : production subjects : - kind : User name : developer roleRef : kind : ClusterRole name : pod-reader apiGroup : rbac.authorization.k8s.io Explanation: RoleBindings are always namespace-scoped. A single RoleBinding cannot grant access across multiple namespaces. To provide access in multiple namespaces, either create multiple RoleBindings or use a ClusterRoleBinding for cluster-wide access.","title":"Correct Approach for Multi-Namespace Access"},{"location":"17-authorization/gotchas/#44-clusterrole-with-rolebinding-scope-limitation","text":"","title":"4.4 ClusterRole with RoleBinding Scope Limitation"},{"location":"17-authorization/gotchas/#misunderstanding","text":"Assuming ClusterRole automatically provides cluster-wide access. apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : pod-manager rules : - apiGroups : [ \"\" ] resources : [ \"pods\" ] verbs : [ \"get\" , \"list\" , \"create\" , \"delete\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-manager-binding namespace : staging subjects : - kind : User name : operator roleRef : kind : ClusterRole name : pod-manager apiGroup : rbac.authorization.k8s.io Result: The operator only has pod management permissions in the staging namespace, not cluster-wide.","title":"Misunderstanding"},{"location":"17-authorization/gotchas/#correct-implementation-for-cluster-wide-access","text":"apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : pod-manager-global subjects : - kind : User name : operator roleRef : kind : ClusterRole name : pod-manager apiGroup : rbac.authorization.k8s.io Explanation: The scope of permissions is determined by the binding type, not the role type. A ClusterRole bound with a RoleBinding becomes namespace-scoped. For cluster-wide access, a ClusterRoleBinding must be used.","title":"Correct Implementation for Cluster-Wide Access"},{"location":"17-authorization/gotchas/#45-subresource-permission-requirements","text":"","title":"4.5 Subresource Permission Requirements"},{"location":"17-authorization/gotchas/#incorrect-implementation_2","text":"Assuming pod access includes subresource access. apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-access namespace : default rules : - apiGroups : [ \"\" ] resources : [ \"pods\" ] verbs : [ \"get\" , \"list\" ] Result: Users can see pods but cannot view logs or execute commands inside containers.","title":"Incorrect Implementation"},{"location":"17-authorization/gotchas/#correct-implementation-with-subresources","text":"apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-access namespace : default rules : - apiGroups : [ \"\" ] resources : [ \"pods\" ] verbs : [ \"get\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"pods/log\" ] verbs : [ \"get\" ] - apiGroups : [ \"\" ] resources : [ \"pods/exec\" ] verbs : [ \"create\" ] Explanation: Subresources like logs, exec, port-forward, and attach require explicit permissions separate from the parent resource. Each subresource must be listed individually with appropriate verbs.","title":"Correct Implementation with Subresources"},{"location":"17-authorization/gotchas/#46-resourcenames-limitation-with-list-verb","text":"","title":"4.6 ResourceNames Limitation with List Verb"},{"location":"17-authorization/gotchas/#incorrect-implementation_3","text":"apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : secret-access namespace : default rules : - apiGroups : [ \"\" ] resources : [ \"secrets\" ] resourceNames : [ \"db-credentials\" , \"api-key\" ] verbs : [ \"get\" , \"list\" ] Result: Users can get specific secrets by name but cannot list all secrets.","title":"Incorrect Implementation"},{"location":"17-authorization/gotchas/#correct-implementation-separating-actions","text":"apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : secret-access namespace : default rules : - apiGroups : [ \"\" ] resources : [ \"secrets\" ] verbs : [ \"list\" ] - apiGroups : [ \"\" ] resources : [ \"secrets\" ] resourceNames : [ \"db-credentials\" , \"api-key\" ] verbs : [ \"get\" ] Explanation: The list verb is incompatible with resourceNames. When resourceNames is specified, users can only access the named resources individually, not list all resources of that type. For list access, a separate rule without resourceNames is required.","title":"Correct Implementation Separating Actions"},{"location":"17-authorization/gotchas/#47-cross-namespace-role-reference-prohibition","text":"","title":"4.7 Cross-Namespace Role Reference Prohibition"},{"location":"17-authorization/gotchas/#incorrect-setup","text":"Attempting to reference a Role from another namespace. # Role exists in development namespace apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-reader namespace : development rules : - apiGroups : [ \"\" ] resources : [ \"pods\" ] verbs : [ \"get\" , \"list\" ] # RoleBinding attempts to reference it from production namespace apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-reader-binding namespace : production subjects : - kind : User name : operator roleRef : kind : Role name : pod-reader apiGroup : rbac.authorization.k8s.io Result: The binding fails validation or becomes invalid.","title":"Incorrect Setup"},{"location":"17-authorization/gotchas/#correct-solution","text":"# Option 1: Create duplicate Role in production namespace apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : pod-reader namespace : production rules : - apiGroups : [ \"\" ] resources : [ \"pods\" ] verbs : [ \"get\" , \"list\" ] # Option 2: Use ClusterRole and RoleBinding apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : pod-reader rules : - apiGroups : [ \"\" ] resources : [ \"pods\" ] verbs : [ \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : pod-reader-binding namespace : production subjects : - kind : User name : operator roleRef : kind : ClusterRole name : pod-reader apiGroup : rbac.authorization.k8s.io Explanation: Role resources are namespace-scoped and cannot be referenced across namespace boundaries. A RoleBinding can only reference a Role in the same namespace. For cross-namespace permission reuse, ClusterRoles should be used.","title":"Correct Solution"},{"location":"17-authorization/gotchas/#48-cluster-scoped-resource-access-with-rolebinding","text":"","title":"4.8 Cluster-Scoped Resource Access with RoleBinding"},{"location":"17-authorization/gotchas/#incorrect-expectation_1","text":"Attempting to grant node access through a RoleBinding. apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : node-viewer rules : - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : node-access namespace : default subjects : - kind : User name : viewer roleRef : kind : ClusterRole name : node-viewer apiGroup : rbac.authorization.k8s.io Result: The user cannot access nodes despite the binding.","title":"Incorrect Expectation"},{"location":"17-authorization/gotchas/#correct-implementation_2","text":"apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : node-access-global subjects : - kind : User name : viewer roleRef : kind : ClusterRole name : node-viewer apiGroup : rbac.authorization.k8s.io Explanation: Cluster-scoped resources like nodes, persistentvolumes, and namespaces cannot be accessed through namespace-scoped bindings. RoleBindings can only grant access to namespace-scoped resources, even when referencing a ClusterRole. For cluster-scoped resource access, ClusterRoleBinding must be used.","title":"Correct Implementation"},{"location":"17-authorization/gotchas/#49-missing-default-serviceaccount-permissions","text":"","title":"4.9 Missing Default ServiceAccount Permissions"},{"location":"17-authorization/gotchas/#common-misunderstanding","text":"Assuming ServiceAccounts have default permissions. apiVersion : v1 kind : Pod metadata : name : my-app namespace : default spec : serviceAccountName : my-app-sa containers : - name : app image : myapp:latest Result: The pod cannot access any Kubernetes API resources.","title":"Common Misunderstanding"},{"location":"17-authorization/gotchas/#required-rbac-setup","text":"# First, create the ServiceAccount apiVersion : v1 kind : ServiceAccount metadata : name : my-app-sa namespace : default --- # Then, create permissions apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : my-app-role namespace : default rules : - apiGroups : [ \"\" ] resources : [ \"pods\" ] verbs : [ \"get\" , \"list\" ] --- # Finally, bind them apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : my-app-binding namespace : default subjects : - kind : ServiceAccount name : my-app-sa namespace : default roleRef : kind : Role name : my-app-role apiGroup : rbac.authorization.k8s.io Explanation: ServiceAccounts have no permissions by default. Unlike some cloud providers' IAM systems, Kubernetes RBAC requires explicit granting of all permissions. Each ServiceAccount starts with zero access and must be explicitly bound to Roles or ClusterRoles.","title":"Required RBAC Setup"},{"location":"17-authorization/gotchas/#410-verb-wildcard-incompatibility","text":"","title":"4.10 Verb Wildcard Incompatibility"},{"location":"17-authorization/gotchas/#incorrect-implementation_4","text":"apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : wildcard-role namespace : default rules : - apiGroups : [ \"\" ] resources : [ \"configmaps\" ] resourceNames : [ \"app-config\" ] verbs : [ \"*\" ] Result: The wildcard verb may not work as expected with resourceNames.","title":"Incorrect Implementation"},{"location":"17-authorization/gotchas/#correct-implementation_3","text":"apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : config-access namespace : default rules : - apiGroups : [ \"\" ] resources : [ \"configmaps\" ] resourceNames : [ \"app-config\" ] verbs : [ \"get\" , \"update\" , \"patch\" , \"delete\" ] Explanation: When using resourceNames, explicitly list the required verbs instead of using wildcards. Some verbs like list and watch may not be compatible with resourceNames restrictions.","title":"Correct Implementation"},{"location":"17-authorization/gotchas/#411-api-group-specification-for-custom-resources","text":"","title":"4.11 API Group Specification for Custom Resources"},{"location":"17-authorization/gotchas/#incorrect-implementation_5","text":"apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : crd-access rules : - apiGroups : [ \"\" ] resources : [ \"customresourcedefinitions\" ] verbs : [ \"get\" , \"list\" ] Result: Cannot access CRDs or custom resources.","title":"Incorrect Implementation"},{"location":"17-authorization/gotchas/#correct-implementation_4","text":"apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : crd-access rules : - apiGroups : [ \"apiextensions.k8s.io\" ] resources : [ \"customresourcedefinitions\" ] verbs : [ \"get\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : custom-resource-access rules : - apiGroups : [ \"mycompany.com\" ] resources : [ \"myresources\" ] verbs : [ \"get\" , \"list\" , \"create\" ] Explanation: CustomResourceDefinitions are in the apiextensions.k8s.io API group, not the core group. Custom resources themselves are in their own API groups defined by their CRD. Both the CRD access and the custom resource access require separate rules with correct API groups.","title":"Correct Implementation"},{"location":"17-authorization/gotchas/#412-role-binding-subject-namespace-requirement","text":"","title":"4.12 Role Binding Subject Namespace Requirement"},{"location":"17-authorization/gotchas/#incorrect-implementation_6","text":"apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : invalid-binding namespace : app-ns subjects : - kind : ServiceAccount name : monitor-sa roleRef : kind : Role name : pod-reader apiGroup : rbac.authorization.k8s.io Result: Binding fails validation or ServiceAccount reference is invalid.","title":"Incorrect Implementation"},{"location":"17-authorization/gotchas/#correct-implementation_5","text":"apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : valid-binding namespace : app-ns subjects : - kind : ServiceAccount name : monitor-sa namespace : monitoring-ns roleRef : kind : Role name : pod-reader apiGroup : rbac.authorization.k8s.io Explanation: When referencing a ServiceAccount in a RoleBinding, the namespace field is required in the subject definition. This is because ServiceAccounts are namespace-scoped resources, and the binding needs to know which namespace contains the ServiceAccount being referenced.","title":"Correct Implementation"},{"location":"17-authorization/gotchas/#413-non-resource-url-access-requirements","text":"","title":"4.13 Non-Resource URL Access Requirements"},{"location":"17-authorization/gotchas/#incorrect-implementation_7","text":"apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : health-checker namespace : default rules : - apiGroups : [ \"\" ] resources : [ \"/healthz\" ] verbs : [ \"get\" ] Result: Cannot access health check endpoints.","title":"Incorrect Implementation"},{"location":"17-authorization/gotchas/#correct-implementation_6","text":"apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : health-checker rules : - nonResourceURLs : [ \"/healthz\" , \"/readyz\" , \"/livez\" ] verbs : [ \"get\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : health-check-binding subjects : - kind : User name : health-monitor roleRef : kind : ClusterRole name : health-checker apiGroup : rbac.authorization.k8s.io Explanation: Non-resource URLs like health check endpoints require ClusterRoles with nonResourceURLs field instead of resources field. They also require ClusterRoleBindings since these endpoints exist at the cluster level, not within namespaces.","title":"Correct Implementation"},{"location":"17-authorization/gotchas/#414-missing-api-group-for-apps-resources","text":"","title":"4.14 Missing API Group for Apps Resources"},{"location":"17-authorization/gotchas/#incorrect-implementation_8","text":"apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : deployment-access namespace : default rules : - apiGroups : [ \"\" ] resources : [ \"deployments\" ] verbs : [ \"get\" , \"list\" ] Result: Cannot access deployments.","title":"Incorrect Implementation"},{"location":"17-authorization/gotchas/#correct-implementation_7","text":"apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : deployment-access namespace : default rules : - apiGroups : [ \"apps\" ] resources : [ \"deployments\" ] verbs : [ \"get\" , \"list\" ] Explanation: Deployments, StatefulSets, and DaemonSets are in the apps API group, not the core API group. Many resources that were originally in extensions/v1beta1 have moved to stable API groups like apps/v1.","title":"Correct Implementation"},{"location":"17-authorization/gotchas/#415-aggregated-clusterrole-rule-inheritance","text":"","title":"4.15 Aggregated ClusterRole Rule Inheritance"},{"location":"17-authorization/gotchas/#incorrect-expectation_2","text":"Assuming aggregated ClusterRoles automatically update. apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : aggregated-role labels : rbac.example.com/aggregate-to-monitoring : \"true\" rules : - apiGroups : [ \"\" ] resources : [ \"pods\" ] verbs : [ \"get\" , \"list\" ] Result: The rules are not aggregated to other ClusterRoles.","title":"Incorrect Expectation"},{"location":"17-authorization/gotchas/#correct-aggregation-setup","text":"# Base ClusterRole that will receive aggregated rules apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : monitoring-role aggregationRule : clusterRoleSelectors : - matchLabels : rbac.example.com/aggregate-to-monitoring : \"true\" rules : [] # Rules will be aggregated here --- # ClusterRole that contributes rules apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : monitoring-pods labels : rbac.example.com/aggregate-to-monitoring : \"true\" rules : - apiGroups : [ \"\" ] resources : [ \"pods\" ] verbs : [ \"get\" , \"list\" ] --- # Another contributing ClusterRole apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : monitoring-services labels : rbac.example.com/aggregate-to-monitoring : \"true\" rules : - apiGroups : [ \"\" ] resources : [ \"services\" ] verbs : [ \"get\" , \"list\" ] Explanation: Aggregated ClusterRoles use the aggregationRule field to dynamically collect rules from other ClusterRoles with matching labels. The base ClusterRole has empty rules initially, and contributing ClusterRoles must have the matching label. The aggregation is not automatic for regular ClusterRoles.","title":"Correct Aggregation Setup"},{"location":"17-authorization/gotchas/#summary-of-key-rbac-principles","text":"Empty string represents the core API group ServiceAccount subjects do not use apiGroup field RoleBindings are always namespace-scoped ClusterRoles become namespace-scoped when bound with RoleBindings Subresources require explicit permissions ResourceNames is incompatible with list verb Roles cannot be referenced across namespaces Cluster-scoped resources require ClusterRoleBindings ServiceAccounts have no default permissions Custom resources have specific API groups Non-resource URLs require ClusterRoles with nonResourceURLs field Apps resources are in the apps API group, not core Each example demonstrates a specific gotcha with both incorrect and correct implementations, showing exactly what fails and how to fix it.","title":"Summary of Key RBAC Principles"},{"location":"17-authorization/role-clusterrole/","text":"RBAC Core Objects 1. Four Objects Role : Namespaced permissions definition ClusterRole : Cluster-wide permissions definition RoleBinding : Grants namespace-scoped access ClusterRoleBinding : Grants cluster-wide access 2. Responsibility Split Permissions (WHAT) : Role/ClusterRole Scope (WHERE) : Binding type determines namespace/global Access (WHO) : Binding links subjects to permissions 3. Key Rules No binding = no access Binding always decides scope RoleBinding always namespace-scoped ClusterRoleBinding always cluster-wide ClusterRole + RoleBinding - Critical Concept Core Principle Permissions come from Role/ClusterRole Scope comes from Binding type Scope Matrix Role Type Binding Type Scope Role RoleBinding Single namespace ClusterRole RoleBinding Single namespace ClusterRole ClusterRoleBinding Entire cluster Why ClusterRole Exists Avoid duplicating same Role across namespaces. ClusterRole can be: - Bound cluster-wide (ClusterRoleBinding) - Limited to namespace (RoleBinding) Gotcha: Cluster-Scoped Resources If ClusterRole contains cluster-scoped resources (nodes, PVs): - RoleBinding \u2192 IGNORES cluster-scoped permissions - ClusterRoleBinding \u2192 Grants them globally Key Takeaway ClusterRole \u2260 cluster-wide access Binding type determines scope, not role type. Below is ONLY SECTION 2.1 . Strictly about scoping, namespaces, and what can / cannot reference what . 2.1 Scoping Rules \u2014 Namespaced vs Cluster-Scoped (Very Important) Fundamental Rule RBAC object scope is fixed and cannot be changed. Some objects are namespaced , some are cluster-scoped . You cannot bend this rule. Which RBAC Objects Are Namespaced These belong to exactly one namespace : Role RoleBinding ServiceAccount You must specify metadata.namespace for them. Which RBAC Objects Are Cluster-Scoped These do not belong to any namespace : ClusterRole ClusterRoleBinding They have no namespace field . What This Means Practically Role Exists in one namespace Can only define permissions for: Namespaced resources Cannot reference cluster-scoped resources (nodes, namespaces) RoleBinding Exists in one namespace Grants access only inside that namespace Can reference: A Role (same namespace only) A ClusterRole (global, but scope limited by RoleBinding namespace) ClusterRole Exists at cluster level Can define permissions for: Namespaced resources Cluster-scoped resources Has no scope until bound ClusterRoleBinding Exists at cluster level Grants access: Across all namespaces To cluster-scoped resources Namespace Crossing Rules (Critical) Can a RoleBinding reference a Role from another namespace? \u274c No roleRef : kind : Role name : some-role That Role must exist in the same namespace as the RoleBinding. Can a RoleBinding reference a ClusterRole? \u2705 Yes roleRef : kind : ClusterRole name : view This is the intended reuse pattern . Can a RoleBinding grant access outside its namespace? \u274c No Even if: Subject is a global User Subject is a Group Subject is a ServiceAccount from another namespace Access is still limited to the RoleBinding namespace. ServiceAccount Namespace Rule (Important) ServiceAccounts are namespaced identities. This identity: system:serviceaccount:monitoring:prometheus Belongs to monitoring Can be referenced in any RoleBinding Gets access only in the RoleBinding namespace Example: Cross-Namespace ServiceAccount Access kind : RoleBinding metadata : namespace : production subjects : - kind : ServiceAccount name : prometheus namespace : monitoring roleRef : kind : ClusterRole name : pod-reader apiGroup : rbac.authorization.k8s.io Result: Prometheus SA can read pods in production Identity remains in monitoring Cluster-Scoped Resources vs RoleBinding (Exam Trap) Cluster-scoped resources: nodes namespaces persistentvolumes clusterroles clusterrolebindings These cannot be granted using a RoleBinding. This silently fails: rules : - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"get\" ] Bound via RoleBinding \u2192 ignored Correct Way You must use ClusterRoleBinding . How to Verify Scope Using CLI (Source of Truth) List All Resources With Scope kubectl api-resources Look at: NAMESPACED Namespaced Only kubectl api-resources --namespaced = true Cluster-Scoped Only kubectl api-resources --namespaced = false RBAC-Specific Check kubectl api-resources | grep -i role Expected: roles \u2192 namespaced rolebindings \u2192 namespaced clusterroles \u2192 cluster-scoped clusterrolebindings \u2192 cluster-scoped Mental Locks for Section 2.1 RoleBinding scope = its namespace Role must be in same namespace as RoleBinding ClusterRole can be reused anywhere RoleBinding cannot grant cluster-scoped resources kubectl api-resources is the authority Say \u201cnext\u201d when you want Section 3 (examples: YAML + CLI mixes) .","title":"RBAC Core Objects"},{"location":"17-authorization/role-clusterrole/#rbac-core-objects","text":"","title":"RBAC Core Objects"},{"location":"17-authorization/role-clusterrole/#1-four-objects","text":"Role : Namespaced permissions definition ClusterRole : Cluster-wide permissions definition RoleBinding : Grants namespace-scoped access ClusterRoleBinding : Grants cluster-wide access","title":"1. Four Objects"},{"location":"17-authorization/role-clusterrole/#2-responsibility-split","text":"Permissions (WHAT) : Role/ClusterRole Scope (WHERE) : Binding type determines namespace/global Access (WHO) : Binding links subjects to permissions","title":"2. Responsibility Split"},{"location":"17-authorization/role-clusterrole/#3-key-rules","text":"No binding = no access Binding always decides scope RoleBinding always namespace-scoped ClusterRoleBinding always cluster-wide","title":"3. Key Rules"},{"location":"17-authorization/role-clusterrole/#clusterrole-rolebinding-critical-concept","text":"","title":"ClusterRole + RoleBinding - Critical Concept"},{"location":"17-authorization/role-clusterrole/#core-principle","text":"Permissions come from Role/ClusterRole Scope comes from Binding type","title":"Core Principle"},{"location":"17-authorization/role-clusterrole/#scope-matrix","text":"Role Type Binding Type Scope Role RoleBinding Single namespace ClusterRole RoleBinding Single namespace ClusterRole ClusterRoleBinding Entire cluster","title":"Scope Matrix"},{"location":"17-authorization/role-clusterrole/#why-clusterrole-exists","text":"Avoid duplicating same Role across namespaces. ClusterRole can be: - Bound cluster-wide (ClusterRoleBinding) - Limited to namespace (RoleBinding)","title":"Why ClusterRole Exists"},{"location":"17-authorization/role-clusterrole/#gotcha-cluster-scoped-resources","text":"If ClusterRole contains cluster-scoped resources (nodes, PVs): - RoleBinding \u2192 IGNORES cluster-scoped permissions - ClusterRoleBinding \u2192 Grants them globally","title":"Gotcha: Cluster-Scoped Resources"},{"location":"17-authorization/role-clusterrole/#key-takeaway","text":"ClusterRole \u2260 cluster-wide access Binding type determines scope, not role type. Below is ONLY SECTION 2.1 . Strictly about scoping, namespaces, and what can / cannot reference what .","title":"Key Takeaway"},{"location":"17-authorization/role-clusterrole/#21-scoping-rules-namespaced-vs-cluster-scoped-very-important","text":"","title":"2.1 Scoping Rules \u2014 Namespaced vs Cluster-Scoped (Very Important)"},{"location":"17-authorization/role-clusterrole/#fundamental-rule","text":"RBAC object scope is fixed and cannot be changed. Some objects are namespaced , some are cluster-scoped . You cannot bend this rule.","title":"Fundamental Rule"},{"location":"17-authorization/role-clusterrole/#which-rbac-objects-are-namespaced","text":"These belong to exactly one namespace : Role RoleBinding ServiceAccount You must specify metadata.namespace for them.","title":"Which RBAC Objects Are Namespaced"},{"location":"17-authorization/role-clusterrole/#which-rbac-objects-are-cluster-scoped","text":"These do not belong to any namespace : ClusterRole ClusterRoleBinding They have no namespace field .","title":"Which RBAC Objects Are Cluster-Scoped"},{"location":"17-authorization/role-clusterrole/#what-this-means-practically","text":"","title":"What This Means Practically"},{"location":"17-authorization/role-clusterrole/#role","text":"Exists in one namespace Can only define permissions for: Namespaced resources Cannot reference cluster-scoped resources (nodes, namespaces)","title":"Role"},{"location":"17-authorization/role-clusterrole/#rolebinding","text":"Exists in one namespace Grants access only inside that namespace Can reference: A Role (same namespace only) A ClusterRole (global, but scope limited by RoleBinding namespace)","title":"RoleBinding"},{"location":"17-authorization/role-clusterrole/#clusterrole","text":"Exists at cluster level Can define permissions for: Namespaced resources Cluster-scoped resources Has no scope until bound","title":"ClusterRole"},{"location":"17-authorization/role-clusterrole/#clusterrolebinding","text":"Exists at cluster level Grants access: Across all namespaces To cluster-scoped resources","title":"ClusterRoleBinding"},{"location":"17-authorization/role-clusterrole/#namespace-crossing-rules-critical","text":"","title":"Namespace Crossing Rules (Critical)"},{"location":"17-authorization/role-clusterrole/#can-a-rolebinding-reference-a-role-from-another-namespace","text":"\u274c No roleRef : kind : Role name : some-role That Role must exist in the same namespace as the RoleBinding.","title":"Can a RoleBinding reference a Role from another namespace?"},{"location":"17-authorization/role-clusterrole/#can-a-rolebinding-reference-a-clusterrole","text":"\u2705 Yes roleRef : kind : ClusterRole name : view This is the intended reuse pattern .","title":"Can a RoleBinding reference a ClusterRole?"},{"location":"17-authorization/role-clusterrole/#can-a-rolebinding-grant-access-outside-its-namespace","text":"\u274c No Even if: Subject is a global User Subject is a Group Subject is a ServiceAccount from another namespace Access is still limited to the RoleBinding namespace.","title":"Can a RoleBinding grant access outside its namespace?"},{"location":"17-authorization/role-clusterrole/#serviceaccount-namespace-rule-important","text":"ServiceAccounts are namespaced identities. This identity: system:serviceaccount:monitoring:prometheus Belongs to monitoring Can be referenced in any RoleBinding Gets access only in the RoleBinding namespace","title":"ServiceAccount Namespace Rule (Important)"},{"location":"17-authorization/role-clusterrole/#example-cross-namespace-serviceaccount-access","text":"kind : RoleBinding metadata : namespace : production subjects : - kind : ServiceAccount name : prometheus namespace : monitoring roleRef : kind : ClusterRole name : pod-reader apiGroup : rbac.authorization.k8s.io Result: Prometheus SA can read pods in production Identity remains in monitoring","title":"Example: Cross-Namespace ServiceAccount Access"},{"location":"17-authorization/role-clusterrole/#cluster-scoped-resources-vs-rolebinding-exam-trap","text":"","title":"Cluster-Scoped Resources vs RoleBinding (Exam Trap)"},{"location":"17-authorization/role-clusterrole/#cluster-scoped-resources","text":"nodes namespaces persistentvolumes clusterroles clusterrolebindings These cannot be granted using a RoleBinding. This silently fails: rules : - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"get\" ] Bound via RoleBinding \u2192 ignored","title":"Cluster-scoped resources:"},{"location":"17-authorization/role-clusterrole/#correct-way","text":"You must use ClusterRoleBinding .","title":"Correct Way"},{"location":"17-authorization/role-clusterrole/#how-to-verify-scope-using-cli-source-of-truth","text":"","title":"How to Verify Scope Using CLI (Source of Truth)"},{"location":"17-authorization/role-clusterrole/#list-all-resources-with-scope","text":"kubectl api-resources Look at: NAMESPACED","title":"List All Resources With Scope"},{"location":"17-authorization/role-clusterrole/#namespaced-only","text":"kubectl api-resources --namespaced = true","title":"Namespaced Only"},{"location":"17-authorization/role-clusterrole/#cluster-scoped-only","text":"kubectl api-resources --namespaced = false","title":"Cluster-Scoped Only"},{"location":"17-authorization/role-clusterrole/#rbac-specific-check","text":"kubectl api-resources | grep -i role Expected: roles \u2192 namespaced rolebindings \u2192 namespaced clusterroles \u2192 cluster-scoped clusterrolebindings \u2192 cluster-scoped","title":"RBAC-Specific Check"},{"location":"17-authorization/role-clusterrole/#mental-locks-for-section-21","text":"RoleBinding scope = its namespace Role must be in same namespace as RoleBinding ClusterRole can be reused anywhere RoleBinding cannot grant cluster-scoped resources kubectl api-resources is the authority Say \u201cnext\u201d when you want Section 3 (examples: YAML + CLI mixes) .","title":"Mental Locks for Section 2.1"},{"location":"17-authorization/system-based-authorization/","text":"","title":"System based authorization"},{"location":"18-basic-bash/bash/","text":"base64 \"$TV ... \" -> can render only man!!","title":"Bash"},{"location":"19-secrets-and-configmap/environment-and-volume/","text":"","title":"Environment and volume"},{"location":"20-services/cli/","text":"create services using cli is must learnable art","title":"Cli"},{"location":"21-storage-class-pv-pvc/example/","text":"provisioner volumeBindingMode accessModes storageClassName capacity hostPath accessModes resources storageClassName https://killercoda.com/sachin/course/CKA/sc-pv-pvc-pod apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: blue-stc-cka provisioner: kubernetes.io/no-provisioner volumeBindingMode: WaitForFirstConsumer apiVersion: v1 kind: PersistentVolume metadata: name: blue-pv-cka spec: capacity: storage: 100Mi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Retain storageClassName: blue-stc-cka local: path: /opt/blue-data-cka nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - controlplane apiVersion: v1 kind: PersistentVolumeClaim metadata: name: blue-pvc-cka spec: accessModes: - ReadWriteOnce resources: requests: storage: 50Mi storageClassName: blue-stc-cka volumeName: blue-pv-cka controlplane:~$ k get sc -o yaml apiVersion: v1 items: - apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\"apiVersion\":\"storage.k8s.io/v1\",\"kind\":\"StorageClass\",\"metadata\":{\"annotations\":{},\"name\":\"local-path\"},\"provisioner\":\"rancher.io/local-path\",\"reclaimPolicy\":\"Delete\",\"volumeBindingMode\":\"WaitForFirstConsumer\"} storageclass.kubernetes.io/is-default-class: \"true\" creationTimestamp: \"2025-11-17T19:11:36Z\" name: local-path resourceVersion: \"821\" uid: 2fa12284-994b-4e2c-811c-6efc5dd0c132 provisioner: rancher.io/local-path reclaimPolicy: Delete volumeBindingMode: WaitForFirstConsumer - allowVolumeExpansion: true apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\"allowVolumeExpansion\":true,\"apiVersion\":\"storage.k8s.io/v1\",\"kind\":\"StorageClass\",\"metadata\":{\"annotations\":{},\"name\":\"nginx-stc-cka\"},\"provisioner\":\"kubernetes.io/no-provisioner\",\"volumeBindingMode\":\"WaitForFirstConsumer\"} creationTimestamp: \"2025-12-16T20:50:53Z\" name: nginx-stc-cka resourceVersion: \"6400\" uid: dc8fa5cb-bf0f-4abb-a10e-8c9517d9d299 provisioner: kubernetes.io/no-provisioner reclaimPolicy: Delete volumeBindingMode: WaitForFirstConsumer","title":"Example"},{"location":"21-storage-class-pv-pvc/selector/","text":"","title":"Selector"}]}